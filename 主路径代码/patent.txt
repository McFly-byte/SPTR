FN Clarivate Analytics Web of Science
VR 1.0
PT P
PN US5291560-A; WO9409446-A1; AU9228080-A; EP664037-A1; JP8504979-W; AU9852778-A; AU709835-B; EP664037-B1; DE69232314-E; KR307792-B1; ES2168261-T3; JP3307936-B2; CA2145659-C
TI Biometric personal identification using iris analysis - using digitised image of iris using video camera and defining polar coordinate system on isolated iris image and annular analysis bands within iris image.
AB       The method involves using iris of the eye as an optical fingerprint. Image analysis algorithms find the iris in a live video image of a person's face, and encode its texture into a compact signature, or "iris code." Iris texture is extracted from the image at multiple scales of analysis by a self-similar set of quadrature (2-D Gabor) bandpass filters defined in a dimensionless polar coordinate system. The sign of the projection of many different parts of the iris onto these multi-scale quadrature filters, determines each bit in an abstract (256-byte) iris code. The degrees-of-freedom in the code are based on the principle forms of variation in a population of irises studied.
   Because of the universal mathematical format and constant length of the iris codes, comparisons between them are readily implemented by the Exclusive-OR (XOR) logical operation. Pattern recognition is achieved by combining special signal processing methods with statistical decision theory, leading to a statistical test of independence based on a similarity metric (the Hamming distance) that is computed from the XOR of any two iris codes. This measure positively establishes, confirms, or disconfirms, the identity of any individual. It also generates an objective confidence level associated with any such identification decision.
   USE/ADVANTAGE -   Rapid and automatic identification of persons, with very high reliability and confidence levels. Enables identification without making physical contact with subject.
PD US5291560-A   01 Mar 1994   G06K-009/00   199409   Pages: 34   English
   WO9409446-A1   28 Apr 1994   G06K-009/00   199418   Pages: 66   English
   AU9228080-A   09 May 1994   G06K-009/00   199432      English
   EP664037-A1   26 Jul 1995   G06K-009/00   199534   Pages: 7   English
   JP8504979-W   28 May 1996   G06T-007/00   199646   Pages: 45   Japanese
   AU9852778-A   23 Apr 1998   G06K-009/00   199828      English
   AU709835-B   09 Sep 1999   G06K-009/00   199949      English
   EP664037-B1   19 Dec 2001   G06K-009/00   200206      English
   DE69232314-E   31 Jan 2002   G06K-009/00   200216      German
   KR307792-B1   30 Nov 2001   G06K-009/00   200247      
   ES2168261-T3   16 Jun 2002   G06K-009/00   200250      Spanish
   JP3307936-B2   29 Jul 2002   G06T-007/00   200256   Pages: 21   Japanese
   CA2145659-C   25 May 2004   G06K-009/20   200441      English
UT DIIDW:1994074707
ER

PT P
PN WO9953430-A1; AU9934904-A; US6301370-B1; US6580811-B2; US2001033675-A1
TI Person's head and hand features recognition method for identifying person from live video.
AB    NOVELTY - A region of interest including a predetermined feature of the person is defined using early vision clues and the region of interest is analyzed using graph matching. The early vision clues include stereo vision, motion, color, convexity, topology and structure.
   USE - For determining state of person in image and recognize his head and hand features for identifying persons from live video, for automated caricaturing, for image encoding, detecting drowsiness etc., for animation of graphical head model.
   ADVANTAGE - Suppresses background. Enables efficient recognition of person's head and hand features.
   DETAILED DESCRIPTION - The region of interest includes background suppression. The stereo vision is used to produce disparity histogram and silhouette images. The current state of a person's face is described by node positions and outputs of Gabor kernels.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of machine vision apparatus and processes.
PD WO9953430-A1   21 Oct 1999   G06K-009/78   199952   Pages: 32   English
   AU9934904-A   01 Nov 1999   G06K-009/78   200013      English
   US6301370-B1   09 Oct 2001   G06K-009/00   200162      English
   US6580811-B2   17 Jun 2003   G06K-009/00   200341      English
   US2001033675-A1   25 Oct 2001   G06K-009/00   201718      English
UT DIIDW:1999611414
ER

PT P
PN US5774591-A
TI Computerized human facial expression recognition method - involves associating first onset of facial expression with first ending component, based on which first facial expression formed during first time period is identified from sequence of image pairs.
AB       The method involves defining a parametric flow model with motion parameters, for each image pair in sequence of images. Based on the defined concepts, rigid and non-rigid movement of facial features between leading and trailing images, are described. A value is assigned for each motion parameter of the parametric flow model. First series of image pairs with motion parameter values having a first temporal relationship, is identified. The first temporal relationship defines a first onset of facial expression. A second series of image pairs with motion parameter values having a second temporal relationship, is identified.
   The second temporal relationship defines a first ending component of the facial expression. The first and second series of image pairs are positioned subsequent to each other. The first onset of facial expression is associated with the first ending component of a facial expression. Based on the association, a first facial expression formed during a first time period is identified in a sequence of images.
   ADVANTAGE -   Improves facial expression recognising accuracy.
PD US5774591-A   30 Jun 1998   G06K-009/00   199833   Pages: 38   English
UT DIIDW:1998387453
ER

PT P
PN US7027621-B1
TI Human operator e.g. surgeon, condition monitoring and assessing system for use in e.g. medical intensive care unit desk, has assess unit providing assessment having reference to data particular to condition and task of operator.
AB    NOVELTY - The system has an infrared camera (302) providing passive thermal image sequences of a face of a human operator (300) e.g. soldier. A spatially orienting unit orients a face on each thermal image in the thermal image sequence. The spatially oriented face is divided into a grid of cells. Each thermal image records a temperature for each cell on the grid. An analyzing unit analyzes the sequence of the thermal images. An assess unit provides assessment having reference to data particular to the condition and the task of the operator and correlated with multiple variations.
   USE - System for monitoring and assessing a condition of a human operator. Uses include but are not limited to a computer operator, a security watchmen, an utility and transportation control center staff, an air traffic controller, a border patrol, a soldier, a pilot, a firefighter, a casino surveillance staff, a surgeon, an emergency response worker, a train driver, a bus driver, a loan officer and a stock broker in a trauma response team, a defense secretary office, a security operations center, a power plant operation center, a call center, a help desk, a medical intensive care unit desk and a computer network activity monitoring desk.
   ADVANTAGE - The system easily monitors and assesses the performance of the operator, and easily determines the condition of the operator while performing a critical task. The system reduces the human error, increases the workability of the operator, avoids accidents caused by inattention of the driver, ensures safety and security to the operator, and facilitates rapid medical treatment process, evacuation process, re-training process, and counseling process. The system has high reliability.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for monitoring and assessing a condition of a human operator.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a process of a human operator condition monitoring and assessing system.
   Human operator (300)
   Infrared camera (302)
   Feature extraction subsystem (310)
   Analysis subsystem (330)
   Black box (352)
PD US7027621-B1   11 Apr 2006   G06K-009/00   200927   Pages: 27   English
UT DIIDW:2009B52773
ER

PT P
PN WO9208202-A1; AU9190375-A; US5164992-A; EP555380-A1; EP555380-A4; SG48965-A1; EP555380-B1; DE69130616-E; US36041-E
TI Face recognition system to identify TV viewing audience - presence and identify of viewer is identified against reference face image set to automate audience viewing habit surveying.
AB       The recognition system comprises an imaging module (4) to generate an image of the audience, a selector module (6) for selecting part (ie the face) of an image. A detector (8) analyses the selected part of the image for ascertaining whether a person is present. A recognition system (10) determines whether any such detected image resembles an individual image from a reference set, if so identifying the individual.
   The reference set of individuals is represented as a set of eigenvectors in a multi-dimensional image space.
   USE/ADVANTAGE -   For identifying individual members of an audience, for monitoring television viewing habits. Reduction is manual intervention by viewer ensures that gathered statistics are complete and error free.
PD WO9208202-A1   14 May 1992      199222   Pages: 24   English
   AU9190375-A   26 May 1992   G06K-009/00   199235      English
   US5164992-A   17 Nov 1992      199249   Pages: 0   English
   EP555380-A1   18 Aug 1993      199333   Pages: 24   English
   EP555380-A4   20 Jul 1994   G06K-009/00   199532      English
   SG48965-A1   18 May 1998   G06K-009/00   199834      English
   EP555380-B1   09 Dec 1998   G06K-009/00   199902      English
   DE69130616-E   21 Jan 1999   G06K-009/00   199909      German
   US36041-E   12 Jan 1999   G06K-009/00   199910      English
UT DIIDW:1992183866
ER

PT P
PN US2004190776-A1; JP2004302992-A; JP2004303014-A; EP1477924-A2; EP1477924-B1; DE602004006190-E; DE602004006190-T2; DE602004006190-T8; JP4153818-B2; JP4153819-B2; US7593552-B2; EP1477924-A3
TI Person gesture/posture recognition apparatus, has posture/gesture recognition unit to compare detected changes of fingertip position with previously stored data to determine and recognize posture/gesture of person.
AB    NOVELTY - The apparatus has a face/fingertip position detection unit (41) detecting a face position and a fingertip position of a person in three-dimensional space based on contour and human skin region information of the person. A posture/gesture recognition unit (42) compares detected changes of the fingertip position with previously stored data to determine and recognize a posture or a gesture.
   USE - Used in a gesture recognition system for recognizing a posture or gesture of an object person.
   ADVANTAGE - The posture/gesture recognition unit determines the posture or gesture with the maximum probability density for the object person by comparing the changes with the previously stored values, thus decreasing the amount of calculations required for the recognition process.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a gesture recognition method
   (B) a gesture recognition program that makes a computer to recognize postures or gestures of an object person.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating whole arrangement of a gesture recognition system.
   Cameras (1)
   Captured image analysis device (2)
   Contour extraction device (3)
   Gesture recognition device (4)
   Face/fingertip position detection unit (41)
   Posture/gesture recognition unit (42)
PD US2004190776-A1   30 Sep 2004   G09G-005/00   200469   Pages: 44   English
   JP2004302992-A   28 Oct 2004   G06T-007/20   200471   Pages: 24   Japanese
   JP2004303014-A   28 Oct 2004   G06T-007/20   200471   Pages: 19   Japanese
   EP1477924-A2   17 Nov 2004   G06K-009/00   200475      English
   EP1477924-B1   02 May 2007   G06K-009/00   200731      English
   DE602004006190-E   14 Jun 2007   G06K-009/00   200741      German
   DE602004006190-T8   10 Apr 2008   G06K-009/00   200827      German
   JP4153818-B2   24 Sep 2008   G06T-007/20   200864   Pages: 24   Japanese
   JP4153819-B2   24 Sep 2008   G06T-007/20   200864   Pages: 20   Japanese
   US7593552-B2   22 Sep 2009   G06K-009/00   200962      English
   EP1477924-A3   12 Oct 2005   G06K-009/00   201731      English
UT DIIDW:2004708435
ER

PT P
PN EP551941-A2; EP551941-A3; US5561718-A; US5793456-A; EP551941-B1; DE69328982-E
TI Face classification system for computer security system - extracts face from two=dimensional representation of three=dimensional scene, produces feature vector by rotation, scaling and translation invariant representation of face and compares result.
AB       The classification system locates a face in a two dimensional representation of a three dimensional scene. The face is located in the representation of the scene. A rotation, scaling, translation and grey level intensity invariant representation of the face is formed and a feature vector is produced.
   The feature vector of the presently located face is compared with the feature vector of a previously located face is compared to determine whether the presently located face matches the previously located face. A homo-morphic filter is provided for producing a grey level intensity invariant representation.
   ADVANTAGE -   Effect of noise on feature vector is reduced. Eliminates effects of lighting changes in scene.
PD EP551941-A2   21 Jul 1993      199329   Pages: 9   English
   EP551941-A3   08 Jun 1994   G06K-009/46   199526      English
   US5561718-A   01 Oct 1996   G06K-009/00   199645   Pages: 8   English
   US5793456-A   11 Aug 1998   G02F-001/1335   199839      English
   EP551941-B1   12 Jul 2000   G06K-009/46   200036      English
   DE69328982-E   17 Aug 2000   G06K-009/46   200047      German
UT DIIDW:1993228766
ER

PT P
PN US5781650-A
TI Automatic human face recognition method with age calculating function - involves analysing wrinkles in face from obtained two- dimensional digital image, based on which features in human face is classified within three age group.
AB       The method involves obtaining the two-dimensional digital image of a human face using a digital camera (10). The recognised wrinkles in the image is detected and by the Hough transform process. The features in the human face is classified within three age group, based on the recognised wrinkles.
   ADVANTAGE -   Enables age calculation with slight age variation. Performs accurate wrinkle analysis on face.
PD US5781650-A   14 Jul 1998   G06K-009/00   199835   Pages: 36   English
UT DIIDW:1998413506
ER

PT P
PN WO9953443-A1; AU9936396-A; BR9909611-A; EP1072018-A1; US6272231-B1; KR2001042673-A; JP2002511620-W; US2002118195-A1; MX2000010044-A1; EP1072018-B1; DE69910757-E; MX218458-B; US6940454-B2; JP2007109255-A; JP3970520-B2; JP4177402-B2; BR9909611-B1; KR530812-B1
TI Wavelet based facial motion capture for animation.
AB    NOVELTY - Wavelet transformation is performed to each image frame and a transformed image frame is formed. Using a model graph of a predetermined pose, node location associated with wavelet jets are initialized in the transformed image and the node locations are tracked by elastic bunch graph matching. The tracked node is reinitialized if the node position deviates from a threshold determined by geometrical position constraint.
   USE - For animation.
   ADVANTAGE - By providing linear motion model of high frame ratio, tracking acceleration is enhanced. By increasing number of Gauss and frequency levels, tracking accuracy is enhanced. By shifting the Gabor filter response frequency, Gabor jets are tracked with sub-pixels accuracy without accounting of rounding errors. The speed of reinitialization is enhanced by identifying the identity of the same tracked person. The speed of reinitialization is also enhanced by using partial bunch graph process.
   DETAILED DESCRIPTION - The tracked node locations such as pupil of right and left eye, right and left corner of both eye brow, top and tip of nose, right and left corner of mouth, center of upper and lower lip, top and bottom of both ears are transmitted to a remote site for animating an image. The track step also determines the status of mouth, eye, tongue, hair etc. INDEPENDENT CLAIMS are also included for the following:
   (a) apparatus for feature screening on a sequence of image frames;
   (b) a model graph for facial image analysis
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of animation system.
PD WO9953443-A1   21 Oct 1999   G06T-007/20   200001   Pages: 71   English
   AU9936396-A   01 Nov 1999   G06T-007/20   200013      English
   BR9909611-A   19 Dec 2000   G06T-007/20   200103      
   EP1072018-A1   31 Jan 2001   G06T-007/20   200108      English
   US6272231-B1   07 Aug 2001   G06K-009/00   200147      English
   KR2001042673-A   25 May 2001   G06T-007/20   200168      
   JP2002511620-W   16 Apr 2002   G06T-007/20   200242   Pages: 55   Japanese
   US2002118195-A1   29 Aug 2002   G06T-013/00   200259      English
   MX2000010044-A1   01 Dec 2001   G06K-009/00   200282      Spanish
   EP1072018-B1   27 Aug 2003   G06T-007/20   200358      English
   DE69910757-E   02 Oct 2003   G06T-007/20   200372      German
   MX218458-B   07 Jan 2004   G06K-009/00   200472      Spanish
   US6940454-B2   06 Sep 2005   G06T-015/00   200558      English
   JP2007109255-A   26 Apr 2007   G06T-007/20   200730   Pages: 21   Japanese
   JP3970520-B2   05 Sep 2007   G06T-007/20   200760   Pages: 22   Japanese
   JP4177402-B2   05 Nov 2008   G06T-007/20   200906   Pages: 22   Japanese
   BR9909611-B1   07 Aug 2012   G06T-007/20   201280      
   KR530812-B1   28 Nov 2005   G06T-007/20   200682      
UT DIIDW:2000013125
ER

PT P
PN WO9524691-A1; US5467406-A; AU9519941-A; JP8110968-A; JP8180239-A; JP8185558-A; JP8235408-A; EP749611-A1; US5633949-A; US5640463-A; US5652802-A; EP805408-A2; EP807904-A2; EP807905-A2; EP807906-A2; US5687963-A; EP749611-A4; US5724438-A; EP807906-A3; EP805408-A3; EP807904-A3; EP807905-A3; MX9603911-A1; US5751840-A; US5815592-A; EP875866-A2; US5822448-A; US5832104-A; US5867589-A; US5870487-A; US5875259-A; CA2215850-C; CA2215864-C; US6072896-A; CA2215853-C; CA2215857-C; EP1022694-A2; CA2215869-C; MX190468-B; CA2184807-C; CA2316138-A1; EP1107167-A1; US2001006557-A1; CA2215887-C; CA2357502-A1; CA2316138-C; EP1158469-A2; CA2215886-C; US6381354-B1; EP1202224-A2; EP749611-B1; EP1209632-A2; RU2183861-C2; EP807904-B1; DE69527136-E; EP805408-B1; EP807905-B1; EP1227447-A2; DE69527546-E; DE69527552-E; DE69527806-E; US6539104-B1; JP2003091759-A; CA2379146-C; EP807906-B1; EP875866-B1; DE69530868-E; DE69530873-E; CA2357502-C; US2003174874-A1; US6628816-B2; EP1022694-B1; DE69534475-E; DE69534475-T2; US7248731-B2; EP1107167-B1; EP875866-B2; DE69535870-E; US6381354-C1; EP807906-B2; EP1022694-A3; EP1158469-A3; EP1202224-A3; EP1227447-A3; EP875866-A3
TI Currency discriminating device - cans both faces of note and compares images with those stored in memory to distinguish denomination.
AB       The appts for currency discrimination has two stationary scanheads, disposed on opposite sides of the note transfer path to scan both sides of the note as it passes. The scanheads provide corresp signals. The notes pass along the path at about 800 notes per minute. A memory stores primary characteristic patterns of denominations of genuine notes. A sampler samples the scanhead output signals.
   A signal processor determines which surface of the note includes the primary characteristic pattern to a predetermined degree of certainty. An output signal associated with the surface of each scanned note which includes the primary characteristic pattern is correlated with the master primary characteristic patterns to identify the denomination of each note.
   USE/ADVANTAGE -   Document identification. Authentification and counting of money. High speed operation. Discriminates notes from different countries.
PD WO9524691-A1   14 Sep 1995   G06K-009/00   199542   Pages: 275   English
   US5467406-A   14 Nov 1995   G06K-009/00   199551   Pages: 13   English
   AU9519941-A   25 Sep 1995   G06K-009/00   199601      English
   JP8110968-A   30 Apr 1996   G07D-007/00   199627   Pages: 14   Japanese
   JP8180239-A   12 Jul 1996   G07D-007/00   199638   Pages: 19   Japanese
   JP8185558-A   16 Jul 1996   G07D-007/00   199638   Pages: 6   Japanese
   JP8235408-A   13 Sep 1996   G07D-007/00   199647   Pages: 12   Japanese
   EP749611-A1   27 Dec 1996   G06K-009/00   199705   Pages: 1   English
   US5633949-A   27 May 1997   G06K-009/00   199727   Pages: 20   English
   US5640463-A   17 Jun 1997   G06K-009/00   199730   Pages: 9   English
   US5652802-A   29 Jul 1997   G06K-009/00   199736   Pages: 37   English
   EP805408-A2   05 Nov 1997   G06K-009/00   199749   Pages: 135   English
   EP807904-A2   19 Nov 1997   G07D-007/00   199751   Pages: 12   English
   EP807905-A2   19 Nov 1997   G07D-007/00   199751   Pages: 45   English
   US5687963-A   18 Nov 1997   B65H-003/06   199801   Pages: 54   English
   EP749611-A4   27 Aug 1997   G06K-009/00   199814      English
   US5724438-A   03 Mar 1998   G06K-009/00   199816   Pages: 26   English
   EP807906-A3   10 Dec 1997   G07D-007/00   199817      English
   EP805408-A3   17 Dec 1997   G06K-009/00   199818      English
   EP807904-A3   17 Dec 1997   G07D-007/00   199818      English
   EP807905-A3   17 Dec 1997   G07D-007/00   199818      English
   MX9603911-A1   01 Mar 1997   G06K-009/00   199820      Spanish
   US5751840-A   12 May 1998   G06K-009/00   199826      English
   US5815592-A   29 Sep 1998   G06K-009/00   199846      English
   EP875866-A2   04 Nov 1998   G07D-007/00   199848      English
   US5822448-A   13 Oct 1998   G06K-009/00   199848      English
   US5832104-A   03 Nov 1998   G06K-009/00   199851      English
   US5867589-A   02 Feb 1999   G06K-009/00   199912      English
   US5870487-A   09 Feb 1999   G06K-009/00   199913      English
   US5875259-A   23 Feb 1999   G06K-009/00   199915      English
   CA2215850-C   15 Feb 2000   G07D-007/00   200028      English
   CA2215864-C   15 Feb 2000   G07D-007/00   200028      English
   US6072896-A   06 Jun 2000   G06K-009/00   200033      English
   CA2215853-C   02 May 2000   G07D-007/00   200037      English
   CA2215857-C   02 May 2000   G07D-007/00   200037      English
   EP1022694-A2   26 Jul 2000   G07D-007/00   200037      English
   CA2215869-C   06 Jun 2000   G07D-007/00   200041      English
   MX190468-B   27 Nov 1998   G06K-009/00   200043      Spanish
   CA2184807-C   26 Sep 2000   G06K-009/62   200055      English
   CA2316138-A1   14 Sep 1995   G07D-007/00   200058      English
   EP1107167-A1   13 Jun 2001   G06K-009/00   200134      English
   US2001006557-A1   05 Jul 2001   G06K-009/00   200139      English
   CA2215887-C   10 Jul 2001   G07D-007/00   200142      English
   CA2357502-A1   14 Sep 1995   G06K-009/62   200174      English
   CA2316138-C   06 Nov 2001   G07D-007/00   200174      English
   EP1158469-A2   28 Nov 2001   G07D-007/00   200201      English
   CA2215886-C   08 Jan 2002   G07D-007/00   200206      English
   US6381354-B1   30 Apr 2002   G06K-009/00   200235      English
   EP1202224-A2   02 May 2002   G07D-007/00   200236      English
   EP749611-B1   19 Jun 2002   G06K-009/00   200240      English
   EP1209632-A2   29 May 2002   G07D-007/00   200243      English
   RU2183861-C2   20 Jun 2002   G07D-007/12   200254      Russian
   EP807904-B1   14 Aug 2002   G06K-009/00   200255      English
   DE69527136-E   25 Jul 2002   G06K-009/00   200256      German
   EP805408-B1   24 Jul 2002   G06K-009/00   200256      English
   EP807905-B1   24 Jul 2002   G06K-009/00   200256      English
   EP1227447-A2   31 Jul 2002   G07D-007/12   200257      English
   DE69527546-E   29 Aug 2002   G06K-009/00   200264      German
   DE69527552-E   29 Aug 2002   G06K-009/00   200264      German
   US6539104-B1   25 Mar 2003   G06K-009/00   200325      English
   JP2003091759-A   28 Mar 2003   G07D-007/12   200331   Pages: 20   Japanese
   CA2379146-C   13 May 2003   G07D-007/12   200339      English
   EP807906-B1   21 May 2003   G06K-009/00   200341      English
   EP875866-B1   21 May 2003   G07D-007/00   200341      English
   DE69530868-E   26 Jun 2003   G06K-009/00   200350      German
   DE69530873-E   26 Jun 2003   G07D-007/00   200350      German
   CA2357502-C   08 Jul 2003   G06K-009/62   200352      English
   US2003174874-A1   18 Sep 2003   G06K-009/00   200362      English
   US6628816-B2   30 Sep 2003   G06K-009/00   200367      English
   EP1022694-B1   21 Sep 2005   G07D-007/00   200563      English
   DE69534475-E   02 Feb 2006      200615      German
   DE69534475-T2   27 Apr 2006   G07D-007/00   200629      German
   US7248731-B2   24 Jul 2007   G06K-009/00   200749      English
   EP1107167-B1   22 Oct 2008   G06K-009/00   200874      English
   EP875866-B2   22 Oct 2008   G07D-007/00   200874      English
   DE69535870-E   04 Dec 2008   G06K-009/00   200882      German
   US6381354-C1   08 Dec 2009   G06K-009/00   200981      English
   EP807906-B2   21 Nov 2012   G06K-009/00   201277      English
   EP1022694-A3   27 Sep 2000   G07D-007/00   201736      English
   EP1158469-A3   29 Jun 2005   G07D-007/00   201736      English
   EP1202224-A3   04 May 2005   G07D-007/00   201736      English
   EP1227447-A3   22 Nov 2006   G07D-007/00   201736      English
   EP875866-A3   03 Nov 1999   G07D-007/00   201741      English
UT DIIDW:1995328410
ER

PT P
PN WO9830017-A2; AU9857115-A; US6111517-A; WO9830017-A3
TI Continues access to restricted environment regulating - by periodically acquiring subsequent facial representations of at least one individual desiring continued access to e.g. computer.
AB       The method involves storing a facial representation of an individual authorised to have access to the restricted environment. A facial representation of an individual desiring continued access is acquired to the restricted environment. It requires determining whether the individual seeking continued access is the authorised individual by comparing the acquired representation to the stored representation. The access to the restricted environment is revoked if the determination indicates that the individual seeking continued access is not the authorised individual.
   The method further entails periodically acquiring subsequent facial representations of at least one individual desiring continued access to the restricted environment, for repeatedly determining the degree to which the most recently acquired representation corresponds to the stored representation of the authorised individual.
   USE -   In real time face recognition software to regulate and monitor access to computers and other restricted environments.
   ADVANTAGE -   Prevents access to computer system etc restricted access environment when it is determined that authorised individual has left vicinity of environment.
PD WO9830017-A2   09 Jul 1998   H04N-000/00   199833   Pages: 47   English
   AU9857115-A   31 Jul 1998   G06T-001/00   199849      English
   US6111517-A   29 Aug 2000   G07D-007/00   200043      English
   WO9830017-A3   08 Oct 1998   H04N-000/00   201729      English
UT DIIDW:1998388522
ER

PT P
PN US5835616-A
TI Human face detection method for automated security and surveillance systems, demographic studies - involves positioning template based on outline and position of chin and sides, having eyes, nose and mouth at preselected points, to judge facial feature ratio.
AB       The method involves blurring a digital image, and sharpening its edges. Then, snake like curves formed in the image are cut off using Hough transfer voting, and its exterior oral shape is judged. The snake like curves within the exterior oral curve are cut off to judge the position of the chin and sides of the face.
   Then an template is positioned based on outline and position of chin and sides of the face, to judge the position of eyes, nose, mouth and face. The template has preselected configuration having eyes, nose and mouth at preselected positions. The facial feature ratio between the position of eyes, nose and mouth is computed and is judged to be within predefined limits.
   USE -   For safety monitoring system, human-computer interfaces, automated photography.
   ADVANTAGE -   Recognises and interprets common features in face, accurately. Eliminates manual adjustment and hence improves quality of photographs.
PD US5835616-A   10 Nov 1998   G06K-009/00   199902   Pages: 33   English
UT DIIDW:1999022854
ER

PT P
PN EP899690-A2; JP11088913-A; JP11136706-A; US2002084974-A1; US6522312-B2; JP3450704-B2; JP2003308514-A; JP3486536-B2; JP3793158-B2; EP899690-B1; DE69835321-E; DE69835321-T2; EP899690-A3
TI Mixed virtual reality shared among e.g. players of computer hockey game - detects position co-ordinates of arbitrary markers provided at game environment and outputs signal indicating 3D position of player, based on which position of operation unit is judged.
AB    NOVELTY - In a mixed reality e.g. hockey game, two players (2000, 3000) face each other holding mallets (260L, 260R) and wearing head-mounted displays (210L, 210R). The mallets have an infrared ray generator at the distal end and their positions are determined by an image processor or can be determined by pattern recognition using shape or color and a camera (230). The viewpoint position determined by a sensor (220) is corrected using the image obtained from cameras (240L, 240R) mounted on the players' heads
   USE - Presenting mixed reality coupled with virtual image generated by computer graphics.
   ADVANTAGE - Improvement of precise detection of head position and/or posture of operator.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for a method of generating three-dimensional virtual images and a position posture determination apparatus and method.
   DESCRIPTION OF DRAWING(S) - The drawing is a side view showing arrangement of game apparatus used in first embodiment of present invention.
   Mallets (260L, 260R)
   Head-mounted displays (210L, 210R)
   Camera (230)
   Sensor (220)
   Head-mounted cameras (240L, 240R)
   Players (2000, 3000)
PD EP899690-A2   03 Mar 1999   G06T-015/00   199915   Pages: 49   English
   JP11088913-A   30 Mar 1999   H04N-013/04   199923   Pages: 19   Japanese
   JP11136706-A   21 May 1999   H04N-013/04   199931   Pages: 26   Japanese
   US2002084974-A1   04 Jul 2002   G09G-005/00   200247      English
   US6522312-B2   18 Feb 2003   G09G-005/00   200317      English
   JP3450704-B2   29 Sep 2003   H04N-013/04   200364   Pages: 24   Japanese
   JP2003308514-A   31 Oct 2003   G06T-001/00   200374   Pages: 22   Japanese
   JP3486536-B2   13 Jan 2004   G06T-017/40   200406   Pages: 20   Japanese
   JP3793158-B2   05 Jul 2006   G06T-001/00   200644   Pages: 27   Japanese
   EP899690-B1   26 Jul 2006   G06T-015/00   200650      English
   DE69835321-E   07 Sep 2006   G06T-015/00   200660      German
   DE69835321-T2   14 Dec 2006   G06T-015/00   200701      German
   EP899690-A3   05 Apr 2000   G06T-015/00   201735      English
UT DIIDW:1999168867
ER

PT P
PN JP9251534-A; US5982912-A; JP3279913-B; JP3279913-B2
TI Person authentication apparatus - has pattern evaluation unit which isolates set of characteristic points in face image having high coherence with specific template pattern.
AB       The apparatus includes an image input unit (11) through which image of the person to be identified is input to a face area extraction unit (12). The image of the face of the person to be identified is output from the face area extraction unit. A characteristic point extraction unit (13), obtains the characteristic points from the extracted face image, using degree of separation filter. Based on preknown face structural constraints, a characteristic point set candidate selection unit (14) chooses some characteristic points from the output of the characteristic points extraction unit.
   A pattern evaluation unit (15) compares the characteristic point sets with a preknown template pattern. The characteristic point sets which have high coherence with the template pattern alone are output from the pattern evaluation unit. A normalization generator (16) forms a normalized image based on the output of the pattern evaluation unit. A recognition unit (17) compares the formed normalized image with some previously registered images and identifies the person.
   ADVANTAGE -   Enables to extract characteristics points e.g. eye and nose without influence of brightness variation. Improves accuracy of person authentication.
PD JP9251534-A   22 Sep 1997   G06T-007/00   199748   Pages: 8   Japanese
   US5982912-A   09 Nov 1999   G06K-009/00   199954      English
   JP3279913-B   30 Apr 2002      200230   Pages: 11   Japanese
   JP3279913-B2   30 Apr 2002   G06T-007/00   200940   Pages: 11   Japanese
UT DIIDW:1997523555
ER

PT P
PN US2007198849-A1; US7487089-B2
TI Client-server security system, has client and server systems, where authorization procedures of systems deny access of client system and client if biometric data does not correspond to client system and client, respectively.
AB    NOVELTY - The system has a client system e.g. pager, receiving biometric data e.g. speech data, from a client using a biometric input device e.g. microphone, where a security authorization procedure e.g. user verification, performed by a client biometric algorithm denies access to the client system if the data does not correspond to the client. Another security authorization procedure e.g. user identification, performed by a server biometric algorithm in a server system denies access to the client if another biometric data e.g. signature, of the client does not correspond to data stored in a database.
   USE - Used for providing security to a client-server system, by corresponding a biometric data e.g. genetic composition, fingerprint pattern, facial shape or facial pattern, hand- print or hand geometry, cornea pattern, iris or retinal pattern, and voice or speech characteristic, gathered by a biometric input device e.g. microphone, fingerprint scanner, retinal scanner, and handprint analyzer, to a client system e.g. personal computer system, thin client computer system, portable phone such as cellular phone, personal digital assistant, portable media player such as MPEG-1 Audio layer 3 (MP3) player or walkman, electronic watch, internet appliance, smart card, electronic wallet, pager, set-top box, and automotive information system.
   ADVANTAGE - The two security authorization procedures are followed to deny access of the client system and the client, if the biometric data does not correspond to the client system and the client, respectively, thus enhancing the security of the overall client-server system.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method of performing a secured transaction on a server system
   (2) a method of performing a secured transaction on a client system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a method of providing a secure environment in a client-server system.
PD US2007198849-A1   23 Aug 2007   H04L-009/32   200779   Pages: 16   English
   US7487089-B2   03 Feb 2009   G10L-017/00   200917      English
UT DIIDW:2007857608
ER

PT P
PN US6292575-B1
TI Real time facial recognition and verification system which compares an obtained image to a stored image to determine if a match exists in an access control system.
AB    NOVELTY - An image acquisition stage (22), a frame grabber stage (26), a head-find stage (28), an eye-find stage (30) and an image manipulation stage (34) acquire and digitize an image of a person and place it in a suitable condition for compression and subsequent comparison to pre-stored image identification information. A compression stage (36) produces eigenvectors from a reference set of images, which are used to characterize the acquired image and generates an input to a discrimination stage (36) to detect a match.
   USE - Verifying identification of an individual in an access control system.
   ADVANTAGE - Providing real time identification with simple processing.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for a system and method for refining an region of interest in a color-based image.
   DESCRIPTION OF DRAWING(S) - The drawing is a block diagram of the system
   Image acquisition stage (22)
   Manipulation stage (34)
   Compression stage (36)
   Discrimination stage (38)
PD US6292575-B1   18 Sep 2001   G06K-009/00   200169   Pages: 28   English
UT DIIDW:2001606700
ER

PT P
PN WO200114910-A2; SE200100186-A; DE10084638-T; US2002059022-A1; US6397136-B1; US2002082756-A1; US6445988-B1; US6529809-B1; SE523753-C2; US6757602-B2; US2004129478-A1; JP2004522932-W; US7243945-B2; US2007280505-A1; US2008069403-A1; US2008234899-A1; US7570785-B2; US7788008-B2; WO200114910-A3; US6397136-C1
TI System developing method for identifying presence and nature of seat occupancy in vehicle using transducers and pattern recognition, algorithm resident in computer and data sets for smart air bag operation.
AB    NOVELTY - The method includes:
(1) Mounting transducers in the vehicle;
(2) forming at least one database comprising multiple data sets, each data set representing a different occupancy state of the state and formed by receiving data from the transducers while the seat is in that occupancy state, and processing the received data; and
(3) creating an algorithm from the database capable of producing an output indicative of the seat occupancy state upon inputting a data set representing a seat occupancy state.
   USE - Provides an improved system for identifying presence and nature of seat occupancy in vehicle.
   ADVANTAGE - Enables improved airbag operation to increase occupant safety in a vehicle involved in a collision. Accurately detects the presence of an occupied rear-facing child seat in order to prevent an occupant apparatus such as an airbag, from deploying, when the airbag would impact against the rear-facing child if deployed, or when the airbag would impact against the head or chest of the occupant during its initial deployment phase causing injury or possible death to an occupant.
   DETAILED DESCRIPTION - The method uses a variety of transducers and transducers and pattern recognition technologies and techniques that applies to any combination of transducers that provide information that seat occupancy. These include weight sensors (6,7), capacitive sensors, inductive sensors, ultrasonic or optical, electromagnetic, motion, infrared, and radar among others. A processor is coupled to the transducers to receive data from the transducers and process the data to obtain an output indicative of the current occupancy state of the seat. The ultrasonic system (11,12) produces a wave pattern which is digitized as measured data to be fed with other data e.g. seat position and tilt of seat back (3), to a neural network (25) of the processor (23). Once seat occupancy is determined, vehicular components e.g. airbags can be controlled for optimum passenger or driver protection in the event of a collision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a seated state detection unit with ultrasonic, electromagnetic, weight and other sensors.
   Seat (1)
   Seat back (3)
   Weight sensors (6,7)
   Ultrasonic system (11,12)
   Processor (23)
   Neural network (25)
PD WO200114910-A2   01 Mar 2001   G01V-000/00   200140   Pages: 77   English
   SE200100186-A   27 Mar 2001   B60R-021/00   200146      Swedish
   DE10084638-T   02 May 2002   G01V-009/00   200237      German
   US2002059022-A1   16 May 2002   B60R-021/32   200237      English
   US6397136-B1   28 May 2002   B60R-021/22   200243      English
   US2002082756-A1   27 Jun 2002   B60R-021/32   200245      English
   US6445988-B1   03 Sep 2002   G06F-007/00   200260      English
   US6529809-B1   04 Mar 2003   G06F-007/00   200320      English
   SE523753-C2   11 May 2004   B60R-021/01   200432      Swedish
   US6757602-B2   29 Jun 2004   G06F-007/00   200443      English
   US2004129478-A1   08 Jul 2004   B60K-028/04   200445      English
   JP2004522932-W   29 Jul 2004   G01V-001/00   200456   Pages: 133   Japanese
   US7243945-B2   17 Jul 2007   B60R-021/16   200748      English
   US2007280505-A1   06 Dec 2007   G06F-017/00   200781      English
   US2008069403-A1   20 Mar 2008   G06K-009/00   200822      English
   US2008234899-A1   25 Sep 2008   B60R-021/015   200866      English
   US7570785-B2   04 Aug 2009   G06K-009/00   200951      English
   US7788008-B2   31 Aug 2010   G06K-009/00   201058      English
   WO200114910-A3   27 Sep 2001   B60R-021/32   201182      English
   US6397136-C1   24 Apr 2012   B60R-021/01   201230      English
UT DIIDW:2001380893
ER

PT P
PN JP8339445-A; US5710833-A; JP3986583-B2
TI Automatic visual angle learning method of target object using peculiar space analysis - in which probability level corresponding to image part containing selected description is estimated by each input vector.
AB       The method involves expressing the input image digitally. The selected description of the input image is detected by assembling peculiar vector in multidimensional image space. The inclination assembling of the selected description is then expressed. The detected description is projected on peculiar space expression of inclination assembling and peculiar vector projection coefficient. A part of the input image is considered as the input vector in image space.
   Each input vector estimates the probability level corresponding to the image part containing the selected description. Arbitrary reformulation modules reformulates the original image.Then, consistency estimation analysis is performed by recognizing image part with probability level with high relevance factor.
   ADVANTAGE -   Detects recognition of selected description like human face, easily. Expresses adjustment and reformulation image efficiently.
PD JP8339445-A   24 Dec 1996   G06T-007/00   199710   Pages: 15   Japanese
   US5710833-A   20 Jan 1998   G06K-009/00   199810   Pages: 13   English
   JP3986583-B2   03 Oct 2007   G06T-007/00   200765   Pages: 22   Japanese
UT DIIDW:1997105271
ER

PT P
PN WO9825229-A1; AU9854635-A; US5991429-A
TI Facial recognition method for security and access identification - involves enrolling image of person's face to be used later for comparison when person voluntarily desires clearance or is covertly detected.
AB       The method involves enroling data information regarding a person into a database and assigning a corresponding identification number in several database locations. Image information of the person is obtained and is processed by extracting a region of interest. A personal identification number is obtained from the person. The number is checked against a set of personal identification numbers that correspond to the data base locations. Image information of a person is scanned and the image information is processed by extracting a region of interest.
   The scanned image region is compared with the stored image regions of interest located in the database locations. A sorted match list is generated which provides a list of persons enroled in the database in order of the likelihood that the person whose image was scanned is that person enroled in the database. Finally the identification number corresponding to most likely person whose image was enroled is correlated with the personal identification number provided by the person whose image was scanned.
   ADVANTAGE -   Allows premises to be surveyed so that presence of particular unwanted persons such as known terrorists and fugitives can be immediately detected.
PD WO9825229-A1   11 Jun 1998   G06K-009/00   199829   Pages: 44   English
   AU9854635-A   29 Jun 1998   G06K-009/00   199845      English
   US5991429-A   23 Nov 1999   G06K-009/32   200002      English
UT DIIDW:1998333563
ER

PT P
PN US6400835-B1
TI Motor vehicle security system comprises infrared camera whose scanning axis is directed along center plane and at rear-view mirror of vehicle, to scan person face at driver seat.
AB    NOVELTY - An infrared camera (12) whose scanning axis is directed along center plane and at rear-view mirror (16) of vehicle, scans a portion of a person face located at a driver seat (21). An electronic storage device stores a set of image signals, produced from the camera, such that a facial recognition system activates an alarm, if the scanned images does not matches with the known person's images stored beforehand.
   USE - For providing security to motor vehicle e.g. car.
   ADVANTAGE - Provides sufficient image information for the facial recognition algorithms to properly recognize authorized driver with improved view, because of the accurate positioning of the camera near the vertical center plane and rear-view mirror of the vehicle, thereby prevents theft of vehicles reliably.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) Motor vehicle securing apparatus;
   (2) Motor vehicle; and
   (3) Motor vehicle retrofitting method.
   DESCRIPTION OF DRAWING(S) - The figure shows a side view of the driving condition of vehicle.
   Infrared camera (12)
   Rear-view mirror (16)
   Driver seat (21)
PD US6400835-B1   04 Jun 2002   G06K-009/00   200268   Pages: 17   English
UT DIIDW:2002634554
ER

PT P
PN US5502774-A
TI Consistent message into computer recognisable message transformation method - using automatic recogniser device which accepts multiple sources of information, and multiple input interface coupling display terminal to digital or analog transformation raw data input.
AB       The method entails transforming consistent message generated by a human in at least two formats into electrical signals representative of the consistent message, producing from the electrical signals a set of parameters for each format,
   generating a likelihood score of recognition for each set of parameters. The sets of parameters are used to train a weighting coefficient for each of the two formats of the consistent message.
   A weighted expression is generated based on the trained weighting coefficient and the likelihood scores of recognition. A candidate message unit is selected that maximizes the weighted expression to transform the electrical signal representative of the consistent message into a computer recognizable message.
   The trained weighting coefficients are used to produce a new set of parameters and the new set of parameters is applied to generate new the trained weighting coefficients.
   USE/ADVANTAGE -   For decoding consistent message using both handwriting and speech recognition. Provides system and method of combining multiple sources of information, e.g.voice print, written word facial movement, gestures, typing, in order to recognise a consistent message.
PD US5502774-A   26 Mar 1996   G06K-009/00   199618   Pages: 17   English
UT DIIDW:1996179562
ER

PT P
PN DE19634768-A1; US5715325-A
TI Face identification system using video image analysis - localises defined region within video camera image containing image of face identified by comparison with stored image patterns of known faces.
AB       The identification system has a video camera (12) providing an image frame of the viewed scene, with a localisation device for localising a defined region containing an image of the face. The defined region is compared with stored image patterns of known faces, for identifying the face in the video camera image. Pref. the localisation device identifies the upper limit of the defined region by comparing the image element blocks within the image lines with a threshold value.
   ADVANTAGE -   Rapid identification of known face independent of colour variables, reliable in continually changeable lighting environment.
PD DE19634768-A1   06 Mar 1997   G06K-009/62   199717   Pages: 13   German
   US5715325-A   03 Feb 1998   G06K-009/00   199812   Pages: 10   English
UT DIIDW:1997181133
ER

PT P
PN WO2007008159-A2; EP1904347-A2; US2008252412-A1; CN101296821-A; JP2009500246-W; BR200613023-A2; EP1904347-B1; JP4852604-B2; EP2428413-A1; EP2436566-A1; WO2007008159-A3; ES2374221-T3; US8344849-B2; EP2428413-B1; EP2436566-B1; CN101296821-B; EP1904347-A4
TI Vehicle e.g. commercial truck, operator`s identity verifying method, involves performing different identification procedures such as fingerprinting and iris scanning with time interval based upon nature of operator`s work.
AB    NOVELTY - The method involves utilizing an onboard, multi-mode identification system to ascertain whether an operator is an authorized driver. An identification procedure such as fingerprinting and iris scanning, and another identification procedure are performed on the operator, to determine whether the operator is authorized or not. The procedures are performed with a time interval based upon a nature of operator work. A remedial measure is exercised to avert negative impact when the operator is determined to be unauthorized based on the performed procedures.
   USE - Used for verifying an identity of an operator of a vehicle e.g. commercial truck and passenger car.
   ADVANTAGE - The method performs different procedures such as fingerprinting and iris scanning to increase the security level when driving in dangerous areas and carrying hazardous goods, thus achieving robustness against aging and facial expression. The method is user friendly, inexpensive and non-intrusive, and eliminates the need for updating a database for the operators, which is time-consuming. The method increases risk awareness for the operator and influences him to be more attentive and careful.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method for verifying an identity of a vehicle operator.
PD WO2007008159-A2   18 Jan 2007   B60R-025/00   200727   Pages: 122   English
   EP1904347-A2   02 Apr 2008   B60R-025/00   200825      English
   US2008252412-A1   16 Oct 2008   G05B-019/00   200869      English
   CN101296821-A   29 Oct 2008   B60R-025/00   200882      Chinese
   JP2009500246-W   08 Jan 2009   B60R-025/00   200906   Pages: 138   Japanese
   BR200613023-A2   14 Dec 2010   B60R-025/00   201109      
   EP1904347-B1   28 Sep 2011   B60R-025/00   201163      English
   JP4852604-B2   11 Jan 2012   B60R-025/00   201205   Pages: 71   Japanese
   EP2428413-A1   14 Mar 2012   B60R-025/00   201219      English
   EP2436566-A1   04 Apr 2012   B60R-025/00   201225      English
   WO2007008159-A3   08 Mar 2007   B60R-025/00   201225      English
   ES2374221-T3   14 Feb 2012   B60R-025/00   201228      Spanish
   US8344849-B2   01 Jan 2013   G05B-019/00   201303      English
   EP2428413-B1   27 Mar 2013   B60R-025/00   201322      English
   EP2436566-B1   03 Apr 2013   B60R-025/00   201324      English
   CN101296821-B   02 Jan 2013   B60R-025/00   201325      Chinese
   EP1904347-A4   02 Jun 2010   B60R-025/00   201742      English
UT DIIDW:2007282678
ER

PT P
PN WO200209025-A1; AU200140332-A; EP1320830-A1; US2003169907-A1; JP2004504684-W; US7043056-B2; EP1320830-B1; DE60140067-E; JP5016175-B2; EP1320830-A4
TI Determining head pose measurement by localizing face within captured images and identifying feature search regions containing identifiable features.
AB    NOVELTY - Method consists in capturing images of the head and face from different viewpoints, computing an initial head pose estimate, computing the 3D location of facial features, and determining rotational and translational displacement best matching the head model to the computed location of the features.
   USE - Method is for determining 3D head pose, eye gaze direction etc. for passive video analysis of the human head and face in e.g. operator monitoring.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) A method of passively determining eye gaze direction.
   (2) A method of determining eye closure.
   (3) A method of determining eye blink.
   (4) A method of measuring operator fatigue.
   DESCRIPTION OF DRAWING(S) - The figure shows a flow chart of the head pose tracking algorithm.
PD WO200209025-A1   31 Jan 2002   G06K-009/46   200240   Pages: 40   English
   AU200140332-A   05 Feb 2002   G06K-009/46   200241      English
   EP1320830-A1   25 Jun 2003   G06K-009/46   200341      English
   US2003169907-A1   11 Sep 2003   G06K-009/00   200367      English
   JP2004504684-W   12 Feb 2004   G06T-001/00   200413   Pages: 79   Japanese
   US7043056-B2   09 May 2006   G06K-009/00   200632      English
   EP1320830-B1   30 Sep 2009   G06K-009/00   200964      English
   DE60140067-E   12 Nov 2009   G06K-009/46   200974      German
   JP5016175-B2   05 Sep 2012   G06T-001/00   201261   Pages: 28   Japanese
   EP1320830-A4   02 May 2007   G06K-009/00   201748      English
UT DIIDW:2002371465
ER

PT P
PN EP944019-A2; JP11296674-A; US6038333-A; EP944019-A3
TI Image processing and information management system for the identification of people.
AB    NOVELTY - An image processing and information management system (10) allows a user to recognize an input face image via an input/output system (11). Face feature data, from a number of face images, are stored with associated person-identifying information in a profile database (13). An analysis system (12) is used to compare face feature data from the database with the face features of the person to be identified. When matching data is obtained the associated person-identifying information is retrieved from the profile database and displayed on the input/output system.
   USE - For the identification of people.
   ADVANTAGE - Relatively accurate and quick identification of people, due to the use of face feature comparisons.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of an image processing and information management system in accordance with the invention.
   Image processing and information management system (10)
   Input/output system (11)
   Analysis system (12)
   Profile database (13)
PD EP944019-A2   22 Sep 1999   G06K-009/00   199945   Pages: 14   English
   JP11296674-A   29 Oct 1999   G06T-007/00   200003   Pages: 9   Japanese
   US6038333-A   14 Mar 2000   G06K-009/00   200020      English
   EP944019-A3   05 Sep 2001   G06K-009/00   201733      English
UT DIIDW:1999529832
ER

PT P
PN GB2346239-A; JP2000222362-A; KR2000053495-A; TW460819-A; GB2346239-B; US6829711-B1; US2005050366-A1; US7571461-B2; KR346615-B1
TI Smart card security, for making electronic transactions over the Internet, that comprises multiple checkpoints in order to protect the personal Web pages contained on the card and any associated personal bank accounts.
AB    NOVELTY - Smart card security, for making electronic transactions over the Internet, is ensured by the following multiple checkpoints. Before a user can access the personal Web pages contained on a Smart card, the user is validated by means of a personal identification number, facial image, eye image, hand image, fingerprints, and/or voice characteristics. Also, further verification is carried out by validating a digital signature embedded in the card. Before an associated bank account can be accessed by the user, the bank's computer system checks the authenticity of the card and the user's identity using Internet security protocols.
   USE - For making secure electronic transactions over the Internet.
   ADVANTAGE - Improved security when making electronic transactions over the Internet.
   DESCRIPTION OF DRAWING(S) - The figure shows a flow diagram illustrating the multiple checkpoint procedure for accessing a personal Web page stored in a Smart card.
PD GB2346239-A   02 Aug 2000   G07F-019/00   200102   Pages: 24   English
   JP2000222362-A   11 Aug 2000   G06F-015/00   200102   Pages: 11   Japanese
   KR2000053495-A   25 Aug 2000   G06F-017/00   200121      
   TW460819-A   21 Oct 2001   G06F-019/00   200248      Chinese
   GB2346239-B   10 Sep 2003   G07F-019/00   200360      English
   US6829711-B1   07 Dec 2004   H04K-001/00   200480      English
   US2005050366-A1   03 Mar 2005   H04L-009/32   200517      English
   US7571461-B2   04 Aug 2009   H04L-009/32   200951      English
   KR346615-B1   26 Jul 2002   G09C-001/00   200309      
UT DIIDW:2001010278
ER

PT P
PN EP1453002-A2; CA2455088-A1; JP2004265406-A; US2004170337-A1; CN1525401-A; AU2004200313-A1; TW200416622-A; US7039222-B2; US2006153470-A1; US2006228037-A1; US2006228038-A1; US2006228039-A1; US2006228040-A1; US7184578-B2; US7187788-B2; US7212657-B2; CN100350431-C; US7602949-B2; US7636485-B2; EP1453002-A3; TW325567-B1
TI Batch processing method for enhancing appearance of face located in digital image, involves determining facially relevant characteristics of different face regions, for selecting enhancement filters each customized for particular region.
AB    NOVELTY - A face is divided into different regions based on location of facial feature points. The facially relevant characteristics of different face regions, for selecting enhancement filters each customized for particular region. The filters are executed to produce an enhanced digital image from the original image. An output script file containing operations performed on enhanced image, is generated.
   USE - For enhancing appearance of face located in digital image stored in RAM, ROM, hard disk and floppy disk or image received through network such as local area network or Internet from digital camera or scanner.
   ADVANTAGE - Efficiently uses automated and semi-automated portrait image enhancement methods in batch process to enable retouching of portraits without requiring skilled operator intervention to make and supervise the retouching corrections. Thus highly manual and time consuming processes are avoided.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart explaining the batch processing procedure.
PD EP1453002-A2   01 Sep 2004   G06T-005/00   200463   Pages: 44   English
   CA2455088-A1   28 Aug 2004   G06T-005/00   200463      English
   JP2004265406-A   24 Sep 2004   G06T-001/00   200463   Pages: 39   Japanese
   US2004170337-A1   02 Sep 2004   G06K-009/40   200463      English
   CN1525401-A   01 Sep 2004   G06T-005/00   200478      Chinese
   AU2004200313-A1   16 Sep 2004   G06T-005/50   200479      English
   TW200416622-A   01 Sep 2004   G06K-009/46   200624      Chinese
   US7039222-B2   02 May 2006   G06K-009/00   200629      English
   US2006153470-A1   13 Jul 2006   G06K-009/40   200646      English
   US2006228037-A1   12 Oct 2006   G06K-009/40   200668      English
   US2006228038-A1   12 Oct 2006   G06K-009/40   200668      English
   US2006228039-A1   12 Oct 2006   G06K-009/40   200668      English
   US2006228040-A1   12 Oct 2006   G06K-009/40   200668      English
   US7184578-B2   27 Feb 2007   G06K-009/00   200718      English
   US7187788-B2   06 Mar 2007   G06K-009/00   200718      English
   US7212657-B2   01 May 2007   G06K-009/00   200730      English
   CN100350431-C   21 Nov 2007   G06T-005/00   200835      Chinese
   US7602949-B2   13 Oct 2009   G06K-009/00   200967      English
   US7636485-B2   22 Dec 2009   G06K-009/40   201001      English
   EP1453002-A3   10 Nov 2010   G06T-005/00   201073      English
   TW325567-B1   01 Jun 2010   G06K-009/46   201174      Chinese
UT DIIDW:2004644237
ER

PT P
PN JP2000003452-A; AU9933982-A; AU9963173-A; AU728290-B; AU739936-B; US2003179911-A1; US6661907-B2; US7218759-B1
TI Facial image identification method in computer application such as for crime prevention - involves identifying face by detecting skin colors of different parts of face.
AB       NOVELTY - The identification is done on the basis of color analysis of skin color of different parts of face and not on complete face analysis. DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following: image judging apparatus; recording medium that stores program for executing facing identification process
   USE -   For computer application such as for processing image from crime prevention camera, ATM etc. DESCRIPTION OF DRAWING(S) - The figure shows condition of image divided into different areas.
PD JP2000003452-A   07 Jan 2000   G06T-007/00   200012   Pages: 8   Japanese
   AU9933982-A   23 Dec 1999   G06T-007/00   200012      English
   AU9963173-A   24 Feb 2000   G06T-007/00   200020      English
   AU728290-B   04 Jan 2001   G06T-007/00   200107      English
   AU739936-B   25 Oct 2001   G06T-007/00   200173      English
   US2003179911-A1   25 Sep 2003   G06K-009/00   200364      English
   US6661907-B2   09 Dec 2003   G06K-009/00   200381      English
   US7218759-B1   15 May 2007   G06K-009/00   200734      English
UT DIIDW:2000132206
ER

PT P
PN US6072894-A
TI Applicants screening method involves examining and updating history of prior requests of applicant after comparing bi level digital facial image of applicant during submission with image provided in his/her details.
AB    NOVELTY - The bi level digital facial image of applicant is compared with bi level digital facial images of prior applicants using computer based face recognition. The history of prior requests of applicants is examined and updated using computer accessible database. The notification of results of examination is provided so as to perform further scrutiny.
   USE - For screening applicants in communication applicants such as Internet.
   ADVANTAGE - Fraudulent applications are rejected effectively using biometric face recognition.
   DETAILED DESCRIPTION - The requests from applicants are received and his/her digital facial image is captured when the applicant hand overs the application in person. The digital facial image is converted to bi level digital facial image. The image along with data identifying request for application is transmitted to computing location.
   DESCRIPTION OF DRAWING(S) - The figure shows the flow chart of the steps involved in biometric facial screening process at applicant screening branch.
PD US6072894-A   06 Jun 2000   G06K-009/00   200035   Pages: 13   English
UT DIIDW:2000411238
ER

PT P
PN WO9311511-A2; AU9340291-A; ZA9301449-A; EP614559-A1; GB2276967-A; HU66345-T; WO9311511-A3; JP7502351-W; GB2276967-B; AU669707-B; US5608387-A; HU214529-B; EP614559-B1; DE69228315-E; ES2129462-T3; JP3530185-B2; CA2123518-C; KR292547-B1
TI Personal identification devices and access control systems - uses store of authorised users and store of complex facial images to enable user identification..
AB       An access control device is controlled by a data processing system (10) which has access to a store of authorised users (14) and a store of complex images (15). The complex images are specifically images of human faces.
   Each authorised user recognises certain complex images and these key images are linked to an identity statement unique to that user. On receipt of the identity statement, a matrix of images including the key images is presented to the user on a display (12). The user must identify which images are his key images in order to confirm authorisation and be permitted access. The user and image stores (18) may be carried in credit card form as a section of an optical disk.
   ADVANTAGE -   Provides an access control system of high potential security which is relatively simple and reliable.
PD WO9311511-A2   10 Jun 1993      199324   Pages: 26   English
   AU9340291-A   28 Jun 1993      199342      English
   ZA9301449-A   29 Dec 1993   A61B-000/00   199405   Pages: 27   English
   EP614559-A1   14 Sep 1994   G07C-009/00   199435   Pages: 2   English
   GB2276967-A   12 Oct 1994   G07C-009/00   199438   Pages: 1   English
   HU66345-T   28 Nov 1994   G07C-009/00   199502      Hungary
   WO9311511-A3   08 Jul 1993   G07C-009/00   199513      English
   JP7502351-W   09 Mar 1995   G06F-015/00   199518   Pages: 11   Japanese
   GB2276967-B   01 Nov 1995   G07C-009/00   199547   Pages: 1   English
   AU669707-B   20 Jun 1996   G06K-009/00   199632      English
   US5608387-A   04 Mar 1997   H04Q-001/00   199715   Pages: 13   English
   HU214529-B   30 Mar 1998   G07C-009/00   199823      Hungary
   EP614559-B1   27 Jan 1999   G07C-009/00   199909      English
   DE69228315-E   11 Mar 1999   G07C-009/00   199916      German
   ES2129462-T3   16 Jun 1999   G07C-009/00   199930      Spanish
   JP3530185-B2   24 May 2004   G06F-015/00   200434   Pages: 12   Japanese
   CA2123518-C   22 Jun 2004   G07C-009/00   200442      English
   KR292547-B1   01 Jun 2001   G06F-015/00   200225      
UT DIIDW:1993197273
ER

PT P
PN US2001017584-A1; JP2001236324-A; DE10105396-A1; US6765470-B2
TI Mobile electronic apparatus with user authentication function used in bank account settlement, displays personal data of user, if extracted biometrics information is in accord with reference information.
AB    NOVELTY - A verification section (30) compares extracted biometrics feature information with reference biometrics feature information of user. If the user's biometrics feature information matches the authorized user's reference information, a display controller (60) reads-out personal data of the user from a memory (40) and controls a display section (50) to display the personal data.
   USE - Mobile electronic apparatus such as portable telephone, portable electronic information terminal with function for authenticating user by biometrics information such as fingerprint, palmprint, finger shape, hand shape, voiceprint, retina, iris, facial recognition, signature dynamics, blood vessel pattern, key strokes, for use in bank account settlement, electronic commerce or security trading.
   ADVANTAGE - An improved degree of security is realized, as user's personal data is safely protected in mobile electronic apparatus against any unauthorized user, since biometric information is used for verifying a user.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of mobile electronic apparatus.
   Verification section (30)
   Memory (40)
   Display section (50)
   Display controller (60)
PD US2001017584-A1   30 Aug 2001   G05B-023/00   200158   Pages: 27   English
   JP2001236324-A   31 Aug 2001   G06F-015/00   200158   Pages: 21   Japanese
   DE10105396-A1   20 Sep 2001   G06K-009/00   200163      German
   US6765470-B2   20 Jul 2004   H04Q-009/00   200448      English
UT DIIDW:2001529250
ER

PT P
PN WO2004049242-A2; AU2003298731-A1; US2004213437-A1; US7804982-B2; WO2004049242-A3
TI Identification document e.g. driver license issuing system for use with biometric recognition system, has user interface indicating whether database has images of individuals resembling digitized images.
AB    NOVELTY - The system has a database for storing digitized images e.g. finger print with a biometric image. A server sends the images to a biometric recognition system (50) that is in communication with another database, which has biometric templates of individual images. A user interface indicates whether the latter database has images of individuals resembling the digitized images.
   USE - Used for issuing identification document e.g. driver license, magnetic disk, credit card, bank card, phone card, stored value card, prepaid card, smart card contact card, contactless card, proximity card, passport, network access card employee badge, debit card, security visa, immigration documentation, national ID card, citizenship card, social security card, security badge, certificate, identification card or document, voter registration and/or identification card, police ID card, border crossing card, security clearance badge and card, legal instrument, gun permit, badge, gift certificate or card membership card or badge, and tag, to a group of individual (claimed) that are utilized in a biometric recognition system for determining an image of an individual.
   ADVANTAGE - The user interface enables to detect patterns of fraud, geographically locate entities committing and/or attempting to commit fraud, thereby helping to prevent fraud, and hence improving accuracy of facial recognition processing, avoiding time consuming and expenses in reckoning an image.
   DETAILED DESCRIPTION - A workstation permits a user to determine whether to issue an individual an identification document or to keep the document in the individuals possession.
   INDEPENDENT CLAIMS are also included for the following:
   (1) a method for screening a group of applicants each seeking to be issued an identification document
   (2) a computer implemented method of creating a biometric template of an individual for facial recognition processing
   (3) a method of searching a database of biometric templates, each biometric template associated with a corresponding facial image, for an image of an individual who substantially resembles an individual in a probe image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a system for biometric searching.
   Biometric searching system (50)
   Facial recognition search system (52)
   Web server (54)
   Investigative workstation (56)
   Intake station (62)
   DMV mainframe (64)
   Capture station (66)
PD WO2004049242-A2   10 Jun 2004   G06K-009/00   200442   Pages: 88   English
   AU2003298731-A1   18 Jun 2004   G06K-009/00   200471      English
   US2004213437-A1   28 Oct 2004   G06K-009/00   200471      English
   US7804982-B2   28 Sep 2010   G06K-009/00   201064      English
   WO2004049242-A3   15 Jul 2004   G06K-009/00   201213      English
UT DIIDW:2004450538
ER

PT P
PN US4625329-A
TI Position analyser for vehicle drivers using three-D coordinate system - has image sensors related to line bisecting drivers seat.
AB       The analyzer comprises a light emitting element for emitting an infrared light into the driver's face porrion and an image detector arranged to receive reflected infrared light to generate an optical image of the driver's face portion. A microcomputer includes a memory for storing an electronic image corresp, to the optical image and a recognition device processes the stored image to determine the position of a facial feature, such as an eye, in a three-dimensional coordinate system. Position indicating data is utilised in a driver assistance system. The light emitter is arranged to emit light at periodic intervals and the recognition device is synchronized with the light emitter to respond to the periodically emitted light.
   The image sensors are solid-state image sensors and the light emitter comprises an infrared light emitting device and the light detector comprises an infrared light detecting device. The infrared light emitter comprises an infrared light flash lamp and is located in such a position that the emitted light is directed at an angle of elevation to the driver's face from in front of the driver and the image sensors are located in front of the driver to receive light reflected from the driver's face.
   ADVANTAGE -   Provides early warning when driver is incapacitated as result of over drinking or distraction.
PD US4625329-A   25 Nov 1986   G06K-009/00   198650   Pages: 18   English
UT DIIDW:1986331953
ER

PT P
PN WO9927838-A2; AU9916135-A; EP1034507-A2; US6381346-B1; US2002122573-A1; US6801641-B2; WO9927838-A3
TI Three-dimensional face identification system.
AB    NOVELTY - The two-dimensional facial image of a suspect is analyzed to determine the facial feature shapes from an inventory best matching the image and the codes of these shapes are identified and stored. A facial image is then generated by a processor using the shapes corresponding to the stored codes and the generated facial image is displayed in two dimensions. The matched codes of a suspect are compared to stored codes of individuals to identify the suspect according to a match.
   USE - Face identification in law enforcement and corrections applications.
   ADVANTAGE - Fast searching of large collection of two-dimensional mug shots to find match for particular image.
   DETAILED DESCRIPTION - Independent claims are included for a three-dimensional face identification image generation system and for a method of identifying an object.
PD WO9927838-A2   10 Jun 1999   A61B-000/00   199937   Pages: 87   English
   AU9916135-A   16 Jun 1999   A61B-000/00   199945      English
   EP1034507-A2   13 Sep 2000   G06K-009/00   200046      English
   US6381346-B1   30 Apr 2002   G06K-009/00   200235      English
   US2002122573-A1   05 Sep 2002   G06K-009/00   200260      English
   US6801641-B2   05 Oct 2004   C06K-009/00   200466      English
   WO9927838-A3   12 Aug 1999   A61B-000/00   201729      English
UT DIIDW:1999443836
ER

PT P
PN US8457367-B1; EP2680192-A2; KR2014001164-A; CN103514440-A; KR1393717-B1; EP2680192-A3; EP2680192-B1
TI Anti-spoofing based facial recognition method involves determining whether to deny authentication to user with respect to accessing functionalities controlled by computing device, based on detecting facial gesture.
AB    NOVELTY - The method (1500) involves receiving (1501,1502) two facial images of a face. A facial landmark in first image and a facial landmark in second image, are identified (1504). The sub-images representing facial landmarks are extracted (1506,1508) from respective images. A facial gesture is detected by determining (1510) whether a sufficient difference exists between sub-images to indicate facial gesture. A determination is made as to whether to deny authentication to user with respect to accessing functionalities controlled by a computing device, based on facial gesture.
   USE - Facial recognition method based on anti-spoofing.
   ADVANTAGE - The computing device can implement anti-spoofing programs to detect suspected attempts to spoof, and prevent erroneous authentication due to spoofing. The anti-spoofing programs can reduce the chances of an unauthorized user causing erroneous authentication by spoofing. The anti-spoofing programs can reduce the usage of the facial recognition programs, thus the computing resources can be conserved and the power consumption can be reduced. The method can be enhanced to improve robustness and accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a computing device; and
   (2) a computer-readable storage device storing program for anti-spoofing in computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the anti-spoofing process performed by the computing device.
   Process for performing facial recognition (1500)
   Steps for receiving facial images (1501,1502)
   Step for identifying facial landmarks in the images (1504)
   Steps for extracting sub-images representing facial landmarks from respective images (1506,1508)
   Step for determining whether sufficient difference exists between the sub-images to indicate the facial gesture (1510)
   Step for denying authentication (1514)
PD US8457367-B1   04 Jun 2013   G06K-009/62   201338   Pages: 39   English
   EP2680192-A2   01 Jan 2014   G06K-009/00   201403      English
   KR2014001164-A   06 Jan 2014   G06K-009/60   201406      
   CN103514440-A   15 Jan 2014   G06K-009/00   201420      Chinese
   KR1393717-B1   13 May 2014   G06K-009/60   201434      
   EP2680192-A3   28 Jun 2017   G06K-009/00   201744      English
   EP2680192-B1   07 Aug 2019   G06K-009/00   201960      English
UT DIIDW:2013K06091
ER

PT P
PN US2004207743-A1; EP1471455-A2; JP2004317699-A; JP2004320284-A; JP2004320285-A; JP2004320286-A; JP2004320287-A; JP4196714-B2; EP1471455-B1; DE602004030390-E; EP1471455-A3
TI Digital still camera system for detecting feature point of digital image, selects each feature point such as eye, nose from input image data, in accordance with user given order, and displays selected feature point information.
AB    NOVELTY - A detector detects the feature points such as eye, nose from the input image data. A selector selects each feature point in the input image data, in accordance with the user specified order, when the several feature points are detected. A display unit displays the feature point information identifying the feature point selected by the selector.
   USE - Digital still camera system for detecting eye, nose, mouth, face, pupils, eyebrows, ear, hand, leg, hair, wrinkle, mole, freckles, skin color and outline of eyeglasses of human being. Also detects hair-style, bone structure and clothes of person to discriminate sex, race and age of person. Also for detecting feature point in dog, cat, bird, house and car.
   ADVANTAGE - Efficiently discriminates the feature point of target object, by simple technique.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the digital still camera system
PD US2004207743-A1   21 Oct 2004   H04N-005/222   200477   Pages: 38   English
   EP1471455-A2   27 Oct 2004   G06K-009/00   200477      English
   JP2004317699-A   11 Nov 2004   G02B-007/28   200477   Pages: 23   Japanese
   JP2004320284-A   11 Nov 2004   H04N-005/238   200477   Pages: 22   Japanese
   JP2004320285-A   11 Nov 2004   H04N-005/232   200477   Pages: 25   Japanese
   JP2004320286-A   11 Nov 2004   H04N-005/232   200477   Pages: 26   Japanese
   JP2004320287-A   11 Nov 2004   H04N-005/232   200477   Pages: 25   Japanese
   JP4196714-B2   17 Dec 2008   H04N-005/232   200903   Pages: 21   Japanese
   EP1471455-B1   08 Dec 2010   G06K-009/00   201081      English
   DE602004030390-E   20 Jan 2011   G06K-009/00   201107      German
   EP1471455-A3   25 May 2005   G06K-009/00   201737      English
UT DIIDW:2004782764
ER

PT P
PN US6108437-A
TI Face recognition system used in security agencies, determines position of face by applying eye template to extract facial features which is compared with stored features to identify face.
AB    NOVELTY - An eye template circuit in face position registration circuit applies eye and mirror image templates on either sides of face to determine face position when the face detector determines the presence of face. Based on the determined position, facial features are extracted from the face. Then, the extracted features are compared with corresponding feature candidate stored in database to identify the face.
   USE - Used by security agencies, low enforcement agencies, air line industry, border patrol, banking, etc,.
   ADVANTAGE - Identifies subjects from images taken by surveillance camera and provides such identification to appropriate person.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) face recognition method;
   (b) face recognition software
   DESCRIPTION OF DRAWING(S) - The figure shows block diagram of face recognition system.
PD US6108437-A   22 Aug 2000   G06K-009/00   200054   Pages: 25   English
UT DIIDW:2000578585
ER

PT P
PN EP844582-A2; JP10320562-A; US6184926-B1; EP844582-A3
TI Video image processing system for detecting human face - has probability computing device performs computation of probability of model based on ellipse or ellipses fitted in identified region, filters out sub-regions that are below selected small size or above selected large size.
AB       The system includes a component analysis device to process the pixels of the image to identify a region of connected components in the foreground of the image. An ellipse fitting device performs an iterative ellipse fitting algorithm to fit one or more ellipses to the connected components in the identified region. A probability computing device performs a computation of the probability of the model based on the ellipse or ellipses fitted in the identified region. Sub-regions within the identified region that are below a selected low intensity threshold are identified. The sub-regions that are below a selected small size or above a selected large size are filtered out. The remaining sub-regions are filtered based on anthropological measures to derive co-ordinates representing eyes.
   ADVANTAGE -   Capable of quickly, reliably, flexibly detect existence of faces or faces within video image.
PD EP844582-A2   27 May 1998   G06K-009/00   199825   Pages: 22   English
   JP10320562-A   04 Dec 1998   G06T-007/00   199908   Pages: 54   Japanese
   US6184926-B1   06 Feb 2001   H04N-005/262   200109      English
   EP844582-A3   26 May 1999   G06K-009/00   201735      English
UT DIIDW:1998274406
ER

PT P
PN US2009262987-A1; WO2009145826-A2; WO2009145826-A3; AU2009251833-A1; EP2260469-A2; KR2010134079-A; CA2719992-A1; JP2011516965-W; CN102067175-A; US8098904-B2; JP5361987-B2; EP2260469-B1; AU2009251833-B2; CN102067175-B; KR1572995-B1; CA2719992-C; EP2260469-A4
TI Identities obscuring method for set of faces in image, involves detecting region corresponding to face in image, selecting portion of region, and blurring portion of region to obscure features of face.
AB    NOVELTY - The method involves detecting a region corresponding to a face in an image by a face detector (112), and selecting a portion of the region, where multiple features of the face are provided in the region. The portion of the region is blurred by a motion blurrer (118) to obscure the features of the face. A verification is made whether the region includes a color consistent with human skin, and the detected region is checked with a stored public image. The detected face region is replaced by a face replacer (116) with substitute facial images.
   USE - Method for obscuring identities of a set of faces in an image.
   ADVANTAGE - The method automatically detects and processes the face regions in the image, and enables an identity masker to use an identity masking algorithm to process the detected face regions to obscure the identifying features within the detected face regions. The identity masker utilizes a motion blur algorithm to blur the detected face region, so that the blurred face region is made to appear, and utilizes a face replacement algorithm to replace the detected face region with a substitute facial image such that the corresponding identity is obscured. The method enables adjustment of the sensitivity of the face detector to detect the regions corresponding to face, and performs a skin color analysis to reject false positives detected by the face detector.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for obscuring identities in an image, comprising a processing pipeline server.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a system for performing identity masking on images viewed through a network.
   Processing pipeline server (110)
   Face detector (112)
   Identity masker (114)
   Face replacer (116)
   Motion blurrer (118)
PD US2009262987-A1   22 Oct 2009   G06K-009/00   200971   Pages: 17   English
   WO2009145826-A2   03 Dec 2009   G06T-007/40   200979      English
   WO2009145826-A3   21 Jan 2010   G06T-007/40   201007      English
   AU2009251833-A1   04 Nov 2010   G06T-007/00   201073      English
   EP2260469-A2   15 Dec 2010   G06T-007/40   201082      English
   KR2010134079-A   22 Dec 2010   G06T-007/00   201102      
   CA2719992-A1   03 Dec 2009   G06T-007/40   201118      English
   JP2011516965-W   26 May 2011   G06T-007/00   201135   Pages: 17   Japanese
   CN102067175-A   18 May 2011   G06T-007/40   201141      Chinese
   US8098904-B2   17 Jan 2012   G06K-009/36   201206      English
   JP5361987-B2   04 Dec 2013   G06T-007/00   201380   Pages: 16   Japanese
   EP2260469-B1   07 May 2014   G06T-007/40   201431      English
   AU2009251833-B2   07 Aug 2014   G06T-007/00   201458      English
   CN102067175-B   27 Aug 2014   G06T-007/40   201477      Chinese
   KR1572995-B1   30 Nov 2015      201609      English
   CA2719992-C   06 Sep 2016   G06T-007/40   201661      English
   EP2260469-A4   06 Mar 2013   G06T-007/40   201772      English
UT DIIDW:2009Q25398
ER

PT P
PN US7551755-B1
TI Processor-based workflow system, has face detection module for identifying group of pixels corresponding to face region within digital image data acquired by acquisition device, and database module archiving data.
AB    NOVELTY - The system has a face detection module for identifying a group of pixels corresponding to a face region within digital image data acquired by an acquisition device. A normalization module generates a normalized version of the face region. A face recognition module extracts a set of face classifier parameter values from the normalized face region that is referred as a faceprint. A workflow module compares an extracted face print to a database of archived faceprints. A database module archives data corresponding to a new faceprint.
   USE - Processor-based workflow system.
   ADVANTAGE - The system enables the face detection module to automatically identify the group of pixels corresponding to the face region when the digital image is received, and automatically processes the detected face regions for normalization when the detected face regions are identified.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for classifying and archiving images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a core system workflow in a processor-based workflow system.
PD US7551755-B1   23 Jun 2009   G06K-009/00   200943   Pages: 64   English
UT DIIDW:2009K63357
ER

PT P
PN US2003100371-A1; WO2003045518-A1; AU2002352872-A1; EP1446206-A1; US6945870-B2; EP1446206-A4
TI Networked gaming system for e.g. bar, creates video images based on directives from internal processor and input from interactive users, received over wide area network.
AB    NOVELTY - A central server (112) creates video images in response to directives from processors for applying game rules and user interaction network service provided by a gaming machine (200) over the wide area network (1402). The gaming machine displays the video images received from the server over Internet.
   USE - Networked gaming system comprising pay computer-controlled game machine, entertainment device, mobile gaming machine for playing games of skill and chance of cash or cashless type in public places, casino, bar, restaurant, swimming pool, hotel room, etc. Also can be used in lottery and pari-mutual wager system.
   ADVANTAGE - Low cost computer hardware is sufficient at the user side, since intensive graphics operation, expensive graphics accelerator are eliminated. Stability and reliability are achieved, as gaming machine computer platform are simple and need not be upgraded. Future proofing is ensured, as no software or hardware upgrades are required to accommodate extremely resource intensive multimedia advances such as future generations of advanced graphics animations, voice recognition, face recognition, avatar creation. Moreover selection of a given microprocessor architecture, operating system platform and supplier do not impact the future capabilities of the gaming machine. The video encoding, transmission, reception and decoding use low cost and mass-produced economical TV and streaming media components.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for method of controlling gaming system.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic model of the networked gaming system.
   central server (112)
   gaming machine (200)
   wide area network (1402)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - Payment and identification devices are coupled to a platform communicating with a central server through RS232/RS485 or similar connections. The connection between specialized devices and processing hardware in the gaming machine, is through RS232, RS422, Parallel, via dedicated add-on board. A network access point such as universal serial bus (USB) to Ethernet converter and RS232 to Ethernet, allows API to be directly supported and controlled over network. Video streams are encoded using motion picture expert group (MPEG) 4 compression standard.
PD US2003100371-A1   29 May 2003   A63F-013/00   200358   Pages: 33   English
   WO2003045518-A1   05 Jun 2003   A63F-013/00   200358      English
   AU2002352872-A1   10 Jun 2003   A63F-013/00   200419      English
   EP1446206-A1   18 Aug 2004   A63F-013/00   200454      English
   US6945870-B2   20 Sep 2005   A63F-009/24   200562      English
   EP1446206-A4   03 Jan 2007   G07F-017/32   201751      English
UT DIIDW:2003616467
ER

PT P
PN US7580551-B1
TI Computer based biometric samples determining method, involves measuring accuracy of model by calculation involving features selected from group consisting of micro-features, macro-features and combination of micro-and macro-features.
AB    NOVELTY - The method involves comparing a biometric sample i.e. handwriting sample, with another biometric sample within a general purpose computer. A determination is made whether samples are from a same source by the computer. The cluster is made to composite based on a model for measuring a distance between vectors of binary feature e.g. line quality. An accuracy of the model is measured by calculation involving features selected from a group consisting of micro-features e.g. gradient, macro-features e.g. vertical slope and a combination of micro-and macro-features.
   USE - Computer based method for determining whether biometric samples e.g. handwriting sample, are from a same source, in a criminal justice system. Can also be used for voice sample, face geometry sample, fingerprint sample, hand geometry sample, iris sample, retinal sample and vein sample.
   ADVANTAGE - The method enables scientific support for an admission of handwriting and other biometric evidence in a court. The method enables examination, analysis and comparison of handwritten documents and other biometric samples for the purpose of authentication and identification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a verification model of general algorithm.
PD US7580551-B1   25 Aug 2009   G06K-009/00   200957   Pages: 68   English
UT DIIDW:2009M85979
ER

PT P
PN JP10334213-A; US6035074-A; JP3222091-B2
TI Automatic facial recognition apparatus in image processor for digital photograph - cuts predetermined facial image from area surrounded by frame which is produced by frame production unit based on size of facial image.
AB       NOVELTY - A colour LCD (5) displays a colour photograph acquired by an image acquisition units (11-6). Based on indication received from an indication unit, a facial image recognition unit (11-1) recognises facial image from the photograph by comparing it with multiple data stored in a memory (12-2). An area indicating unit indicates area around facial image to be cut-out, and a frame production unit (11-7) produces a frame depending on size of facial image. A facial image cut- out unit (11-4) cuts the facial image from area surrounded by the frame.
   USE -   In image processor for digital photograph.
   ADVANTAGE -   The editing process of photograph is performed irrespective of its brightness and background images. DESCRIPTION OF DRAWING(S) - The figure shows block diagram of image processor. (5) Colour LCD device. (11-1) Image recognising unit. (11-4) Facial image cut-out unit. (11-6) Image acquisition unit. (11-7) Frame production unit. (12-2) Memory.
PD JP10334213-A   18 Dec 1998   G06T-001/00   199910   Pages: 25   Japanese
   US6035074-A   07 Mar 2000   G06K-009/20   200019      English
   JP3222091-B2   22 Oct 2001   G06T-001/00   200169   Pages: 26   Japanese
UT DIIDW:1999111070
ER

PT P
PN WO9202000-A; EP539439-A1; JP5508951-W; CA2087523-C; US5719951-A; SG47939-A1; EP539439-B1; DE69131350-E; JP3040466-B2
TI Processing method for recognition of face image - modifying image data of located features to reduce deviation from standard feature shape using deformable template technique.
PD WO9202000-A   06 Feb 1992      199208      English
   EP539439-A1   05 May 1993      199318      English
   JP5508951-W   09 Dec 1993   G06F-015/62   199403      Japanese
   CA2087523-C   15 Apr 1997   G06K-009/46   199728      English
   US5719951-A   17 Feb 1998   G06K-009/00   199814      English
   SG47939-A1   17 Apr 1998   G06K-009/46   199827      English
   EP539439-B1   16 Jun 1999   G06K-009/46   199928      English
   DE69131350-E   22 Jul 1999   G06K-009/46   199935      German
   JP3040466-B2   15 May 2000   G06T-007/00   200028   Pages: 14   Japanese
UT DIIDW:1992065102
ER

PT P
PN US7039221-B1
TI Non-invasive human user identification and verification system, has silicon-based video camera embedded within card for gathering facial image data, and digitizer integrated within smart card for digitizing facial image data.
AB    NOVELTY - The system has a smart card, and a silicon-based video camera (111) embedded within the card for gathering facial image data. A digitizer (112) integrated within the smart card digitizes the facial image data, and a smart-card docking station with a port receives the smart card. A central processor receives and manipulates the stored image data to produce an output signal for identifying and verifying the human user.
   USE - Used for identifying a human user utilizing automated facial image recognition.
   ADVANTAGE - The silicon-based video camera embedded within the card gathers the facial image data of the user, and the data is manipulated to produce the output, thus enabling to prevent fraud in point of sale and Internet-based financial transactions.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for identification and verification of a human user.
   DESCRIPTION OF DRAWING(S) - The drawing shows a human user identification and verification system.
   Video camera (111)
   Digitizer (112)
   Central processor (116)
   Smart card docking station (118)
   Smart card (127)
   Verification software (130)
PD US7039221-B1   02 May 2006   G06K-009/00   200634   Pages: 16   English
UT DIIDW:2006327196
ER

PT P
PN DE19613614-A1; JP8300978-A; US5859921-A; DE19613614-C2; JP3452685-B2
TI Image of face processing apparatus - with grey scale conversion unit extracting black level in direction parallel to pixel scan direction.
AB       The apparatus includes a camera for taking the picture of the face. An image signal input device (1) receives the image from the camera. A grey scale conversion device (2) converts the grey level of a face image.
   A variable binary conversion device (3) converts the output image from the grey scale conversion device into a binary image based on a variable threshold. An eye search region setting device (4) sets the eye search region in the binary image. A target region setting device (5) sets one or more target regions in the eye search region. An eye region detection device (6) detects an eye region from the target region.
   USE/ADVANTAGE -   For use as security device for driver recognition, e.g. to detect if driver falls asleep. Correctly records by artificial light, sun light regardless of direction, height, filtering of sun, extracts eye even if driver wears glasses,
PD DE19613614-A1   14 Nov 1996   G06K-009/36   199651   Pages: 64   German
   JP8300978-A   19 Nov 1996   B60K-028/06   199705   Pages: 36   Japanese
   US5859921-A   12 Jan 1999   G06K-009/00   199910      English
   DE19613614-C2   21 Oct 1999   G06K-009/36   199948      German
   JP3452685-B2   29 Sep 2003   G06T-007/00   200364   Pages: 39   Japanese
UT DIIDW:1996507056
ER

PT P
PN EP735510-A1; JP8272948-A; US5870138-A; EP735510-B1; DE69524204-E
TI Facial image processing used in real=time facial image recognition - involves capturing subject facial image , generating tracking signal containing feature extraction data and using memory store to generate substitute face.
AB       The image processing involves receiving a subject facial image signal, generating a feature extraction tracking signal and processing the tracking signal to provide a processed output signal An image signal representing a substitute face is generated. The substitute face image signal is modified in real time according to the tracing signal to generate an output signal representing the substitute face with facial features of the subject face.
   The output signal is transmitted from an output image memory which is modified by overwriting pixel values transferred from an update image memory in response to the tracking signal.
   ADVANTAGE -   Obtains more useful outputs for improved versatility. Outputs are achieved in real time. Complexity of hardware is limited.
PD EP735510-A1   02 Oct 1996   G06T-007/20   199644   Pages: 28   English
   JP8272948-A   18 Oct 1996   G06T-001/00   199701   Pages: 22   Japanese
   US5870138-A   09 Feb 1999   H04N-007/18   199913      English
   EP735510-B1   28 Nov 2001   G06T-007/20   200201      English
   DE69524204-E   10 Jan 2002   G06T-007/20   200211      German
UT DIIDW:1996435901
ER

PT P
PN EP676899-A2; US5500673-A; US5512939-A; US5548322-A; US5550580-A; US5550581-A; US5596362-A; CN1118961-A; EP676899-A3; EP676899-B1; DE69523503-E
TI Audio and video coding method for communication system - determines significance of audio and video signal, as well as motion in video signal of scene, and allocates bits according to determined accuracy based on significance of information detail.
AB       The encoding method analyses the audio signal to determine the content of the audio signal and analyses the video signal to determine the content of the video signal. Available bits are allocated (80) among the audio and video signals based on the perceptual significance of the content of the audio and video signals to a user.
   The audio and video signals are encoded (60 and 30) with the allocated bits. The video signal includes a facial region of one person which includes a lip region. The analysing of the video signal step determines if the lips are moving. The allocating step allocates a sufficient number of available bits to ensure quality transmission of the audio signal and the lip region of the video signal if audio is present while the lips are moving.
   USE/ADVANTAGE -   E.g. video tele-conference. Allocates amount of available bits efficiently, according to significance of detail information in communication scene.
PD EP676899-A2   11 Oct 1995   H04N-007/26   199545   Pages: 30   English
   US5500673-A   19 Mar 1996   H04N-007/18   199617   Pages: 24   English
   US5512939-A   30 Apr 1996   H04N-007/52   199623   Pages: 26   English
   US5548322-A   20 Aug 1996   H04N-007/14   199639   Pages: 25   English
   US5550580-A   27 Aug 1996   H04N-007/14   199640   Pages: 25   English
   US5550581-A   27 Aug 1996   H04N-007/14   199640   Pages: 25   English
   US5596362-A   21 Jan 1997   H04N-007/14   199710   Pages: 26   English
   CN1118961-A   20 Mar 1996   H04N-001/46   199743      Chinese
   EP676899-A3   19 Nov 1997   H04N-007/26   199816      English
   EP676899-B1   31 Oct 2001   H04N-007/26   200169      English
   DE69523503-E   06 Dec 2001   H04N-007/26   200203      German
UT DIIDW:1995346430
ER

PT P
PN WO200036546-A1; EP1053530-A1; US6263113-B1; US2001026633-A1; KR2001040852-A; JP2002532807-W; US6574354-B2; KR677177-B1
TI Digital image detection method, involves classifying mapped binary image components, as facial and non-facial types.
AB    NOVELTY - A binary image is produced by detecting skin colored pixels in a digital image. The pixels corresponding to edges in luminance component of the binary image, are removed, to produce binary image components which are mapped into a graph. The mapped image components are classified as facial and non-facial types.
   USE - For face detection in security systems, criminal identification, digital image capturing and teleconferences.
   ADVANTAGE - Enables easily detecting a face disposed within a complex background.
   DESCRIPTION OF DRAWING(S) - The figure shows the sequence of images illustrating the application of heuristic.
PD WO200036546-A1   22 Jun 2000   G06K-009/00   200046   Pages: 26   English
   EP1053530-A1   22 Nov 2000   G06K-009/00   200061      English
   US6263113-B1   17 Jul 2001   G06K-009/36   200142      English
   US2001026633-A1   04 Oct 2001   G06K-009/00   200161      English
   KR2001040852-A   15 May 2001   G06T-001/00   200167      
   JP2002532807-W   02 Oct 2002   G06T-007/00   200279   Pages: 25   Japanese
   US6574354-B2   03 Jun 2003   G06K-009/00   200339      English
   KR677177-B1   05 Feb 2007   G06K-009/00   200850      
UT DIIDW:2000514518
ER

PT P
PN JP2002133446-A; US2004201586-A1; US2004204927-A1; US2004210427-A1; US6850872-B1; US2005089199-A1; US2006013449-A1; US2006015308-A1; US7124066-B2; US7152024-B2; US7356447-B2; US7363201-B2; US7433807-B2; US7492927-B2
TI Facial image processing method involves capturing prescribed structural information by light sources, based on which animation processing of face, expression recognition and conversion is performed.
AB    NOVELTY - The structural data such as range, depth, mirror reflection characteristics are captured by the light sources (204,206). The animation processing of the face, expression recognition and expression conversion is performed based on the captured information.
   USE - For animating features of face.
   ADVANTAGE - Enables animating face features and converting the face expressions accurately.
   DESCRIPTION OF DRAWING(S) - The figure shows an explanatory drawing of the facial image processing method. (Drawing includes non-English language text).
   light sources (204,206)
PD JP2002133446-A   10 May 2002   G06T-015/70   200321   Pages: 25   Japanese
   US2004201586-A1   14 Oct 2004   G06T-015/60   200468      English
   US2004204927-A1   14 Oct 2004   G06F-009/455   200468      English
   US2004210427-A1   21 Oct 2004   G06F-017/10   200470      English
   US6850872-B1   01 Feb 2005   G06F-007/60   200511      English
   US2005089199-A1   28 Apr 2005   G06K-009/00   200530      English
   US2006013449-A1   19 Jan 2006   G06K-009/00   200607      English
   US2006015308-A1   19 Jan 2006   G06F-017/10   200607      English
   US7124066-B2   17 Oct 2006   G06F-017/10   200668      English
   US7152024-B2   19 Dec 2006   G06K-009/00   200702      English
   US7356447-B2   08 Apr 2008   G06K-009/40   200826      English
   US7363201-B2   22 Apr 2008   G06F-007/60   200832      English
   US7433807-B2   07 Oct 2008   G06F-007/60   200867      English
   US7492927-B2   17 Feb 2009   G06K-009/00   200914      English
UT DIIDW:2003213713
ER

PT P
PN US2003190076-A1; WO2003085589-A1; AU2003214523-A1; EP1497786-A1; KR2004105846-A; JP2005521975-W; CN1656503-A; US7369685-B2; CN100385448-C; EP1497786-B1; JP4198602-B2; DE60326036-E; SG108356-B
TI Determining method for identifying if acquired image matches reference image, involves comparing relative locations of selected pixel groups with reference images and pixel groups in acquired image to determine preset probabilities.
AB    NOVELTY - Relative locations of selected pixel groups are compared with reference images and pixel groups in an acquired image to determine the probability that the relative locations randomly occur.
   USE - For identifying if acquired image matches reference image. For allowing entry of individual into office or home based on facial recognition.
   ADVANTAGE - Limits instances of false positive matches to arbitrary low level. Prevents entry of unauthorized user into home or office.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) a processor-based image match determining system;
   (b) a processor-based image match determining system providing method; and
   (c) a vision-based system.
   DESCRIPTION OF DRAWING(S) - The figure shows the operation flow diagram of a processor-based image match determining system.
PD US2003190076-A1   09 Oct 2003   G06K-009/00   200377   Pages: 36   English
   WO2003085589-A1   16 Oct 2003   G06K-009/00   200378      English
   AU2003214523-A1   20 Oct 2003   G06K-009/00   200436      English
   EP1497786-A1   19 Jan 2005   G06K-009/00   200506      English
   KR2004105846-A   16 Dec 2004   G06K-009/00   200525      
   JP2005521975-W   21 Jul 2005   G06T-007/00   200549   Pages: 37   Japanese
   CN1656503-A   17 Aug 2005   G06K-009/00   200572      Chinese
   US7369685-B2   06 May 2008   G06K-009/00   200834      English
   CN100385448-C   30 Apr 2008   G06K-009/00   200857      Chinese
   EP1497786-B1   28 Jan 2009   G06K-009/00   200918      English
   JP4198602-B2   17 Dec 2008   G06T-007/00   200918   Pages: 27   Japanese
   DE60326036-E   19 Mar 2009   G06K-009/00   200921      German
   SG108356-B   31 Oct 2006   G06K-009/00   201408      English
UT DIIDW:2003831590
ER

PT P
PN US4712103-A
TI Person recognition door lock control system - has memory coupled to processor storing data of key owners face for comparison with signal from camera.
AB       The door lock control system includes keys each having a memory storing information of the key number and the key owner's face as video data. When the key is inserted into the door lock, the face image is reproduced on a CRT according to the face information stored in the key. A TV camera also takes the face image of the key handling person for displaying same on another CRT so that one can judge, by comparison of the face images on CRT, whether the person is the same as the key owner.
   The system further includes a central storage unit in which a portion of the video data in each key is stored as a collation word. Upon insertion of the key into the door lock, the collation word is read out according to the information of the key number stored in the key. The read out word is compared with the portion of the video data stored in the key to check whether the key is one of those owned by the predetermined key owners.
   USE/ADVANTAGE -   By disabled persons. Prevents use of forged keys.
PD US4712103-A   08 Dec 1987   G06F-007/04   198751   Pages: 10   English
UT DIIDW:1987362538
ER

PT P
PN WO9741527-A1; EP950227-A1; US6122394-A
TI Image building scanner giving time varying electrical signal of fingerprint - uses total internal reflection of beam from scanner surface at fingerprint valleys and its frustration at their edges to produce time varying electrical signal representing fingerprint, scanned surface may be tinted resilient material.
AB       The scanner uses a beam of electromagnetic radiation deflected by a moving mirror plate to produce a two dimensional raster on a scanned surface (28) of a block. This block is transparent to electromagnetic radiation of predetermined wavelengths. A radiation inlet face of the block admits the beam to impinge on the scanned surface to exit the block via a radiation outlet face.
   The beam then impinges on a radiation detector, and the total internal reflection of the beam from the scanner surface at fingerprint valleys and its frustration at their edges produces a time varying electrical signal representing the fingerprint. The scanned surface may be formed by a patch of resilient material, tinted to be transparent only at the predetermined wavelength.
   USE -   Relates to fingerprint scanners used for personal verification and or identification.
   ADVANTAGE -   Provides low cost fingerprint scanner that can be used for biometrically identifying individuals that is extremely compact, inexpensive, rugged, easy to manufacture and maintain and economical to manufacture. Scanner has finer resolution and better optical performance than existing scanners.
PD WO9741527-A1   06 Nov 1997   G06K-009/00   199750   Pages: 50   English
   EP950227-A1   20 Oct 1999   G06K-009/00   199948      English
   US6122394-A   19 Sep 2000   G06K-009/00   200048      English
UT DIIDW:1997549971
ER

PT P
PN US6757008-B1
TI Video surveillance system for monitoring exchange of illicit drugs and money, has line scan camera with a sensor producing analog video signal that is converted into digital signal by A/D converter.
AB    NOVELTY - The system has a camera (41) with a sensor producing an analog video signal that is converted into a digital signal by an A/D converter (93). A compressor compresses the digital signal and a memory stores the compressed signal. Full-field images are selectively recalled from the memory and converted to a lower resolution for display on a monitor. An operator selects a region-of- interest from a display (103) by using a mouse.
   USE - Used for electronic video surveillance for recording information that allows identification of previously unknown persons by their facial features and body marks e.g. tattoos and scars and automobiles by reading their license plates, recognizing their make and model, and recording distinguishing marks e.g. body damage, and monitoring the actions of person`s hands e.g. exchange of illicit drugs and money, the brandishing of weapons, and the manipulation or removal of property.
   ADVANTAGE - The selected image area is then displayed on the monitor in its full acquired resolution, thereby allowing the operator to view high- resolution images of a large area under surveillance. The system acquires video data in a spatial and temporal format that is matched to the specific needs of video surveillance, rather than broadcast television, thus improving the electronic video surveillance. The system stores and manipulates the surveillance image data in a digital form, thus preventing degradation of the recorded information from long-tern storage. The system provides a spatial image resolution capable of recognizing faces, automobile license plates, actions of the hands, and similar items, while simultaneously monitoring large areas. The system provides surveillance image data with an improved signal- to-noise ratio and a high aspect-ratio surveillance image that is matched to the area being monitored. The system provides an operator interface that facilitates the extraction of relevant subsections of the surveillance record. The system facilitates the use of digital image processing to aid in the extraction of information from the surveillance record.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method of inspecting sub regions in a surveillance video recording.
   DESCRIPTION OF DRAWING(S) - DESCRIPTION OF DRAWING - The drawing shows a video surveillance system.
   Linescan camera (41)
   Camera assembly (90)
   Scanner (91)
   A/D converter (93)
   Digital memory (97)
   Display (103)
PD US6757008-B1   29 Jun 2004   H04N-007/18   200451   Pages: 21   English
UT DIIDW:2004532384
ER

PT P
PN WO200191038-A1; AU200163316-A; US6496595-B1; AU2001263316-A8; WO200191038-A9
TI Distributed biometric access control apparatus using a local access unit at respective entrances to a restricted area to download identification data.
AB    NOVELTY - A biometric sensing device (76) senses the same parameters as the sensing device (56) of a server (40) and a sensing device (77) can be a camera for sensing facial parameters, while local access units (60) have a control program stored in a memory device (64). A finger is placed on a device (56) and the appropriate menu is selected on a display (50), while other data are input via an input device (48) or a person is scanned by the remote access unit, registering the face using the camera to identify a person and operate a control device (78) to grant access if the person is identified.
   USE - Identifying authorized persons using biometric parameters.
   ADVANTAGE - Maximum identification speed and accuracy with minimum effects of environmental variables.
   DETAILED DESCRIPTION - AN INDEPENDENT CLAIM is included for a method of controlling access into a restricted area.
   DESCRIPTION OF DRAWING(S) - The drawing is a block diagram of the system
   Sensing devices (56,76,77)
   Local access unit (60)
   Display (50)
   Control device (78)
PD WO200191038-A1   29 Nov 2001   G06K-009/00   200211   Pages: 31   English
   AU200163316-A   03 Dec 2001   G06K-009/00   200221      English
   US6496595-B1   17 Dec 2002   G06K-009/00   200307      English
   AU2001263316-A8   08 Sep 2005      200568      English
   WO200191038-A9   08 Aug 2002   G06K-009/00   201729      English
UT DIIDW:2002083140
ER

PT P
PN US6128398-A
TI Automated face verification system for automated teller machine, verifies read image by comparing photographed image of user using neural network training procedure done by automated face verifier.
AB    NOVELTY - An automated face locator (26) produces a test facial image (28) by clipping the portion of image in user's photograph with respect to that read from access card (12). The automated face verifier (18) compares test image and read image (16) using neural network training procedure that defines predetermined values for digitized images.
   USE - For face verification required in ATM or computer controlled access for buildings.
   ADVANTAGE - Since neural network is used for face image verification, each time the database need not be verified, thereby storage requirements are minimized. Has high tolerance to facial expression and orientation and hence high degree of accuracy in facial image verification is achieved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for method of determining likelihood of different object patterns.
   DESCRIPTION OF DRAWING(S) - The figure shows the automated face verifier.
   Access card (12)
   Image (16)
   Face verifier (18)
   Face locator (26)
   Facial image (28)
PD US6128398-A   03 Oct 2000   G06K-009/00   200061   Pages: 17   English
UT DIIDW:2000637776
ER

PT P
PN US2008037838-A1; US7403643-B2
TI Face recognition method for an image stream acquired by e.g. digital still camera involves applying face recognition using database consisting of identifier and associated parameters for each face to be recognized in candidate face regions.
AB    NOVELTY - Face detection is applied to a portion of a new acquired image from an image stream to provide a set of one or more candidate face regions. Face recognition using a database consisting of an identifier and associated parameters for each of the faces to be recognized is applied to candidate face regions to provide an identifier for a face recognized in a candidate face region. A portion of image that includes the recognized face in association with an image of the image stream is stored.
   USE - Face recognition method for an image stream acquired by an image acquisition device. Uses include but are not limited to a digital still camera, a video camera, a cell phone equipped with an image capturing mechanism or a hand help computer equipped with an internal or external camera.
   ADVANTAGE - The method provides improved real-time face tracking in which only high quality face candidate regions are sent for recognition and the success rate of the face recognizer is greatly improved. Calculations of a complete highest resolution integral image for every acquired image in an image stream are avoided, which reduces integral image calculations. This also reduces processing overhead for face detection and tracking and allows longer classifier chains to be employed during the frame-to-frame processing interval to provide higher quality results and enhanced face tracking. Also, the process of face detection can be spread across multiple frames. Significant improvements in efficiency are provided, while the reduction in computation does not impact very significantly on the initial detection of faces.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for
   (1) image processing apparatus;
   (2) method of tracking faces in an image stream with a digital image acquisition device
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the operation of the image processing apparatus.
PD US2008037838-A1   14 Feb 2008   G06K-009/00   200818   Pages: 16   English
   US7403643-B2   22 Jul 2008   G06K-009/00   200850      English
UT DIIDW:2008C46023
ER

PT P
PN WO2003023695-A1; US2003053664-A1; EP1425708-A1; AU2002333617-A1; JP2005502961-W; CN1585957-A; US7027619-B2; EP1425708-B1; DE60226884-E; JP4187651-B2; CN100449564-C
TI Detecting facial features by using reflected infra-red radiation to identify eyes and eyebrows.
AB    NOVELTY - Method consists in providing image data for reflected light of a scene using IR bandwidths, extracting regions corresponding to eyebrows or eyes, combining them to make a composite image and searching it based on model data. The eyebrows reflectivity is higher than that for skin and that for the eyes is lower through use of two IR bandwidths. The facial search area is defined by isolating skin relative to background, facial orientation is determined by whether one or both eyes are detected and scene luminance level is adjusted dynamically.
   USE - Method is for detecting facial features in e.g. security applications.
   ADVANTAGE - Method provides more reliable face detection.
   DETAILED DESCRIPTION - There is an INDEPENDENT CLAIM for a system for face detection.
   DESCRIPTION OF DRAWING(S) - The figure shows a near-infrared person identification system.
PD WO2003023695-A1   20 Mar 2003   G06K-009/00   200328   Pages: 76   English
   US2003053664-A1   20 Mar 2003   G06K-009/00   200331      English
   EP1425708-A1   09 Jun 2004   G06K-009/00   200438      English
   AU2002333617-A1   24 Mar 2003   G06K-009/00   200460      English
   JP2005502961-W   27 Jan 2005   G06T-007/00   200510   Pages: 112   Japanese
   CN1585957-A   23 Feb 2005   G06K-009/00   200537      Chinese
   US7027619-B2   11 Apr 2006   G06K-009/00   200626      English
   EP1425708-B1   28 May 2008   G06K-009/00   200838      English
   DE60226884-E   10 Jul 2008   G06K-009/00   200848      German
   JP4187651-B2   26 Nov 2008   G06T-007/00   200903   Pages: 32   Japanese
   CN100449564-C   07 Jan 2009   G06K-009/00   200966      Chinese
UT DIIDW:2003290340
ER

PT P
PN EP751473-A1; CA2177639-A; JP9102043-A; KR97002751-A; US5805745-A; CA2177639-C; EP751473-B1; DE69612700-E; MX201272-B; KR405846-B1
TI Locating features in image, for example, facial image used for security, speech recognition - by band pass filtering image and performing morphological operations followed by threshold varying operation.
AB       The method involves band pass filtering the image to produce a band pass filtered image. The filtered image is morphologically processed to produce an enhanced image. The enhanced image is threshold processed to form a binary image having a number of connected components. A classifier is used to identify at least one connected component corresponding to the feature.
   Band pass filtering includes low pass filtering the image to produce a low pass filtered image, high pass filtering to produce a high pass filtered image and subtracting the two to produce the band pass filtered image.
   ADVANTAGE -   Method is tolerant of varying lighting conditions and varying camera positions.
PD EP751473-A1   02 Jan 1997   G06K-009/00   199706   Pages: 12   English
   CA2177639-A   27 Dec 1996   G06T-007/00   199717      English
   JP9102043-A   15 Apr 1997   G06T-007/00   199725   Pages: 8   Japanese
   KR97002751-A   28 Jan 1997   G06T-011/00   199805      
   US5805745-A   08 Sep 1998   G06K-009/36   199843      English
   EP751473-B1   09 May 2001   G06K-009/00   200128      English
   DE69612700-E   13 Jun 2001   G06K-009/00   200141      German
   MX201272-B   10 Apr 2001   G06K-009/36   200223      Spanish
   KR405846-B1   19 Feb 2004   G06T-011/00   200441      
UT DIIDW:1997054885
ER

PT P
PN US2008037827-A1; WO2008017343-A1; WO2008018887-A1; US7469055-B2; EP2052347-A1; EP2052349-A1; CN101512549-A; EP2052349-B1; US2009263022-A1; DE602007002923-E; JP2010500687-W; JP2010500836-W; JP4472781-B2; EP2256666-A2; EP2052347-B1; DE602006021349-E; EP2256666-A3; CN101512549-B; JP5149293-B2; US8055029-B2
TI Image e.g. digital image, stream`s face tracking method for digital image acquisition device, involves applying adjusted face detection to portion of image in regions vicinity as function of movement, to provide updated regions.
AB    NOVELTY - The method involves receiving an acquired image e.g. digital image, from an image stream including a set of face regions. An indication of relative movement of the acquired image relative to a previously acquired image is received, where the previously acquired image includes an associated set of candidate face regions (141). Each candidate face region is provided with a given size and a respective location. Adjusted face detection is applied to a portion of the acquired image in a vicinity of the candidate face regions as a function of the movement, to provide an updated set of face regions.
   USE - Method for tracking a face in an image e.g. digital image, stream for a digital image acquisition device using an image processing apparatus (claimed) e.g. digital still camera (DSC), video camera, and cell phone equipped with an image capturing mechanism or a hand help computer equipped with an internal or external camera, for indicating a location of the faces in an image, to a photographer.
   ADVANTAGE - The adjusted face detection is applied to the portion of the acquired image in the vicinity of the candidate face regions as the function of the movement, to provide the updated set of candidate face regions, thus avoiding calculations of a complete highest resolution integral image for every acquired image, and hence reducing integral image calculations in a face tracking system, and reducing processing overhead for face detection and tracking, and improving the performance and/or accuracy of real-time face detection and tracking.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating principle components of an image processing apparatus.
   Face tracking module (111)
   Variable sized detector (121)
   Candidate face regions (141)
   Face recognition module (160)
   Light-weight database (161)
PD US2008037827-A1   14 Feb 2008   G06K-009/00   200815   Pages: 12   English
   WO2008017343-A1   14 Feb 2008   G06K-009/00   200815      English
   WO2008018887-A1   14 Feb 2008   G06K-009/00   200815      English
   US7469055-B2   23 Dec 2008   G06K-009/00   200903      English
   EP2052347-A1   29 Apr 2009   G06K-009/00   200931      English
   EP2052349-A1   29 Apr 2009   G06K-009/00   200931      English
   CN101512549-A   19 Aug 2009   G06K-009/00   200959      Chinese
   EP2052349-B1   21 Oct 2009   G06K-009/00   200969      English
   US2009263022-A1   22 Oct 2009   G06K-009/46   200969      English
   DE602007002923-E   03 Dec 2009   G06K-009/00   200980      German
   JP2010500687-W   07 Jan 2010   G06T-001/00   201004   Pages: 29   Japanese
   JP2010500836-W   07 Jan 2010   H04N-005/232   201004   Pages: 28   Japanese
   JP4472781-B2   02 Jun 2010   G06T-001/00   201037   Pages: 21   Japanese
   EP2256666-A2   01 Dec 2010   G06K-009/00   201080      English
   EP2052347-B1   13 Apr 2011   G06K-009/00   201127      English
   DE602006021349-E   26 May 2011   G06K-009/00   201135      German
   EP2256666-A3   16 Nov 2011   G06K-009/00   201175      English
   CN101512549-B   04 Jul 2012   G06K-009/00   201273      Chinese
   JP5149293-B2   20 Feb 2013   H04N-005/232   201316   Pages: 24   Japanese
   US8055029-B2   08 Nov 2011   G06K-009/62   201656      English
UT DIIDW:2008C05689
ER

PT P
PN US2003133599-A1; CA2414743-A1; US6879709-B2; IL153925-A; CA2414743-C
TI Neutral/expressionless face image detection system for facial expression analysis system, detects set of geometrical facial features related to nose, eyes, eyebrows and mouth corners.
AB    NOVELTY - The detectors detect a set of geometrical facial features such as eyes, nose, eyebrows and mouth corners respectively. The pose and position of a face (100) detected by a face detector is normalized to a standard size face. A neutral/expressionless face, is detected in accordance with detected features fed to a classifier.
   USE - Neutral/expressionless face image detection system for facial expression analysis system used in security monitoring system in airports, location tracking system, and for department stores, forensic applications.
   ADVANTAGE - Enables analyzing neutral/expressionless faces in digital images, automatically with improved overall system performance.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for automatically detecting neutral/expressionless face images; and
   (2) a computer program product having computer program for automatically detecting neutral/expressionless face images.
   DESCRIPTION OF DRAWING(S) - The figure shows a perspective view of the user in front of computer attached with camera that views the user's face.
   user (100)
   user's face (105)
   camera (110)
   computer monitor (150)
PD US2003133599-A1   17 Jul 2003   G06K-009/00   200378   Pages: 30   English
   CA2414743-A1   17 Jul 2003   G06K-009/00   200378      English
   US6879709-B2   12 Apr 2005   G06K-009/00   200525      English
   IL153925-A   17 Jun 2007   G06K-009/00   200743      English
   CA2414743-C   09 Sep 2008   G06K-009/00   200863      English
UT DIIDW:2003843250
ER

PT P
PN EP851397-A2; CA2221282-A; JP10275203-A; KR98042776-A; US6024287-A; SG72770-A1; JP3075221-B2; CA2221282-C; EP851397-B1; DE69739017-E; EP851397-A3; KR274714-B1
TI Card type recording medium e.g. for credit card, identification card - has data area containing ID information of holder and image area where image data are recorded with second image area recorded which are generated by transforming first image data to frequency area data.
AB       The medium comprises a data area where data containing ID information of a holder are recorded and an image area where image data are recorded. Contents in the image area record second image data which are generated by transforming the image data specifying the holder to frequency-area data, adding the transformed frequency-area data with the ID information and inversely transforming the added data to image signals.
   The conversion processing to the frequency-area data is a Fourier transform. The conversion processing to the frequency-area data is discrete cosine transform which is performed on each blocks into which the first image data are divided.
   ADVANTAGE -   ID information can be verified dually on basis of information of magnetic card.
PD EP851397-A2   01 Jul 1998   G07F-007/08   199830   Pages: 37   English
   CA2221282-A   28 May 1998   G06K-019/073   199838      English
   JP10275203-A   13 Oct 1998   G06K-017/00   199851   Pages: 20   Japanese
   KR98042776-A   17 Aug 1998   G06K-019/00   199938      
   US6024287-A   15 Feb 2000   G06K-019/06   200016      English
   SG72770-A1   23 May 2000   G06K-019/18   200033      English
   JP3075221-B2   14 Aug 2000   G06K-019/10   200043   Pages: 20   Japanese
   CA2221282-C   31 Jul 2001   G06K-019/073   200147      English
   EP851397-B1   01 Oct 2008   G07F-007/08   200866      English
   DE69739017-E   13 Nov 2008   G07F-007/08   200877      German
   EP851397-A3   17 Mar 2004   G07F-007/08   201732      English
   KR274714-B1   15 Dec 2000   G06K-019/00   200174      
UT DIIDW:1998335674
ER

PT P
PN US5012522-A
TI Autonomous face recognition machine - has camera, Micro-Vax computer, A=D converter and hard copy print out to process video scenes using original computer program.
AB       The machine uses images obtained from a video camera and is insensitive to variations in brightness, scale, focus, and operates without any human intervention or inputs. When a motion detection feature is included, the location and recognition events occur in less than 1 minute. The system includes a camera, a Micro-Vax computer, an analog-to-digital converter, and a hard copy print out to process video scenes with random content using an original computer program to locate human faces and identify them.
   The camera converts the video scenece into an analog electrical signal, which is converted into digital and forwarded to the computer. The computer performs an original pattern recognition algorith, to search for facial components, identify a gestalt face. The gestalt-face's detected facial characteristics are compared with a stored set of facial characteristics of known human faces, to identify the face.
   ADVANTAGE -   Capable of locating human faces in video scenes with random content within two minutes, and capable of recognising faces that it locates.
PD US5012522-A   30 Apr 1991      199119      English
UT DIIDW:1991140544
ER

PT P
PN EP1388802-A2; JP2004062560-A; CN1472691-A; US2004081338-A1; JP4036051-B2; EP1388802-B1; DE60320721-E; US7440594-B2; EP1388802-A3
TI Face identification device for e.g. searching for criminals has abstraction unit that makes detected face image unrecognizable by applying abstraction process when no match is determined between detected face image and stored face image.
AB    NOVELTY - A computer has a detection unit that detects face images from human body images taken by surveillance cameras. A determination unit determines a match between a detected face image of a specific person (10A) and a face image stored in a storage device. An abstraction unit applies an abstraction process to the detected face image to make the face image unrecognizable when no match is determined.
   USE - For e.g. searching for criminals on wanted list.
   ADVANTAGE - Facilitates confirmation of information other than face of specific person and situation around specific person, while protecting right of portrait or privacy of people other than specific person.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face identification method.
   DESCRIPTION OF DRAWING(S) - The figure shows an image example to which a mosaic process has been applied.
   Specific person (10A)
   Persons (10B-10F)
   Face (11)
   Marker (13)
PD EP1388802-A2   11 Feb 2004   G06K-009/00   200415   Pages: 16   English
   JP2004062560-A   26 Feb 2004   G06T-001/00   200416   Pages: 12   Japanese
   CN1472691-A   04 Feb 2004   G06K-009/00   200427      Chinese
   US2004081338-A1   29 Apr 2004   G06K-009/00   200429      English
   JP4036051-B2   23 Jan 2008   G06T-001/00   200809   Pages: 12   Japanese
   EP1388802-B1   07 May 2008   G06K-009/00   200833      English
   DE60320721-E   19 Jun 2008   G06K-009/00   200843      German
   US7440594-B2   21 Oct 2008   G06K-009/00   200872      English
   EP1388802-A3   05 Apr 2006   G06K-009/00   201731      English
UT DIIDW:2004146193
ER

PT P
PN US2001043717-A1; US6449384-B2
TI Recognition method for road signs involves applying fuzzy logic filter to two image frames and filtering out and saving frame portions which contain an approved road sign color.
AB    NOVELTY - Fuzzy logic color filter applied to two image frames. Image frame portions containing preselected approved road sign color are filtered out and saved along with location identifiers e.g. camera number, camera location coordinates, frame number camera orientation.
   USE - As a method of recognizing and determining location of road signs (claimed).
   ADVANTAGE - Overcomes problem of requiring complex processing, special scene illumination and continual tuning of filter or systematic comparison of aspects of unknown object with variety of shapes stored in memory. Efficiently and accurately retrieves and catalogues information from vast amount of video data.
   DETAILED DESCRIPTION - Image frames may be converted from native color space to L asterisk u asterisk v asterisk color space. Database created containing road sign type (if known), approximate location, direction it faces and portion of bitmap of at least one frame of digital video signal containing road sign.
   DESCRIPTION OF DRAWING(S) - Drawing is a flow diagram of the method.
PD US2001043717-A1   22 Nov 2001   G06K-009/00   200229   Pages: 21   English
   US6449384-B2   10 Sep 2002   G06K-009/00   200267      English
UT DIIDW:2002238334
ER

PT P
PN EP1510973-A2; JP2005078646-A; US2005063582-A1; KR2005022306-A; CN1607551-A; CN100345165-C; KR682889-B1; RU2358319-C2; US7835568-B2; JP4723834-B2; EP1510973-A3
TI Image-based photo-realistic three-dimensional face modeling method in computer graphics, involves generating 3D head model by fitting 3D genetic model using facial features detected from input frontal and profile images.
AB    NOVELTY - The frontal and profile features are detected from the input frontal and profile images, and a three-dimensional (3D) head model is generated by fitting a 3D genetic model using the detected facial features. A realistic texture is generated from the input frontal and profile images, and mapped onto the 3D head model.
   USE - For creating three-dimensional (3D) photo-realistic head model from input images, in computer graphic field widely applied in fields such as virtual space creation, computer gaming, video conferencing and animation.
   ADVANTAGE - Enables performing image-based photo-realistic three-dimensional face modeling, using the data obtained from an inexpensive device such as digital camera and processed in an automated manner, and a robust face analysis algorithm.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) apparatus for creating three-dimensional (3D) photo-realistic head model; and
   (2) computer readable medium having computer program for creating 3D photo-realistic head model.
   DESCRIPTION OF DRAWING(S) - The figure illustrates a detailed modeling process along with photographs.
PD EP1510973-A2   02 Mar 2005   G06T-017/00   200521   Pages: 68   English
   JP2005078646-A   24 Mar 2005   G06T-017/40   200522   Pages: 62   Japanese
   US2005063582-A1   24 Mar 2005   G06K-009/00   200526      English
   KR2005022306-A   07 Mar 2005   G06T-015/00   200553      
   CN1607551-A   20 Apr 2005   G06T-017/00   200554      Chinese
   CN100345165-C   24 Oct 2007   G06T-017/00   200830      Chinese
   KR682889-B1   15 Feb 2007   G06T-015/00   200850      
   RU2358319-C2   10 Jun 2009      200944      Russian
   US7835568-B2   16 Nov 2010   G06K-009/00   201075      English
   JP4723834-B2   13 Jul 2011   G06T-019/00   201145   Pages: 61   Japanese
   EP1510973-A3   16 Aug 2006   G06T-017/00   201731      English
UT DIIDW:2005198189
ER

PT P
PN US8856541-B1
TI Method for liveness detection for facial recognition, involves determining challenge pattern against which to match authentication input to detect liveness, receiving image of face, and detecting eye movements based on images of face.
AB    NOVELTY - The method (500) involves determining (502) a challenge pattern against which to match an authentication input to detect liveness. A graphical user interface including an element is displayed (504) using a display device coupled to a computing device. The element is moved (506) according to the challenge pattern within the graphical user interface. An image of a face and another image of the face is received (508). The eye movements are detected (510) based on the images of the face. A respective histogram of gradients corresponding to each respective subimage is generated.
   USE - Method for liveness detection for facial recognition.
   ADVANTAGE - The challenge pattern against which to match an authentication input to detect liveness is determined, and the eye movements are detected based on the images of the face, thus enables to enhance the anti-spoofing programs, which offers improved robustness and resistance to erroneous authentication caused by spoofing.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a computer-readable storage device encoded with instructions; and
   (2) a computing device comprises a memory and multiple processors.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method for liveness detection for facial recognition.
   Liveness detection for facial recognition method (500)
   Determining challenge pattern (502)
   Displaying graphical user interface (504)
   Moving element (506)
   Receiving image of face (508)
   Detecting eye movements based on images of face (510)
PD US8856541-B1   07 Oct 2014   G06F-021/00   201470   Pages: 22   English
UT DIIDW:2014T59879
ER

PT P
PN WO200208023-A2; EP1355807-A2; JP2004504219-W; US6724920-B1; EP1355807-B1; DE60114238-E; DE60114238-T2; JP2007033439-A; JP4025643-B2; WO200208023-A3
TI Facial feature recognition system for automobiles using infrared sensors and recognition processor.
AB    NOVELTY - The camera system uses a processor with face recognition software to identify and track the person using a database for comparison. The camera also detects the distance a person is from the airbag in the event of an accident allowing the airbag to deploy at reduced velocity if a person is close to it.
   USE - Identifying human face in vehicle for safety purposes.
   ADVANTAGE - This system prevents theft of vehicles by recognizing the authorized drivers. The system also allows setting such as seat position, mirror alignment etc. to be stored and automatically changed by each driver. In the event of a crash the system determines the distance a person is from the airbag , allowing it to be deployed at a lower velocity when a person is close to the airbag storage area. Thus injury due to airbag deployment is avoided.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for
   (1) an airbag personalization system
   (2) a drowsy driver detection system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a person in a passenger seat.
   Passenger (16)
   Airbag (20)
   Instrument panel (22)
   Infrared illuminator (26)
PD WO200208023-A2   31 Jan 2002   B60R-021/01   200223   Pages: 27   English
   EP1355807-A2   29 Oct 2003   B60R-021/01   200379      English
   JP2004504219-W   12 Feb 2004   B60R-021/32   200413   Pages: 48   Japanese
   US6724920-B1   20 Apr 2004   G06K-009/00   200427      English
   EP1355807-B1   19 Oct 2005   B60R-021/01   200569      English
   DE60114238-E   02 Mar 2006      200618      German
   JP2007033439-A   08 Feb 2007   G01B-011/24   200716   Pages: 17   Japanese
   JP4025643-B2   26 Dec 2007   B60R-021/16   200803   Pages: 15   Japanese
   WO200208023-A3   04 Sep 2003   B60R-021/01   201208      English
UT DIIDW:2002179890
ER

PT P
PN DE3719373-A; GB2192275-A; FR2600191-A; DE3719373-C; US4881268-A; GB2192275-B; KR9006579-B1
TI Banknote tester detecting two different colour components - performs pattern recognition using processing elements determining signal ratio.
AB       A bank note tester identifies the type of a note by identifying the colours in light reflected from or transmitted by an illuminated note (5). The arrangement contains two optical fibre cables one (5) facing a light source and the other (6) colour sensors. The other ends of the optical fibre cables face a test region on a note support surface (1).
   A colour fitter is arranged in the region of the colour sensor. Detectors identify two different colour components in light from a number of points across a note. Processing elements determine the ratios of associated colour component signals. A pattern recognition technique is applied to the signals from a water mark note region.
   ADVANTAGE -   Can distinguish real note from black and white copy without requiring additional identifying equipment.
PD DE3719373-A   23 Dec 1987   G07D-007/00   198801   Pages: 7   German
   GB2192275-A   06 Jan 1988   G07D-007/00   198801      English
   FR2600191-A   18 Dec 1987   G07D-007/00   198807      French
   DE3719373-C   18 Jan 1990      199004      German
   US4881268-A   14 Nov 1989      199004      English
   KR9006579-B1   13 Sep 1990   G07D-007/00   201558      
UT DIIDW:1988000610
ER

PT P
PN US7564994-B1
TI Processor-based system e.g. desktop computer, for classifying and archiving images to identify person, has workflow module confirming whether normalized face region associated with new face print corresponds to known identity.
AB    NOVELTY - The system has a workflow module determining proximities of face classifier parameters of a new face print image with values corresponding to archived face prints. A face recognition module automatically determines that a new face print corresponds to a known identity based on geometric distance proximities within a predetermined proximity threshold. The workflow module confirms whether normalized face region associated with the new face print corresponds to the known identity when each geometric distance proximity is outside threshold greater than another threshold.
   USE - Processor-based system such as desktop computer and personal computer, for classifying and archiving images to identify a person. Can also be used identifying pet, automobile, and rooms in a house.
   ADVANTAGE - The system allows consumers to organize and archive personal image collection based on people present in each image, thus facilitating access to a personal image collection and enhancing the value of the images. The system provides workflow solution to allow users to transparently train to recognize faces. The workflow solution incorporates automated face detection and recognition methods to assist the grouping and classification of images based on the persons or identities.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for classifying and archiving images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating main face detection workflow process.
PD US7564994-B1   21 Jul 2009   G06K-009/00   200949   Pages: 65   English
UT DIIDW:2009L79967
ER

PT P
PN WO9726615-A1; AU9718316-A; US5890141-A; CA2243600-C
TI Cheque alteration detection system for fraud prevention - in which check digit is added to face of cheque, and used by drawee bank to validate presented cheques.
AB       The cheque alteration detection system converts payee information (10) issue date (20) and the magnetic ink character recognition (MICR) line information (account number, cheque number and amount) (30,40,50) to a check digit which is then placed into the MICR line of a check, printed on its face or transmitted via the pad issuance file to the drawee bank.
   The drawee bank, upon presentation of the cheque, uses a transformation algorithm to convert the printed payee information and issue date on the cheque, into a numerical value that is combined with MICR line information. A check digit is then calculated based upon the pre-agreed logic.
   USE/ADVANTAGE -   Determining whether information printed on face of cheque has been modified. Ensures that cheques issue and presented through banking system contain no material alterations to printed information, e.g. payee name, issue date, cheque number and cheque amount.
PD WO9726615-A1   24 Jul 1997      199735   Pages: 22   English
   AU9718316-A   11 Aug 1997   G06F-157/00   199747      English
   US5890141-A   30 Mar 1999   G06F-157/00   199920      English
   CA2243600-C   17 Mar 2015   G07D-007/12   201521      English
UT DIIDW:1997385531
ER

PT P
PN US6563950-B1
TI Face feature analysis method involves comparing jet extracted from particular pixel position of video image using Gabor wavelets, with bunch graph including multiple reference jets.
AB    NOVELTY - A pixel position is selected from the video image of a person's face and a jet (42) is extracted from that pixel position using Gabor wavelets. The extracted jet is compared with bunch graph (44) containing multiple labeled reference jets showing the attribute of each feature.
   USE - For analyzing the features of person using video image of the person.
   ADVANTAGE - Requires less computational power and time and facilitates easy implementation.
   DESCRIPTION OF DRAWING(S) - The figure shows an explanatory view of the face feature analysis method.
   jet (42)
   bunch graph (44)
PD US6563950-B1   13 May 2003   G06K-009/00   200353   Pages: 12   English
UT DIIDW:2003566697
ER

PT P
PN JP10021406-A; US6038337-A
TI Face recognition method for individual identification in somatometry - involves classifying object by extracting object characteristics based on vector whose dimensions are reduced.
AB       The method involves sampling an image (100) of an object to generate a sample vector. The dimensions of the vector is reduced. The object is classified by extracting the object characteristics based on the vector whose dimensions are reduced.
   ADVANTAGE -   Enables quick and correct recognition of face.
PD JP10021406-A   23 Jan 1998   G06T-007/00   199814   Pages: 8   Japanese
   US6038337-A   14 Mar 2000   G06K-009/62   200020      English
UT DIIDW:1998150525
ER

PT P
PN US5375195-A
TI Composited of human face generating e.g. for identification of criminal suspect - generating set of facial composites and identifying best fit facial composite of set.
AB       The method involves during initial step generating a set of facial composites. Subsequent steps entail identifying a fittest facial composite of the set, then combining probabilistically the fittest facial composite and another facial composite from the set to create an intermediate facial composite with placing the intermediate facial composite in the set. Performing that series of steps until the intermediate facial composite meets set criteria for a satisfactory facial composite.
   The initial step of generating a set of facial composites comprises the step of randomly generating the set and the step of limiting a universe from which the set of facial composites is generated.
   USE/ADVANTAGE -   In criminology to identify criminal suspect. Provision for use ny computer unskilled person and allows additional attribute such colour to be added.
PD US5375195-A   20 Dec 1994   G06K-009/00   199505   Pages: 15   English
UT DIIDW:1995035895
ER

PT P
PN US4975960-A
TI Electronic facial tracking appts. for automated speech recognition - includes computer for electronically forming window around nostrils of grey scaled image to isolate other features e.g. eyes or mouth.
AB       The appts. includes circuitry for obtaining a video image of an individual's face, circuitry for electronically locating and tracking a first feature, such as the nostrils, of the facial image for use as reference coordinates and circuitry responsive to the reference coordinates for locating and tracking a second facial feature, such as the mouth, of the facial image with respect to the first feature. By tracking the location of the nostrils, the appts. can follow the movement of the mouth, and thus, automatically recognise speech.
   Pref., the video image is gray scale encoded and the raster lines are smoothed to eliminate noise. The transitions between gray levels of the smoothed image are encoded and the resulting transition code is used to form a contour map of the image from which region parameters are computed which can be compared against stored speech templates to recognise speech. A coustic speech recognition is combined with visual speech recognition.
   ADVANTAGE -   Improves accuracy.
PD US4975960-A   04 Dec 1990      199051      English
UT DIIDW:1990382891
ER

PT P
PN JP7302327-A; US6181805-B1
TI Object image detection method e.g. human face detection - determining object area corresp. to max. collation position through repetitive comparison of test image data in collation position to dictionary image data.
AB       The method involves determining the object area in an input test image. The image of a predetermined standard size area is extracted from the input test image provided on a comparison position (X,Y). Similarity of the collation image to a dictionary image data containing L number of categories at M arbitrary directions are detected.
   Through repeated comparison process, the max. comparison position (Xmax,Ymax) where similarity occurs is obtained. Hence, the object domain in the test image is detected.
   USE/ADVANTAGE -   For recognising employee or customer permitted to enter specific room or zone. Ensures automatic human face detection.
PD JP7302327-A   14 Nov 1995   G06T-001/00   199610   Pages: 17   Japanese
   US6181805-B1   30 Jan 2001   G06K-009/00   200108      English
UT DIIDW:1996090738
ER

PT P
PN JP10003548-A; US5901244-A; US6430307-B1; JP3841482-B2
TI Face image recognition apparatus for authentication of e.g. ID card, license, passport - has second model selector which chooses model face vector having minimum fluctuation from predetermined point of projected object face vector.
AB       The apparatus has a first model selector (7) which chooses a model characteristic vector, among the respective model characteristic vectors held by a first vector holder and a second vector holder, which has the minimum difference with an object image characteristic vector.
   A vector space, which is held by a vector space holder, is set for a model face vector selected by the model selector. The model face vector is limited by a vector controller corresponding to the set vector space. A second model selector (9) is provided to select the model face vector which has the minimum fluctuation from a predetermined point of a projected object face vector.
   ADVANTAGE -   Prevents reduction in face image recognition rate by narrowing down number of candidate models through feature extraction. Attains highly accurate face image recognition result.
PD JP10003548-A   06 Jan 1998   G06T-007/00   199811   Pages: 9   Japanese
   US5901244-A   04 May 1999   G06K-009/46   199925      English
   US6430307-B1   06 Aug 2002   G06K-009/00   200254      English
   JP3841482-B2   01 Nov 2006   G06T-007/00   200672   Pages: 11   Japanese
UT DIIDW:1998116614
ER

PT P
PN WO9216910-A1; US5163094-A; EP533891-A1; JP6500177-W; EP533891-A4; CA2083380-C; EP533891-B1; DE69232024-E
TI Identifying individuals from elemental shapes derived from biosensor data - by analysing shapes presented in N dimensions particularly facial thermograms after normalising digitised thermal image and equalising to standard histogram.
AB       The method involves generating a thermal image of a portion of an individual's body in accordance with the thermal energy patterns. The thermal image is processed to produce a digital representation comprising a matrix of pixels. The intensity of each pixel corresponds with the level of thermal energy of a corresponding portion of the image.
   The digital representation of the image is stored whereby a unique digital representation is created and stored for every individual for subsequent identification. The method further involves normalising the image in order to eliminate variables to produce an image containing elemental shapes comprising contours corresponding with unique structural features of the individual.
   USE/ADVANTAGE -   Ascertaining identity of individual. Improved results.
PD WO9216910-A1   01 Oct 1992      199242   Pages: 30   English
   US5163094-A   10 Nov 1992      199248   Pages: 15   English
   EP533891-A1   31 Mar 1993      199313      English
   JP6500177-W   06 Jan 1994   G01B-011/24   199406      Japanese
   EP533891-A4   07 Jul 1993   G06K-009/00   199527      English
   CA2083380-C   10 Aug 1999   G06K-009/00   199952      English
   EP533891-B1   29 Aug 2001   G06T-011/00   200150      English
   DE69232024-E   04 Oct 2001   G06T-011/00   200166      German
UT DIIDW:1992349425
ER

PT P
PN CN102622588-A; WO2013131407-A1; CN102622588-B
TI Visible light-based double-checking human face anti-fake detecting method, involves determining target human face collection and identification, and obtaining validity of target human face according to human-computer interaction.
AB    NOVELTY - The method involves collecting a target human face for living body detection. Judgment is made to check whether the target human face has biological activity. Target human face collection and identification is determined corresponding to anti-fake human face detecting result. The target human face is considered as a false face sample of a designated person under specific condition. A collection of the false face sample is determined for extracting a texture characteristic of the target human face. Validity of the target human face is obtained according to human-computer interaction.
   USE - Visible light-based double-checking human face anti-fake detecting method.
   ADVANTAGE - The method enables ensuring living body detection and identity authentication so as to ensure accurate and reliable anti-fake human face detecting result.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a double-checking human face anti-fake device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a visible light based double-checking human face anti-fake detecting method. '(Drawing includes non-English language text)'
PD CN102622588-A   01 Aug 2012   G06K-009/00   201274   Pages: 24   Chinese
   WO2013131407-A1   12 Sep 2013   G06K-009/00   201360      Chinese
   CN102622588-B   09 Oct 2013   G06K-009/00   201402      Chinese
UT DIIDW:2012N22574
ER

PT P
PN JP2005044330-A; US2005102246-A1; US7379568-B2; US2008235165-A1; US2008247598-A1; US7587069-B2; US7624076-B2
TI Weak hypothetical data generator for robot for face expression recognition, selects highly-efficient weak postulate or new weak postulate generated by adding preset transformation to highly-efficient weak postulate, for face recognition.
AB    NOVELTY - A selector selects a weak postulate with high face detection capability among several weak postulates, as highly-efficient weak postulate. A generation unit generates a new weak postulate by adding predetermined transformation with respect to selected highly-efficient weak postulate. Another selector selects one of the highly-efficient weak postulate and generated new weak postulate for face recognition.
   USE - For generating weak hypothetical data for robot (claimed) such as humanoid robot and four-legged robot used for face expression recognition in security, law-enforcement, psychiatry, education and telecommunication applications.
   ADVANTAGE - Improves the face expression recognition precision effectively.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) weak hypothetical data generation method;
   (2) face expression learning apparatus;
   (3) face expression learning method;
   (4) face detector;
   (5) face detection method;
   (6) face expression recognition apparatus;
   (7) face expression recognition method; and
   (8) robot.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart explaining the face expression recognition process. (Drawing includes non-English language text).
PD JP2005044330-A   17 Feb 2005   G06T-007/00   200515   Pages: 75   Japanese
   US2005102246-A1   12 May 2005   G06F-019/00   200532      English
   US7379568-B2   27 May 2008   G06K-009/00   200843      English
   US2008235165-A1   25 Sep 2008   G06F-015/18   200866      English
   US2008247598-A1   09 Oct 2008   G06K-009/00   200868      English
   US7587069-B2   08 Sep 2009   G06K-009/00   200959      English
   US7624076-B2   24 Nov 2009   G06N-005/00   200978      English
UT DIIDW:2005137982
ER

PT P
PN EP609996-A2; TW229290-A; EP609996-A3; US6002798-A; SG75792-A1; EP609996-B1; DE69434620-E; DE69434620-T2
TI Creation indexing and viewing of abstracted documents - processing and identifying types of image on document, converting to text, indexing text and storing for retrieval by reference to indexed text.
AB       The method involves capturing document image by a scanner (31) or being input by modem (21). The data is subjected to rule-based block selection techniques to identify regions for e.g. titles, text, lines, bold face and colour regions. The identification is used to create structural information and both the document image and structural information are stored. A word-based retrieval index is created based on title and text-type regions and used in conjunction with a search query to retrieve documents.
   The retrieved documents are displayed in full image mode or rapid browsing mode. In the browsing mode only an abstract structural view of the document image is displayed, based on the stored structural image. The level of abstraction is user selected. The document may be displayed visually on a display (17) or printer (32), or converted by a text-to-speech converter (27) and output audibly.
   USE/ADVANTAGE -   For creation of collection of indexed document images. Designates abstraction level of document display. Display may be visual or audible.
PD EP609996-A2   10 Aug 1994   G06F-017/30   199431   Pages: 21   English
   TW229290-A   01 Sep 1994   G06F-015/40   199439      Chinese
   EP609996-A3   15 Feb 1995   G06F-015/403   199540      English
   US6002798-A   14 Dec 1999   G06K-009/00   200005      English
   SG75792-A1   24 Oct 2000   G06K-009/00   200060      English
   EP609996-B1   08 Feb 2006   G06F-017/30   200612      English
   DE69434620-E   20 Apr 2006   G06F-017/30   200628      German
UT DIIDW:1994250845
ER

PT P
PN US2002089516-A1; DE10164540-A1; GB2373123-A; GB2373123-B; US7034848-B2
TI Automatic graphics image cropping system determines position of specific object image within graphics image, based on analysis and identification for cropping, automatically.
AB    NOVELTY - A detector analyzes a prestored digital data and automatically identifies a portion that defines an image of an object within a graphics image. A cropper automatically crops digital data based on a position of an object image determined according to the identification.
   USE - For processing digital photographic images.
   ADVANTAGE - Enables user to obtain more pleasing photographs using very less efforts, as the image cropping is automatic.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for automatic graphics image cropping method.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram illustrating a picture that displays a relatively large facial image.
PD US2002089516-A1   11 Jul 2002   G09G-005/00   200301   Pages: 14   English
   DE10164540-A1   29 Aug 2002   H04N-001/387   200301      German
   GB2373123-A   11 Sep 2002   H04N-001/38   200301      English
   US7034848-B2   25 Apr 2006   G09G-005/00   200628      English
UT DIIDW:2003015760
ER

PT P
PN CA2218793-A; US5864630-A; CA2218793-C
TI Multi classifier based method for object location within moving or still image - has three channels performing motion, shape and colour analysis; builds intermediate representation during tracking and only continues tracking with channel that requires more information.
AB       The method involves taking images (10) (either a frame or sequence of frames) and tracking designated objects (e.g. heads/facial features) within the images using three channels. One channel (11) performs a shape analysis on grey level images to determine the location and outline of heads/facial features; a second (12) performs a colour analysis using a clustering algorithm to determine areas of skin colour; whilst the third channel (13) performs a motion analysis, by extracting motion information from frame differences.
   After one or more iterations an intermediate representation of the collected tracking output is obtained.A software or hardware system controller then scores the channels and selects one channel for additional tracking, based on these scores. This channel then remains active for the remainder of the tracking duration. The representations from each channel are then combined to give tracked output.
   USE -   Surveillance video cameras; model-based image compression for video telephone; intelligent computer-user interfaces.
   ADVANTAGE -   Provides robust accurate output; faster than existing algorithms based on multiple classifiers; simultaneously maximises tracking speed and output precision.
PD CA2218793-A   20 May 1998   G06T-001/00   199848   Pages: 44   English
   US5864630-A   26 Jan 1999   G06K-009/00   199911      English
   CA2218793-C   15 Jan 2002   G06T-001/00   200215      English
UT DIIDW:1998558085
ER

PT P
PN WO9423390-A1; JP6521907-X; CN1108035-A; US5995639-A; KR158038-B1; US6181806-B1; CN1045129-C
TI Facial image identification appts. for identity card in security system - divides image into patches and defines face as series of data representing image characteristics of each patch.
AB       The facial image of a person is input from a video camera (2). The positions of the characteristic reference points are extracted from the facial image (13) and compared to the corresponding points on a stored standard model (14). The standard model is then revised in accordance with the input image (15).
   The resulting image is divided into a predetermined pattern of patches, and patch values are obtained based on their average brightness. The image, defined by these values, is compared in an identification unit (18) to images stored in a database (17).
   ADVANTAGE -   Provides accurate identification of input facial image, regardless of minor differences in camera angles, etc..
PD WO9423390-A1   13 Oct 1994   G06F-015/62   199441   Pages: 24   Japanese
   JP6521907-X   11 May 1995   G06F-015/62   199527   Pages: 1   Japanese
   CN1108035-A   06 Sep 1995   G06T-007/00   199732      Chinese
   US5995639-A   30 Nov 1999   G06K-009/00   200003      English
   KR158038-B1   15 Dec 1998   G06T-007/00   200036      
   US6181806-B1   30 Jan 2001   G06K-009/00   200108      English
   CN1045129-C   15 Sep 1999   G06T-007/00   200460      Chinese
UT DIIDW:1994333432
ER

PT P
PN WO2008015586-A2; US2008031498-A1; WO2008015586-A3; EP2050043-A2; US7515740-B2; US2009238410-A1; US8050466-B2; US2012230554-A1; US8897503-B2
TI Face recognition method for consumer digital camera, involves comparing representation of current facial image with representations of facial images and determining whether images within collection match current facial image.
AB    NOVELTY - The method involves determining a modified representation framework based on statistical properties of original facial image samples of a collection of facial images and stored representation of another collection of facial images. The two collections of facial images are combined without using the samples. A representation of the combined image collection is stored using the framework. A representation of a current facial image is compared with representations of facial images. A determination is made whether the facial images within the combined collection match the current facial image.
   USE - Face recognition method for working with collections of facial images in a consumer digital camera.
   ADVANTAGE - The two collections of facial images are combined without using the original facial image samples, where the samples are not saved in memory and only principle components of the samples are stored in the memory, thus preventing stored eigenface data from being lost for a large image collection, and hence providing a complete retraining process which is effective and efficient.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a processor readable medium having instructions for programming a processor to perform a face recognition method for working with collections of facial images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block representation illustrating the application of principle component analysis to a raw dataset.
PD WO2008015586-A2   07 Feb 2008      200813   Pages: 28   English
   US2008031498-A1   07 Feb 2008   G06K-009/00   200813      English
   WO2008015586-A3   07 Aug 2008   G06K-009/62   200854      English
   EP2050043-A2   22 Apr 2009   G06K-009/62   200929      English
   US7515740-B2   07 Apr 2009   G06K-009/00   200929      English
   US2009238410-A1   24 Sep 2009   H04N-005/222   200963      English
   US8050466-B2   01 Nov 2011   G06K-009/00   201172      English
   US2012230554-A1   13 Sep 2012   G06K-009/68   201261      English
   US8897503-B2   25 Nov 2014   G06K-009/00   201478      English
UT DIIDW:2008B78522
ER

PT P
PN JP2000207565-A; US6463163-B1
TI Reserve selection procedure of input image for face detection system, involves identifying candidate pattern area of input image based on correlation of input image with standard data representing target pattern.
AB    NOVELTY - The image pattern detection and recognition procedure involves determining the relationship of an input image with a standard data representing target image pattern. The candidate pattern area of the input image is then identified and chosen based on the correlation of input image and standard data.
   USE - For face detection system.
   ADVANTAGE - The accuracy and the speed of image pattern detection are increased by the reserve selection process.
   DESCRIPTION OF DRAWING(S) - The figure shows the internal component of image pattern selection unit of a face detection system.
PD JP2000207565-A   28 Jul 2000   G06T-007/00   200049   Pages: 16   Japanese
   US6463163-B1   08 Oct 2002   G06H-009/00   200269      English
UT DIIDW:2000536895
ER

PT P
PN JP2004192378-A; US2004136574-A1; JP3954484-B2; US7324670-B2; US2008137919-A1; US7502496-B2
TI Face image processor in security application, calculates feature value based on distance between face distinctive points detected from photographed images picked up by multiple cameras.
AB    NOVELTY - A calculating unit (103) calculates the feature value based on the distance between the face distinctive points such as eyes and nose detected from the photographed face images picked up by multiple cameras (101). A person recognition unit (104) compares the calculated feature value and the feature value.
   USE - For face image processing in security application.
   ADVANTAGE - Performs highly precise face recognition using simple calculation.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for face image processing method.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the face image processor. (Drawing includes non-English language text).
   cameras (101)
   face area detector (102)
   feature value calculating unit (103)
   person recognition unit (104)
   output device (106)
PD JP2004192378-A   08 Jul 2004   G06T-007/00   200451   Pages: 14   Japanese
   US2004136574-A1   15 Jul 2004   G06K-009/00   200451      English
   JP3954484-B2   08 Aug 2007   G06T-007/00   200754   Pages: 14   Japanese
   US7324670-B2   29 Jan 2008   G06K-009/00   200810      English
   US2008137919-A1   12 Jun 2008   G06K-009/00   200841      English
   US7502496-B2   10 Mar 2009   G06K-009/00   200918      English
UT DIIDW:2004529334
ER

PT P
PN US2002136448-A1; US6681032-B2
TI Image processing system compares skin color of person in photographed image, with skin color information stored in memory.
AB    NOVELTY - A memory stores information related to skin color of multiple people. The skin color of person in a photographed image, is defined, and compared with the stored skin color information, to process the photographed image.
   USE - For processing image of person in photograph, to authenticate person to provide access to automated teller machine (ATM).
   ADVANTAGE - Enables providing an enhanced image processing system which simplifies the processing of photographed image in real time.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) method of refining object within image;
   (2) facial recognition and identification system; and
   (3) facial recognition system.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the facial recognition system.
   image acquisition unit (22)
   frame grabber (26)
   image manipulation unit (34)
   compression unit (36)
   discrimination unit (38)
PD US2002136448-A1   26 Sep 2002   G06K-009/00   200373   Pages: 31   English
   US6681032-B2   20 Jan 2004   G06K-009/00   200407      English
UT DIIDW:2003776672
ER

PT P
PN EP1139270-A2; JP2001283229-A; US6580821-B1; EP1139270-B1; DE60023447-E; DE60023447-T2; EP1139270-A3
TI Object location and orientation computation e.g. for human face recognition involves computing possible 3D rigid motion solutions of preset points in 3D space and computing error for each solution to select best solution.
AB    NOVELTY - Several feature points are marked on a three-dimensional model and corresponding points on a 2D query image. The possible 3D rigid motion solutions of preset points in a 3D space are computed such that after each of rigid motion, the 3D points are mapped precisely to the corresponding 2D points. An error for each solution is computed using points other than preset point, to select best solution.
   USE - For computing orientation of human face in 3D space for face recognition.
   ADVANTAGE - The computation is extremely fast and simple, as there is no need for iteration and heuristics solutions. Requires no knowledge a camera model and is quite robust.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) Computer program for computing location and orientation of object;
   (b) Program storage device with program for computing location of orientation of object
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart explaining the method of computing the location and orientation of object in 3D space.
PD EP1139270-A2   04 Oct 2001   G06K-009/00   200217   Pages: 24   English
   JP2001283229-A   12 Oct 2001   G06T-007/60   200217   Pages: 15   Japanese
   US6580821-B1   17 Jun 2003   G06K-009/00   200341      English
   EP1139270-B1   26 Oct 2005   G06K-009/00   200571      English
   DE60023447-E   01 Dec 2005   G06K-009/00   200580      German
   DE60023447-T2   10 Aug 2006   G06K-009/00   200654      German
   EP1139270-A3   20 Aug 2003   G06K-009/00   201731      English
UT DIIDW:2002123998
ER

PT P
PN CN104951077-A; EP3109800-A1; US2016379107-A1; JP2017010516-A; KR2017000752-A; JP6625418-B2
TI Artificial intelligence based human-computer interaction method, involves receiving multi-mode input signal, processing multi-mode input signal, and transmitting processing result to user.
AB    NOVELTY - The method involves receiving multi-mode input signal for determining intention of a user, where the multi-mode input signal includes voice signal, image input signal of the user and environment sensor signal. The multi-mode input signal is processed. The processing result is transmitted to the user. A user voice recognition process is carried out. User personal information is obtained based on camera auxiliary face identification function and the voice recognition result.
   USE - Artificial intelligence based human-computer interaction method.
   ADVANTAGE - The method enables realizing better human-computer interaction function.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an artificial intelligence based human-computer interaction device
   (2) a terminal device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an artificial intelligence based human-computer interaction method. '(Drawing includes non-English language text)'
PD CN104951077-A   30 Sep 2015   G06F-003/01   201578   Pages: 28   Chinese
   EP3109800-A1   28 Dec 2016   G06N-003/00   201702      English
   US2016379107-A1   29 Dec 2016   G06N-003/00   201702      English
   JP2017010516-A   12 Jan 2017   G06F-003/01   201706   Pages: 32   Japanese
   KR2017000752-A   03 Jan 2017   G06F-003/01   201706      
   JP6625418-B2   25 Dec 2019   G06F-003/01   202003   Pages: 32   Japanese
UT DIIDW:201569099S
ER

PT P
PN US2013177248-A1; US8867798-B2
TI Method for identifying and connecting people with photograph, involves selecting digital images from database, and returning selected digital images to user through web site.
AB    NOVELTY - The method involves receiving (120) a search request including a digital image specified by a user of a web site. The digital image is analyzed (110) to identify features present in the digital image. The database including the digital images is searched using the identified features. The digital images are selected from the database. The selected digital images are returned to the user through the web site.
   USE - Method for identifying and connecting people with photograph.
   ADVANTAGE - The errant photographs stored on the computer database can be characterized reliably, so that amount of manual review necessary to identify and connect people with the photographs can be reduced efficiently.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for computing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the identifying and connecting process of people with photograph.
   Step for analyzing digital image (110)
   Step for performing facial recognition (112)
   Step for providing data to database (118)
   Step for receiving search request including digital image (120)
   Step for performing database query (122)
PD US2013177248-A1   11 Jul 2013   G06F-017/30   201348   Pages: 6   English
   US8867798-B2   21 Oct 2014   G06K-009/00   201470      English
UT DIIDW:2013L74873
ER

PT P
PN EP922959-A1; DE19755529-A1; AU9896071-A; JP11242024-A; CN1227921-A; SG66918-A1; KR99063003-A; US6176119-B1; US6378702-B1; CN1123773-C; EP922959-B1; DE59813778-G; ES2275294-T3; JP4160674-B2
TI Sample fluid analysis for medical diagnosis has test strip and supply vessel.
AB    NOVELTY - The system to analyze liquid samples has a test strip (7) with an edge profile, and a supply vessel (11) for at least two test strips which is impermeable to moisture and tightly sealed. It has a guide (13), a spacer (14) between the guide and the first test strip to be taken and a movement system for the test strip.
   USE - The system is for the analysis of liquids for medical diagnosis, such as for the presence of glucose in blood, and analysis of the environment.
   ADVANTAGE - The apparatus gives a fault-free analysis of the sample, in a simple and effective apparatus, which can be operated by unskilled personnel.
   DETAILED DESCRIPTION - A manual measurement unit (1), independent of the mains power supply, has a guide groove for the test strip, a guide groove facing the test unit guide at the supply vessel with grooves of different dimensions, and a holder for the test strip.
   An INDEPENDENT CLAIM is included for an analysis operation, where the measurement unit takes a test strip from the vessel. The sample liquid is taken automatically into the detection zone for a characteristic detectable reaction with the components of the test field. The reaction is measured and displayed, and the test strip is removed from the measurement unit.
   DESCRIPTION OF DRAWING(S) - The drawing shows a test strip being taken by the measurement unit.
   Measurement unit housing (1)
   Test strip (7)
   Test strip supply housing (11)
   Guide with a code (13)
   Spacer (14)
TF TECHNOLOGY FOCUS - INSTRUMENTATION AND TESTING - The measurement unit has an evaluation system for the test strip. The guide at the test strip supply vessel contains a code, which is registered automatically by the measurement unit. The test unit has a transfer system to move the sample from the sampling point to the detection point, as a capillary channel. The test apparatus has a recess to hold the measurement unit, which has a ratchet holding mechanism. The test strip is moved at the supply vessel by a spring mechanism. The base of the vessel can be removed. The code at the guide is a barcode or a read only memory (ROM) component, to be scanned by the measurement unit.
PD EP922959-A1   16 Jun 1999   G01N-035/00   199928   Pages: 15   German
   DE19755529-A1   17 Jun 1999   G01N-033/48   199930      German
   AU9896071-A   01 Jul 1999   G01N-021/78   199937      English
   JP11242024-A   07 Sep 1999   G01N-031/22   199947   Pages: 10   Japanese
   CN1227921-A   08 Sep 1999   G01N-035/00   199954      Chinese
   SG66918-A1   21 Dec 1999   G01N-035/00   200006      English
   KR99063003-A   26 Jul 1999   G01N-033/52   200043      
   US6176119-B1   23 Jan 2001   G01N-021/47   200107      English
   US6378702-B1   30 Apr 2002   B65D-085/38   200235      English
   CN1123773-C   08 Oct 2003   G01N-035/00   200553      Chinese
   EP922959-B1   25 Oct 2006   G01N-035/00   200670      German
   DE59813778-G   07 Dec 2006   G01N-035/00   200681      German
   ES2275294-T3   01 Jun 2007   G01N-035/00   200738      Spanish
   JP4160674-B2   01 Oct 2008   G01N-031/22   200866   Pages: 13   Japanese
UT DIIDW:1999329491
ER

PT P
PN WO8911136-A; AU8935684-A; US4916435-A; EP413745-A
TI Remote confinement monitoring system with alcohol breath analyser - allows confinees with behavioral restrictions and placed under home arrest to be regularly checked from central monitoring station.
AB       The remote confinement monitoring system consists of a central office (15) connected via a telephone switching network to a number of remote monitoring locations (11). Each monitoring station is equipped with an alcohol breath analyser (50), operation of which actuates a slow scan television camera (45) which sends a picture of the confinee and the result of the breath test to the central office over a conventional telephone line (43).
   The personnel at the central office can than identify the confinee either manually or automatically by comparing the received image with the archive picture of the confinee in the computer. To avoid an invasion of privacy, the camera is arranged to transmit only a full screen image of the confinee without including may undesirable background.
   USE/ADVANTAGE -   Vertification of the identity of confinees in home arrest by comparing a high quality pictures with archive images.
PD WO8911136-A   16 Nov 1989   G08B-023/00   198948   Pages: 148   English
   AU8935684-A   29 Nov 1989   G08B-023/00   199007      English
   US4916435-A   10 Apr 1990      199020      English
   EP413745-A   27 Feb 1991      199109      English
UT DIIDW:1989356599
ER

PT P
PN US2013279744-A1; WO2013163098-A1; US9633186-B2
TI Method for controlling output of content using electronic device, involves attempting to detect human feature data, determining whether detected human feature data satisfies rule, and performing action to control output of content.
AB    NOVELTY - The method (400) involves attempting (404) with an electronic device to detect human feature data, and determining (406) with the electronic device whether the detected human feature data satisfies the condition of a rule in response to detecting the human feature data. An action is performed (408) to control the output of the content in response to a determination that the detected human feature data satisfies the condition.
   USE - Method for controlling output of content based on human recognition data captured by sensors of an electronic device (Claimed), such as computers. Uses include but are not limited to laptops, tablets, cellular telephones and personal digital assistants.
   ADVANTAGE - The method involves attempting with the electronic device to detect human feature data, and thus provides face localization and efficient recognition. The action is performed to control the output of the content in response to a determination that the detected human feature data satisfies the condition, and thus reduces the amount of processing that is required for the detection of individuals relevant for the particular content, and conserves power of device.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an electronic device, which comprises a detector.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method for controlling output of content.
   Content controlling method (400)
   Starting content controlling method (402)
   Attempting with the electronic device to detect human feature data (404)
   Determining whether the detected human feature data satisfies the condition of a rule (406)
   Performing an action to control the output of the content (408)
PD US2013279744-A1   24 Oct 2013   G06K-009/00   201373   Pages: 24   English
   WO2013163098-A1   31 Oct 2013   G06F-021/32   201373      English
   US9633186-B2   25 Apr 2017   G06K-009/00   201729      English
UT DIIDW:2013S62175
ER

PT P
PN US2004022423-A1; EP1391842-A2; JP2004078912-A; US7110575-B2; EP1391842-B1; DE60330299-E; EP1391842-A3
TI Digital image processing method, involves locating faces in color image by using integral image to perform correlation test between mean grid pattern elements and digital image at effective resolutions.
AB    NOVELTY - The method involves generating a mean grid pattern element (MGPe) image from sample face images. An integral image is generated from a digital color image. The digital image is reduced to a grid pattern element image (GPes) at different effective resolutions to locate faces in a color digital image by using the integral image to perform a correlation test between the MGPe and digital image at the resolutions.
   USE - Used for processing a digital image.
   ADVANTAGE - The method provides locating faces in the color image by using integral image, thereby assigning useful meaning to human understandable objects, attributes or conditions, providing rapid execution and requiring very little memory space.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer program product to perform the method of processing a digital image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an image processing system.
   Image processor (102)
   CRT display (104)
   Keyboard (106)
   Computer readable storage medium (107)
   Output device (109)
PD US2004022423-A1   05 Feb 2004   G06K-009/00   200415   Pages: 24   English
   EP1391842-A2   25 Feb 2004   G06K-009/00   200415      English
   JP2004078912-A   11 Mar 2004   G06T-001/00   200419   Pages: 21   Japanese
   US7110575-B2   19 Sep 2006   G06K-009/00   200662      English
   EP1391842-B1   02 Dec 2009   G06K-009/00   200979      English
   DE60330299-E   14 Jan 2010   G06K-009/00   201005      German
   EP1391842-A3   01 Feb 2006   G06K-009/00   201731      English
UT DIIDW:2004156151
ER

PT P
PN US7587068-B1
TI Processor-readable medium for storing face print image data, has appearance table/list provided with identity entries, and face regions normalized with respect to standard size based on distances between facial features.
AB    NOVELTY - The medium has an appearance table/list (2340) provided with identity entries for an individual known identity. Identity tables (2322-2236) corresponding to the identity entries are provided in the appearance table/list. Face class tables are provided with face print image entries corresponding to face- prints determined from normalized face regions that are identified within an acquired digital image. The face regions are normalized with respect to a standard size based on distances between facial features e.g. eye.
   USE - Processor-readable medium for storing face print image data.
   ADVANTAGE - The medium enables managing and organizing the image collections and ad-hoc collections during a constant state of change of the image in an effective manner, thus improving image quality and accuracy of a face recognition process. The medium allows consumers to organize and archive the personal image collections, thus facilitating an access to the collections and greatly enhancing the value of the images contained in the collections.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a face recognition data component of a database.
   Face recognition data (2300)
   Identity tables (2322-2236)
   Appearance table/list (2340)
PD US7587068-B1   08 Sep 2009   G06K-009/00   200959   Pages: 60   English
UT DIIDW:2009N33243
ER

PT P
PN CN101236599-A; CN100568262-C
TI Face recognition and detection device, has large-scale video monitoring camera utilized for monitoring multi-target human objects in large space scale, and multiple quick ball cameras utilized for snatching face of person.
AB    NOVELTY - The device has a large-scale video monitoring camera utilized for monitoring multi-target human objects in a large space scale. Multiple quick ball cameras are utilized for snatching a face of a person. A micro processor is utilized for an information combination and face recognition and a detection process. A large-scale monitoring camera calibrating module is utilized for establishing a corresponding relation between images of a monitoring space and acquired video images. A video data combining module is placed between the large-scale monitoring camera and the quick ball cameras.
   USE - Face recognition and detection device.
   ADVANTAGE - The device has high recognition precision, quick detection speed, low request to equipment and well practicability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a face recognition and detection device.
PD CN101236599-A   06 Aug 2008   G06K-009/00   200862   Pages: 46   Chinese
   CN100568262-C   09 Dec 2009   G06K-009/00   201009      Chinese
UT DIIDW:2008K27580
ER

PT P
PN WO9508159-A1; US5537488-A; US5703964-A
TI Pattern recognition system for e.g. motor vehicle components such as fender, tyre - has observation class histogram formed by accumulating training class histograms associated with each test data frame from subject.
AB       In the pattern recognition system, multiple training input patterns from multiple classes of subjects are grouped into clusters within categories by computing correlations between the training patterns and present category definitions. After training, each category is labelled in accordance with the peak class of patterns received within the cluster of the category. If the domination of the peak class over the other classes in the category exceeds a preset threshold, then the peak class defines the category. If the contrast does not exceed the threshold, then the category is defined as unknown.
   The class statistics for each category are stored in the form of a training class histogram for the category. During testing, frames of test data are received from a subject and are correlated with the category definitions. Each frame is associated with the training class histogram for the closest correlated category. For multiple-frame processing, the histograms are combined into a single observation class histogram which identifies the subject with its peak class within a predefined degree of confidence.
   USE/ADVANTAGE -   For facial recognition. Able to identify features which may change according to angle of e.g. image acquisition.
PD WO9508159-A1   23 Mar 1995   G06K-009/62   199517   Pages: 70   English
   US5537488-A   16 Jul 1996   G06K-009/00   199634   Pages: 30   English
   US5703964-A   30 Dec 1997   G06K-009/62   199807   Pages: 35   English
UT DIIDW:1995131492
ER

PT P
PN US5784503-A
TI Electronic front or rear cheque imaging arrangement in cheque processor - generates FAULT signal indicating malfunction occurrence when sync-tag bits do not match.
AB       The arrangement has an imaging unit which generates imaging bits representing front or rear faces of a given cheque via a digitizer with a common array of sync-tag bits. It is assumed that the tag bits for imaging bits of a given cheque are the same for both front or rear faces. The set of imaging bits are then transferred on per-document basis to various successive electronic compression and other processing stages.
   The compression is performed and completed in real time using N pairs of parallel compression paths and a FAULT signal indicating a malfunction is generated when the sync-tag bits do not match. A sync-tag creates sync tag bits unique for each imaged cheque face by operating in real time. The sync-tag bits are transferred with the imaging bits to processing stage for handling the front or rear imaging bits on per-document basis and finally, to DBS interface for final matching between faces and removal of the sync-tag bits.
   USE -   For detecting malfunction in automatic high speed document imaging and identification system.
   ADVANTAGE -   Sync tag ensures integrity of image data and ensures tha images are processed in correct order and that image data is correctly registered in database.
PD US5784503-A   21 Jul 1998   G06K-009/00   199836   Pages: 16   English
UT DIIDW:1998427391
ER

PT P
PN EP194783-A; FI8600862-A; US4728186-A; CA1246179-A; FI88752-B; EP194783-B1; DE3688339-G; KR9006061-B1
TI Fingerprint pattern identification apparatus - has transparent plate against which finger is pressed and optical element to guide light reflected by part of plate to detector.
AB       The detector includes transparent plate (1) which has an uneven surface contact portion (1a) against which the finger, of which the print is to be identified, is pressed. The surface is illuminated by a source (2). Light waves reflected from a projection (9) propagate in a different direction from those reflected from a recess (10).
   Light waves (7), scattered by the projection, and which have an angle in excess of the critical angle are reflected at the glass-air interface and pass through the plate to a hologram (31). The hologram diffracts the totally-internally reflected light and passes it to a detector (4). Alternatively, a prism may be used instead of the hologram.
   ADVANTAGE -   Contrast of fingerprint image is enhanced and image is free from distortion.
PD EP194783-A   17 Sep 1986   G02B-006/26   198638   Pages: 36   English
   FI8600862-A   04 Sep 1986   G06K-000/00   198650      Finnish
   US4728186-A   01 Mar 1988   G06K-009/74   198812      English
   CA1246179-A   06 Dec 1988   G01N-021/88   198902      English
   FI88752-B   15 Mar 1993   G02B-006/26   199316      Finnish
   EP194783-B1   28 Apr 1993      199317   Pages: 42   English
   DE3688339-G   03 Jun 1993   G02B-006/26   199323      German
   KR9006061-B1   20 Aug 1990   G06K-009/00   201558      
UT DIIDW:1986246911
ER

PT P
PN US2003081173-A1; WO2003079097-A1; US6682195-B2; AU2002367536-A1; EP1446694-A1; JP2005520205-W; AU2002367536-B2; JP4361806-B2; EP1446694-B1
TI Spectacles manufacturing method involves processing photographed front and side views of patient's head to determine parameters including pupil location, face width and ear location.
AB    NOVELTY - The front and side views of a patient's head (202), are photographed using the cameras positioned around the patient's head. The photographed images are processed to determine the parameters such as pupil location, center of pupils, pupil distance, width of face, ear location and corneal apex distance. A pair of eyeglasses is manufactured using the determined parameters.
   USE - For manufacturing spectacles for correcting e.g. spherical, cylindrical and axial aberrations of patients.
   ADVANTAGE - Enables efficient manufacture of eyeglasses that precisely fit to the patient.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) wavefront measurement device; and
   (2) test frame.
   DESCRIPTION OF DRAWING(S) - The figure shows a side view of the wavefront measurement system.
   Main housing (101)
   Slots (102,104)
   Pupil (201)
   Patient's head (202)
   Patient's ear (204)
   Patient's nose (206)
   Patient's eyes (208)
   Pupil (210)
TF TECHNOLOGY FOCUS - POLYMERS - Preferred Method: A moldable material is applied to the nose, ears and temples of the patient. The moldings are then used to create custom hinges, ear pads and nose pads. The moldings comprise silicone.
PD US2003081173-A1   01 May 2003   A61B-003/10   200361   Pages: 13   English
   WO2003079097-A1   25 Sep 2003   G02C-013/00   200373      English
   US6682195-B2   27 Jan 2004   A61B-003/10   200408      English
   AU2002367536-A1   29 Sep 2003   G02C-013/00   200432      English
   EP1446694-A1   18 Aug 2004   G02C-013/00   200454      English
   JP2005520205-W   07 Jul 2005   G02C-013/00   200545   Pages: 15   Japanese
   AU2002367536-B2   12 Jun 2008   G02C-013/00   200862      English
   JP4361806-B2   11 Nov 2009   G02C-013/00   200975   Pages: 10   Japanese
   EP1446694-B1   08 Aug 2018   G02C-013/00   201853      English
UT DIIDW:2003644846
ER

PT P
PN CN103824054-A; CN103824054-B
TI Cascade depth neural network based human face property identifying method, involves establishing multi-independent of convolution depth neural network, and obtaining human face image data of progressive cascade depth neural network.
AB    NOVELTY - The method involves establishing a multi-independent convolution depth neural network. Human face image data of the progressive cascade depth neural network is obtained. Identify property of the human face image is determined by using the neural network structure to obtain an output identifying result. The convolution depth neural network is provided with multiple layers that are provided with a convolution layer, a sample layer, an unshared convolution layer, a connecting layer and a soft-max layer.
   USE - Cascade depth neural network based human face property identifying method.
   ADVANTAGE - The method enables improving performance of a final network to effectively improve the human facial attribute recognition speed and accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a cascade depth neural network based human face property identifying method.'(Drawing includes non-English language text)'
PD CN103824054-A   28 May 2014   G06K-009/00   201449   Pages: 9   Chinese
   CN103824054-B   07 Aug 2018   G06K-009/00   201856      Chinese
UT DIIDW:2014N87553
ER

PT P
PN US7555148-B1
TI Processor-based system for digital camera, has workflow module requesting user confirmation whether normalized face region associated with face print corresponds to known identity when geometric distance proximity is within range.
AB    NOVELTY - The system has a workflow module comparing face prints with a database of archived face prints previously determined for known identities. Proximities of values of a new face print image are determined with values of the archived face prints for being within a predetermined proximity threshold. The workflow module requests user confirmation whether a normalized face region associated with the new face print corresponds to the known identity when geometric distance proximity is outside the threshold and within another threshold that is greater than the previous threshold.
   USE - Processor-based system for classifying and archiving images in a digital camera.
   ADVANTAGE - The system allows consumers to organize and archive personal image collections based on known people present in each image, thus facilitating access to personal image collection and greatly enhancing the value of the images. The system provides fully manual, user assisted or fully automated workflow solution, thus allowing users to transparently train the program to recognize faces without performing any initial training of the recognition engine. The system does not require re-training of the fundamental basis vectors set used to describe face prints, thus allowing sharing of face recognition data stored in the main system database with other users. The system provides a searching unit for searching image collections using queries, which include the identity of an individual or a group of people and allows face-based enhancement of individual images.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for classifying and archiving images.
   DESCRIPTION OF DRAWING(S) - The drawing shows flow diagram illustrating a core system workflow of a processor-based system.
PD US7555148-B1   30 Jun 2009   G06K-009/00   200944   Pages: 65   English
UT DIIDW:2009K95653
ER

PT P
PN US2008037840-A1; US7460695-B2
TI Face tracking method for image stream of e.g. digital still camera involves applying face detection to integral image and adjusting resolution of produced and previously detected candidate regions to sub-sample subsequent acquired images.
AB    NOVELTY - An acquired image from an image stream that includes one or more face regions is received and sub-sampled at a specified resolution. A corresponding integral image for the sub-sampled image is calculated. Face detection is applied to the integral image calculated for the sub-sampled image to provide a set of one or more candidate face regions (141). Responsive to the set of candidate face regions produced and any previously detected candidate face regions, the resolution is adjusted for sub-sampling a subsequent acquired image.
   USE - Method for tracking faces in an image stream acquired by a digital still camera, a video camera, a cell phone equipped with an image capturing mechanism or a hand help computer equipped with an internal or external camera (all claimed).
   ADVANTAGE - The method provides improved real-time face tracking in which only high quality face candidate regions are sent for recognition and the success rate of the face recognizer is greatly improved. Calculations are avoided of a complete highest resolution integral image for every acquired image in an image stream, thus reducing integral image calculations. This reduces processing overhead for face detection and tracking and allows longer classifier chains to be employed during the frame-to-frame processing interval to provide higher quality results and enhanced face tracking. This significantly improves the performance and accuracy of real-time face detection and tracking. Also, the process of face detection can be spread across multiple frames. Significant improvements in efficiency are provided, while the reduction in computation does not impact very significantly on the initial detection of faces.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for
   (1) image processing apparatus;
   (2) method of detecting faces in an image stream using a digital image acquisition device;
   (3) digital image acquisition device
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating principle components of an image processing apparatus.
   Integral image generator (115)
   Fixed size face detector (120)
   Candidate face regions (141)
   Face recognition (160)
   Fast face detector (280)
PD US2008037840-A1   14 Feb 2008   G06K-009/62   200818   Pages: 16   English
   US7460695-B2   02 Dec 2008   G06K-009/00   200882      English
UT DIIDW:2008C46024
ER

PT P
PN WO2007060980-A1; JP2007148691-A; JP2007150601-A; JP2007150602-A; JP2007150603-A; JP2007150604-A; EP1962497-A1; CN101313565-A; US2009135269-A1; JP2009153219-A; JP4315148-B2; JP4360369-B2; JP4432886-B2; CN101313565-B; CN102231801-A; JP2012200023-A; JP5093178-B2; EP1962497-B1; US8488847-B2; CN102231801-B; JP5423851-B2; EP1962497-A4
TI Electronic camera has control section to adjust imaging parameter of imaging optical system according to position of face region, and generation unit to generate face recognition data based on feature points from face region.
AB    NOVELTY - An image processing section creates face registration image data and through image data from an image signal sent from an image sensor. A detecting section detects the face region on an imaging screen based on the through image data. A control section adjusts imaging parameter of an imaging optical system (11) according to the position of the face region. A generation unit generates a face recognition data based on the feature points from face region. A recording section records the face recognition or face image data.
   USE - Fore electronic camera.
   ADVANTAGE - The person's facial recognition data is generated easily. The registration photography is performed easily at the time of generating facial recognition data. The profile of the facial recognition data, correspondence of the facial recognition data and registration person are recognized easily and rapidly. The facial recognition processing is adjusted appropriately according to the provided condition. The user's convenience is improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for image processing device.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the electronic camera.(Drawing includes non-English language text)
   Imaging optical system (11)
   Zoom lens (11a)
   Focusing lens (11b)
   LCD monitor (24)
   Recording medium (29)
PD WO2007060980-A1   31 May 2007   H04N-005/232   200759   Pages: 83   Japanese
   JP2007148691-A   14 Jun 2007   G06T-001/00   200759   Pages: 15   Japanese
   JP2007150601-A   14 Jun 2007   H04N-005/232   200759   Pages: 22   Japanese
   JP2007150602-A   14 Jun 2007   H04N-005/232   200759   Pages: 13   Japanese
   JP2007150603-A   14 Jun 2007   H04N-005/232   200759   Pages: 14   Japanese
   JP2007150604-A   14 Jun 2007   H04N-005/225   200759   Pages: 16   Japanese
   EP1962497-A1   27 Aug 2008   H04N-005/232   200857      English
   CN101313565-A   26 Nov 2008   H04N-005/232   200920      Chinese
   US2009135269-A1   28 May 2009   H04N-005/228   200935      English
   JP2009153219-A   09 Jul 2009   H04N-005/232   200945   Pages: 20   Japanese
   JP4315148-B2   19 Aug 2009   H04N-005/232   200955   Pages: 21   Japanese
   JP4360369-B2   11 Nov 2009   H04N-005/232   200974   Pages: 13   Japanese
   JP4432886-B2   17 Mar 2010   H04N-005/232   201020   Pages: 13   Japanese
   CN101313565-B   21 Sep 2011   H04N-005/232   201177      Chinese
   CN102231801-A   02 Nov 2011   H04N-005/232   201180      Chinese
   JP2012200023-A   18 Oct 2012   H04N-005/232   201268   Pages: 18   Japanese
   JP5093178-B2   05 Dec 2012   H04N-005/232   201280   Pages: 19   Japanese
   EP1962497-B1   16 Jan 2013   H04N-005/232   201306      English
   US8488847-B2   16 Jul 2013   G06K-009/00   201347      English
   CN102231801-B   10 Jul 2013   H04N-005/232   201371      Chinese
   JP5423851-B2   19 Feb 2014   H04N-005/232   201414   Pages: 19   Japanese
   EP1962497-A4   03 Mar 2010   H04N-005/232   201744      English
UT DIIDW:2007623553
ER

PT P
PN US2001043727-A1; US6606398-B2
TI Facial image cataloging method for digital photograph, associates identification parameter with facial image stored in database, based on which new image is cataloged according to pre-existing match.
AB    NOVELTY - An image associated with at least one facial image in face database (112), is associated with one identification parameter. A new image is automatically cataloged into face database according to pre-existing matching facial images located in face database.
   USE - For cataloging facial images in digital photograph and digital video using graphical user interface (GUI).
   ADVANTAGE - Automatically searches catalog of known faces from matching faces and enter new facial image into face database with high efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) Computer readable medium storing program for facial image catalog;
   (b) Facial image cataloging apparatus
   DESCRIPTION OF DRAWING(S) - The figure shows the explanatory components of the face image catalog apparatus.
   Face database (112)
PD US2001043727-A1   22 Nov 2001   G06K-009/00   200206   Pages: 8   English
   US6606398-B2   12 Aug 2003   G06K-009/00   200355      English
UT DIIDW:2002048827
ER

PT P
PN EP1136937-A2; US2001031072-A1; JP2001266153-A; JP2001338296-A; US6882741-B2; EP1136937-B1; DE60119418-E; DE60119418-T2; EP1136937-A3
TI Facial image recognition apparatus for passenger entry control apparatus, photographs image of human face illuminated by preset number of light sources.
AB    NOVELTY - A camera (101) photographs image of a human face (100) illuminated by two light sources (102,103). An extraction section (106) extracts feature value of human face from photographed image, which is then collated with preset value for recognition.
   USE - For passenger entry control apparatus (claimed) and in security management system.
   ADVANTAGE - Reduces deterioration in the recognition rate due to fluctuation in ceiling illumination, solar light by providing two light sources. Facial image is easily acquired regardless of human height.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for passenger entry control apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of facial image recognition apparatus.
   Human face (100)
   Camera (101)
   Light sources (102,103)
   Extraction section (106)
PD EP1136937-A2   26 Sep 2001   G06K-009/20   200169   Pages: 42   English
   US2001031072-A1   18 Oct 2001   G06K-009/00   200169      English
   JP2001266153-A   28 Sep 2001   G06T-007/00   200172   Pages: 14   Japanese
   JP2001338296-A   07 Dec 2001   G06T-007/00   200202   Pages: 20   Japanese
   US6882741-B2   19 Apr 2005   G06K-009/00   200527      English
   EP1136937-B1   10 May 2006   G06K-009/20   200634      English
   DE60119418-E   14 Jun 2006   G06K-009/20   200642      German
   DE60119418-T2   24 May 2007   G06K-009/20   200735      German
   EP1136937-A3   02 Apr 2003   G06K-009/20   201731      English
UT DIIDW:2001604138
ER

PT P
PN WO9629674-A1; AU9651359-A; EP815531-A1; JP11502344-W; KR98703120-A; US2001026631-A1; CN1184542-A; US6430306-B2
TI System for identifying facial images in databases of ID cards - has image capture unit and eigenvector based analysis to give data to be searched in database for duplications.
AB       The system includes an image analysis that allows image comparisons. A facial image is captured (30) and held in the system. A processor (22) stores a number of eigenvectors defining a multi-dimensional space. The processor projects the image onto the space and encodes it as a weighted function of the eigenvectors. A demographic database (24) stores other data related to the projected image.
   A recognition process is provided that compares the projected image with others in the database. Similar images can be extracted for visual comparison. Acceptable data is then used to produce (14,12,16) identity cards.
   USE/ADVANTAGE -   For producing identity cards, driver's licenses, military ID cards, welfare ID cards, permits. Allows images to be stored in form that enables efficient storage and comparison of images to detect similar faces and prevent issuing multiple cards under different names to single applicant.
PD WO9629674-A1   26 Sep 1996   G06K-009/00   199644   Pages: 34   English
   AU9651359-A   08 Oct 1996   G06K-009/00   199704      English
   EP815531-A1   07 Jan 1998   G06K-009/00   199806      English
   JP11502344-W   23 Feb 1999   G06T-007/00   199918   Pages: 37   Japanese
   KR98703120-A   15 Oct 1998   C10M-115/08   199950      
   US2001026631-A1   04 Oct 2001   G06K-009/00   200161      English
   CN1184542-A   10 Jun 1998   G06K-009/00   200254      Chinese
   US6430306-B2   06 Aug 2002   G06K-009/00   200254      English
UT DIIDW:1996443383
ER

PT P
PN US2003209893-A1; US7415126-B2
TI Occupant sensing system for vehicle e.g. automobile, truck has transducers that detect presence of occupant on front passenger seat, and send output to processor which produces information based on transducer output.
AB    NOVELTY - Transducers (6,8) are used alone or alternately in combination with antenna near field monitoring sensors or transducers (12,14) to detect the presence of an occupant (30) on a front passenger seat (4). A processor (20) generates information based on the output of the transducers.
   USE - For vehicle e.g. automobile, truck. For controlling vehicular system, subsystem or component.
   ADVANTAGE - Obtains information about occupancy of vehicle and conveys information to remotely situated assistance personnel to optimize their response to crash involving vehicle or enable proper assistance to be rendered to occupants after crash. Improves identification of presence, position and orientation of object in vehicle. Detects if rear facing child seat is occupied to prevent occupant protection apparatus e.g. airbag from deploying if apparatus would impact against child seat. Detects if occupant is out of position to prevent occupant protection apparatus from deploying and hitting head or chest of occupant. Utilizes reflection, scattering, absorption or transmission of waves. Determines presence of child in child seat based on motion of child. Determines passenger's velocity relative to passenger compartment and uses information to affect operation of another vehicle system. Activates vehicular warning system to prevent injury to life form.
   DESCRIPTION OF DRAWING(S) - The figure is a side view showing schematically an interface between a vehicle interior monitoring system and a vehicle cellular or other telematics communication system.
   Front passenger seat (4)
   Transduces (6,8)
   Antenna near field monitoring sensors or transducers (12,14)
   Processor (20)
   Occupant (30)
PD US2003209893-A1   13 Nov 2003   B60R-021/32   200382   Pages: 287   English
   US7415126-B2   19 Aug 2008   G06K-009/00   200901      English
UT DIIDW:2003901271
ER

PT P
PN EP1416425-A1; US2004091137-A1; JP2004158013-A; KR2004039788-A; EP1416425-B1; DE60317025-E; US7321670-B2
TI Face detection system in automatic teller machine, has recognition unit which extracts eigen vectors and weight from input facial image using principle component analysis.
AB    NOVELTY - A memory (100) stores eigen vector and weight extracted from training images and is classified into normal and occluded facial images. A recognition unit (200) extracts eigen vectors and weight from an input facial image using principle component analysis (PCA) to classify normal and occluded face using support vector machine. A decision unit (300) decides the occluded facial image using stored image.
   USE - For detecting face of financial criminals in automatic teller machine.
   ADVANTAGE - Since the input facial image is divided into higher and lower regions to restrict the search region of the respective facial components, processing time needed for extracting eigen vector and weight of the respective facial components are reduced.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) face detection method; and
   (2) facial image authentication method.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the face detection system.
   memory (100)
   facial image recognition unit (200)
   monochrome unit (210)
   facial image division unit (240)
   facial image decision unit (300)
PD EP1416425-A1   06 May 2004   G06K-009/00   200433   Pages: 17   English
   US2004091137-A1   13 May 2004   G06K-009/00   200433      English
   JP2004158013-A   03 Jun 2004   G06T-007/00   200436   Pages: 16   Japanese
   KR2004039788-A   12 May 2004   G06K-009/46   200459      
   EP1416425-B1   24 Oct 2007   G06K-009/00   200770      English
   DE60317025-E   06 Dec 2007   G06K-009/00   200781      German
   US7321670-B2   22 Jan 2008   G06K-009/00   200807      English
UT DIIDW:2004349747
ER

PT P
PN US2003223622-A1; CA2424963-A1; EP1372109-A2; JP2004005660-A; CN1475969-A; AU2003204466-A1; TW200402231-A; US7082211-B2; TW268097-B1; AU2003204466-B2; EP1372109-B1; DE60320178-E; DE60320178-T2; JP4294371-B2; CN1475969-B; EP1372109-A3
TI Enhancement method of face appearance in digital image, involves selecting enhancement filters customized for particular segment in face and corresponding default parameters for producing enhanced digital image.
AB    NOVELTY - The location of facial feature points such as skin, eyes, eyebrows, in digital face image is detected. The face is segmented based on detected location, to determine facially relevant characteristics. The enhancement filters customized for a particular region and corresponding default parameters are selected based on the determined characteristics and applied to the region, to produce enhanced digital image.
   USE - For enhancing portrait-type images picked up during occasions such as weddings, graduations, school and sports functions and birthdays.
   ADVANTAGE - Retouching of portraits does not require skilled operator. Highly manual and time consuming retouching processes are also avoided.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) computer storage medium storing face appearance enhancement program;
   (2) user interactive retouching method;
   (3) computer storage medium storing user interactive retouching program;
   (4) face appearance enhancement system;
   (5) user interactive retouching system;
   (6) automatic retouching method;
   (7) computer storage medium storing automatic retouching program;
   (8) skin texture improvement method;
   (9) computer storage medium storing skin texture improvement program;
   (10) skin tone enhancement method;
   (11) computer storage medium storing skin tone enhancement program;
   (12) facial region luminance and chrominance enhancement method;
   (13) computer storage medium storing facial region luminance and chrominance enhancement program;
   (14) graphical user interface method;
   (15) computer storage medium storing graphical user interface program;
   (16) face shape features appearance enhancement method; and
   (17) computer storage medium storing face shape features appearance enhancement program.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart of the face appearance enhancement process.
PD US2003223622-A1   04 Dec 2003   G06K-009/40   200409   Pages: 35   English
   CA2424963-A1   30 Nov 2003   G06T-005/00   200409      English
   EP1372109-A2   17 Dec 2003   G06T-005/00   200409      English
   JP2004005660-A   08 Jan 2004   G06T-011/80   200409   Pages: 29   Japanese
   CN1475969-A   18 Feb 2004   G06T-005/10   200430      Chinese
   AU2003204466-A1   18 Dec 2003   G06T-005/00   200442      English
   TW200402231-A   01 Feb 2004   H04N-001/46   200568      Chinese
   US7082211-B2   25 Jul 2006   G06K-009/00   200649      English
   TW268097-B1   01 Dec 2006   H04N-000/00   200755      Chinese
   AU2003204466-B2   07 Jun 2007   G06T-011/80   200765      English
   EP1372109-B1   09 Apr 2008   G06T-005/00   200827      English
   DE60320178-E   21 May 2008   G06T-005/00   200836      German
   DE60320178-T2   07 May 2009   G06T-005/00   200931      German
   JP4294371-B2   08 Jul 2009   G06T-011/80   200945   Pages: 28   Japanese
   CN1475969-B   13 Apr 2011   G06T-005/10   201167      Chinese
   EP1372109-A3   09 Feb 2005   G06T-005/00   201730      English
UT DIIDW:2004089382
ER

PT P
PN EP1343107-A2; US2003169908-A1; JP2004005456-A; KR2003072191-A; KR2003072192-A; US7254257-B2; JP4271964-B2; EP1343107-A3; KR486714-B1; KR480783-B1
TI Face recognition method for detecting and identifying human face involves performing principal component analysis (PCA) and independent component analysis (ICA) on segmented images and residual images respectively.
AB    NOVELTY - An input facial image is segmented and a principal component analysis (PCA) is performed on the segmented images. Low-pass filtered images are subtracted from the segmented images to obtain residual images. Independent component analysis (ICA) is performed on the residual images to generate feature information.
   USE - For detecting and identifying human face.
   ADVANTAGE - The recognition performance can be increased even when facial image is segmented by components to be processed without compensation for the facial image with pose changes.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMs are also included for the following:
   (1) A method of searching for a face;
   (2) An apparatus for extracting a face;
   (3) An apparatus for searching a face;
   (4) A computer-readable recording medium having program for executing the method; and
   (5) A computer software product to cause a computer system execute the method.
   DESCRIPTION OF DRAWING(S) - The figure shows a flow chart diagram of the method of extracting feature information of a predetermined image using ICA.
   Primary component analysis (PCA)
   Independent component analysis. (ICA)
PD EP1343107-A2   10 Sep 2003   G06K-009/00   200367   Pages: 28   English
   US2003169908-A1   11 Sep 2003   G06K-009/00   200367      English
   JP2004005456-A   08 Jan 2004   G06T-007/00   200405   Pages: 28   Japanese
   KR2003072191-A   13 Sep 2003   G06T-007/00   200405      
   KR2003072192-A   13 Sep 2003   G06K-009/46   200405      
   US7254257-B2   07 Aug 2007   G06K-009/00   200753      English
   JP4271964-B2   03 Jun 2009   G06T-007/00   200936   Pages: 27   Japanese
   EP1343107-A3   23 Mar 2005   G06K-009/00   201738      English
   KR486714-B1   03 May 2005   G06T-007/00   200657      
   KR480783-B1   06 Apr 2005   G06K-009/46   200568      
UT DIIDW:2003699413
ER

PT P
PN WO200028469-A2; AU200017256-A; EP1131775-A2; CN1321279-A; KR2001089417-A; US6381347-B1; US2002110266-A1; JP2002529864-W; TW511038-A; US6917695-B2; EP1131775-B1; CA2351086-C; DE69926908-E; ES2249047-T3; DE69926908-T2; JP2006268878-A; CN100373398-C; JP4294872-B2; WO200028469-A3; KR429302-B1
TI Low distortion optical acquisition system for image capturing and recognition system, includes prism with imaging surface on which light strikes after reflection from scattering surface.
AB    NOVELTY - Light from light source (312) reaches the light receiving surface (320) of prism (310) and passes into the prism. Since the light source is located adjacent to edge (338), incident light strikes scattering surface (322), which is then reflected and strikes the imaging surface (318). The light (330) scattered from the imaging surface is focussed onto the image sensor (316) via a focussing lens (314).
   USE - Low distortion optical acquisition system for image capturing and recognition system.
   ADVANTAGE - Since the refractor is an isosceles triangular prism, trapezoidal distortion in an image of object is reduced.
   DETAILED DESCRIPTION - The object to be photographed is placed facing the imaging surface. A light receiving surface is placed adjacent to the imaging surface. An INDEPENDENT CLAIM is also included for patterned object imaging method.
   DESCRIPTION OF DRAWING(S) - The figure shows schematic diagram of image acquisition system.
   Prism (310)
   Light source (312)
   Focussing lens (314)
   Image sensor (316)
   Imaging surface (318)
   Receiving surface (320)
   Scattering surface (322)
   Light (330)
   Edge (338)
PD WO200028469-A2   18 May 2000   G06K-009/00   200033   Pages: 57   English
   AU200017256-A   29 May 2000   G06K-009/00   200041      English
   EP1131775-A2   12 Sep 2001   G06K-009/00   200155      English
   CN1321279-A   07 Nov 2001   G06K-009/20   200216      Chinese
   KR2001089417-A   06 Oct 2001   G06K-009/00   200220      
   US6381347-B1   30 Apr 2002   G06K-009/00   200235      English
   US2002110266-A1   15 Aug 2002   G06K-009/00   200256      English
   JP2002529864-W   10 Sep 2002   G06T-001/00   200274   Pages: 59   Japanese
   TW511038-A   21 Nov 2002   G06K-009/00   200353      Chinese
   US6917695-B2   12 Jul 2005   G06K-009/00   200546      English
   EP1131775-B1   24 Aug 2005   G06K-009/00   200556      English
   CA2351086-C   16 Aug 2005   G06K-009/20   200557      English
   DE69926908-E   29 Sep 2005   G06K-009/00   200564      German
   ES2249047-T3   16 Mar 2006   G06K-009/00   200622      Spanish
   DE69926908-T2   29 Jun 2006   G06K-009/00   200643      German
   JP2006268878-A   05 Oct 2006   G06T-001/00   200667   Pages: 65   Japanese
   CN100373398-C   05 Mar 2008   G06K-009/20   200840      Chinese
   JP4294872-B2   15 Jul 2009   G06T-001/00   200951   Pages: 24   Japanese
   WO200028469-A3   12 Oct 2000   G06K-009/00   201729      English
   KR429302-B1   29 Apr 2004   G02B-005/04   200457      
UT DIIDW:2000387488
ER

PT P
PN US2015220772-A1; US9773151-B2
TI Communication system for facilitating collection of biometric data for identification purposes, has biometric source attachment which is spaced from lens plate by spacing supports and has face on which human body part is firmly rested.
AB    NOVELTY - The system (100) has spacing supports (120) to which biometric source attachment (130) and lens plate (110) which is positioned adjacent to mobile device are rotatably connected. The biometric source attachment is spaced from lens plate by spacing supports and has a face (130A) on which a human body part is firmly rested. The biometric source attachment is rotatable such that attachment is positioned at a distance from a lens of a camera so that distortion free images of human body part is captured by operation of camera and processed for collection of biometric data.
   USE - Communication system such as telephone, tablets, travel electronic gaming systems and e-readers for facilitating collection of biometric data for identification purposes.
   ADVANTAGE - The system is provided which facilitates the identification of an individual through the use of highly particularized data. The secured system is developed that permits access only to those who provide verification of enrollment in the system or who is identified by the system. While the biometric data used for enrollment purposes is erased from the system, the information is retained for security purposes that is submitted or collected when unauthorized access to the secured system is attempted. The processing of the data that is inputted into the system 11 for enrollment purposes reduces the memory demands on the system and permits the enrollment, verification, and authentication routines to operate more efficiently and quickly.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for facilitating secure enrollment in a secured system through use of biometric data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an apparatus.
   System (100)
   Lens plate (110)
   Spacing supports (120)
   Biometric source attachment (130)
   Face (130A)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - Computer system includes a communication interface such as Ethernet and wired or wireless systems such as WiFi, and Bluetooth.
PD US2015220772-A1   06 Aug 2015   G06K-009/00   201553   Pages: 18   English
   US9773151-B2   26 Sep 2017   G06K-009/00   201765      English
UT DIIDW:2015447455
ER

PT P
PN US2009185723-A1; WO2009094109-A1; EP2238563-A1; JP2011510420-W; US8180112-B2; JP5517952-B2; EP2238563-B1
TI Method for recognition of particular person in images acquired by electronic device, involves generating innovative facial recognition model based on identified changes in person recognition confidence values over certain time.
AB    NOVELTY - The facial images of person acquired by electronic device are analyzed using facial recognition model, to identify changes in facial appearance by comparing the acquired images. The person recognition confidence values and metrics (375) for the facial images are calculated, and are compared with recognition model to identify changes in the values. The innovative facial recognition model for person is generated based on analyzing the identified changes in values. The changes in person recognition confidence values over time are tracked using the innovative model.
   USE - Method for recognition of particular person in images acquired by electronic device e.g. camera, camcorder, cell phone, web cam and video phone.
   ADVANTAGE - Since the image data of particular individual of known identity is acquired to obtain innovative facial recognition model, the robust and persistent identification of individuals in subsequent images can be facilitated without requiring complex facial recognition models.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) storage device including program for persistent recognition of particular person in acquired image; and
   (2) system for generating innovative facial recognition model.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the system for performing facial recognition of individual in acquired digital images.
   Event manager (320)
   Feature detector (336)
   Individual recognition classifier (340)
   Confidence values and metrics (375)
   Personal feature (388)
PD US2009185723-A1   23 Jul 2009   G06K-009/00   200950   Pages: 29   English
   WO2009094109-A1   30 Jul 2009   G06K-009/00   200950      English
   EP2238563-A1   13 Oct 2010   G06K-009/00   201067      English
   JP2011510420-W   31 Mar 2011   G06T-007/00   201124   Pages: 40   Japanese
   US8180112-B2   15 May 2012   G06K-009/00   201233      English
   JP5517952-B2   11 Jun 2014   G06T-007/00   201438   Pages: 37   Japanese
   EP2238563-B1   07 Mar 2018   G06K-009/00   201817      English
UT DIIDW:2009L92592
ER

PT P
PN US8542879-B1; EP2680191-A2; KR2014001163-A; CN103514439-A; KR1415287-B1; EP2680191-A3; EP2680191-B1
TI Method for facial recognition technology, involves detecting liveness gesture of image, where liveness gesture is compared with threshold value to determine deny authentication to user.
AB    NOVELTY - The method (1100) involves receiving (1101,1102) a primary and secondary image of a face of a user. The liveness gesture is detected (1104) based on the yaw angle of the secondary image and the pitch angle of the secondary image, where the yaw angle corresponds to a transition centered on a vertical axis and the pitch angle corresponds to a transition centered on a horizontal axis. A liveness score is generated (1106) based on yaw angle magnitude. The liveness score is compared (1110) to a threshold value, where the deny authentication is determined (1114) to the user.
   USE - Method for facial recognition technology for preventing erroneous authentication caused by spoofing.
   ADVANTAGE - The liveness gesture is detected based on the yaw angle of the secondary image and the pitch angle of the secondary image, and thus if the computing device fails to detect the corneal glint, the computing device deny access by facial recognition authentication and prevent erroneous authentication by spoofing. The liveness score is compared to a threshold value, where the deny authentication is determined to the user, and thus photographs used for spoofing fail to generate a specular highlight and utilize the phenomenon of specular highlights to detect suspected spoofing attempts. The method reduce the occurrence of erroneous authentication caused by spoofing and conserve computing resources and reduces power consumption as the anti-spoofing programs reduce the usage of the facial recognition programs.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a computer-readable storage device for storing encoded instructions; and
   (2) a computing device comprises a memory.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of the method for facial recognition.
   Method for facial recognition (1100)
   Receiving primary and secondary facial image (1101,1102)
   Detecting liveness gesture (1104)
   Generating liveness score (1106)
   Comparing liveness score with threshold (1110)
   Determining deny authentication (1114)
PD US8542879-B1   24 Sep 2013   G06K-009/00   201365   Pages: 39   English
   EP2680191-A2   01 Jan 2014   G06K-009/00   201403      English
   KR2014001163-A   06 Jan 2014   G06K-009/68   201406      
   CN103514439-A   15 Jan 2014   G06K-009/00   201420      Chinese
   KR1415287-B1   04 Jul 2014   G06K-009/68   201447      
   EP2680191-A3   28 Jun 2017   G06K-009/00   201745      English
   EP2680191-B1   20 Nov 2019   G06K-009/00   201991      English
UT DIIDW:2013N58270
ER

PT P
PN US7912246-B1
TI Method for determining age category of people based on facial images, involves determining target outputs of learning machines, so as to map input data, whose images belonging and not belonging to demographics class respectively.
AB    NOVELTY - The method involves annotating facial image database based on demographics classes of the individual face. Learning machines are trained so that auxiliary demographics class information and age information of image are outputted. Image is detected and tracked from image frame, and obtained image features are processed from image, to determine age. The target outputs of machines are determined, so as to map input data, whose facial images belonging and not belonging to class, to specific vector-valued points on manifold and away from manifold, respectively.
   USE - Method for determining age category of people based on facial images.
   ADVANTAGE - Since the input face belonging to the given auxiliary categories of the machine, and the age vector are outputted by the learning machine, the final age vector of the input face is determined by the average of the age outputs from all the machines. The age or the age group of the input face is determined by averaged age vector.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for apparatus for determining the age category of people.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of the hybrid multi-classifier architecture.
   Input image frame (330)
   Face detection (360)
   Data collection (680)
   Specialized classifier N (818)
   Classifier fusion (819)
PD US7912246-B1   22 Mar 2011   G06K-009/00   201122   Pages: 28   English
UT DIIDW:2011C94503
ER

PT P
PN EP1416427-A2; CA2438980-A1; JP2004157602-A; US2004086157-A1; CN1494037-A; KR2004038617-A; TW200407792-A; KR550739-B1; TW254254-B1; US7266224-B2; JP4314016-B2; EP1416427-A3
TI Person recognizing apparatus for security management, judges whether similarity between biometric information of subject person and registered information is within prescribed updating range, to update registered information.
AB    NOVELTY - A recognizer identifies the similarity between biometric information of a person subject to recognition, and registered information, and recognizes a person based on the similarity. An updating unit judges whether the identified similarity is within prescribed updating range, so as to update registered information using the biometric information of subject person.
   USE - For recognizing person using biometric information such as face image, fingerprint image, iris information, hand geometry image, finger images and voice information, for controlling passage of person to/from a security facility.
   ADVANTAGE - Makes it possible to effectively update registered information at less number of updates, without burdening recognized persons with load, by having the system learn feature information of low similarity. Reduces erroneous recognizing rates of registered persons for their secular changes or fluctuated input information at the time of recognition.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) person recognizing method; and
   (2) passage controller.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic block diagram of the person recognizing apparatus.
PD EP1416427-A2   06 May 2004   G06K-009/62   200436   Pages: 24   English
   CA2438980-A1   01 May 2004   A61B-005/117   200436      English
   JP2004157602-A   03 Jun 2004   G06T-007/00   200436   Pages: 22   Japanese
   US2004086157-A1   06 May 2004   G06K-009/00   200436      English
   CN1494037-A   05 May 2004   G06T-007/00   200447      Chinese
   KR2004038617-A   08 May 2004   G06T-007/00   200459      
   TW200407792-A   16 May 2004   G06K-009/00   200628      Chinese
   KR550739-B1   08 Feb 2006   G06T-007/00   200703      
   TW254254-B1   01 May 2006   G06K-009/00   200723      Chinese
   US7266224-B2   04 Sep 2007   G06K-009/00   200758      English
   JP4314016-B2   12 Aug 2009   G06T-007/00   200953   Pages: 19   Japanese
   EP1416427-A3   19 Apr 2006   G06K-009/62   201731      English
UT DIIDW:2004378570
ER

PT P
PN US7319779-B1
TI Method for extracting multi-class age category information for human-computer interaction system, involves processing user face features using tree of binary classification systems.
AB    NOVELTY - A face region of a person is extracted from a digital image. Face features are extracted and processed using a tree of binary classifications system to determine the multi-class age category, in which the face region is the collection of digital image pixels in the face image, the face features represent the face region as direct pixel information or any representations obtained by transformation into other spaces by algebraic manipulation, and the multi-class age categories are any partition of the entire age spectrum into any number of groups.
   USE - For extracting multi-class age category information used in various applications such as human-computer interaction system, passive surveillance system, immersive computer games, business promotion, identity verification, advertising, etc.
   ADVANTAGE - Enables to efficiently and automatically categorize a person from his/her image into a particular age category by processing face features using tree of binary classification systems.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for multi-class age category information extraction system.
   DESCRIPTION OF DRAWING(S) - The drawing shows the tree architecture for multi-class age classification module.
   Intermediate classifier (303)
PD US7319779-B1   15 Jan 2008   G06K-009/00   200808   Pages: 13   English
UT DIIDW:2008B25298
ER

PT P
PN EP992933-A2; JP2000123192-A; US6735566-B1; JP3633399-B2; EP992933-A3
TI Generating realistic facial animation directly from speech, analyses voice with regard to learned categories of facial gesture and trains hidden Markov models to obtain optimal representation for new vocal actions.
AB    NOVELTY - Dynamical models of facial and vocal action are learnt by system from face and facial gestures during speech. It trains hidden Markov models to obtain optimal representation of vocal and facial action. Entropy minimizing training is used so models have dynamic information to synthesize realistic facial motion to new vocal action. System outputs facial control parameters to drive animation.
   USE - To provide more realistic voice driven animation, provides facial control parameters to drive warped photo realistic images, 3D cartoon characters.
   ADVANTAGE - No need of voice track analysis, fast
   DESCRIPTION OF DRAWING(S) - Flow chart showing learning using entropic hidden Markov model training and production of synthetic face trajectory using vocal/facial hidden Markov model.
PD EP992933-A2   12 Apr 2000   G06K-009/00   200023   Pages: 42   English
   JP2000123192-A   28 Apr 2000   G06T-013/00   200032   Pages: 10   Japanese
   US6735566-B1   11 May 2004   G10L-015/14   200431      English
   JP3633399-B2   30 Mar 2005   G06T-013/00   200522   Pages: 16   Japanese
   EP992933-A3   14 Aug 2002   G06K-009/00   201733      English
UT DIIDW:2000259225
ER

PT P
PN US9785796-B1
TI Method for automated privacy protection in distributed images, involves receiving an image from a client device and executing a facial recognition technique against an individual face to obtain a recognized face.
AB    NOVELTY - The method involves receiving an image from a client device and executing a facial recognition technique against an individual face within the image to obtain a recognized face. The privacy protected version of the image is distributed to an ephemeral gallery including a collection of ephemeral images shown in sequence, where the ephemeral gallery is available for an ephemeral period of time.
   USE - Method for automated privacy protection in distributed images.
   ADVANTAGE - The privacy protected version of the image is distributed to an ephemeral gallery including a collection of ephemeral images shown in sequence, thus ensuring improved automated privacy protection in digital image distribution.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a system for automated privacy protection in distributed images.
   Client devices (102-1 to 102-N)
   Server (104)
   Network (106)
   Input-output device (112)
   Bus (114)
PD US9785796-B1   10 Oct 2017   G06F-021/62   201772   Pages: 18   English
UT DIIDW:2017688022
ER

PT P
PN US7742623-B1
TI Method for estimating gaze target within visual target in retail space, involves determining target grid of visual target, and estimating gaze target of person based on estimated gaze direction and head position of person.
AB    NOVELTY - The method involves determining a target grid of a visual target, and detecting and tracking a face of a person from an input image captured by an image capturing unit. A two-dimensional pose and a three-dimensional pose of the face are estimated. Facial features are localized to extract an eye image of the face. A person is detected and tracked by another input image captured by another image capturing unit. A head position is estimated based on top-down view calibration, and a gaze target of the person is estimated based on an estimated gaze direction and the head position of the person.
   USE - Method for estimating a gaze target within a visual target based on automatic image measurements in a retail space.
   ADVANTAGE - The method adopts a series of machine learning-based approaches to accurately and robustly estimate the gaze under realistic imaging conditions without using specialized imaging devices, close-range images and three-dimensional face models such that the robust facial pose estimation and eye gaze estimation are performed in an effective manner. The eye gaze is processed by varying facial pose, so that the changing of appearance of the eyes is properly handled. The method enables top-down view image analysis to locate the head of the viewer to achieve accurate gaze target estimation, comprehensive framework for site calibration, performance characterization and site-specific data collection in an effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an apparatus for estimating a gaze target within a visual target, comprising a gaze direction estimation error distribution estimating unit.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating process for estimating a gaze target within a visual target.
PD US7742623-B1   22 Jun 2010   G06K-009/00   201043   Pages: 32   English
UT DIIDW:2010H04588
ER

PT P
PN US2002159627-A1; US6829384-B2
TI Three-dimensional object detection method e.g. for human face, ear, involves calculating wavelet transform of 2D image, that generates transform coefficients applied to each view-based detector.
AB    NOVELTY - A wavelet transform of digitized version of a 2D image, for generating transform coefficients is calculated for each view-based detector. The detectors detect specific orientation and location of a 3D object in the 2D image based on visual information received from corresponding transform coefficients applied to the detectors.
   USE - For detecting presence of three-dimensional (3D) objects such as human face, ear, airplane, cats, trees, etc., in a two-dimensional (2D) image containing 2D representation of the 3D objects.
   ADVANTAGE - The object detection excels in accuracy and computational properties, by using wavelet transform for object detection. The 3D object is detected over a wide range in angular variation through the combination of several view-based detectors each specialized to a small range within this range of angular variation.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) Three-dimensional object finder program storage medium;
   (2) Three-dimensional object detection system;
   (3) Computer system; and
   (4) Method for providing assistance in three-dimensional object detection.
   DESCRIPTION OF DRAWING(S) - The figure shows a view-based classification approach utilized by the object finder program to detect object locations and orientations.
PD US2002159627-A1   31 Oct 2002   G06K-009/00   200313   Pages: 39   English
   US6829384-B2   07 Dec 2004   G06K-009/00   200480      English
UT DIIDW:2003139270
ER

PT P
PN US2004240711-A1; WO2005008570-A2; EP1629415-A2; EP1629415-B1; DE602004005984-E; JP2007516503-W; DE602004005984-T2; US7421097-B2; WO2005008570-A3
TI Checkpoint screening method for use in airport, involves comparing image features of each of view images of face to be matched, to profile, and determining match based on compared image features.
AB    NOVELTY - The method involves obtaining a front view image of a face to be matched. View images of the face to be matched are obtained from an angle off the front view. Image features are generated from each view image. The image features of each view image are compared to a profile. A match is determined based on the compared image features. The profile is updated if the match is determined.
   USE - Used for verifying that a person presenting an identification badge or other form of ID is actually the person identified at a security checkpoint in situations e.g. providing access to buildings during busy times, airport, and large crowds of people.
   ADVANTAGE - The method speeds the task of positively identifying members and matching them with their photos using face identification technology to check them in through access checkpoints in an automated and reliable manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for checking identity at a security check- point.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block flowchart of a process utilized at a security checkpoint.
PD US2004240711-A1   02 Dec 2004   G06K-009/00   200503   Pages: 14   English
   WO2005008570-A2   27 Jan 2005   G06K-009/00   200510      English
   EP1629415-A2   01 Mar 2006   G06K-009/00   200617      English
   EP1629415-B1   18 Apr 2007   G06K-009/00   200729      English
   DE602004005984-E   31 May 2007   G06K-009/00   200736      German
   JP2007516503-W   21 Jun 2007   G06T-007/00   200742   Pages: 23   Japanese
   US7421097-B2   02 Sep 2008   G06K-009/00   200859      English
   WO2005008570-A3   06 May 2005   G06K-009/00   201217      English
UT DIIDW:2005029862
ER

PT P
PN EP1298597-A2; US2003063794-A1; JP2004038918-A; US7634103-B2; EP1298597-A3
TI Enabling method for a beauty analysis using a three-dimensional facial image constructing image using captured image of subject's face and simulating use of aesthetic feature.
AB    NOVELTY - The method involves constructing the three-dimensional facial image using at least one captured image of a subject's face. Use of an aesthetic feature on the three-dimensional facial image and/or processing the three-dimensional facial image is simulated to enable as beauty analysis. The three-dimensional image is constructed by software.
   USE - For beauty analysis using a facial image.
   ADVANTAGE - Facilitates beauty analysis or simulation of an aesthetic feature.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) a computer-readable medium;
   (b) a system.
   DESCRIPTION OF DRAWING(S) - The figure shows a method for enabling simulated use of an aesthetic feature on a simulated facial image.
PD EP1298597-A2   02 Apr 2003   G06T-017/40   200335   Pages: 42   English
   US2003063794-A1   03 Apr 2003   G06K-009/00   200335      English
   JP2004038918-A   05 Feb 2004   G06T-017/40   200411   Pages: 104   Japanese
   US7634103-B2   15 Dec 2009   G06K-009/00   200982      English
   EP1298597-A3   12 Jan 2005   G06T-017/40   201738      English
UT DIIDW:2003365137
ER

PT P
PN WO2004004320-A1; AU2003280516-A1; US2006187305-A1; US8599266-B2
TI Video system for digital imaging and machine vision, has video camera to capture warped panoramic video images of scene, and digital processor to unwrap images to produce rectilinear images and to extract images in persons view.
AB    NOVELTY - The system has a video camera to capture warped panoramic video images of a scene and produce a video stream. A digital processor has a module to unwrap the warped images to produce rectilinear images of the scene and provide pan, tilt and zoom adjustments to allow for customized viewing of the scene. Another module detects and tracks a persons head in the rectilinear images to extract video images in the persons view.
   USE - Used for digital imaging and machine vision.
   ADVANTAGE - The video system has separate video camera and processor so that the video captured by the camera at one location is processed and viewed through the processor at a different location, thereby allowing for remote sensing.
   DESCRIPTION OF DRAWING(S) - The drawing shows one implementation of video-based face recognition.
PD WO2004004320-A1   08 Jan 2004   H04N-005/225   200408   Pages: 86   English
   AU2003280516-A1   19 Jan 2004   H04N-005/225   200447      English
   US2006187305-A1   24 Aug 2006   H04N-005/225   200656      English
   US8599266-B2   03 Dec 2013   H04N-007/18   201379      English
UT DIIDW:2004083413
ER

PT P
PN EP1085366-A2; CN1288174-A; JP2001215605-A; KR2001029936-A; US6483993-B1; KR2002073115-A; KR2002073116-A; US2003026605-A1; US6636694-B1; US2004105672-A1; EP1085366-B1; DE60013717-E; US6813439-B2; US6829432-B2; DE60013717-T2; JP2009175744-A; JP2009175745-A; JP2009181130-A; JP4762324-B2; EP1085366-A3; KR522514-B1; KR408366-B1; KR408365-B1
TI Face image photographing apparatus to form certificate photograph attached to identification card or various types of licenses; displays at least position in which person sets observing point on photographing device.
AB    NOVELTY - A guidance display device (11-14) displays at least a position in which a person (1) sets an observing point on the photographing device (3), displays a message instructing the person to wait for photographing, an end of photographing, and a direction in which the person should move after the end of photographing.
   USE - For photographing at least a face image of a to-be-photographed person to form a certificate photograph attached to an identification card or various types of licenses.
   ADVANTAGE - Capable of guiding to-be-photographed persons as to the photographing state and dealing with applicants making a queue waiting for photographing as efficiently as possible. Capable of automatically recognizing the size of a face of and automatically effecting the zooming process to keep the size of the face of the output image constant. Automatically excludes a photograph such as a face image with the eyes closed which is not suitable for an output photograph and always outputting an optimum image without calling back a to-be-photographed person again even if a poor-image photographing operation is effected. Always outputs an optimum image by directly indicating the position of the face on the display screen without effecting the complicated operation for trimming the photographed image.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for:
   (a) a face image photographing method
   DESCRIPTION OF DRAWING(S) - The drawing shows the whole construction of the face image photographing system according to the present invention.
   person (1)
   photographing device (3)
   guidance display device (11-14)
PD EP1085366-A2   21 Mar 2001   G03B-017/53   200124   Pages: 68   English
   CN1288174-A   21 Mar 2001   G03B-015/00   200137      Chinese
   JP2001215605-A   10 Aug 2001   G03B-017/53   200154   Pages: 34   Japanese
   KR2001029936-A   16 Apr 2001   H04N-005/222   200163      
   US6483993-B1   19 Nov 2002   G03B-017/00   200280      English
   KR2002073115-A   19 Sep 2002   H04N-005/222   200311      
   KR2002073116-A   19 Sep 2002   H04N-005/222   200311      
   US2003026605-A1   06 Feb 2003   G03B-015/00   200313      English
   US6636694-B1   21 Oct 2003   G03B-015/00   200370      English
   US2004105672-A1   03 Jun 2004   G03B-015/00   200436      English
   EP1085366-B1   15 Sep 2004   G03B-017/53   200460      English
   DE60013717-E   21 Oct 2004   G03B-017/53   200469      German
   US6813439-B2   02 Nov 2004   G03B-015/00   200472      English
   US6829432-B2   07 Dec 2004   G03B-015/00   200480      English
   DE60013717-T2   29 Sep 2005   G03B-017/53   200568      German
   JP2009175744-A   06 Aug 2009   G03B-017/53   200952   Pages: 39   Japanese
   JP2009175745-A   06 Aug 2009   G03B-017/53   200952   Pages: 38   Japanese
   JP2009181130-A   13 Aug 2009   G03B-017/53   200953   Pages: 38   Japanese
   JP4762324-B2   31 Aug 2011   G03B-017/53   201158   Pages: 40   Japanese
   EP1085366-A3   16 Jan 2002   G03B-017/53   201740      English
   KR522514-B1   18 Oct 2005   H04N-005/222   200680      
   KR408366-B1   03 Dec 2003   H04N-005/222   200424      
   KR408365-B1   03 Dec 2003   H04N-005/222   200424      
UT DIIDW:2001228310
ER

PT P
PN WO2005008593-A1; JP2005056387-A; JP2005056388-A; EP1650711-A1; US2006115157-A1; CN1839410-A; JP4612806-B2; JP2011018362-A; JP4743823-B2; US8515136-B2; JP2013178816-A; US2013301885-A1; JP5517858-B2; JP5629803-B2; US8942436-B2; EP1650711-B1; CN1839410-B; EP2955662-A1; EP1650711-A4; EP2955662-B1; EP3358501-A1; EP3358501-B1
TI Image processor used in image recognition application, discriminates facial expression in input image, based on deviation of detected local characteristics with respect to each of local characteristics of reference image.
AB    NOVELTY - The processor detects the local characteristics from the input image, based on which an area of the face in the input image is specified. A facial expression is discriminated based on deviation of detected local characteristics with respect to each of the local characteristics of a reference image.
   USE - For processing images in image recognition application.
   ADVANTAGE - The facial expression in the input image is judged correctly.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) image processing method;
   (2) image processing program;
   (3) computer readable medium storing image processing program; and
   (4) image pickup device.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart of image processing method. (Drawing includes non-English language text).
PD WO2005008593-A1   27 Jan 2005   G06T-007/20   200513   Pages: 140   Japanese
   JP2005056387-A   03 Mar 2005   G06T-007/00   200517   Pages: 36   Japanese
   JP2005056388-A   03 Mar 2005   G06T-007/00   200517   Pages: 48   Japanese
   EP1650711-A1   26 Apr 2006   G06T-007/20   200628      English
   US2006115157-A1   01 Jun 2006   G06K-009/46   200637      English
   CN1839410-A   27 Sep 2006   G06T-007/20   200706      Chinese
   JP4612806-B2   12 Jan 2011   G06T-007/20   201105   Pages: 30   Japanese
   JP2011018362-A   27 Jan 2011   G06T-007/20   201108   Pages: 34   Japanese
   JP4743823-B2   10 Aug 2011   G06T-007/00   201152   Pages: 35   Japanese
   US8515136-B2   20 Aug 2013   G06K-009/00   201355      English
   JP2013178816-A   09 Sep 2013   G06T-007/20   201359   Pages: 33   Japanese
   US2013301885-A1   14 Nov 2013   G06K-009/00   201376      English
   JP5517858-B2   11 Jun 2014   G06T-007/20   201438   Pages: 33   Japanese
   JP5629803-B2   26 Nov 2014   G06T-007/20   201477   Pages: 33   Japanese
   US8942436-B2   27 Jan 2015   G06K-009/00   201517      English
   EP1650711-B1   04 Mar 2015   G06K-009/00   201518      English
   CN1839410-B   20 May 2015   G06T-007/20   201550      Chinese
   EP2955662-A1   16 Dec 2015   G06K-009/00   201582      English
   EP1650711-A4   14 May 2008   G06K-009/00   201746      English
   EP2955662-B1   04 Apr 2018   G06K-009/00   201823      English
   EP3358501-A1   08 Aug 2018   G06K-009/00   201853      English
   EP3358501-B1   01 Jan 2020   G06K-009/00   202002      English
UT DIIDW:2005122965
ER

PT P
PN US2005013467-A1; US7283647-B2
TI Analysis tool for sports training, has analysis module calculating static and dynamic characteristics of putter head motion, and using input from calibration module to correct physical conditions e.g. camera lens distortion.
AB    NOVELTY - The tool has a video capture module collecting video image-data from a video image recording device (101). A detection module processes each of frames (102) of data to detect a light reflective unit position. An analysis module calculates static and dynamic characteristics of a putter head motion. The analysis module uses input from a calibration module to correct physical conditions e.g. camera lens distortion.
   USE - Used for capturing and analyzing physical motion e.g. shaft length, head weight, head face angle, and head offset, of a golf club e.g. pitching wedge or sand wedge, during a putting stroke in a sports training.
   ADVANTAGE - The analysis module uses input from the calibration module to correct various physical conditions e.g. camera lens distortion, thus improving the performance of the putting stroke.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method of using a computer processor based analysis tool for recording and analyzing physical motion of a golf putter during a putting stroke.
   DESCRIPTION OF DRAWING(S) - The drawing shows a functional diagram and major physical components of an analysis tool.
   Video image recording device (101)
   Video image-data frames (102)
   Student (103)
   Computer processing environment (104)
   Light reflective unit (105)
PD US2005013467-A1   20 Jan 2005   G06K-009/00   200514   Pages: 18   English
   US7283647-B2   16 Oct 2007   A63B-069/36   200768      English
UT DIIDW:2005130741
ER

PT P
PN US2003198368-A1; EP1357520-A2; JP2003317101-A; KR2003083510-A; US7187786-B2; EP1357520-B1; DE60312427-E; DE60312427-T2; JP4156430-B2; EP1357520-A3; KR438841-B1
TI Face verification method e.g. for credit card identification, involves shifting position of eyes from input image and extracting recognition feature values and determining whether input image is similar to stored image.
AB    NOVELTY - The background of input face image is separated to detect the face region. The position of eyes are shifted in a preset direction by a preset distance to generate new co-ordinate points of eyes. The face region is normalized based on new co- ordinate points, and the recognition feature values are extracted, calculated and stored, based on which whether the input face is similar to that stored in database is determined.
   USE - For verifying the identity using biometric information e.g. fingerprints, iris, face and shape of vein for use in verification of credit cards, cash cards, and electronically resident cards. Also used in terminal access control, control system on a public place, electronic album and for identifying criminals.
   ADVANTAGE - Optimum feature classifier is designed, thereby prevents degradation of recognition performance. Prevents inconvenience to user and prevents updating a database during image input, so as to increase recognition perform.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) recorded medium storing face verification method; and
   (2) face verification system.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the face verification system.
   preprocessor (310)
   eye position shifting unit (330)
   face region normalizer (340)
   recognition feature extractor (350)
   database (380)
PD US2003198368-A1   23 Oct 2003   G06K-009/00   200377   Pages: 15   English
   EP1357520-A2   29 Oct 2003   G06K-009/00   200379      English
   JP2003317101-A   07 Nov 2003   G06T-007/00   200381   Pages: 11   Japanese
   KR2003083510-A   30 Oct 2003   G06K-009/46   200415      
   US7187786-B2   06 Mar 2007   G06K-009/00   200718      English
   EP1357520-B1   14 Mar 2007   G06K-009/00   200722      English
   DE60312427-E   26 Apr 2007   G06K-009/00   200730      German
   JP4156430-B2   24 Sep 2008   G06T-007/00   200864   Pages: 16   Japanese
   EP1357520-A3   03 Aug 2005   G06K-009/00   201731      English
   KR438841-B1   05 Jul 2004   G06K-009/46   200471      
UT DIIDW:2003832195
ER

PT P
PN JP9062865-A; US5850463-A; JP3799633-B2
TI Face image processing method e.g. for image synthesis - involves interpolating shape and colour of geometric models of two faces in background image model.
AB       The method involves using geometric models of the two cases (G1,G2) and the background image (G3). The ratio in which the coordinates of the different points of the two faces are synthesized is determined beforehand. The vertical and horizontal vectors for the corresponding points of the background image are calculated from the data for the geometric models of the faces as per this ratio and model of the background image is altered accordingly.
   The ratio in which the colour of different parts of the two faces are applied to synthetic image is predetermined. Magnitude of the brightness is evaluated as per this ratio and colour interpolation is carried out in the corresponding pixels. Age variation is included for based on ratio of skin areas.
   USE/ADVANTAGE -   For e.g. image synthesis of male face and female face. Produces realistic and natural images. Includes heredity and racial features.
PD JP9062865-A   07 Mar 1997   G06T-017/00   199720   Pages: 21   Japanese
   US5850463-A   15 Dec 1998   G06K-009/00   199906      English
   JP3799633-B2   19 Jul 2006   G06T-017/40   200648   Pages: 24   Japanese
UT DIIDW:1997217602
ER

PT P
PN EP611160-A2; JP7004934-A; EP611160-A3; US5544254-A
TI Classification appts for sorting crystalline objects - obtains image of object and compares it to various templates to determine its form and allow it to be sorted accordingly.
AB       The appts for sorting crystalline objects includes an imaging system and a computer. The crystalline objects are transported (60) to the appts. Each object will lie with one surface on the transport system. A colour or black and white camera (20) is positioned perpendicular to the transport system and hence perpendicular to one of the crystal surfaces.
   The object is backlit (40) with diffuse light (30) to produce an image with an inner and outer outline. The inner outline is the face of the object facing the camera. By comparing the image against a number of templates it can be identified and sorted.
   ADVANTAGE -   Provides a quick and precise classification of crystalline objects suited to a production line.
PD EP611160-A2   17 Aug 1994   B07C-005/10   199432   Pages: 18   English
   JP7004934-A   10 Jan 1995   G01B-011/24   199511   Pages: 11   Japanese
   EP611160-A3   31 May 1995   B07C-005/10   199610      English
   US5544254-A   06 Aug 1996   G06K-009/00   199637   Pages: 18   English
UT DIIDW:1994256926
ER

PT P
PN US7440590-B1; US2008279446-A1
TI Depth information retrieving method for e.g. video teleconferencing, involves capturing image reflected from surface, and recovering pattern information from reflected image for modulated structured light patterns.
AB    NOVELTY - The method involves modulating a set of structured light patterns using a respective carrier frequency along an orthogonal dimension. A composite image comprising a set of modulated structured light patterns is projected at an object e.g. inanimate object. An image reflected from the surface is captured. Pattern information is recovered from the reflected image for each of the modulated structured light patterns, where the information comprises the depth information. A demodulation of the reflected image is performed.
   USE - Method for retrieving depth information about a surface of an object such as animal feature, face and inanimate object, in various applications. Uses include but are not limited to three dimensional video teleconferencing, telecolloboration, commercial, industrial and scientific applications, for security or identification purpose.
   ADVANTAGE - The method provides ease of operability and design simplicity. The method reduces the overall system cost and manufacturing cost. The method reduces the time required to acquire and generate depth information and associated maps of a surface. The method increases the speed of the system and permits reliable display information to be communicated to a viewer. The method allows measuring of surface topologies with a single projected composite image that addresses issues of ambiguities, higher accuracy, and less sensitivity to albedo variations.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for retrieving depth information
   (2) a computer readable storage medium with a set of instructions for performing the method for retrieving depth information.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a method for retrieving depth information about a surface of an object.
PD US7440590-B1   21 Oct 2008   G06K-009/00   201032   Pages: 20   English
   US2008279446-A1   13 Nov 2008   G06K-009/00   201032      English
UT DIIDW:2009B54342
ER

PT P
PN WO200033240-A1; AU200012879-A; EP1138011-A1; JP2002531901-W; AU762625-B; EP1138011-B1; DE69914370-E; ES2212857-T3; US6876755-B1
TI Facial sub-spaces variation determining method involves obtaining projection on each sub-space and recalculating spaces.
AB    NOVELTY - The initial estimates of sub-spaces such as lighting, pose, identity and expression are performed using principle component analysis on appropriate groups of faces. An iterated algorithm is applied to image coding to maximize the probability of coding across the non-orthogonal sub-spaces. the projection on each sub-space is obtained and spaces are recalculated.
   USE - For determining the sub-spaces of facial variation.
   ADVANTAGE - The faces are divided into a set of non-orthogonal projections, to allow and iterative approach to a set of pure, but overlapping spaces, hence improves identity recognition.
   DESCRIPTION OF DRAWING(S) - The figure shows the two-dimensional view of face space.
PD WO200033240-A1   08 Jun 2000   G06K-009/00   200048   Pages: 22   English
   AU200012879-A   19 Jun 2000   G06K-009/00   200049      English
   EP1138011-A1   04 Oct 2001   G06K-009/00   200158      English
   JP2002531901-W   24 Sep 2002   G06T-007/00   200278   Pages: 23   Japanese
   AU762625-B   03 Jul 2003   G06K-009/00   200354      English
   DE69914370-E   26 Feb 2004   G06K-009/00   200419      German
   ES2212857-T3   01 Aug 2004   G06K-009/00   200451      Spanish
   US6876755-B1   05 Apr 2005   G06K-009/00   200523      English
UT DIIDW:2000532451
ER

PT P
PN DE29821644-U1; EP1006479-A2; JP2000215296-A; US6592031-B1; EP1006479-B1; DE59910983-G; EP1006479-A3
TI Authentication system for PC cards - has built in smart card reader and fingerprint detector for identification provided at entry point for slide module.
AB       NOVELTY - The system includes a card shaped housing (2) for accommodating electronic components, e.g. chip card reading unit. The housing has a face ledge connector (3) that engages contacts in a computer. The housing has an entry point for a slide module (4) that has a sensor (5) for fingerprint detection.
   An entry point is provided for insertion of a smart card into a reader and the PC card has processing capability to handle the read data.
   USE -   For computers, modems.
   ADVANTAGE -   Provides reliable identification of user. DESCRIPTION OF DRAWING(S) - (1) PC card; (2) Housing; (3) connector; (4) Slide in module; (5) Fingerprint sensor.
PD DE29821644-U1   18 Feb 1999   G06F-012/14   199914   Pages: 20   German
   EP1006479-A2   07 Jun 2000   G06K-019/077   200032      German
   JP2000215296-A   04 Aug 2000   G06K-019/10   200042   Pages: 8   Japanese
   US6592031-B1   15 Jul 2003   G06K-005/00   200348      English
   EP1006479-B1   03 Nov 2004   G06K-019/077   200475      German
   DE59910983-G   09 Dec 2004   G06K-019/077   200481      German
   EP1006479-A3   09 Oct 2002   G06K-019/077   201777      German
UT DIIDW:1999155501
ER

PT P
PN US2006140455-A1; IE84153-B3; US7715597-B2
TI Image recognition method for use in desktop computer, involves retrieving sub-set of images from image collection, based on feature vectors determined for each face region and reference region.
AB    NOVELTY - The feature vectors are determined for each face region identified from each digital image in a collection. The feature vectors are determined for reference region containing the face to be recognized and selected from an image. The sub-set of images are retrieved from the collection, based on the determined feature vectors.
   USE - For image recognition in desktop computer connected with digital camera.
   ADVANTAGE - The automatic recognition of face regions within newly acquired images or sets of images, is enabled.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) image recognition apparatus; and image browser.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of image processing system.
PD US2006140455-A1   29 Jun 2006   G06K-009/00   200652   Pages: 33   English
   IE84153-B3   22 Feb 2006   G06T-007/60   200652      English
   US7715597-B2   11 May 2010   G06K-009/00   201031      English
UT DIIDW:2006510762
ER

PT P
PN WO2005124659-A1; EP1766547-A1; JP2008502989-W; US2008069413-A1; EP1766547-B1; DE602004017911-E; JP4604087-B2; US7864992-B2
TI Fingerprint sensor element for recognition of person for computer access, has capacitance formed between sensor electrode with upper side facing finger, and lower electrode that is connected to output terminal of charge amplifier.
AB    NOVELTY - A sensor electrode (11) formed in upper conducting layer (M3), has an upper side (11a) facing a finger (5) and a lower side (11b) facing a lower electrode (17). The lower electrode formed in lower conducting layer, is connected to an output terminal of a charge amplifier (13). An insulating layer (8) is formed between upper and lower conducting layers, and a capacitance (Cref) is formed between sensor and lower electrodes.
   USE - In fingerprint sensor (claimed), for recognition of person using fingerprint, for access control for buildings, smart card, weapon enable/disable arrangements and computer.
   ADVANTAGE - Reduces lateral parasitic capacitance effects and signal to noise ratio, while acquiring large gain, thereby increasing resolution. Enables effective utilization of layers of fingerprint structure.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for fingerprint sensor.
   DESCRIPTION OF DRAWING(S) - The figure shows a cross section of the finger print sensor element.
   finger (5)
   insulating layer (8)
   ensor electrode (11)
   upper side of sensor electrode (11a)
   lower side of sensor electrode (11b)
   charge amplifier (13)
   lower electrode (17)
   capacitance (Cref)
   lower conducting layer (M2)
   upper conducting layer (M3)
PD WO2005124659-A1   29 Dec 2005   G06K-009/00   200606   Pages: 35   English
   EP1766547-A1   28 Mar 2007   G06K-009/00   200725      English
   JP2008502989-W   31 Jan 2008   G06T-001/00   200810   Pages: 18   Japanese
   US2008069413-A1   20 Mar 2008   G06K-009/00   200822      English
   EP1766547-B1   19 Nov 2008   G06K-009/00   200878      English
   DE602004017911-E   02 Jan 2009   G06K-009/00   200914      German
   JP4604087-B2   22 Dec 2010   G06T-001/00   201101   Pages: 16   Japanese
   US7864992-B2   04 Jan 2011   G06K-009/00   201104      English
UT DIIDW:2006057448
ER

PT P
PN US2014079297-A1; US8873813-B2
TI Method for recognizing faces of person or human from still image or video frame in movie for movie and music recording industries, involves outputting identification number of first face as identification for detected faces from video frame.
AB    NOVELTY - The method involves detecting faces in a still image or video frame with a Z-factor e.g. bias factor, by a computing processor e.g. CPU. The detected faces are compared with the Z-factor against a library of faces stored in a face storage by the computing processor. Identity or identification number of a first face is output as identification for the detected faces from the still image or video frame along with another Z-factor corresponding to matching or correspondence to the first face if the detected faces match or correspond to the first face in the library of faces.
   USE - Method for recognizing faces of a person or human from a still image or video frame in a movie for applications for movie and music recording industries. Uses include but are not limited to analytics applications, big data processing applications, natural language processing applications, economy forecasting applications, face recognition applications, medical diagnosis applications, pattern recognition applications, object recognition applications, biometrics applications and security analysis applications.
   ADVANTAGE - The method enables using neural networks, perceptrons including gradient descent and delta rule, back propagation algorithm, feed-forward networks and hypothesis space search and inductive bias with generalization and overfitting considerations, Q learning algorithm, or reinforcement learning to be combined with the method for machine learning to improve performance or efficiency of recognizing the faces of the person. The method enables using restricted-centered theory of reasoning and computation in an environment of uncertainty and imprecision so as to enhance capability of reasoning and computation in the environment of uncertainty, imprecision and partiality of truth. The method enables fuzzy type rendering to avoid making false correlations with other irrelevant features in image/data by representing features of location of interest as coarse.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a Z-number estimator or calculator device or system.
PD US2014079297-A1   20 Mar 2014   G06K-009/00   201423   Pages: 159   English
   US8873813-B2   28 Oct 2014   G06K-009/00   201471      English
UT DIIDW:2014F21559
ER

PT P
PN US2001002932-A1; EP1107166-A2; JP2001222719-A; US6697503-B2; EP1107166-A3
TI Face image extraction device for image processor, has analysis unit for defining face in target image by position and size, based on voting values that are increased or decreased based on template.
AB    NOVELTY - The extraction unit (1) generates image by extracting edge of target image. The storage unit (2) stores template composed of concentric shapes. The storage unit (3) stores voting values and corresponding coordinates of pixels of edge image. The voting unit (4) increases or decreases voting values based on template. Analysis unit (5) defines face in target image by position and size based on stored voting values.
   USE - For image processors to extract human face from target images such as still and moving pictures.
   ADVANTAGE - Real-time face region extraction is achieved with available processing capabilities of existing PCs as the processing load is reduced by using face image extraction device. Face region is stably extracted at a high speed even if facial expression changes with help of template for detection, voting process and evaluation of voting results.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) Face image extraction method;
   (b) Recording medium having recorder face image extraction method
   DESCRIPTION OF DRAWING(S) - The figure is the block diagram showing structure of face image extraction device.
   Extraction unit (1)
   Storage unit (2)
   Storage unit (3)
   Voting unit (4)
   Analysis unit (5)
PD US2001002932-A1   07 Jun 2001   G06K-009/00   200138   Pages: 16   English
   EP1107166-A2   13 Jun 2001   G06K-009/00   200141      English
   JP2001222719-A   17 Aug 2001   G06T-007/60   200155   Pages: 12   Japanese
   US6697503-B2   24 Feb 2004   G06K-009/00   200415      English
   EP1107166-A3   06 Aug 2008   G06K-009/00   200854      English
UT DIIDW:2001366779
ER

PT P
PN US5410609-A
TI Individuals identification system based on analysis of facial image - has visual image input unit for inputting facial image of individual to be recognised and characteristics extracting unit.
AB       The appts includes a visual image input device (1) for inputting a facial image of a subject to be identified. A characteristics extracting device (2) is provided for extracting characteristic points which represent configurations and positions of various parts of the inputted facial image. A data base of individuals (3) is also used for storing expressionless facial images.
   A difference detecting device (4) is used for determining differences between the characteristic points of the inputted image extracted from the characteristics extracting device and the expressionless facial images stored in the individuals data base. The difference detecting device also outputs difference vector information indicative of the difference.
   USE/ADVANTAGE -   For identification of individuals from their facial images including changing facial expressions. Enhanced recognition accuracy.
PD US5410609-A   25 Apr 1995   G06K-009/00   199522   Pages: 7   English
UT DIIDW:1995169889
ER

PT P
PN WO2009123711-A1; US2009252383-A1; KR2010129783-A; EP2281248-A1; CN101990667-A; JP2011516966-W; US8358811-B2; US2013251217-A1; CN101990667-B; EP2618289-A2; EP2618290-A2; EP2618289-A3; EP2618290-A3; US8897508-B2; JP2014222519-A; JP5869054-B2; KR1618735-B1
TI Facial image database creating and updating method using collection of digital images, involves updating separately displayed similarity groups of facial images with user input which is received to confirm or reject individual facial images.
AB    NOVELTY - The creating and updating method involves detecting a first set of facial images in images from a collection of digital images (107), and grouping the first set of facial images into similarity groups (104). Facial recognition templates of facial images are provided within a predetermined range within each similarity group. One or more of the similarity groups are separately displayed in a graphical user interface (GUI) (121). User input is received to confirm or reject individual facial images in the displayed similarity groups. The displayed similarity groups are updated with the user input.
   USE - Facial image database creating and updating method using a collection of digital images. Can be used in automatic face recognition within digital image collections.
   ADVANTAGE - Integrates automatic face recognition into organization of digital image collections. Provides a method to integrate facial images of separate collections, by comparing additional information elements in the comparison of image templates. Ensures that the tags to be assigned to the facial images in the integrated collection may be determined by various ways of comparing the set of information elements.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) Naming tool for assigning names to faces detected in a digital image collection;
   (2) Digital image accessing method; and
   (3) Image searching method.
   DESCRIPTION OF DRAWING(S) - The drawing is a schematic block diagram showing the system view of the method for creating and updating a facial image database from a collection of digital images.
   Server (101)
   Similarity groups (104)
   Collection of digital images (107)
   Database of facial images (108)
   Client (120)
   GUI (121)
PD WO2009123711-A1   08 Oct 2009   G06F-017/30   200968   Pages: 49   English
   US2009252383-A1   08 Oct 2009   G06K-009/00   200968      English
   KR2010129783-A   09 Dec 2010   G06F-017/40   201082      
   EP2281248-A1   09 Feb 2011   G06F-017/30   201112      English
   CN101990667-A   23 Mar 2011   G06F-017/30   201129      Chinese
   JP2011516966-W   26 May 2011   G06T-007/00   201135   Pages: 26   Japanese
   US8358811-B2   22 Jan 2013   G06K-009/00   201307      English
   US2013251217-A1   26 Sep 2013   G06K-009/00   201363      English
   CN101990667-B   28 Aug 2013   G06F-017/30   201378      Chinese
   EP2618289-A2   24 Jul 2013   G06K-009/00   201402      English
   EP2618290-A2   24 Jul 2013   G06K-009/00   201402      English
   EP2618289-A3   30 Jul 2014   G06K-009/00   201450      English
   EP2618290-A3   06 Aug 2014   G06K-009/00   201452      English
   US8897508-B2   25 Nov 2014   G06K-009/00   201477      English
   JP2014222519-A   27 Nov 2014   G06F-003/048   201478   Pages: 28   Japanese
   JP5869054-B2   24 Feb 2016   G06F-003/0484   201617   Pages: 27   English
   KR1618735-B1   09 May 2016   G06F-017/40   201635      English
UT DIIDW:2009P53076
ER

PT P
PN WO8606527-A; GB2174831-A; EP218668-A; JP62502575-W; GB2174831-B; US4805223-A
TI Human skin-pattern recognition method - comparing current and stored images using contact surface being hypotenuse face of right angled prism.
AB       The recognition method involves taking a current skin pattern using a contact surface. An image of the print of the skin pattern is projected onto a photo-detector. The intensity variators of the image in at least one region is assessed digitally in several different sub regions of each region to create a digital signal train. A second digital signal train is derived from the stoned skin pattern information.
   The two trains are compared to determine the degree of coincidence The degree of coincidence is used to authenticate the person.
   USE -   Validation of persons presenting creditor bankers card.
PD WO8606527-A   06 Nov 1986   G07C-009/00   198646   Pages: 22   English
   GB2174831-A   12 Nov 1986   G06K-009/60   198646      English
   EP218668-A   22 Apr 1987   G07C-009/00   198716      English
   JP62502575-W   01 Oct 1987      198745      Japanese
   GB2174831-B   14 Dec 1988   G06K-009/60   198850      English
   US4805223-A   14 Feb 1989   G06K-009/00   198909      English
UT DIIDW:1986305232
ER

PT P
PN US7711155-B1; US45768-E
TI Human faces three-dimensional modeling method for virtual teleconferencing application, involves combining demographic recognition with affine coordinate based mesh adjustment technique for face modeling.
AB    NOVELTY - The method involves processing a set of images to obtain demographic recognition of a person in captured images using a support vector machine (SVM) based demo-graphic classifiers. A face model specific to the demographic recognition of the person is chosen as an approximate face model. The demographic recognition is combined with affine coordinate based mesh adjustment technique for face modeling, where the demographic recognition comprises gender and ethnicity recognition, and the face modeling is followed by a view generation of the face using rendering tools.
   USE - Method for three-dimensional (3D) modeling of human faces, from images captured from a single or a set of images capturing systems at different times, in computer vision and graphics field, for virtual teleconferencing application.
   ADVANTAGE - The method enables determination of the demographics of the person being imaged and enables selection of an approximate three dimensional face model from a set of models, thus adjusting the model leading to a more accurate face model using the initial model and properties of camera projection. The method enables detection of face and facial feature in a robust, reliable, and efficient manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an apparatus for three-dimensional (3D) modeling of human faces, comprising a single or a set of image capturing systems.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating steps of different important subcomponents in the face modeling system.
PD US7711155-B1   04 May 2010   G06K-009/00   201031   Pages: 12   English
   US45768-E   20 Oct 2015   G06K-009/00   201570      English
UT DIIDW:2010E82788
ER

PT P
PN US7558408-B1
TI Image acquisition and processing system for detection and recognition of human face images, determines whether faceprint related to face classifier parameter values, corresponds to known identity, based on predetermined condition.
AB    NOVELTY - The system has programming instructions comprising a face detection module for identifying a group of pixels corresponding to a face region within digital image data acquired by an acquisition device. A face recognition module extracts a set of values of face classifier parameter values collectively known as a faceprint, from normalized face region and determines whether the faceprint corresponds to a known identity when comparisons of parameter values with multiple archived faceprints correspond to a same known identity.
   USE - Image acquisition and processing system for use in automatic or semiautomatic grouping and classification of images in database based on occurrence of faces in images, for detection and recognition of human face images.
   ADVANTAGE - A new and improved tool which can manage collection of images and organize image collections which are in a constant state of change and growth, is provided.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) computer-readable recording medium storing image acquisition and processing program; and
   (2) method for classifying and archiving images including face regions that are acquired with image acquisition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart explaining the automatic or semiautomatic grouping and classification of images.
   Transmission of image to face detection module (4120)
   Transmission of face image to normalization mode (4155)
   Transmission of face image to face recognition mode (4160)
   Addition of faceprint to search list (4190)
   Loading of manual recognition list (4310)
PD US7558408-B1   07 Jul 2009   G06K-009/00   200945   Pages: 58   English
UT DIIDW:2009L13883
ER

PT P
PN US2010149305-A1; WO2010071442-A1; NO200805227-A; EP2380349-A1; NO331287-B1; CN102265612-A; US8390669-B2; CN102265612-B; EP2380349-A4; EP2380349-B1
TI Streaming/archiving unit for video conference equipment used in e.g. organization, stores identity of detected face as searchable metadata in content database, when detected face matches with facial image in subsets.
AB    NOVELTY - A conversion unit converts video conference format coded data stream into multimedia stream. A face in a video stream included in multimedia stream is detected (S4). The subsets including facial images of known users are defined and ranked according to a probability of known users appearing in the video stream. The detected face is compared with the subsets in consecutive order, until a match is found. An identity of the detected face is stored as searchable metadata in a content database (304), when detected face matches with a facial image in the subsets.
   USE - Streaming/archiving unit for video conference equipment used in organization and company.
   ADVANTAGE - Since identity of detected face is stored as searchable metadata in content database in response to detected face matching facial image in subsets, close-up images of conference participants can be displayed in graphical user interface, so that manual browsing for archiving conference containing particular person can be made easier. Since fewer comparisons are required to find true identity of detected face, computational load of the recognition process can be reduced and probability that the comparison between the recorded face and correct face can be increased, so that chance of false match can be reduced while improving reliability. Thus facial recognition process can be improved by improving robustness and decreasing computational cost.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for method for identifying individual in multimedia stream originating from video conference terminal or multipoint control unit.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram explaining the process of identifying individual in multimedia stream originating from video conference terminal.
   Call handling unit (302)
   Content database (304)
   Face recognition unit (305)
   User database (306)
   Step for detecting face in video stream included in multimedia stream (S4)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The Streaming/archiving unit receives video conference format coded data stream that conforms to H.323 and H.320 coded data stream, and MPEG-4 format.
PD US2010149305-A1   17 Jun 2010   H04N-007/14   201042   Pages: 22   English
   WO2010071442-A1   24 Jun 2010   H04N-007/15   201043      English
   NO200805227-A   16 Jun 2010   G06K-009/62   201055      Portuguese
   EP2380349-A1   26 Oct 2011   G06K-009/00   201170      English
   NO331287-B1   14 Nov 2011   G06K-009/62   201178      Portuguese
   CN102265612-A   30 Nov 2011   H04N-007/15   201182      Chinese
   US8390669-B2   05 Mar 2013   H04N-007/14   201318      English
   CN102265612-B   27 May 2015   H04N-007/15   201552      Chinese
   EP2380349-A4   27 Jun 2012   G06K-009/00   201745      English
   EP2380349-B1   24 Jan 2018   G06K-009/00   201807      English
UT DIIDW:2010G87427
ER

PT P
PN US2009175509-A1; US8600120-B2
TI Personal computing device for supporting user presence sensing control of application, has processor controlling generation of user information outputs and reception of user information inputs in response to face identification in images.
AB    NOVELTY - The device has a user interface generating multiple user information outputs, and receiving multiple user information inputs, and an image sensor i.e. camera, for capturing multiple images. A processor detects multiple faces in the images by employing a pattern recognition algorithm comprising a statistical model. The processor controls generation of the user information outputs and reception of the user information inputs in response to the identification of one of the detected faces in the images, which is associated with an authorized user of the device.
   USE - Personal computing device such as personal computer, portable computer, cellular telephone, wireless communication device, media player, MPEG-1 Audio layer 3 player, video player, personal digital assistant (PDA), personal media device, and personal communication device (all claimed), for supporting user presence sensing control of an application such as electronic-mail, texting, word processing, interface navigation, data searching, web surfing, database management, remote control system, and multimedia application, using face detection and recognition. Can also be used for personal display device, vehicle control system, financial transactions system, desktop computer, laptop computer, workstation, server interfaces, and handheld computer, and consumer electronic devices such as TV, stereo system, video gaming system, camera, video camera, task-specific computing device, media capable cellular telephone, and satellite media player, satellite phone, vehicle operating system, vehicle monitoring system, automatic teller machine (ATM), store purchase/check-out system, credit card transaction system and remote purchase system.
   ADVANTAGE - The device is sized in such a way that the device fits relatively easily into the pocket or hand of a user. The device is small and can be easily handled and utilized during traveling. The image sensor enables the device to capture an image or series of images continuously, periodically at select time and/or under select conditions. The device enables more efficient control and interaction between the user and the device by supporting user presence sensing control of an application using face detection and recognition.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a personal media device
   (2) a personal communication device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a process by which a personal computing device performs face detection and/or face recognition to control input/output interface of the device and/or to control an application.
PD US2009175509-A1   09 Jul 2009   G06K-009/00   200947   Pages: 18   English
   US8600120-B2   03 Dec 2013   G06K-009/00   201379      English
UT DIIDW:2009L26178
ER

PT P
PN US7103211-B1
TI Generation method for fully textured three-dimensional model of human face, involves texturing three-dimensional mesh model with respect to generated images from camera.
AB    NOVELTY - The method involves texturing a three-dimensional mesh model with respect to the generated images from a camera. The three-dimensional mesh model is generated in accordance with camera pose information determined in each image with respect to the features being tracked across the sequence of images.
   USE - Use for generating a fully-textured three-dimensional model of a human face without depending on a deformable generic face.
   ADVANTAGE - Eases deployment in a home computer with a web camera to generate a fully textured three-dimensional model of the user's face.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for generating a fully -textured three-dimensional model of a human face; and
   (2) a software product encoded on a computer readable medium.
   DESCRIPTION OF DRAWING(S) - The figures show a coarse mesh with vertices directly triangulated from points tracked on the image frames, and a mesh after being incrementally refined and extended.
PD US7103211-B1   05 Sep 2006   G06K-009/00   200671   Pages: 35   English
UT DIIDW:2006686487
ER

PT P
PN US2004151371-A1; US7120279-B2
TI Digital image processing method for determining human face orientation, involves locating face in color digital image and determining orientation using mean grid pattern unit image with different orientations in correlating images.
AB    NOVELTY - The method involves generating a mean grid pattern unit image from sample face images, and an integral image from a digital color image. A face is located in the digital image to perform a correlation test between the mean grid pattern unit image provided at different orientations and the digital color image, at effective resolutions. An orientation of the face is determined by using the images with different orientations.
   USE - Used for determining orientation of faces located in a digital color image (claimed).
   ADVANTAGE - The method automatically determines the orientations of the human face located in the color digital image. The method rapidly executes and requires very little memory space in order to locate faces in different orientations in the digital image.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer program product to perform a digital image processing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a face detection method.
PD US2004151371-A1   05 Aug 2004   G06K-009/00   200459   Pages: 29   English
   US7120279-B2   10 Oct 2006   G06K-009/00   200667      English
UT DIIDW:2004614202
ER

PT P
PN US6944319-B1
TI Computerized face recognition involves simultaneously inputting principal components analysis coefficient into respective face recognition neural network, and assigning active output of fusing neural network as corresponding to person.
AB    NOVELTY - The principal components analysis coefficient vectors generated from dimensional column vector are input into the respective face recognition neural network associated with the vector's particular pose range group, and the active output of the fusing neural network is assigned as corresponding to the particular person and pose associated with the model image used to create the set of PCA coefficient vectors, simultaneously.
   USE - For identifying people depicted in non-staged image such as frame from video camera.
   ADVANTAGE - Enables accurate reconstruction of face from images of a scene.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) face recognition system; and
   (2) computer readable memory recorded with face recognition program.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart of the face recognition process.
PD US6944319-B1   13 Sep 2005   G06K-009/00   200566   Pages: 36   English
UT DIIDW:2005646657
ER

PT P
PN US2002076088-A1; US6697504-B2
TI Multi-level facial image recognition method involves passing decomposed sub-images through map neural networks, and performing recognition decision process of sub-images with required resolution.
AB    NOVELTY - An input original image (11) of a face is trimmed into a facial image (12) which is decomposed into sub-images (101-104,111,112,121-124) with different resolutions. The decomposed sub-images are passed through self-organizing map neural networks to perform a non-supervisory classification learning. A recognition decision is performed for the sub-images having a low resolution. If the candidates are not identified in low resolution, the recognition is performed in a high level of resolution.
   USE - For recognizing multi-level facial image.
   ADVANTAGE - Reduces the amount of data to be compared in the recognition process, by image division and the multi-level decision process, thereby greatly increasing the recognition speed. By performing a non-supervisory classification learning of a self-organizing map (SOM) neural network, the features are unnecessary to be selected or extracted manually and therefore the insufficiency or impropriety in the featured selection is avoided.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for multi-level facial image recognition system.
   DESCRIPTION OF DRAWING(S) - The figure shows the structural view of multi-level facial image recognition system.
   Input original image (11)
   Facial image (12)
   Sub-images (101-104,111,112,121-124)
PD US2002076088-A1   20 Jun 2002   G06K-009/00   200260   Pages: 10   English
   US6697504-B2   24 Feb 2004   G06K-009/00   200415      English
UT DIIDW:2002566185
ER

PT P
PN US5978494-A
TI Master enroll image determination method for selecting master enroll image to identify individuals from eye image.
AB    NOVELTY - N set of images having focus value in predefined range is selected and each image is compared with (N- 1) images in image comparison processor (52) to determine the matching images. The successful matches are recorded in memory (43). S set of images are formed. A master enroll image is selected from the S set.
   USE - For selecting master enroll image which is used to identify individuals from eye image. In automatic teller machine for conducting banking transactions. Also used to compare finger print, hand print or full facial images, and to identify products by symbols on packages.
   ADVANTAGE - A range of images with various degrees of defocus representative of results of a range of focus settings on actual imaging system can be produced from single input image by digitally effecting low pass spatial filtering. As the imperfections in video capture facilities used for iris identification are reduced, the master enroll image will move closer in hamming distance and spatial frequency content to a sharply focussed image of the subject's eye.
   DETAILED DESCRIPTION - A set of M infrared images at different focus settings are collected and focus value F of each image is computed. N set of images having focus value in the range 0.30 less than F less than 0.60 are selected. An INDEPENDENT CLAIM is also included for a master enroll image selecting apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of an master enroll image selection apparatus.
   Memory (43)
   Image comparison processor (52)
PD US5978494-A   02 Nov 1999   G06K-009/00   200001   Pages: 6   English
UT DIIDW:2000012170
ER

PT P
PN WO2004072802-A2; US2004190760-A1; US7162076-B2; WO2004072802-A3
TI Image representation method e.g. for face image detection, involves combining input image with one-dimensional Harr wavelet representation and amplitude projections.
AB    NOVELTY - The input image is combined with calculated 1-D Harr wavelet representations and amplitude projection, by forming discriminating feature analysis (DPA) vector of the image.
   USE - For image representation e.g. for face image detection in government and security applications. Also used in other image processing and detection system.
   ADVANTAGE - Required calculations are minimized thereby increasing face image detection speed and efficiency while reducing computational cost. Allows face image detection with relatively low probability of error and false detection rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for the classification method of input image.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart illustrating the face detection algorithm.
PD WO2004072802-A2   26 Aug 2004   G06F-000/00   200463   Pages: 10   English
   US2004190760-A1   30 Sep 2004   G06K-009/00   200465      English
   US7162076-B2   09 Jan 2007   G06K-009/62   200705      English
   WO2004072802-A3   09 Jun 2005   G06K-009/00   201217      English
UT DIIDW:2004652682
ER

PT P
PN US2003053663-A1; EP1296279-A2; JP2003108981-A; US7058209-B2; EP1296279-B1; DE60215743-E; DE60215743-T2; JP4234381-B2; EP1296279-A3
TI Digital image processing method for human face recognition, involves selecting squared difference method based on image pixels and accordingly detected eye positions are added with respect to iris pixel clusters.
AB    NOVELTY - A group of iris pixels is clustered and accordingly eye position is detected by applying geometric reasoning. The detected positions are added by selecting a specific squared difference method according to the image pixels.
   USE - For detecting human facial features in automatic human facial morphing and warping, expression recognition, hair segmentation, face recognition and classification, red-eye detection and facial image compression. Also for identifying various organs in medical imaging and locating circuit board components in industrial system.
   ADVANTAGE - Ensures precise detection of facial features by automating selection of eye detection process. Quality of search results is significantly improved by limiting the number of feature points.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for computer program product for facial feature detection.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart of eye detection method of digital image.
PD US2003053663-A1   20 Mar 2003   G06K-009/00   200342   Pages: 29   English
   EP1296279-A2   26 Mar 2003   G06K-009/46   200342      English
   JP2003108981-A   11 Apr 2003   G06T-001/00   200342   Pages: 17   Japanese
   US7058209-B2   06 Jun 2006   G06K-009/00   200638      English
   EP1296279-B1   02 Nov 2006   G06K-009/00   200672      English
   DE60215743-E   14 Dec 2006   G06K-009/46   200705      German
   JP4234381-B2   04 Mar 2009   G06T-001/00   200919   Pages: 22   Japanese
   EP1296279-A3   14 Jan 2004   G06K-009/46   201736      English
UT DIIDW:2003447927
ER

PT P
PN WO200208022-A2; EP1355806-A2; JP2004517512-W; US7110570-B1; JP4589600-B2; WO200208022-A3
TI Facial feature recognition system for automobiles for identifying and tracking persons using reflected infrared illumination to identify person.
AB    NOVELTY - The camera system uses a processor with face recognition software to identify and track the person using a database for comparison. The camera also detects the distance a person is from the airbag in the event of an accident allowing the airbag to deploy at reduced velocity if a person is close to it.
   USE - Identifying human face for vehicle security and convenience.
   ADVANTAGE - This system prevents theft of vehicles by recognizing the authorized drivers. The system also allows setting such as seat position, mirror alignment etc. to be stored and automatically changed by each driver. In the event of a crash the system determines the distance a person is from the airbag , allowing it to be deployed at a lower velocity is a person is close to the airbag storage area. Thus injury due to airbag deployment is avoided.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for
   (1) a driver imaging and identification system,
   (2) an intruder detection system
   (3) a driver convenience system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a person in a passenger seat.
   Person (16)
   Instrument panel (22)
   Illumination unit (26)
PD WO200208022-A2   31 Jan 2002   B60R-021/01   200223   Pages: 28   English
   EP1355806-A2   29 Oct 2003   B60R-021/01   200379      English
   JP2004517512-W   10 Jun 2004   H04N-007/18   200438   Pages: 50   Japanese
   US7110570-B1   19 Sep 2006   G06K-009/00   200662      English
   JP4589600-B2   01 Dec 2010   H04N-007/18   201079   Pages: 15   Japanese
   WO200208022-A3   04 Sep 2003   B60R-021/01   201208      English
UT DIIDW:2002179889
ER

PT P
PN GB2343945-A; DE19955714-A1; JP2000163600-A; CN1254904-A; KR2000035050-A; GB2343945-B; KR2001002097-A; TW466452-A; KR347058-B1; KR326203-B1
TI Method for photographing a face for generating alarm conditions if a dangerous character is recognized on a security system involves tracking selected area that contains face.
AB    NOVELTY - Face data is sampled from a camera image (10) and used to recognize a face (32), the area containing the face is then selected for tracking (33). Pan/tilt and zoom control (34) move the camera left, right, up, down, forward and backward to track the selected area. Any abnormal conditions of the face e.g. if it is masked, are determined by analysis of the face contours. The face image is then stored (50) or transmitted for recognition (36).
   USE - Method for photographing a face for generating alarm conditions when a dangerous character is recognized on a security system. The tracking system can also be applied to picture telephones, on-line gaming and virtual reality systems.
   ADVANTAGE - The camera position is automatically altered to get the best view of the face possible.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for;
   (1) a method of recognizing a face, and
   (2) an apparatus for photographing a face.
   DESCRIPTION OF DRAWING(S) - The drawing shows a functional block diagram of the main controller from the system.
   Camera (10)
   Determine face location (32)
   Select area (33)
   Tracking controller (34)
   face recognition system (36)
   Image storage (50)
PD GB2343945-A   24 May 2000   G01S-003/786   200028   Pages: 55   English
   DE19955714-A1   31 May 2000   G06K-009/78   200033      German
   JP2000163600-A   16 Jun 2000   G06T-009/20   200036   Pages: 16   Japanese
   CN1254904-A   31 May 2000   G06K-009/20   200045      Chinese
   KR2000035050-A   26 Jun 2000   G06K-009/00   200111      
   GB2343945-B   28 Feb 2001   G01S-003/786   200113      English
   KR2001002097-A   05 Jan 2001   G06T-001/00   200144      
   TW466452-A   01 Dec 2001   G06K-009/78   200252      Chinese
   KR347058-B1   03 Aug 2002   G06K-009/00   200309      
   KR326203-B1   27 Feb 2002   G06T-001/00   200258      
UT DIIDW:2000320932
ER

PT P
PN CN102201061-A; CN102201061-B
TI Multilevel-filtration face recognition based security monitoring system, has database management system receiving signal from feature extraction system and managing and storing client information stored in database.
AB    NOVELTY - The system has a feature extraction system for extracting a feature point of a face image passing through image quality detection threshold of an acquisition terminal. A multilevel quick filtration search system carries out multilevel filtration search and match operations of a large-scale face image and template data to judge specific identity information of a customer. A database management system receives a signal from the feature extraction system, and manages and stores client information stored in a background database.
   USE - Intelligent security monitoring system based on multilevel-filtration face recognition.
   ADVANTAGE - The system compares a to-be-recognized face detected by a client with known face database based on multilevel filtration search algorithm through a background server in real time, finds and judges the face template with highest matching score according to a preset threshold, thus determining the identification information of a recorded person in real time. The system has high reliability and flexibility.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multilevel-filtration face recognition based security monitoring method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a multilevel-filtration face recognition based security monitoring system.'(Drawing includes non-English language text)'
PD CN102201061-A   28 Sep 2011   G06K-009/00   201175   Pages: 20   Chinese
   CN102201061-B   31 Oct 2012   G06K-009/00   201306      Chinese
UT DIIDW:2011N80744
ER

PT P
PN EP2031544-A1; US2009060290-A1; JP2009053916-A; CN101377814-A; EP2031544-B1; JP4946730-B2; US8331616-B2; CN101377814-B
TI Face image processing apparatus used for person identification has face identifying unit that identifies person by calculating feature of input face image at each feature point after position correction is performed.
AB    NOVELTY - The face image processing apparatus has a face identifying unit that identifies a person by calculating the feature of an input face image at each feature point after position correction is performed by a feature point position correcting unit () and checking the feature against the feature of a registered face. A face pose estimation unit estimates face pose based on the detected positions of face parts. A feature point position correcting unit corrects the position of each feature point used for identifying the person based on the estimation result.
   USE - Face image processing apparatus used for person identification.
   ADVANTAGE - Achieve high recognition performance even when the pose of the image to be checked changes. Ensures light and cost-effective processing operation.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a face image processing method; and
   (2) a computer program.
   DESCRIPTION OF DRAWING(S) - The drawing shows the three-dimensional positional relationship which the average face parts enter when they are rotated by 45 degrees about a yaw axis.
PD EP2031544-A1   04 Mar 2009   G06K-009/00   200918   Pages: 36   English
   US2009060290-A1   05 Mar 2009   G06K-009/00   200918      English
   JP2009053916-A   12 Mar 2009   G06T-001/00   200919   Pages: 32   Japanese
   CN101377814-A   04 Mar 2009   G06K-009/00   200921      Chinese
   EP2031544-B1   11 May 2011   G06K-009/00   201131      English
   JP4946730-B2   06 Jun 2012   G06T-001/00   201237   Pages: 32   Japanese
   US8331616-B2   11 Dec 2012   G06K-009/00   201302      English
   CN101377814-B   26 Dec 2012   G06K-009/00   201325      Chinese
UT DIIDW:2009F42641
ER

PT P
PN US2005055582-A1; WO2005059812-A2; EP1661063-A2; JP2007504557-W; US7183895-B2; TW200523810-A; IL174089-A; JP4667377-B2; WO2005059812-A3; TW334102-B1
TI Dynamic security verification providing method for gate security system, involves comparing facial image with face print retrieved corresponding to data read from radio frequency identification device.
AB    NOVELTY - A final facial image is obtained by combining skin image and facial image of a person obtained from the scanning of the person's face with low and high near-infrared (IR) light beams. Final facial image is compared with a face print retrieved from a database corresponding to the data read from a radio frequency identification (RFID) device.
   USE - For gate security system for large facility such as military base, governmental office and other locations.
   ADVANTAGE - Provides fast and secure verification system identifying vehicles and personnel dynamically, by combining RFID and advanced facial detection and recognition techniques. Enables to perform security verification, without requiring vehicle and individual to be in particular place or posture or come into physical contact with biometric device. Enables to quickly locate face out of surrounding backgrounds for enabling efficient capture of biometric data, and reduces processing speed of system required for facial recognition.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) method of dynamically verifying vehicle and person riding in vehicle at control gate; and
   (2) system for dynamic stand-off verification.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart explaining the security identification process.
PD US2005055582-A1   10 Mar 2005   H04L-009/32   200525   Pages: 13   English
   WO2005059812-A2   30 Jun 2005   G06K-009/00   200544      English
   EP1661063-A2   31 May 2006   G06K-009/00   200636      English
   JP2007504557-W   01 Mar 2007   G06K-017/00   200718   Pages: 20   Japanese
   US7183895-B2   27 Feb 2007   H04B-003/00   200718      English
   TW200523810-A   16 Jul 2005   G06K-019/067   200957      Chinese
   IL174089-A   31 May 2010   G06K-009/00   201041      English
   JP4667377-B2   13 Apr 2011   G06K-017/00   201127   Pages: 13   Japanese
   WO2005059812-A3   15 Dec 2005   G06K-009/00   201219      English
   TW334102-B1   01 Dec 2010   G06K-019/067   201227      Chinese
UT DIIDW:2005240651
ER

PT P
PN EP828230-A2; JP10075823-A; TW340791-A; KR98024383-A; US6091836-A; JP2001224430-A; EP828230-B1; CN1179293-A; DE69710137-E; JP3529954-B2; JP3614783-B2; CN1105523-C; EP828230-A3; KR342787-B1
TI Face feature classification method - extracting characters of face from view point of form, and classifying features in accordance with extracted characters, and features map produced by coordinate axes.
AB       The method for classifying a face involves analysing facial features so that the face is appropriately classified or recognized in order to facilitate an exact and easy creation of an image produced by applying make-up. A first index represents one of a length of the face and a configuration of formational elements of the face, the formational elements including an eye, an eyebrow, a mouth and a nose.
   A second index represents one of a contour of the face and a contour of each of the formational elements of the face. A face is classified into one of groups of features each of which provides similar impressions by using the first index and the second index.
   USE/ADVANTAGE -   Using features map for recognising characters of face, and as guide line when applying make-up, e.g. as assistance tool in cosmetic counselling with client at cosmetic shop or beauty salon, or as educational tool or practical work of cosmetic consultant or beautician.
PD EP828230-A2   11 Mar 1998   G06T-007/00   199814   Pages: 21   English
   JP10075823-A   24 Mar 1998   A45D-044/00   199822   Pages: 10   Japanese
   TW340791-A   21 Sep 1998   A45D-044/00   199903      Chinese
   KR98024383-A   06 Jul 1998   G06T-007/00   199927      
   US6091836-A   18 Jul 2000   G06K-009/00   200037      English
   JP2001224430-A   21 Aug 2001   A45D-044/00   200155   Pages: 10   Japanese
   EP828230-B1   30 Jan 2002   G06T-007/00   200209      English
   CN1179293-A   22 Apr 1998   A45D-044/00   200222      Chinese
   DE69710137-E   14 Mar 2002   G06T-007/00   200226      German
   JP3529954-B2   24 May 2004   A45D-044/00   200434   Pages: 10   Japanese
   JP3614783-B2   26 Jan 2005   A45D-044/00   200510   Pages: 12   Japanese
   CN1105523-C   16 Apr 2003   A45D-044/00   200538      Chinese
   EP828230-A3   11 Nov 1998   G06T-007/00   201735      English
   KR342787-B1   22 Aug 2002      200313      
UT DIIDW:1998147617
ER

PT P
PN US7848548-B1
TI Method for automatically performing demographic classification for detecting human faces, involves building multiple pose-dependent facial appearance models for tracking and classification based on estimated pose of face.
AB    NOVELTY - The method involves detecting (335) faces from several captured images of peoples. The two-dimensional and three dimensional facial poses of detected faces are estimated (510,520). The faces are tracked (361) and multiple pose-dependent appearance models of the faces are constructed. The demographic classification based on a pose-independent is performed in which multiple pose-dependent facial appearance models are built for tracking and classification based on estimated pose of face. The demographic information is comprised of age, gender and ethnicity information.
   USE - Method for automatically performing demographic classification for detecting human faces in still images or in videos.
   ADVANTAGE - The speed of the classification is improved by executing demographic classification. Tracking accuracy is also improved by constructing pose-dependent appearance model. Better accuracy demographic classification is achieved by performing three dimensional pose-dependent classifications.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for apparatus for performing demographic classification.
   DESCRIPTION OF DRAWING(S) - The drawing shows an explanatory block diagram of operation of the face-based automatic demographic classification system.
   Step for detecting face sequence (335)
   Step for tracking (361)
   Step for estimating two-dimensional and three dimensional facial poses (510,520)
   Step for estimation training of two-dimensional facial pose (511)
   Step for estimation training of three dimensional facial pose (521)
PD US7848548-B1   07 Dec 2010   G06K-009/00   201101   Pages: 19   English
UT DIIDW:2010Q03240
ER

PT P
PN US7283650-B1
TI Person/customer`s facial image printing method for e.g. kiosk system, involves processing read and write transaction, and input images to superimpose facial images of person, onto promotional printed material e.g. coupon.
AB    NOVELTY - The method involves capturing input images of a customer (408) in an uncontrolled background, by an image capturing unit (110) e.g. camera. Communication between a processing and controlling unit and product and customer databases are performed. A read and write transaction is processed to the databases. The input images are processed to superimpose facial images of the customer, onto a promotional printed material e.g. coupon (801). The content of the material is matched based on demographic information from a demographic classification of the customer using the input images.
   USE - Used for printing a facial image of person or customer, from a sequence of images automatically captured by a image capturing unit, onto a promotional printed material e.g. coupon, postcard, stamp, promotional brochure, and ticket for a movies or a show, in an existing checkout counter of a retail store environment, and a stand alone system e.g. kiosk system (claimed).
   ADVANTAGE - The method improves the value and quality of the material. The method allows verification of the legitimacy of coupon redemption, thus preventing coupon fraud, duplicate redemption of the same coupon to the same customer, and inefficiency of issuing coupons to the same customer. The method allows issue of new coupons, thus promoting products to the customers at a redemption stage. The input images are processed to superimpose the facial images onto the material, thus increasing the advertisement effect for a product, and stimulating the customer`s desire to buy the product. The method prevents the customer from throwing away coupons, thus saving resources and maximizing the efficiency of coupon resources. The method increases the amusement level to the customers.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an apparatus for printing facial images of person, from a sequence of images automatically captured by an image capturing unit, onto a promotional printed material or coupon.
   DESCRIPTION OF DRAWING(S) - The drawing shows an overall view of a UCOUPON as a store-integrated system.
   Image capturing unit (110)
   Digital content displaying unit (111)
   Coupon printing unit (118)
   Customer (408)
   Coupon (801)
   Retail store environment (880)
PD US7283650-B1   16 Oct 2007   G06K-009/00   200778   Pages: 23   English
UT DIIDW:2007842545
ER

PT P
PN WO2015088141-A1; KR2015068013-A; CN105874405-A; EP3080678-A1; US2017004828-A1; EP3080678-A4; US10269344-B2; US2019267004-A1; KR2188090-B1; EP3761309-A1
TI Smart home appliance such as air conditioner for improving voice recognition rate, has memory unit for mapping text recognized by voice input unit and setting function and is used by control unit for determining performance of voice.
AB    NOVELTY - The home appliance (10) has a voice input unit (110) for collecting voice. A voice recognition unit (170) recognizes text of voice that is collected through the voice input unit. A capturing unit (50) collects image for detecting user visage. A memory unit (130) maps the text that is recognized by the voice recognition unit and setting function and stores the mapped information. A control unit (120) determines performing of voice recognition service using information of image that is collected by the capturing unit and information that is collected by the voice input unit.
   USE - Smart home appliance such as air conditioner for improving voice recognition rate using voice recognition system (claimed). Can also be used in air conditioner, cooker and vacuum cleaner.
   ADVANTAGE - The usability of smart home appliance is improved by enabling control operation by the user through voice. The voice misrecognition is prevented during an operation of an electronic product by recognize of user face or action for manipulating mobile device on determination that user has an intention for speaking a voice command. Thus, user convenience is improved. The reliability of a product operation is increased by providing a setting for determining possibility of usage of voice recognition function and security setting. The usage of mapped information of voice characteristics for distinguishing voice information related to an operation of an appliance reduces misrecognition of user voice. The control of operation of smart home appliance by the user through noise improves the usability.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) an operating method of a smart home appliance; and
   (2) a voice recognition system using smart home appliance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram explaining operation of the air conditioner.
   Air conditioner (10)
   Capturing unit (50)
   Voice input unit (110)
   Control unit (120)
   Memory unit (130)
   Voice recognition unit (170)
PD WO2015088141-A1   18 Jun 2015   G06F-003/00   201542   Pages: 63   English
   KR2015068013-A   19 Jun 2015   G06F-003/00   201543      
   CN105874405-A   17 Aug 2016   G06F-003/00   201659      Chinese
   EP3080678-A1   19 Oct 2016   G06F-003/00   201669      English
   US2017004828-A1   05 Jan 2017   G10L-015/22   201704      English
   EP3080678-A4   24 Jan 2018   G06F-003/00   201807      English
   US10269344-B2   23 Apr 2019   G06F-003/01   201930      English
   US2019267004-A1   29 Aug 2019   G10L-015/22   201966      English
   KR2188090-B1   04 Dec 2020   G06F-003/00   202000      
   EP3761309-A1   06 Jan 2021   G10L-015/28   202105      English
UT DIIDW:201535572L
ER

PT P
PN US2015123967-A1; WO2015065928-A1; US9508197-B2
TI Method for automatically generating facial avatar resembling user in defined art style, involves generating facial avatar in defined art style, and displaying facial avatar by display communicatively connected to processors.
AB    NOVELTY - The method involves capturing three-dimensional (3D) image data of user head features including facial features by a 3D capture device (20). A user 3D head model is generated for a user (18) based on the image data. A set of transferable user head features is identified from the model. A set of avatar character head features is identified to represent the set of transferable user head features based on transferable head feature rules for a defined art style. A facial avatar is generated in the art style. The facial avatar is displayed by a display (14) communicatively connected to processors.
   USE - Method for automatically generating a facial avatar resembling a user in a defined art style.
   ADVANTAGE - The method allows a user to use a 3D facial recognition and reconstruction engine to identify the head features including facial features from the image data of a head location identified by an object recognition engine, thus automatically generating the facial avatar resembling the user in the defined art style in an easy and efficient manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a processor readable storage devices comprising a set of instructions for automatically generating a facial avatar resembling a user in a defined art style
   (2) a system for automatically generating a facial avatar resembling a user in a defined art style
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a target recognition, analysis and tracking system.
   Computer system (12)
   Display (14)
   Display monitor (16)
   User (18)
   3D image capture device (20)
PD US2015123967-A1   07 May 2015   G06T-019/20   201532   Pages: 24   English
   WO2015065928-A1   07 May 2015   G06K-009/00   201533      English
   US9508197-B2   29 Nov 2016   G06T-019/20   201679      English
UT DIIDW:201528358V
ER

PT P
PN US8396265-B1
TI Method for performing facial recognition of user based on e.g. authentication image, involves denying authentication of user independent of performing facial recognition when detected distance is less than threshold distance.
AB    NOVELTY - The method involves capturing an image of a face of a user by an image capture device i.e. front-facing camera (214), coupled to a computing device (200), and detecting whether a distance between the image capture device and an object represented by a portion of the image is less than a threshold distance. Authentication of the user is denied with respect to accessing functionalities independent of performing facial recognition based on part of the captured image when the detected distance is less than the threshold distance.
   USE - Method for performing facial recognition of a user based on an authentication image and an enrollment image.
   ADVANTAGE - The method enables authenticating the user attempts to gain access to functionalities of the computing device, transferring a door lock from locked state to unlocked state, determining physical distance between the computing device and the object of the facial image, tightening error margin, preventing erroneous authentication caused by spoofing, and reducing occurrence of an unauthorized user causing erroneous authentication by spoofing.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a computing device for detecting spoofing in captured facial images.
   Computing device (200)
   Processor (202)
   Memory (204)
   Front-facing camera (214)
   Proximity sensor (216)
PD US8396265-B1   12 Mar 2013   G06K-009/00   201319   Pages: 20   English
UT DIIDW:2013D40112
ER

PT P
PN WO2003049033-A1; US2004017932-A1; AU2002347641-A1; EP1464031-A1; JP2005512201-W; CN1599917-A; US7054468-B2; CN1302437-C; EP1464031-B1; DE60228999-E; JP4589625-B2; EP1464031-A4
TI Representing set of reference face images corresponding to set of first vectors in input space of first dimension by projecting first vectors to high dimensional feature space of a second dimension using projection function.
AB    NOVELTY - First vectors are projected to a high dimensional feature space of a second dimension using a projection function to generate a set of second vectors in the high dimensional feature space. The second dimension having more dimensions than the first dimension while generating Kernel Fisherfaces for the second vectors.
   USE - In face recognition using Kernel Fisher Linear Discriminant analysis or Kernel Fisherfaces.
   ADVANTAGE - Can process face image data having wide variations and an enormous amount of image data such that higher order dependencies of the face image can be used to obtain more representative features of the face image without introducing a huge computational burden on the face recognition system. Utilizes the discriminant features of the face images and maximizes the class separation when these features are projected to a lower dimensional face image space.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:
   (a) a method of identifying an input face image as corresponding to a particular face image in a set of reference images
   (b) a computer program product
   (c) a face recognition system
   DESCRIPTION OF DRAWING(S) - The drawing is a block diagram illustrating the structure of the face recognition system according to one embodiment of the present invention.
PD WO2003049033-A1   12 Jun 2003   G06T-007/00   200345   Pages: 42   English
   US2004017932-A1   29 Jan 2004   G06K-009/00   200413      English
   AU2002347641-A1   17 Jun 2003   G06T-007/00   200419      English
   EP1464031-A1   06 Oct 2004   G06T-007/00   200465      English
   JP2005512201-W   28 Apr 2005   G06T-007/00   200530   Pages: 25   Japanese
   CN1599917-A   23 Mar 2005   G06T-007/00   200545      Chinese
   US7054468-B2   30 May 2006   G06K-009/00   200636      English
   CN1302437-C   28 Feb 2007   G06T-007/00   200749      Chinese
   EP1464031-B1   17 Sep 2008   G06K-009/00   200862      English
   DE60228999-E   30 Oct 2008   G06T-007/00   200874      German
   JP4589625-B2   01 Dec 2010   G06T-007/00   201079   Pages: 22   Japanese
   EP1464031-A4   23 May 2007   G06K-009/00   201747      English
UT DIIDW:2003482878
ER

PT P
PN GB2395778-A; JP2004192637-A; US2004151381-A1; US7336830-B2
TI Video face image detection apparatus compares pixels in test image defined by pixel mask, with gaussian model to detect face image from test image at mask position.
AB    NOVELTY - Specific pixels are selected from the region containing a face image in the preceding image which matches a gaussian model, to derive a pixel mask. The pixels in test image defined by the pixel mask, are compared with the model by applying the mask at different positions in image. A face image is detected in the image at the mask position where average difference between the model and the pixels, is minimum.
   USE - For video face image detection for use in video conferencing.
   ADVANTAGE - The face image detection is performed reliably.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) video conferencing apparatus;
   (2) video face image detection method;
   (3) surveillance apparatus; and
   (4) computer software having instructions for video face image detection and recording medium storing video face image detection program.
   DESCRIPTION OF DRAWING(S) - The figure shows a presentation of mask for skin color detection.
   pixel area (600)
   face window (610)
PD GB2395778-A   02 Jun 2004   G06T-007/20   200446   Pages: 54   English
   JP2004192637-A   08 Jul 2004   G06T-001/00   200446   Pages: 77   Japanese
   US2004151381-A1   05 Aug 2004   G06K-009/68   200452      English
   US7336830-B2   26 Feb 2008   G06K-009/68   200816      English
UT DIIDW:2004482639
ER

PT P
PN EP840250-A1; FR2755526-A1; CA2217779-A; JP10187954-A; US6061464-A; JP4048246-B2
TI Digital finger print reading system for authentication - has sensor pad with heaters which produce transient heat pulse, with sensors detecting impact of heat pulse on matrix of sensing elements beneath finger under test.
AB       The system comprises a sensor (10,20) which has a matrix of sensitive elements (41,42,43) and a multiplexer (23) which allow individual measurement of electrical charges generated within the sensor. The matrix is contained in an open housing (13) with the user's finger applied to one of the housing faces. The matrix provides electrical signals corresponding to a matrix design consisting of a complete image of the finger print.
   The sensor has heating elements which create a transient thermal change within the sensor so the effect on the sensor varies according to the contact of elements of the finger print surface with the sensor surface. The heating elements are operated with the sensor reading system, in order to detect the applied finger print pattern.
   ADVANTAGE -   Low cost implementation as sensor and data processor can be integrated. Determines automatically which image is best and should be used.
PD EP840250-A1   06 May 1998   G06K-009/00   199822   Pages: 9   French
   FR2755526-A1   07 May 1998   G06K-009/00   199825      French
   CA2217779-A   05 May 1998   G06K-009/28   199837      English
   JP10187954-A   21 Jul 1998   G06T-001/00   199839   Pages: 6   Japanese
   US6061464-A   09 May 2000   G06K-009/00   200030      English
   JP4048246-B2   20 Feb 2008   G06T-001/00   200821   Pages: 9   Japanese
UT DIIDW:1998242940
ER

PT P
PN US9911073-B1
TI Method for identifying and decoding optical barcode using custom functional pattern, involves accessing resource that corresponds to decoded data through network, and presenting output corresponding to accessed resource at display device.
AB    NOVELTY - The method involves accessing an image at a computing device. A determination is made to check that the accessed image includes a face inside a geometric shape using a facial recognition module. An orientation of the geometric shape is determined using the face inside the geometric shape. A data encoded is decoded within the geometric shape based on the determined orientation of the geometric shape. A resource that corresponds to the decoded data is accessed through a network. A graphical output is presented corresponding to the accessed resource at a display device of the computing device.
   USE - Method for identifying and decoding an optical barcode using a custom functional pattern by a client device. Uses include but are not limited to remote devices, work stations, computers, general purpose computers, internet appliances, hand-held devices, wireless devices, portable devices, wearable computers, and cellular or mobile phones.
   ADVANTAGE - The method enables utilizing a finder module to extract candidate shape features from a modified low resolution image to improve detection of a custom graphic in the image and improve computational efficiency of identifying the custom graphic in the image. The method enables utilizing an alignment module that uses a mismatch to infer or determine a perspective of the image or other spatial attributes of the image that can be utilized by a decoder module to more accurately decode data from the image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a non-transitory machine-readable medium comprising a set of instructions for identifying and decoding an optical barcode using a custom functional pattern by a client device
   (2) a system for identifying and decoding an optical barcode using a custom functional pattern by a client device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a system for identifying and decoding an optical barcode employing a custom functional pattern.
   Scene (402)
   Poster (404)
   Optical barcode (406)
   Optical signal (408)
   User (410)
   Callout (412)
   User device (414)
PD US9911073-B1   06 Mar 2018   G06K-009/00   201818   Pages: 38   English
UT DIIDW:201818525J
ER

PT P
PN US8411909-B1; EP2680190-A2; EP2680190-A3; DE202013012646-U1
TI Method for recognizing facial image input using facial recognition technology in computing device e.g. mobile phone, involves determining whether reference image from light beam is associated with captured test image.
AB    NOVELTY - The method involves initiating emission of light beam by computing device. The light beam is received from an image capture device coupled to computing device and identifies (602) reference image of an eye in an image of the face. The computing device is provided to determine (604) whether reference image from the light beam is associated with the captured test image of the eye in image of the face, for denying (614) or accessing (616) the authentication of a user with respect to accessing functionalities controlled by computing device.
   USE - Method for recognizing facial image input using facial recognition technology in computing device such as mobile phone, tablet computer, notebook, laptop, desktop, personal digital assistant (PDA), set-top box, TV, biometric door lock, watch and vehicle ignition and presence verification device.
   ADVANTAGE - The access to the computing device can be prevented when the expected glint is not detected. The computing device can implement anti-spoofing programs that detect suspected attempts to spoof, so that the erroneous authentication due to spoofing the antispoofing programs can be prevented. Since the anti-spoofing program reduces the usage of the facial recognition programs, the computing resources conserved and the power consumption can be reduced in efficient manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a computer-readable storage device storing instructions for; and
   (2) a computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the process for detecting spoofing in captured facial image.
   Step for identifying reference image (602)
   Step for comparing images (604)
   Step for capturing test image (606)
   Step for denying functionality (614)
   Step for accessing functionality (616)
PD US8411909-B1   02 Apr 2013   G06K-009/00   201325   Pages: 17   English
   EP2680190-A2   01 Jan 2014   G06K-009/00   201403      English
   EP2680190-A3   22 Apr 2015   G06K-009/00   201529      English
   DE202013012646-U1   29 Mar 2018   G06K-009/62   201825      German
UT DIIDW:2013E50186
ER

PT P
PN EP1480157-A2; GB2401979-A; CA2467889-A1; US2005259851-A1; GB2401979-B; US7505611-B2; US2009221272-A1; US7646897-B2; US2010112984-A1; US7881507-B2; CA2467889-C; US2011182487-A1; US8160313-B2; EP1480157-B1; EP1480157-A3
TI Integrated touch pad/fingerprint recognition apparatus for two-way pager, has finger print resolution portion that selectively operates in fingerprint resolution mode and in touch input mode.
AB    NOVELTY - The finger print resolution portion (420) distinct from the touch resolution portion of a touch screen (415), selectively operates in fingerprint resolution mode for sensing fingerprint on touch screen and in touch input mode for detecting touch input on the touch screen.
   USE - For handheld electronic device (claimed) e.g. data messaging device, two-way pager, wireless email device, cellular telephone, wireless Internet appliance and data communication device.
   ADVANTAGE - Provides both input and fingerprint function, without complicating device structure.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) method of input and finger print recognition; and
   (2) handheld electronic device.
   DESCRIPTION OF DRAWING(S) - The figure shows a rear elevational view of the hand electronic device.
   handheld electronic device (400)
   touch screen (415)
   finger print resolution portion (420)
   rear face (410)
   match screen (490)
PD EP1480157-A2   24 Nov 2004   G06K-009/00   200482   Pages: 11   English
   GB2401979-A   24 Nov 2004   G06K-009/00   200482      English
   CA2467889-A1   21 Nov 2004   G06K-009/00   200501      English
   US2005259851-A1   24 Nov 2005   G06K-009/00   200577      English
   GB2401979-B   21 Mar 2007   G06K-009/00   200723      English
   US7505611-B2   17 Mar 2009   G06K-009/00   200924      English
   US2009221272-A1   03 Sep 2009   H04M-003/42   200958      English
   US7646897-B2   12 Jan 2010   G06K-009/00   201005      English
   US2010112984-A1   06 May 2010   G08B-029/16   201031      English
   US7881507-B2   01 Feb 2011   G06K-009/00   201110      English
   CA2467889-C   04 Jan 2011   G06K-009/00   201120      English
   US2011182487-A1   28 Jul 2011   G06K-009/00   201151      English
   US8160313-B2   17 Apr 2012   G06K-009/00   201227      English
   EP1480157-B1   24 Sep 2014   G06K-009/00   201465      English
   EP1480157-A3   29 Mar 2006   G06K-009/00   201738      English
UT DIIDW:2004824009
ER

PT P
PN WO2003073359-A2; US2003169906-A1; AU2003219926-A1; AU2003219926-A8; WO2003073359-A3
TI Occupant face recognizing method for automobiles, involves obtaining depth distance between each region on object surface and reference, to classify object in particular category.
AB    NOVELTY - The method involves obtaining a depth distance between each region on an object (162) surface e.g. candidates face and a reference, by processing a depth image of a depth perceptive sensor (110). An identification feature of the object is determined by an identification module (140) using the measured depth distance to classify the object in a particular category.
   USE - Used for recognizing occupant face in security mechanisms of automobiles.
   ADVANTAGE - The depth distance is used by the identification module to classify the object, thereby efficiently determining if the object is an authenticated user of the automobile.
   DESCRIPTION OF DRAWING(S) - The drawing shows a system for recognizing objects.
   Electronic system (105)
   Depth perceptive sensor (110)
   Classification module (120)
   Detection module (130)
   Identification module (140)
PD WO2003073359-A2   04 Sep 2003   G06K-000/00   200368   Pages: 47   English
   US2003169906-A1   11 Sep 2003   G06K-009/00   200382      English
   AU2003219926-A1   09 Sep 2003   G06K-000/00   200428      English
   AU2003219926-A8   20 Oct 2005   G06K-009/00   200615      English
   WO2003073359-A3   06 Nov 2003   G06K-009/00   201209      English
UT DIIDW:2003721866
ER

PT P
PN WO2003060846-A2; AU2002357929-A1; US2006177109-A1; WO2003060846-A3
TI Combination casino table game imaging system used in casino has computer which associates information corresponding to recognition of face of player and wagered gaming chip based on received light by imaging device.
AB    NOVELTY - At least one imaging device (16) receives the reflected light from the face of a player (14) and the light from a respective playing position including the light from the wagered gaming chips (50). A computer receives the output information of the imaging device based on the received light to process and associate the player's face and waging chip information.
   USE - Applicable for automatic recognition of faces of players playing e.g. blackjack, and wagered chips in casino.
   ADVANTAGE - Enables automatic identification of each player playing on a casino game table during player even without requiring swipe of the magnetic identification card of the player. Enables automatic calculation of amount of bets of player since wagered gaming chips during play can be recognized by capturing image of wagered gaming chips with imaging device. Enables accurate rating of each player without requiring a casino employees to make manual estimates of a player's betting activity since movement and activity of player during play can be monitored, hence preventing casino cheats and other undesirable activities on casino game table during play. Allows identification of wanted criminals who play in casino due to image recognition capability of system during play.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for associating wagered gaming chip information to playing positions on a game table associating.
   DESCRIPTION OF DRAWING(S) - The figure shows the side view of imaging device mounted to an assembly plate located behind a transparent protection plate and under a dealer's chip tray.
   Player (14)
   Imaging device (16)
   Wagered gaming chips (50)
PD WO2003060846-A2   24 Jul 2003   G08B-000/00   200356   Pages: 28   English
   AU2002357929-A1   30 Jul 2003      200421      English
   US2006177109-A1   10 Aug 2006   G06K-009/00   200654      English
   WO2003060846-A3   16 Oct 2003   G06K-009/00   201208      English
UT DIIDW:2003598624
ER

PT P
PN WO200019895-A1; AU200014447-A; US6290654-B1; WO200019895-A9
TI Patient breath pattern detection using a sensor close to the face to monitor airflow subject to pattern recognition to monitor shallow and wide ranges of airflows for obstructive sleep apnea detection.
AB    NOVELTY - A sensor close to the face monitors airflow subject to pattern recognition to monitor shallow and wide ranges of airflows for obstructive sleep apnea detection.
   USE - Breathing pattern analysis.
   ADVANTAGE - Allows unattended studies of obstructive sleep apnea and hypopnea and/or airway obstruction.
   DETAILED DESCRIPTION - Signal from sensor (17) is digitized for analysis with a waveform extractor (36) providing a continuous-time wavelet transform of the estimated volume of air. This is used to ascertain whether a normal breathing pattern is recognized and provides a breathing pattern signal. A neural network pattern recognizer (38) operates on the breathing pattern signal to ascertain abnormal breathing and to provide a signal which is operated on to classify the abnormal breathing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the obstructive sleep apnea detection apparatus.
   Air sensor (17)
   Waveform extractor (36)
   Neural network pattern recognizer (38)
PD WO200019895-A1   13 Apr 2000   A61B-005/08   200027   Pages: 23   English
   AU200014447-A   26 Apr 2000   A61B-005/08   200036      English
   US6290654-B1   18 Sep 2001   A61B-005/08   200157      English
   WO200019895-A9   24 Aug 2000   A61B-005/08   201732      English
UT DIIDW:2000317612
ER

PT P
PN WO2016070354-A1; US2016300379-A1; CN107004287-A; EP3216008-A1; US9898849-B2; EP3216008-A4; EP3216008-B1; EP3614304-A1; CN107004287-B
TI Device for animating and rendering avatar, has processors and tongue-out detector, which is operated by one or multiple processors to detect tongue-out condition in image frame.
AB    NOVELTY - The device has the processors and a tongue-out detector, which is operated by the one or multiple processors to detect a tongue-out condition in an image frame. A mouth region detector is provided for identifying locations of multiple facial landmarks associated with identifying a mouth in the image frame. A mouth region extractor is provided to extract a mouth region from the image frame.
   USE - Device for animating and rendering an avatar.
   ADVANTAGE - The device has the processors and a tongue-out detector, which is operated by the one or multiple processors to detect a tongue-out condition in an image frame, and hence ensures that the expressions are customized according to the concept and characteristics of the avatar, and thus the avatar models are made more funny and attractive to users in a cost effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for animating and rendering an avatar.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an avatar video generation system.
   Avatar video generation system (100)
   Avatar animation-rendering engine (104)
   Video generator (106)
   Video driven facial expression engine (112)
   Video recognition facial expression engine (114)
   Text based facial expression engine (116)
PD WO2016070354-A1   12 May 2016   G06T-013/40   201633   Pages: 42   English
   US2016300379-A1   13 Oct 2016   G06T-013/40   201668      English
   CN107004287-A   01 Aug 2017   G06T-013/40   201753      Chinese
   EP3216008-A1   13 Sep 2017   G06T-013/40   201761      English
   US9898849-B2   20 Feb 2018   G06T-013/40   201814      English
   EP3216008-A4   27 Jun 2018   G06K-009/00   201843      English
   EP3216008-B1   26 Feb 2020   G06K-009/00   202018      English
   EP3614304-A1   26 Feb 2020   G06K-009/00   202018      English
   CN107004287-B   23 Oct 2020   G06T-013/40   202088      Chinese
UT DIIDW:201628392G
ER

PT P
PN CN103824049-A
TI Cascaded neural network based human face key point detection method, involves establishing training personnel face image unit, determining key point position of marked human face, and detecting human face key preliminary point.
AB    NOVELTY - The method involves establishing a training personnel face image unit. A key point position of marked human face is determined. A first layer depth neural network and a human face area estimation model are constructed. A second layer depth neural network is constructed. A human face key preliminary point is detected. A third layer depth neural network is constructed. A local area of a human face key point detector is obtained. A rotation angle of a partial area is estimated. A picture collection area is obtained according to the estimation of the rotating angle.
   USE - Cascaded neural network based human face key point detection method.
   ADVANTAGE - The method enables effectively improving the human face detection key points and achieving high performance accuracy of a particularly dense human face key point in real time.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a cascaded neural network based human face key point detection method. '(Drawing includes non-English language text)'
PD CN103824049-A   28 May 2014   G06K-009/00   201449   Pages: 8   Chinese
UT DIIDW:2014N87558
ER

PT P
PN CN102572388-A; CN102572388-B
TI Human face identification based network video monitoring device, has wireless network communication module connected with audio/video signal processing module, alarm signal output interface and clock interface module.
AB    NOVELTY - The device has an audio/video signal processing module, a storage module, a clock module and a power supply module that are connected with a CPU and an alarm module. A camera is connected with a charge coupled device (CCD) image sensor. The camera is utilized for shooting a human face image. A wireless network communication module is connected with the audio/video signal processing module, an alarm signal output interface and a clock interface module. A G711 or G726 compression standard audio signal is transmitted to a human face image decoding circuit.
   USE - Human face identification based network video monitoring device.
   ADVANTAGE - The device improves safety and video monitoring efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification based network video monitoring method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face identification based network video monitoring device.'(Drawing includes non-English language text)'
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The wireless network communication module is formed as a wireless fidelity (Wi-Fi) communication module and a third generation (3G) communication module.
PD CN102572388-A   11 Jul 2012   H04N-007/18   201302   Pages: 12   Chinese
   CN102572388-B   20 May 2015   H04N-007/18   201549      Chinese
UT DIIDW:2012K86488
ER

PT P
PN WO2011017653-A1; CA2770239-A1; AU2010279248-A1; EP2462522-A1; KR2012058539-A; CN102667763-A; JP2013501978-W; AU2010279248-B2; US2014172881-A1; JP5557911-B2; JP2014194810-A; CN104021150-A; US9208177-B2; US2016055182-A1; AU2013205924-B2; US2011038512-A1; US8670597-B2; JP5985535-B2; KR2016108832-A; KR2016108833-A; JP2016201135-A; KR1686613-B1; AU2016200659-B2; KR1760853-B1; KR1760855-B1; BR112012002823-A2; CN104021150-B; US2018322147-A1; CA2770239-C; JP6470713-B2; JP2019023923-A; US10515114-B2; BR112012002823-B1
TI Computer-implemented method for processing visual query in facial recognition search system, involves generating ordered list of persons by ranking identified persons, and sending person identifier from ordered list to requester.
AB    NOVELTY - The method involves identifying potential image matches that potentially match a respective facial image in a visual query in accordance with visual similarity criteria. Persons associated with the potential image matches are identified, and person-specific data is retrieved for each identified person. An ordered list of persons is generated by ranking the identified persons in accordance with one or more metrics of visual similarity between the respective facial image and the potential image matches, and a person identifier is sent from the ordered list to a requester.
   USE - Computer-implemented method for processing a visual query in a facial recognition search system.
   ADVANTAGE - The method allows person-specific data about the person identified from the facial image in the visual query to be provided to the requester.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a server system for processing a visual query, comprising one or more processors
   (2) a non-transitory computer readable storage medium storing one or more programs for processing a visual query.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a process of responding to a visual query including a facial image.
PD WO2011017653-A1   10 Feb 2011   G06F-017/30   201114   Pages: 83   English
   CA2770239-A1   10 Feb 2011   G06F-017/30   201226      English
   AU2010279248-A1   15 Mar 2012   G06F-017/30   201230      English
   EP2462522-A1   13 Jun 2012   G06F-017/30   201238      English
   KR2012058539-A   07 Jun 2012      201240      
   CN102667763-A   12 Sep 2012   G06F-017/30   201274      Chinese
   JP2013501978-W   17 Jan 2013   G06F-017/30   201306   Pages: 47   Japanese
   US2014172881-A1   19 Jun 2014   G06F-017/30   201441      English
   JP5557911-B2   23 Jul 2014   G06F-017/30   201449   Pages: 45   Japanese
   JP2014194810-A   09 Oct 2014   G06F-017/30   201466   Pages: 50   Japanese
   CN104021150-A   03 Sep 2014   G06F-017/30   201479      Chinese
   US9208177-B2   08 Dec 2015   G06F-007/00   201582      English
   US2016055182-A1   25 Feb 2016   G06F-017/30   201618      English
   AU2013205924-B2   24 Dec 2015   G06F-017/30   201650      English
   US2011038512-A1   17 Feb 2011   G06K-009/00   201650      English
   US8670597-B2   11 Mar 2014   G06K-009/00   201655      English
   JP5985535-B2   06 Sep 2016   G06F-017/30   201660   Pages: 44   Japanese
   KR2016108832-A   20 Sep 2016   G06F-017/30   201665      
   JP2016201135-A   01 Dec 2016   G06F-017/30   201680   Pages: 46   Japanese
   KR1686613-B1   14 Dec 2016   G06F-017/30   201701      
   AU2016200659-B2   22 Jun 2017   G06F-017/30   201747      English
   KR1760853-B1   24 Jul 2017   G06F-017/30   201755      
   BR112012002823-A2   05 Sep 2017   G06F-017/30   201775      English
   CN104021150-B   19 Dec 2017   G06F-017/30   201803      Chinese
   US2018322147-A1   08 Nov 2018   G06F-017/30   201876      English
   CA2770239-C   22 Jan 2019   G06F-016/53   201909      English
   JP6470713-B2   13 Feb 2019   G06F-016/50   201913   Pages: 44   Japanese
   JP2019023923-A   14 Feb 2019   G06F-016/00   201914   Pages: 50   Japanese
   US10515114-B2   24 Dec 2019   G06K-009/00   201999      English
   BR112012002823-B1   22 Jun 2021      202160      English
UT DIIDW:2011B51571
ER

PT P
PN WO2007083600-A1; JP2007216000-A; EP1975870-A1; TW200805175-A; KR2008086996-A; CN101371272-A; US2010226531-A1; US8107672-B2; CN101371272-B; HK1125726-A1; JP5191665-B2; JP2013101633-A; KR1363691-B1; JP5465768-B2; TW421781-B1; EP1975870-A4; EP1975870-B1
TI Makeup simulation system for makeup processing e.g. lipstick processing for face in moving image, has makeup processing module performing makeup processing for face image recognized according to predetermined tracking point.
AB    NOVELTY - An analog camera images the user and outputs a moving image file. The simulator main application has a facial recognition module recognizing the face in the moving image according to a predetermined tracking point using fast Fourier transform (FFT). A makeup processing module performs makeup processing for the recognized face. A moving image server transmits the original image, FFT image and makeup image to the display.
   USE - For makeup processing like lipstick processing, shadow processing, cheek processing, eyebrow processing, and foundation processing, for face in moving image.
   ADVANTAGE - The makeup processing for the face in the moving image is accurate and processing burden is reduced.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) makeup simulation apparatus;
   (2) makeup simulation method; and
   (3) makeup simulation program.
   DESCRIPTION OF DRAWING(S) - The figure shows the makeup processing for moving image. (Drawing includes non-English language text)
PD WO2007083600-A1   26 Jul 2007   G06T-001/00   200756   Pages: 65   Japanese
   JP2007216000-A   30 Aug 2007   A45D-044/00   200758   Pages: 36   Japanese
   EP1975870-A1   01 Oct 2008   G06T-001/00   200866      English
   TW200805175-A   16 Jan 2008   G06K-009/00   200908      Chinese
   KR2008086996-A   29 Sep 2008   G06T-011/80   200912      
   CN101371272-A   18 Feb 2009   G06T-001/00   200921      Chinese
   US2010226531-A1   09 Sep 2010   G06K-009/00   201059      English
   US8107672-B2   31 Jan 2012   G06K-009/00   201209      English
   HK1125726-A1   18 Jan 2013   G06T-000/00   201326      Chinese
   JP5191665-B2   08 May 2013   G06T-001/00   201330   Pages: 35   Japanese
   JP2013101633-A   23 May 2013   G06T-001/00   201336   Pages: 35   Japanese
   KR1363691-B1   14 Feb 2014   G06T-011/80   201416      
   JP5465768-B2   09 Apr 2014   G06T-001/00   201425   Pages: 35   Japanese
   TW421781-B1   01 Jan 2014   G06K-009/00   201426      Chinese
   EP1975870-A4   25 May 2011   G06T-001/00   201744      English
   EP1975870-B1   26 Aug 2020   G06T-001/00   202070      English
UT DIIDW:2007587577
ER

PT P
PN WO2003060814-A1; KR2003062377-A; AU2003235641-A1; US2005084137-A1; CN1618079-A; CN100350420-C; US2009041309-A1; US7715595-B2
TI Iris identification system for individual identification has iris recognition cameras that photograph irises of person using stereoscopic face information created according to photographed face images of person.
AB    NOVELTY - A recognition system receives face images of an person photographed by face recognition cameras (16,18), and creates stereoscopic face information based on the face images. Iris recognition cameras (12,14) are controlled by the recognition system to photograph the irises of the person using the created stereoscopic face information.
   USE - For individual identification.
   ADVANTAGE - Allows identification of person using stereoscopic face image without iris recognition of person.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) a method for iris recognition using stereoscopic face recognition; and
   (b) a method for individual identification using stereoscopic face recognition.
   DESCRIPTION OF DRAWING(S) - The figure is a control block diagram of a camera system for iris and face recognition.
   Iris recognition camera (12,14)
   Face recognition camera (16,18)
PD WO2003060814-A1   24 Jul 2003   G06K-009/00   200358   Pages: 35   English
   KR2003062377-A   25 Jul 2003   G06K-009/00   200381      
   AU2003235641-A1   30 Jul 2003   G06K-009/00   200421      English
   US2005084137-A1   21 Apr 2005   G06K-009/00   200531      English
   CN1618079-A   18 May 2005   G06K-009/00   200558      Chinese
   CN100350420-C   21 Nov 2007   G06K-009/00   200831      Chinese
   US2009041309-A1   12 Feb 2009   G06K-009/62   200914      English
   US7715595-B2   11 May 2010   G06K-009/00   201033      English
UT DIIDW:2003618217
ER

PT P
PN CN106529402-A; CN106529402-B
TI Convolutional neural network multi-task learning based human face attribute analysis method involves obtaining trained multi-task convolutional neural network model, and detecting face key point on input image.
AB    NOVELTY - The method involves performing single task model analysis, multiple task model training and a face attribute judging single task model analysis process. The slowest convergence rate of single task is obtained. The convolutional neural network weights are obtained. The partial data sharing and information exchange of multi-task learning are performed. The trained multi-task convolutional neural network model is obtained, for estimating age, identifying sex and for race classifications. The face key point is detected on the input image.
   USE - Convolutional neural network multi-task learning based human face attribute analysis method.
   ADVANTAGE - The human face attribute analysis is performed quickly and accurately.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the human face attribute analysis process. (Drawing includes non-English language text)
PD CN106529402-A   22 Mar 2017   G06K-009/00   201725   Pages: 14   Chinese
   CN106529402-B   28 May 2019   G06K-009/00   201941      Chinese
UT DIIDW:201721502V
ER

PT P
PN US2012139830-A1; KR2012059994-A; US9298257-B2; KR2017016901-A; KR1815995-B1
TI Apparatus for controlling avatar using expression control points, has expression tracking unit, feature point position resetting unit, and control point generating unit which generates expression control points using expression parameter.
AB    NOVELTY - The apparatus includes processor which controls one or more processor-executable units; expression tracking unit which tracks positions of feature points of user facial expression and head movement; feature point position resetting unit which resets positions of feature points of facial expression, by removing head movement from tracked positions of feature points; and control point generating unit which generates expression control points using expression parameter, based on reset positions of feature points.
   USE - Apparatus for controlling avatar using expression control points.
   ADVANTAGE - Positions of feature points are tracked without markers, where change of expressions of user may be detected, thus realistic control of avatar expression may be achieved with single camera. Head movement can be removed from positions of feature points tracked from expression of user, thus realistic control of avatar expression is robust to facial expression. The expression parameter may be generated by adjusting motion capture data collected using markers, based on positions of feature points extracted from expression of user, then expression control points are generated, thus avatar expression may be more naturally reproduced.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:
   (1) method for controlling avatar using expression control points; and non-transitory computer readable recording medium.
   DESCRIPTION OF DRAWING(S) - The drawing is a block diagram of avatar controlling method.
PD US2012139830-A1   07 Jun 2012   G06F-003/01   201244   Pages: 16   English
   KR2012059994-A   11 Jun 2012   G06T-007/00   201244      
   US9298257-B2   29 Mar 2016   G06T-013/00   201623      English
   KR2017016901-A   14 Feb 2017   G06F-003/01   201716      
   KR1815995-B1   21 Feb 2018   G06F-003/01   201817      
UT DIIDW:2012G37625
ER

PT P
PN WO2004032061-A2; US2004076313-A1; AU2003272048-A1; EP1550082-A2; US2005180613-A1; US6947579-B2; JP2006502478-W; KR2005059247-A; US2006251298-A1; US7421098-B2; US2008292147-A1; US7623687-B2; EP1550082-B1; DE60330880-E; JP4445864-B2; KR1007276-B1; WO2004032061-A3; US8155400-B2; EP1550082-A4
TI Three-dimensional data processing apparatus for facial recognition, has geodesic converter to convert data into series of geodesic distances and multi-dimensional scalar to form low dimensional Euclidean representation.
AB    NOVELTY - The apparatus has a triangulator (14) to convert the scanned data of a body into a triangulated manifold data. A geodesic converter (20) utilizes a fast marching method to convert the manifold into a series of geodesic distances between the pairs of points of data. A multi-dimensional scaler (22) forms a low dimensional Euclidean representation of the distances to give a bending invariant representation of body.
   USE - Used for processing 3-dimensional representation of a face for facial matching, that is used in computer vision, biometric and security application.
   ADVANTAGE - The 3-D approach provides face geometry information independent of viewpoint and lighting conditions. The geodesic converter of the data processor converts the data into a canonical form representation enabling efficient matching.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method of image preprocessing of 3-dimensional (3-D) topographical data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a device for gathering 3D topographical data of a body and processing the data into a canonical form representation for efficient matching.
   3D scanner (12)
   Triangulator (14)
   Preprocessor (16)
   Subsampler (18)
   Geodesic converter (20)
   Multi-dimensional scaler (22)
   Canonical form of 3D face (24)
PD WO2004032061-A2   15 Apr 2004   G06T-015/00   200432   Pages: 85   English
   US2004076313-A1   22 Apr 2004   G06K-009/00   200432      English
   AU2003272048-A1   23 Apr 2004   G06T-015/00   200465      English
   EP1550082-A2   06 Jul 2005   G06T-015/00   200544      English
   US2005180613-A1   18 Aug 2005   G06K-009/00   200555      English
   US6947579-B2   20 Sep 2005   G06K-009/00   200562      English
   JP2006502478-W   19 Jan 2006   G06T-007/00   200607   Pages: 69   Japanese
   KR2005059247-A   17 Jun 2005   G06T-017/30   200644      
   US2006251298-A1   09 Nov 2006   G06K-009/00   200674      English
   US7421098-B2   02 Sep 2008   G06K-009/00   200859      English
   US2008292147-A1   27 Nov 2008   G06K-009/00   200881      English
   US7623687-B2   24 Nov 2009   G06K-009/00   200978      English
   EP1550082-B1   06 Jan 2010   G06K-009/00   201004      English
   DE60330880-E   25 Feb 2010   G06T-015/00   201015      German
   JP4445864-B2   07 Apr 2010   G06T-007/00   201024   Pages: 66   Japanese
   KR1007276-B1   13 Jan 2011   G06T-017/00   201107      
   WO2004032061-A3   06 May 2004   G06T-015/00   201212      English
   US8155400-B2   10 Apr 2012   G06K-009/00   201225      English
   EP1550082-A4   10 Oct 2007   G06K-009/00   201747      English
UT DIIDW:2004348053
ER

PT P
PN US5612928-A
TI Scene classifier for objects in sonar images - develops selected characteristics of selected snippets and Bayesian Classifier classifies feature as man-made or natural.
AB       The object images consist of a matrix of pixels, each pixel has a known grey level. A snippet selection device selects portions of the images for classification. A scene classifier receives input from the snippet selector. The scene classifier modules are edge parameter module (41), smoothness module (42), frame parameter module (43), cuer detection module (44), highlight and shadow module (45) and texture module (46). They all receive data corresponding to the snippets (32). The cuer detection module only receives the locations of the snippets. Each module generates an output (47) to a Bayesian Classifier (48).
   The bayesian Classifier receives and uses the characteristics to classify a feature as man made or natural. The classifier output is used to mark the locations of the selected snippets on the sonar image.
   USE/ADVANTAGE -   Classifies objects as man-made or natural features, determine presence of features in images not corresponding to object, for face recognition.
PD US5612928-A   18 Mar 1997   G06K-009/00   199717   Pages: 13   English
UT DIIDW:1997192345
ER

PT P
PN WO2005001753-A1; EP1636735-A1; JP2007526542-W; US2008123908-A1; US7728959-B2; US2014241598-A1
TI Fingerprint sensor in domestic security system, has waveguide to which light beam is directed, and holographic optical element disposed at top/bottom surface of waveguide to diffract light beam to skin contact layer.
AB    NOVELTY - A skin contact layer is disposed at/near top/bottom surface of waveguide to which light beam is directed. Holographic optical element (HOE) with grating and supporting layers having similar thermal expansion coefficients or thermo-optical coefficients or both, is disposed at top/bottom surface, to diffract light beam to skin layer. Sensor array detects light reflected from interface between skin and skin layer.
   USE - For acquiring image of fingerprint, ear shape and structure, facial or hand thermogram, iris or retinal structure, palm print, foot print, toe print and hair, used in domestic security system for identifying credit card holder, driver license holder and passport holder, point-of-sales application, airport security station, customs and border crossings, police vehicle, home and office computing and entrance control sites of secure buildings.
   ADVANTAGE - Provides a compact, high resolution fingerprint sensor that reliably operates over a broad range of temperature.
   DETAILED DESCRIPTION - AN INDEPENDENT CLAIM is also included for method of acquiring image of topological features of skin surface.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the fingerprint sensor.
   active control circuit (225)
   feedback signals (391,392,393,395)
PD WO2005001753-A1   06 Jan 2005   G06K-009/00   200509   Pages: 76   English
   EP1636735-A1   22 Mar 2006   G06K-009/00   200621      English
   JP2007526542-W   13 Sep 2007   G06T-001/00   200762   Pages: 41   Japanese
   US2008123908-A1   29 May 2008   G06K-009/00   200838      English
   US7728959-B2   01 Jun 2010   G06K-009/74   201036      English
   US2014241598-A1   28 Aug 2014   G06K-009/00   201456      English
UT DIIDW:2005081774
ER

PT P
PN US2002090108-A1; US6757406-B2
TI Encoding method of digital image to permit its later identification by making corresponding change to image depending on values of each bit of N-bit data string to be embedded in image.
AB    NOVELTY - An N-bit data string to be embedded in the image and having binary values is received, where N is at least two. If the first bit of the string has a first value, a corresponding change is made to the image, but if the first bit of the string has a second value, a corresponding change to the image is not made. The change is essentially imperceptible to a human receiver of the image.
   USE - For identification/authentication coding of electronic imagery, serial data signals, emulsion film, and paper currency.
   ADVANTAGE - Enables cost-effective monitoring of unauthorized uses of material and perform quick checks. Allows identification coding to be much more robust in the face of thousands of real world degradation processes and material transformations, such as cutting and cropping of imagery.
   DETAILED DESCRIPTION - The second step is performed for the second through the Nth bits of the string. An INDEPENDENT CLAIM is also included for a decoding method of image data; Recorded medium
   DESCRIPTION OF DRAWING(S) - The figure is a simple and classic depiction of a one-dimensional digital signal which is discretized in both axes.
PD US2002090108-A1   11 Jul 2002   G06K-009/00   200272   Pages: 46   English
   US6757406-B2   29 Jun 2004   G06K-009/00   200443      English
UT DIIDW:2002673567
ER

PT P
PN JP2000259814-A; US6690814-B1; US2004042644-A1; US7127086-B2
TI Image processor for image pattern recognition apparatus, has camera provided below face of person such that it optical axis is inclined at elevated angle.
AB    NOVELTY - The view position of a video camera (9) is provided below the face of person for recognition by photography. The camera is positioned inclinedly so that horizontal optical axis of camera is at elevated angle.
   USE - For facial image pattern recognition apparatus.
   ADVANTAGE - Person's face can be recognized reliably and stably as camera is positioned suitably during photography.
   DETAILED DESCRIPTION - The differential image of input image and a predetermined image is generated. A detector detects recognition objective area with more than predetermined threshold value within the differential image. A generator converts the recognition objective area into predetermined input. A similarity calculator compares output of generator and predetermined registered information. An INDEPENDENT CLAIM is also included for image processing method.
   DESCRIPTION OF DRAWING(S) - The figure shows the component of the image processor.
   Video camera (9)
PD JP2000259814-A   22 Sep 2000   G06T-001/00   200061   Pages: 14   Japanese
   US6690814-B1   10 Feb 2004   G06K-009/00   200413      English
   US2004042644-A1   04 Mar 2004   G06K-009/00   200417      English
   US7127086-B2   24 Oct 2006   G06K-009/00   200670      English
UT DIIDW:2000633704
ER

PT P
PN CN106951867-A; CN106951867-B
TI Convolutional neural network based human face recognition method, involves calculating distance between characteristics, and determining similarity or distance ordering by face recognition result according to threshold value.
AB    NOVELTY - The method involves detecting a human face for different imaging conditions and face size by utilizing a multi-layer structure. A needed human face key point position is obtained from a given human face image by using a deep learning cascade multiple reference frame regression network. An input image is pre-processes to obtain fixed size of the human face image. A characteristic representative vector is obtained. Distance between characteristics is calculated. Similarity or distance ordering are determined by a face recognition result according to a threshold value.
   USE - Convolutional neural network (CNN) based human face recognition method.
   ADVANTAGE - The method enables increasing combination of multi-layered CNN characteristics with traditional CNN single-layer framework to deal different imaging conditions, training human face detection network with stronger robustness under monitoring environment based on a deep convolutional neural network algorithm, reducing error rate, and improving detection response speed.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a convolutional neural network based human face recognition device
   (2) a convolutional neural network based human face recognition system for an electronic device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolutional neural network based human face recognition method. '(Drawing includes non-English language text)'
PD CN106951867-A   14 Jul 2017   G06K-009/00   201761   Pages: 14   Chinese
   CN106951867-B   23 Aug 2019   G06K-009/00   201968      Chinese
UT DIIDW:201751743W
ER

PT P
PN US9607138-B1
TI Device for facilitating user authentication and verification through video analysis by electronic devices, has processor for authenticating head portion of physical person to determine distance information and red chroma color component.
AB    NOVELTY - The device has a first camera comprising a first field of view. A second camera is separated from the first camera by distance. The second camera comprises a second field of view that overlaps the first field of view. A memory comprises instructions. The first camera and the second camera respectively acquire a first image and a second image. A processor authenticates a head portion of a physical person to determine distance information and a red chroma color component corresponding to a signature representative of the physical person.
   USE - Device for facilitating user authentication and verification through video analysis by electronic devices.
   ADVANTAGE - The device analyzes features of the images to accurately predict or identify corresponding features of faces in images and provides a starting point for feature detection in a subsequent image so as to increase detection speed, so that amount of processing and power required can be reduced. The device locates the head position in the image to reduce amount of resources by performing facial recognition so as to improve accuracy of facial tracking and verify face represented in the image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for facilitating user authentication and verification through video analysis by electronic devices
   (2) a non-transitory computer readable storage medium comprising a set of instructions for facilitating user authentication and verification through video analysis by electronic devices.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for facilitating user authentication and verification through video analysis by electronic devices. '(Drawing includes non-English language text)'
   Step for performing activate authentication (902)
   Step for beginning image capture (904)
   Step for detecting presence of head image information (906)
   Step for determining chroma value of red channel (914)
   Step for determining stereo disparity signature (916)
PD US9607138-B1   28 Mar 2017   G06F-021/00   201723   Pages: 25   English
UT DIIDW:2017205881
ER

PT P
PN WO2014146258-A1; CN104995662-A; US2015379752-A1; EP2976749-A1; EP2976749-A4; US9792714-B2; CN104995662-B
TI Apparatus for managing manage avatars, has avatar module generating avatar animation data based on facial expressions, and audio module incorporating avatar animation data into audio file associated with video signal.
AB    NOVELTY - The apparatus has a recognition module (120a) for identifying facial expressions of a subject e.g. person, in a video signal. An avatar module (120b) generates avatar animation data based on the facial expressions. An audio module (120c) incorporates the avatar animation data into an audio file associated with the video signal. A communications module (120d) sends the audio file to a remote client device via a messaging application. An icon module (120e) generates an avatar icon based on the facial expressions. A list module (120f) adds the avatar icon to an icon list.
   USE - Apparatus for managing manage avatars that are utilized in virtual worlds i.e. game environments.
   ADVANTAGE - The apparatus incorporates the avatar animation data into the audio file so as to enable the avatar animation data to be transferred across a network e.g. Internet, without incurring high bandwidth costs or relying on dedicated avatar support in a remote peer i.e. remote client device. The method enables utilizing the audio file and avatar animation data to render an avatar animation that mimics the facial expressions of the subject and audible content spoken by the subject, without revealing the true identity or likeness of the subject.
   DETAILED DESCRIPTION - The facial expressions are mouth shapes. INDEPENDENT CLAIMS are also included for the following:
   (1) a computer readable storage medium comprising a set of instructions to perform a method for managing avatars
   (2) a method for managing avatars.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a client device.
   Recognition module (120a)
   Avatar module (120b)
   Audio module (120c)
   Communications module (120d)
   Icon module (120e)
   List module (120f)
PD WO2014146258-A1   25 Sep 2014   G06T-015/00   201467   Pages: 32   English
   CN104995662-A   21 Oct 2015   G06T-013/40   201575      Chinese
   US2015379752-A1   31 Dec 2015   G06T-013/40   201602      English
   EP2976749-A1   27 Jan 2016   G06T-015/00   201608      English
   EP2976749-A4   26 Oct 2016   G06T-015/00   201675      English
   US9792714-B2   17 Oct 2017   G06K-009/00   201769      English
   CN104995662-B   11 Aug 2020   G06T-013/40   202068      Chinese
UT DIIDW:2014R62687
ER

PT P
PN WO8912867-A; US4899391-A; AU8938505-A; EP381721-A; JP3501234-W; US5127532-A; CA1331524-C; EP381721-B1; DE68918261-E; EP381721-A4
TI Automatic key blank identification system - compares video image of key with images of known blanks.
AB       The key holder (10) positions an original key (1) to be duplicated horizontally along its longitudinal axis, with the key's flat sides vertical. A lensing subsystem (30) lights and forms an image of the front profile of the key. A video camera (50) converts the light image into an electrical signal which is digitised (70) and stored in a computer (90).
   The image is either saved or identified. If it is saved, it is stored both in the computer memory and on a disk (92) along with a label chosen by the user. If it is identified, it is compared with and matched to, the saved images, and the matched label output (110) to the user.
   ADVANTAGE -   Reduces error and time in identifying key blanks.
PD WO8912867-A   28 Dec 1989   G06K-009/00   199003   Pages: 20   English
   US4899391-A   06 Feb 1990      199012   Pages: 11   English
   AU8938505-A   12 Jan 1990   G06K-009/00   199013      English
   EP381721-A   16 Aug 1990      199033      English
   JP3501234-W   22 Mar 1991      199118      Japanese
   US5127532-A   07 Jul 1992      199230   Pages: 10   English
   CA1331524-C   23 Aug 1994   G06K-009/18   199435      English
   EP381721-B1   14 Sep 1994   G06K-009/00   199435   Pages: 14   English
   DE68918261-E   20 Oct 1994   G06K-009/00   199441      German
   EP381721-A4   03 Apr 1991   G06K-009/00   199515      English
UT DIIDW:1990022683
ER

PT P
PN GB2179596-A; FR2590707-A; US4795894-A; KR9006767-B1
TI Visiting card, partic. for business use - has discriminating symbols optically read with identification items and converted into electric signals for input to computer.
AB       Symbols (14,17) e.g. characters, numerals and figures of any kind are provided on the card for discriminating the bearer's identification items, such as the company name, section name or own name, from other items. The symbols are marked in the vicinity of the identification items or in the margin of the card. Additional symbols (15,17,16,18) may indicate the size and form of type face.
   Camouflage print is made on the symbols to conceal them, by the use of a coloured/colourless ink, so that they are readable only by an optical reader. The discriminating symbols optically read together with the identification items are converted into binary optical or electrical signals and input to a computer.
   ADVANTAGE -   Obtains swift and correct retrieval of data required.
PD GB2179596-A   11 Mar 1987   B42D-015/02   198710   Pages: 11   English
   FR2590707-A   29 May 1987   G09F-003/02   198728      French
   US4795894-A   03 Jan 1989   G06K-007/10   198904   Pages: 10   English
   KR9006767-B1   21 Sep 1990   G06K-009/74   201558      
UT DIIDW:1987067043
ER

PT P
PN US2005207622-A1; WO2005091639-A1; WO2005091211-A1; EP1725974-A1; EP1733560-A1; US7529411-B2; CA2568049-C; EP2378461-A2; EP2378462-A2; EP2378463-A2; EP2378464-A2; HK1163302-A0; HK1163303-A0; HK1163304-A0; HK1163305-A0; CA2559381-C; EP2378461-A3; EP2378462-A3; EP2378463-A3; EP2378464-A3
TI Method of determining best match between target profile and stored profile in multicamera environment, involves weighting generated confidence scores using information external to confidence scores to select best matching stored profile.
AB    NOVELTY - The confidence scores are generated, based on comparison between the target profile and set of stored profiles. The generated confidence scores are weighted using information external to confidence scores. A stored profile is selected as best match for target profile, based on the weighted confidence scores.
   USE - For determining best match between target profile and set of stored profile for face recognition in multicamera naturalistic environment such as surveillance system in secure office building.
   ADVANTAGE - Improves the accuracy of the object recognition system in naturalistic environment. Provides accurate facial recognition in video surveillance system.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) method of maintaining associations between profiles and objects in object recognition system;
   (2) computer readable medium storing instructions for determining best match between target profile and stored profiles; and
   (3) computer readable medium storing instructions for maintaining associations between profiles and objects.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart explaining the method of processing video data in multi-camera image recognition system.
PD US2005207622-A1   22 Sep 2005   G06K-009/00   200569   Pages: 20   English
   WO2005091639-A1   29 Sep 2005   H04N-007/18   200569      English
   WO2005091211-A1   29 Sep 2005   G06K-009/00   200572   Pages: 50   English
   EP1725974-A1   29 Nov 2006   G06K-009/00   200681      English
   EP1733560-A1   20 Dec 2006   H04N-007/18   200702      English
   US7529411-B2   05 May 2009   G06K-009/68   200932      English
   CA2568049-C   15 Feb 2011   H04N-007/18   201120      English
   EP2378461-A2   19 Oct 2011   G06K-009/00   201168      English
   EP2378464-A2   19 Oct 2011   G06K-009/00   201168      English
   HK1163302-A0   07 Sep 2012   G06K-000/00   201310      English
   HK1163303-A0   07 Sep 2012   G06K-000/00   201310      English
   CA2559381-C   13 Aug 2013   G06K-009/00   201358      English
   EP2378461-A3   12 Aug 2015   H04N-007/18   201553      English
   EP2378462-A3   12 Aug 2015   H04N-007/18   201553      English
   EP2378463-A3   12 Aug 2015   H04N-007/18   201553      English
   EP2378464-A3   19 Aug 2015   H04N-007/18   201555      English
UT DIIDW:2005674731
ER

PT P
PN US2005105803-A1; WO2005052850-A1; EP1685521-A1; JP2007513413-W; US7382903-B2
TI Image selecting method for e.g. photographic application, involves scoring image based on relative frequency of occurrence of recognized face for producing emphasis image characteristic of most frequently occurring face.
AB    NOVELTY - The method involves detecting image patterns indicative of presence of faces (304) in digital images to identify detected faces. A set of faces are recognized from the detected faces for each image. An image is scored based on a relative frequency of occurrence of a recognized face within the collection of images for producing an emphasis image characteristic of a most frequently occurring face in the images.
   USE - Used in a computer system for selecting an image for photographic application and face recognition application.
   ADVANTAGE - The method automatically selects the emphasis image from the collection of images for properly and concisely characterizing the content of the collection of the images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a data structure used in an emphasis image selection process.
   Data structures (300, 310)
   Database (301)
   Face record (302)
   Faces (304)
   Pointers (314)
PD US2005105803-A1   19 May 2005   G06K-009/62   200543   Pages: 16   English
   WO2005052850-A1   09 Jun 2005   G06K-009/00   200543      English
   EP1685521-A1   02 Aug 2006   G06K-009/00   200650      English
   JP2007513413-W   24 May 2007   G06T-001/00   200735   Pages: 21   Japanese
   US7382903-B2   03 Jun 2008   G06K-009/00   200839      English
UT DIIDW:2005423788
ER

PT P
PN US2002136435-A1; US6920236-B2
TI Biometric recognition method for ATM, involves creating composite image with encoded image and image captured by visual imager and thermal infrared camera, for identifying subject.
AB    NOVELTY - A visual imager and a thermal infrared camera are combined into a dual system using which an image of human face is generated. The anatomical characteristics of the face are determined to detect the degraded portion of the visual image and the detected portions are encoded. A composite image is created using the encoded image and image captured by the visual imager and thermal infrared camera, to identify the subject.
   USE - For personal identification in ATM, airport security, information assurance or computer network security, etc.
   ADVANTAGE - Since composite image is generated using images captured by visual imager and infrared camera, the likelihood of identifying the subject in a variety of circumstances is improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for biometric recognition system.
   DESCRIPTION OF DRAWING(S) - The figure shows the interconnection of components in a biometric identification apparatus.
PD US2002136435-A1   26 Sep 2002   G06K-009/00   200303   Pages: 27   English
   US6920236-B2   19 Jul 2005   G06K-009/00   200547      English
UT DIIDW:2003039994
ER

PT P
PN US2002050988-A1; US7065242-B2
TI Three-dimensional model construction method for digital imaging, involves revising set of vertices of projected face that fall within boundaries of silhouette contour polygons so as to create set of 3D coordinates for vertices.
AB    NOVELTY - An initial three-dimensional (3D) model of an object is created using silhouette contour polygons created from images captured at different angles. The 3D model is refined by projecting each face onto a two-dimensional plane of image collection. The set of vertices of the projected face which fall within the boundary are revised to create a set of 3D coordinates for each of the vertices.
   USE - For digital imaging, computer animation, special effects in film, prototype imaging in marketing and product development, topography, reconstructive and plastic surgery, dentistry, architecture, industrial design, anthropology, milling and object production, biology and internal medicine.
   ADVANTAGE - As 3D coordinates are determined from revised vertices after refining the initial 3D image, it is possible to create 3D models that are highly accurate in terms of spatial dimension.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) Two-dimensional image capture method;
   (2) Silhouette contour identification method;
   (3) Polygonal shaped silhouette contour creation method;
   (4) 3D model construction system;
   (5) Computer based 3D model construction system;
   (6) Computer based two-dimensional image capture system;
   (7) Computer based two-dimensional image process system;
   (8) Computer based contour identification system; and
   (9) Polygonal shaped silhouette creation system.
   DESCRIPTION OF DRAWING(S) - The figure shows an exemplary system for image capture.
PD US2002050988-A1   02 May 2002   G06T-001/00   200255   Pages: 135   English
   US7065242-B2   20 Jun 2006   G06K-009/00   200641      English
UT DIIDW:2002518047
ER

PT P
PN WO200219137-A1; US2002039447-A1; AU200182483-A; US7203367-B2; US2007230799-A1; US7376276-B2
TI Indexing method for images of persons for electronic archiving using three suitably programmed computer functionalities to provide face recognition, to group images and to provide an index of groups.
AB    NOVELTY - The method involves providing image data relating to images of persons to a suitably programmed computer. A suitably programmed computer functionality is employed to provide face recognition of the images. A second suitably programmed computer functionality is employed to group the images according to faces recognized in them.
A third suitably programmed computer functionality is employed to provide an index of groups of the images organized according to faces recognized. The index is used for retrieving images of an individual person.
   USE - For electronic archiving, indexing and retrieving images.
   ADVANTAGE - Improved system and methodology.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for a system for indexing images of persons, for a method for classifying images of persons in photographs and for a system for classifying images.
   DESCRIPTION OF DRAWING(S) - The figure shows a system and methodology for archiving images.
PD WO200219137-A1   07 Mar 2002   G06F-017/00   200237   Pages: 31   English
   US2002039447-A1   04 Apr 2002   G06K-009/64   200237      English
   AU200182483-A   13 Mar 2002   G06F-017/00   200249      English
   US7203367-B2   10 Apr 2007   G06K-009/62   200726      English
   US2007230799-A1   04 Oct 2007   G06K-009/62   200766      English
   US7376276-B2   20 May 2008   G06K-009/62   200834      English
UT DIIDW:2002339707
ER

PT P
PN US8913004-B1
TI Method for controlling computing device, involves causing display element to be in third power state and deactivating functionality input elements, based on gaze direction being determined to be directed towards third region.
AB    NOVELTY - The method involves causing the display element to be in first power state and activating input elements of device (102), based upon the gaze direction being determined to be directed towards first region. The display element is caused to be in second power state and functionality of input elements are altered, based upon gaze direction being determined to be directed towards second region. The display element is caused to be in third power state, and functionality of input elements are deactivated, based upon the gaze direction being determined to be directed towards third region.
   USE - Method for controlling computing device (claimed) such as personal computer, laptop computer, notebook, netbook, television set top box, cellular phone, personal digital assistant, electronic book reading device and video game system or portable media player.
   ADVANTAGE - By not rendering the entire display in high resolution, the device reduces the amount of processing power needed or focus the processing power on rendering that portion of the display that is noticeable to the user. The relative movement is more accurately interpreted, when the input from an accelerometer or similar element is used along with the input from the camera, and thus allows for a more precise input and/or a less complex image analysis algorithm.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a computing device; and
   (2) a non-transitory computer-readable storage medium storing program for controlling a computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating the portable device including element operable to image facial region of the user.
   Computing device (102)
   Imaging element (104)
   User (106)
   Viewable area (108)
PD US8913004-B1   16 Dec 2014   G09G-005/00   201501   Pages: 26   English
UT DIIDW:2014W33821
ER

PT P
PN US2013083207-A1; US8588527-B2
TI Interactive system for using digitally captured images to identify e.g. two-dimensional or three-dimensional objects, has processing platform causing result set related to query to be returned to interactive device.
AB    NOVELTY - The system (400) has a sensor device including a sensor for acquiring a digital representation of a scene. The representation comprises non-visual information (413). A processing platform is coupled with the sensor device to receive the representation from the device. The platform derives a query from one of the representation and the information, where the query includes key information that allows a database to access an indexing system. The platform causes a result set (434) related to the query to be returned to an interactive device upon submission of the query to the database.
   USE - Interactive system for using digitally captured images to identify two-dimensional or three-dimensional objects within the images and using the identifications to retrieve information e.g. non-visual information such as natural sound and speech (all claimed), from databases e.g. relational databases, object databases and XML databases, by using local device e.g. cell phone, digital camera and personal digital assistant (PDA). Uses include but are not limited to animated objects such as faces of people, biometric information such as fingerprint pattern on a human finger and iris of person, inanimated objects, physical objects, target object such as beverage can, a music compact disc (CD) box, a DVD video box, a magazine advertisement, a poster, a theatre, a store, a building and a car, security application target object such as person, passport and driver's license, and industrial application target object such as part in a machine, part on assembly line, box in warehouse and spacecraft in orbit.
   ADVANTAGE - The system performs decomposition of a high-resolution input image into several different types of quantifiable salient parameters so as to allow independent convergent search processes of the database to occur in parallel, thus improving image match speed and match robustness in database matching.
   DETAILED DESCRIPTION - The sensor device is selected from a group consisting of a terminal, a digital camera, a computer, a vehicle, a portable device and a mobile phone. The image sensor is a Charge-Coupled Device (CCD) or complementary metal-oxide semiconductor (CMOS) digital camera sensor. The result set comprises video data, image data, audio data, streamed data and text data. The key information comprises non-text key information such as image, sound and machine-readable information. The interactive system comprises a machine, a device local to sensor device, a TV, an assembly line, a server and a spacecraft.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of an interactive system in which a local device captures and image, where a search term automatically derived from an image is submitted to a search engine to produce a results set, and information from the results set is sent back to the device.
   Interactive system (400)
   Portable imaging device (410)
   Non-visual information (413)
   Object (415)
   Server (420)
   Result set (434)
PD US2013083207-A1   04 Apr 2013   G06K-009/78   201325   Pages: 19   English
   US8588527-B2   19 Nov 2013   G06K-009/00   201377      English
UT DIIDW:2013F15240
ER

PT P
PN US2003108241-A1; WO2003051033-A1; AU2002366670-A1; EP1459515-A1; KR2004068210-A; JP2005512248-W; CN1602620-A; US6931147-B2; CN1320806-C
TI Virtual photo album providing method, involves retrieving set of photos that matches user mood by comparing image with images stored in pattern recognition module and transmitting set to electronic photo album..
AB    NOVELTY - The method involves providing an image of a facial expression to a pattern recognition module having stored images. The mood of the user is determined (125) by comparing the image with the previously stored images, each having an associated emotional identifier to indicate mood .A set of photos that matches the user mood is retrieved and transmitted to an electronic photo album for display.
   USE - Used for reminding people about the special events or persons from the past.
   ADVANTAGE - The electronic photo album takes into account the mood of the user while he/she pages through the pictures.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for mood based virtual photo album.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart that provides an overview of a method of providing a mood based virtual photo album.
PD US2003108241-A1   12 Jun 2003   G06K-009/00   200362   Pages: 9   English
   WO2003051033-A1   19 Jun 2003   H04N-001/00   200362      English
   AU2002366670-A1   23 Jun 2003   H04N-001/00   200420      English
   EP1459515-A1   22 Sep 2004   H04N-001/00   200462      English
   KR2004068210-A   30 Jul 2004   G06T-007/00   200475      
   JP2005512248-W   28 Apr 2005   G06T-001/00   200530   Pages: 12   Japanese
   CN1602620-A   30 Mar 2005   H04N-001/00   200547      Chinese
   US6931147-B2   16 Aug 2005   G06K-009/00   200554      English
   CN1320806-C   06 Jun 2007   H04N-001/00   200808      Chinese
UT DIIDW:2003659045
ER

PT P
PN US6137896-A
TI Three dimensional facial image comparing method involves filtering transformed image to obtain correlation results substantially independent of illumination angle of face within two dimensional intensity image.
AB    NOVELTY - Filter has three orthogonal components derived from normal vectors derived from 3D facial image surface defining points. 2D intensity image other than transform of intensity image is transformed into 3D image by correlation, which is filtered to obtain correlation results independent of illumination angle of face within intensity image. When results are within specific range, match signal is output.
   USE - For comparing 3D facial image with 2D intensity image used for human facial recognition.
   ADVANTAGE - Discrimination provided by phase-only filter used in range image recognition is satisfactory. The 3D facial image comparing method is easily implemented in digital domain.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for system for performing facial recognition.
   DESCRIPTION OF DRAWING(S) - The figure shows the facial range image in the top left and its three normal components.
PD US6137896-A   24 Oct 2000   G06K-009/00   200101   Pages: 26   English
UT DIIDW:2001006250
ER

PT P
PN WO9743677-A1; US5717512-A; AU9730062-A; EP1010029-A1
TI Compact image steering and focusing apparatus e.g. for image recognition - has mirror drive motor fixed to frame on one side of tilt plane and lens motor fixed to frame on other side, with frame rotatably fixed to stand connected to frame drive motor.
AB       The imaging apparatus has a frame with end with two parallel spaced arms (32 and 33) extending from one end face, equidistant from the arms is a tilt plane. A primary mirror is pivotally attached between the arms rotating around a pan axis normal to the tilt plane. A motor (45) on one side of the tilt plane is attached to the frame and connected to the mirror to rotate it about the pan axis.
   A primary lens is located between the arms aligned with the primary mirror. A lens motor (47) is attached to the frame and to the lens to move it away from the mirror, positioned opposite to the mirror drive motor. The frame rotates about a stand transverse to the pan axis. A frame drive motor (52) in on the stand and connected to the frame to rotate the frame about the frame axis.
   USE -   Relates to apparatus for directing light reflected from object in scene through lens and into camera for use in biometrics e.g. fingerprint or signature recognition.
   ADVANTAGE -   Can obtain clear image of small region on object located from one to three feet from optical system.
PD WO9743677-A1   20 Nov 1997   G02B-017/08   199801   Pages: 40   English
   US5717512-A   10 Feb 1998   G02B-026/08   199813   Pages: 14   English
   AU9730062-A   05 Dec 1997   G02B-017/08   199814      English
   EP1010029-A1   21 Jun 2000   G02B-017/08   200033      English
UT DIIDW:1998009075
ER

PT P
PN US2015169938-A1; WO2015089436-A1; US9361510-B2; CN105981075-A; EP3080779-A1; EP3080779-A4; CN105981075-B
TI Method for detection of facial landmark points on e.g. eyes, involves computing set of final facial landmark points in first video frame by applying second transform to facial landmark points.
AB    NOVELTY - The method involves detecting a facial image in a first video frame using a face classifier. The detected facial image is normalized by applying a first transform to the detected facial image. A set of facial landmark points of the normalized facial image is detected (1050,1100) using a trained shape regressor. A set of final facial landmark points is computed in the first video frame by applying a second transform to the facial landmark points, where the second transform is an inverse transform of the first transform.
   USE - Method for detection of facial landmark points on eyes, nose, mouth or chin for facial expression recognition, facial tracking or three dimensional modeling.
   ADVANTAGE - The geometric and temporal cues are jointly used to enable the offline trained shape regressor capable of online, real-time tracking of facial landmark points, after adjusting the detected facial region with respect to the spatial layout structure of facial landmark points. The method is enabled to show favorable capacity to suppress slight jitter and temporary drift, and lay a solid base for achieving sufficiently stable animation interaction in facial land-marking applications. More stable performance is obtained when tracking landmark points on facial components such as eyes and mouth. The method is enabled to produce robust results in generic facial mapping, even in situations where the visual tracking outputs are of low dimension and noisy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a device for detection of facial landmark points; and
   (2) a machine readable medium storing program for detection of facial landmark points.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method of utilizing shape regression techniques in real time applications.
   Step for successfully detecting user face (1010)
   Step for obtaining shape bounding box (1020)
   Steps for computing transform parameters for further padded shape region (1030,1080)
   Steps for padded shape region transformation (1040,1090)
   Steps for detecting set of facial landmark points of normalized facial image using trained shape regressor (1050,1100)
PD US2015169938-A1   18 Jun 2015   G06K-009/00   201542   Pages: 20   English
   WO2015089436-A1   18 Jun 2015   G06T-007/20   201542      English
   US9361510-B2   07 Jun 2016   G06K-009/00   201638      English
   CN105981075-A   28 Sep 2016   G06T-007/20   201668      Chinese
   EP3080779-A1   19 Oct 2016   G06T-007/20   201669      English
   EP3080779-A4   27 Sep 2017   G06K-009/00   201765      English
   CN105981075-B   12 Nov 2019   G06K-009/00   201989      Chinese
UT DIIDW:201535191L
ER

PT P
PN CN103246869-A; CN103246869-B
TI Face and voice recognition based crime monitoring method, involves recording video by camera, collecting picture information, determining position of camera by global positioning system, and sending alarm information to nearby police.
AB    NOVELTY - The method involves recording video by a camera. Picture information is collected. A picture information unit is used for obtaining identification and comparison action of crime according to a behavior model. Pre-alarm information is verified by a duty police. Position of the camera is determined by a global positioning system (GPS). Alarm information is sent to a nearby police. A monitoring area is arranged with an image pickup area of the camera. Human face characteristic information of an operator is obtained. The behavior model is used for capturing human face in monitoring mode.
   USE - Face and voice recognition based crime monitoring method.
   ADVANTAGE - The method enables increasing detection rate, preventing deterioration event and reducing detection time.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a face and voice recognition based crime monitoring method.'(Drawing includes non-English language text)'
PD CN103246869-A   14 Aug 2013   G06K-009/00   201376   Pages: 14   Chinese
   CN103246869-B   06 Jul 2016   G06K-009/00   201647      Chinese
UT DIIDW:2013U71923
ER

PT P
PN CN101510257-A; CN101510257-B
TI Human faces similarity matching method, involves integrating similarity fraction between key points of human face image and matched key points of another human face image, and judging whether key points are matched.
AB    NOVELTY - The method involves picking up a human face image by a pickup unit, and extracting characteristic data of the human face image and stored multiple key points of another human face image by an extracting unit. A matched key point is searched in the key points of the latter human face image for each key point of the former human face image. A similarity fraction between the key points of the former human face image and the matched key points of the latter human face image is calculated and integrated. A determination is made whether the key points are matched.
   USE - Method for matching similarity of human faces.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for matching similarity of human faces comprising a pickup unit.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a human faces similarity matching method.'(Drawing includes non-English language text)'
PD CN101510257-A   19 Aug 2009   G06K-009/00   200957   Pages: 34   Chinese
   CN101510257-B   10 Aug 2011   G06K-009/00   201177      Chinese
UT DIIDW:2009N03766
ER

PT P
PN CN101344919-A; CN101344919-B
TI Sight tracking method for disables, involves collecting human face images reflected by infrared source, and confirming blinking action via quantity of black pixel relation for front and black frame.
AB    NOVELTY - The method involves collecting human face images reflected by an infrared source and detecting a position of a Pulcinell spot by utilizing a geometrical characteristic of the Pulcinell spot. A centre of pupils is positioned by horizontal and vertical grey projecting method after a smooth process. A position relation of the centre of pupil and a Pulcinell spot is solved via a geometrical calculation to judge a sight direction. An eye state is identified, and blinking action is confirmed via a quantity of a black pixel relation for a front and black frames.
   USE - Method for tracking a sight of disables by utilizing an assistant system (Claimed).
   ADVANTAGE - The method processes the identification for the eyes state to achieve the functions of wheelchair control and music playing control, text reading and web rolling. The method enables the assistant system to be utilized conveniently by the disables and to achieve high-precise for the disables.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an assistant system for disables utilizing a method for tracking a sight.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a method for tracking a sight.'(Drawing includes non-English language text)'
PD CN101344919-A   14 Jan 2009   G06K-009/00   200909   Pages: 15   Chinese
   CN101344919-B   22 Aug 2012   G06K-009/00   201355      Chinese
UT DIIDW:2009E31585
ER

PT P
PN US7162073-B1
TI Detecting method for spot defects in object image involves forming candidate defects in defect image, provided by applying threshold image to foreground image within foreground region, using connected component analysis.
AB    NOVELTY - A threshold image is computed for a foreground region in a foreground image which is provided by removing a background image from an object image, acquired at only a single plane of focus, within one aligned object region. The computed threshold image is applied to the foreground image within the foreground region so as to provide a defect image. Candidate defects are formed in the defect image using a connected component analysis.
   USE - For detecting defects in image of object, such as in polished end face of fiber-optic cable.
   ADVANTAGE - Detects ,spot defects on object when an allowable variation in the appearance of the object in an image can be modeled and removed. Measures and classifies detected defects.
   DETAILED DESCRIPTION - An alignment model is aligned with an object region within the acquired image so as to provide the aligned object region. The background image representing allowable and expected appearance within the aligned object region using the image and a median filter of a larger spatial scale to provide a mask for the use with a median filter of a smaller spatial scale.
   DESCRIPTION OF DRAWING(S) - The figure is the top-level flowchart of the detecting method for spot defects.
PD US7162073-B1   09 Jan 2007   G06K-009/00   200722   Pages: 23   English
UT DIIDW:2007216954
ER

PT P
PN US2003123713-A1; WO2004059573-A2; AU2003297239-A1; AU2003297239-A8; US7221809-B2; WO2004059573-A3
TI Security surveillance method used in field of security and surveillance comparing two-dimensional images for individuals in first and second groups such that two-dimensional images are independently produced from three-dimensional image.
AB    NOVELTY - The three-dimensional image data are produced for facial picture of each individual in a first group. Angularly offset two-dimensional images are produced from the three-dimensional image data. Number of two-dimensional picture images is produced for individual in a second group in a surveillance field. The two-dimensional images for individuals in the first and second groups are compared.
   USE - Used in field of security and surveillance.
   ADVANTAGE - Enables rapid and accurate face identification by using three-dimensional features e.g. length of nose, surface profile of chin and forehead, on a human face together with two-dimensional texture information. Allows customizing a generic model based on three-dimensional surface images acquired by three-dimensional cameras.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) a surveillance system;
   (b) an automatic human face recognition method;
   (c) an automatic human face recognition system; and
   (d) a computer software for generating two-dimensional images from a three-dimensional facial model.
   DESCRIPTION OF DRAWING(S) - The figure shows an automated facial recognition system.
PD US2003123713-A1   03 Jul 2003   H04N-013/02   200365   Pages: 25   English
   WO2004059573-A2   15 Jul 2004   G06T-000/00   200446      English
   AU2003297239-A1   22 Jul 2004   H04N-013/02   200476      English
   AU2003297239-A8   03 Nov 2005   G06K-009/00   200629      English
   US7221809-B2   22 May 2007   G06K-009/36   200734      English
   WO2004059573-A3   23 Dec 2004   G06K-009/00   201214      English
UT DIIDW:2003688400
ER

PT P
PN US6459974-B1; EP1262376-A1; CA2387076-A1; JP2003025953-A; KR2002091801-A; MX2002005422-A1; MX228534-B
TI Occupant classification system for airbag deployment system of vehicle, generates confidence factor representing relative accuracy of occupant type classification by expert system classifier.
AB    NOVELTY - An expert system classifier (44) applies an internal set of rules to vector of features relating to an occupant from an image, such that the occupant is identified as one of the predefined occupant type categories comprising adult, children, rear and front facing infant seat. A confidence factor (50) represents the relative accuracy of the occupant type classification by the classifier.
   USE - For airbag deployment system used in vehicles.
   ADVANTAGE - Allows users to choose the most effective features for expert system processing. Facilitates timely and accurate classification determinations, while minimizing resource requirements for the classification system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for method for associating predetermined occupant type classification to occupant image.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of rule-based occupant classifier.
   Expert system classifier (44)
   Confidence factor (50)
PD US6459974-B1   01 Oct 2002   G06F-007/00   200305   Pages: 15   English
   EP1262376-A1   04 Dec 2002   B60R-021/01   200305      English
   CA2387076-A1   30 Nov 2002   G06T-007/00   200309      English
   JP2003025953-A   29 Jan 2003   B60R-021/32   200319   Pages: 14   Japanese
   KR2002091801-A   06 Dec 2002   B60R-021/00   200324      
   MX2002005422-A1   01 Dec 2002   B60R-022/10   200373      Spanish
   MX228534-B   16 Jun 2005   G06F-007/00   200627      Spanish
UT DIIDW:2003056869
ER

PT P
PN US5129010-A
TI System for measuring three=dimensional shapes and dimensions - has TV camera imaging unit, surface feature extraction unit with slit light centre detector, coordinate look=up table and characteristic points detector.
AB       The imaging unit faces an object with at least two end portions forming a gap or flushness. A slit line formed on the object surface by a slit light projected from a slit light source at a fixed angle is imaged by a TV camera. The surface feature extraction unit contains the slit light centre detector which performs a centroid calculation of an intensity distribution of the slit line corresp. to scanning lines from the camera.
   The coordinate look-up table stores the relationship between the position of the image and XYZ coordinates calculated by triangulation, and output XYZ coordinates corresp to the detected slit light centre position. The characteristic points detector extracts a pair of points representing a shape change of the object surface based coordinate difference values and comparison of difference calculation results and standard values. The coordinates of the pair of points is detected. A relative position relationship between two characteristic points, based on coordinate differences, is calculated, thereby measuring a gap or flushness between the two end portions.
   USE/ADVANTAGE -   Measures coordinates reliably at high speed and accuracy and measures numerous points at once. For measurement of constructional dimensions of car body or gap/flushness between car parts and standard blocks or checking fixtures curve fitting.
PD US5129010-A   07 Jul 1992      199230   Pages: 24   English
UT DIIDW:1992249720
ER

PT P
PN CN106815566-A; CN106815566-B
TI Task convolutional neural network based human face searching method, involves recording set of candidate face image in candidate face image list, and obtaining face recognition search result according to candidate face image list.
AB    NOVELTY - The method involves outputting set of a candidate face images stored in a face feature database. A set of similarity scores and set of attribute features expression vectors are determined corresponding to each candidate face image in a real time candidate face image list to calculate similarity. An acquired fusion similarity score is obtained from the each candidate face image. The set of candidate face image in the candidate face image list is reordered. A face recognition search result is obtained according to the candidate face image list.
   USE - Task convolutional neural network based human face searching method.
   ADVANTAGE - The method ensures identification quality of the human face image, quickly and effectively identifying and judging user identity and satisfying human face image recognition requirements.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a task convolutional neural network based human face searching method. '(Drawing includes non-English language text)'
PD CN106815566-A   09 Jun 2017   G06K-009/00   201754   Pages: 17   Chinese
   CN106815566-B   16 Apr 2021   G06K-009/00   202141      Chinese
UT DIIDW:201740456M
ER

PT P
PN US2007005988-A1; US8079079-B2
TI Authentication system for smartphone for use as credit card for financial transaction, uses information related to face, handwriting, speech, gait, retinal scan, handprint or thumbprint of user.
AB    NOVELTY - A sensor outputs information related to face recognition, handwriting recognition, speech recognition, gait recognition, retinal scan recognition, handprint or thumbprint. An authentication component compares the output information of sensor, with predetermined user authentication data and accordingly authenticates the user.
   USE - For authentication of portable wireless device (claimed) such as cellular telephone, personal digital assistant (PDA), printer, scanner, desktop or portable computer, communication satellite and smartphone for use as credit card for financial transaction, using wireless fidelity and Bluetooth (RTM: not defined) wireless technologies.
   ADVANTAGE - The combined use of several biometric information of user avoids unauthorized authentication.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) computer-readable medium for storing authentication program; and
   (2) authentication method.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the multi-modal authentication system.
   Multi-modal authentication system (100)
   Sensed inputs (102)
PD US2007005988-A1   04 Jan 2007   H04L-009/32   200722   Pages: 24   English
   US8079079-B2   13 Dec 2011   G06F-007/04   201181      English
UT DIIDW:2007218736
ER

PT P
PN EP1134691-A2; JP2001331799-A; US2003206645-A1; US6873713-B2; EP1134691-B1; DE60133788-E; DE60133788-T2; EP1134691-A3
TI Image processor for personal authentication apparatus, has processor for computing feature pattern based on feature points extracted from object images.
AB    NOVELTY - The capture boards extract feature points from the object images, which are sensed from different directions using video cameras. A processor computes a feature pattern based on the extracted feature points.
   USE - For entrance/exit management, and access management of a computer. For personal authentication apparatus to authenticate a person based on vital information such as a facial image, fingerprints, palm prints, voice prints, signature, retina and iris image.
   ADVANTAGE - Reduces load on user upon registration and verification, thus improving verification performance. Minimizes complicated computation and processing cost without changing pattern verification process algorithm from a single direction.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for image processing method.
   DESCRIPTION OF DRAWING(S) - The figure shows a view explaining a normalization process.
PD EP1134691-A2   19 Sep 2001   G06K-009/46   200172   Pages: 23   English
   JP2001331799-A   30 Nov 2001   G06T-007/00   200202   Pages: 16   Japanese
   US2003206645-A1   06 Nov 2003   G06K-009/00   200374      English
   US6873713-B2   29 Mar 2005   G06K-009/00   200522      English
   EP1134691-B1   30 Apr 2008   G06K-009/46   200831      English
   DE60133788-E   12 Jun 2008   G06K-009/46   200841      German
   DE60133788-T2   25 Jun 2009   G06K-009/46   200942      German
   EP1134691-A3   24 Nov 2004   G06K-009/46   201731      English
UT DIIDW:2001618391
ER

PT P
PN US5659625-A
TI Facial configuration analysing method - using overlay system composed of composite of pentagons of size related by phi proportions to describe points and lines.
AB       The method uses overlays of mathematically inter-related pentagons and parts of the pentagons to establish a reference system to analyze human faces. The overlay is applied to analyze the form and proportion of faces and components of the faces. The subject face is aligned relative to an image-recording device and an image is recorded. The major anthropometric points on the image of the face are compared with the overlay system.
   The overlay system consists of line segments and points forming the features of an aesthetically ideal human face. It is created by selecting portions from regular pentagons and their radials to form the features. The size and relative size of each regular pentagon is related by the proportion. Line segments from at least one pentagon complex of the regular pentagons and its radials are selected and combined to form the features of an ideal human face. The ideal features are fixed in a transparency. The ideal face features are compared with the actual features of the subject's face to evaluate the subject's facial components.
   USE/ADVANTAGE -   To plan surgical corrections/treatments, cosmetic applications, and establish feature layout for identification purposes. For three dimensional analysis as describes lateral and frontal view of face.
PD US5659625-A   19 Aug 1997   G06K-009/00   199739   Pages: 85   English
UT DIIDW:1997424494
ER

PT P
PN US4932776-A
TI Fingerprint acquisition system for electronic analysis - has source projecting light onto deflecting end face to illuminate characteristic pattern.
AB       The fingerprint acquisition system has a fibre-optic block having an end face for detecting a fingerprint pattern. A second imaging end face projects an image of a fingerprint pattern. The fibre optic block has a bundle of optical fibres having light ray-guided cores. The optical fibres have opposite end faces terminating in and respectively forming the detecting and imaging end faces. A solid state image sensor electronically transmits the image. The solid state image sensor is bonded to the fibre optic block imaging end face. The electronically transmitted fingerprint pattern image is compared with another image and a match is determined. A light source projects light onto a fibre optic block detecting end face to transilluminate the fingerprint pattern.
   ADVANTAGE -   Rigid construction for high durability. Eliminates problem of light source reflection.
PD US4932776-A   12 Jun 1990      199031      English
UT DIIDW:1990238294
ER

PT P
PN CN106022317-A
TI Human face identification method, involves obtaining convolutional neural network model extracting face image, measuring face character vector calculating degree according to face image sample, and obtaining face image recognition result.
AB    NOVELTY - The method involves obtaining human face feature vector information, where human face feature vector information is provided with an image characteristic information and face attribute characteristic information. A convolutional neural network model extracting face image is obtained. A face character vector calculating degree is measured according to face image sample. A face image recognition result is obtained. The face attribute characteristic information is provided with age, gender and race. A face template database is established.
   USE - Human face identification method.
   ADVANTAGE - The method enables realizing human face identification process in a simple manner and increasing face recognition accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face identification method. '(Drawing includes non-English language text)'
PD CN106022317-A   12 Oct 2016   G06K-009/00   201674   Pages: 22   Chinese
UT DIIDW:2016676896
ER

PT P
PN EP1167127-A2; JP2002083287-A; KR2002002221-A; US6810135-B1; EP1167127-A3; KR492765-B1
TI Imaging system for vehicle airbag deployment system, determines presence of person's face from filtered image obtained by subtracting images generated when IR source is pulsed ON and OFF respectively.
AB    NOVELTY - The processor responsive to image signals from an infrared detector, generates initial and final images at specific time periods when an infrared source (26) is pulsed ON and OFF respectively. The initial image is subtracted from the final image to obtain a filtered image. A face recognition algorithm used by the processor determines the presence of person's face from the filtered image.
   USE - For use in vehicle airbag deployment system to identify the presence of passengers.
   ADVANTAGE - Eliminates the background interference in a human presence detection system by generating a filtered image and thus ensures effective, accurate and unique detection using face recognition algorithm.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) a human presence detection system; and
   (b) a filtered image generation method.
   DESCRIPTION OF DRAWING(S) - The figure shows a side, cut-away, plan views of a person in the passenger seat of a vehicle with the imaging system.
   Infrared source (26)
PD EP1167127-A2   02 Jan 2002   B60R-021/01   200224   Pages: 11   English
   JP2002083287-A   22 Mar 2002   G06T-001/00   200236   Pages: 11   Japanese
   KR2002002221-A   09 Jan 2002   G01V-008/00   200246      
   US6810135-B1   26 Oct 2004   G06K-009/00   200470      English
   EP1167127-A3   07 May 2003   B60R-021/01   201731      English
   KR492765-B1   07 Jun 2005   G01V-008/00   200659      
UT DIIDW:2002180932
ER

PT P
PN DE4414216-C1; GB2288680-A; GB2288680-B; US6252978-B1
TI Unauthorised use protection device for motor vehicle - uses CCD camera and object recognition to detect face of driver for comparison with stored image of authorised user.
AB       The unauthorised use protection device includes a vehicle equipment unit (9), controlled from a vehicle start blocking state to a vehicle start release state by a driver authorisation signal (12). An image comparator system detects a body region of the driver by a camera, compares the detected image with stored image, and generates the driver entitlement signal for coincidence.
   The detected body region is a region of the driver's face. The image comparator system is an object recognition system in which the camera is connected to an image processing unit for object recognition image information comparison. The camera is positioned at the side of the vehicle so that it can detect the driver's face region when the driver is seated.
   ADVANTAGE -   Allows reliable detection of entitled drives as well as being secure against manipulation.
PD DE4414216-C1   06 Apr 1995   B60R-025/00   199518   Pages: 8   German
   GB2288680-A   25 Oct 1995   B60R-025/04   199546   Pages: 22   English
   GB2288680-B   02 Jul 1997   B60R-025/04   199729      English
   US6252978-B1   26 Jun 2001   G06K-009/00   200138      English
UT DIIDW:1995132225
ER

PT P
PN US2018027307-A1; US10573048-B2
TI Method for sharing emotional reaction, involves sending landmark points to user reaction distribution service to identify facial expression of user for display through client device to another user based on landmark points.
AB    NOVELTY - The method involves initializing a camera of a client device (110) to capture frames of video of a user (112) in response to determining that the user is viewing content through the client device. A frame of the video is evaluated to identify facial features of the user. Landmark points are generated to represent the facial features within the frame. The landmark points are sent to a user reaction distribution service to identify a facial expression of the user for display through another client device to another user based on the landmark points.
   USE - Method for sharing an emotional reaction of a user by utilizing a computing device (claimed). Uses include but are not limited to camera, phone, text chatting device, laptop form factor computer, laptop, tablet, convertible tablet, palmtop device, eyeglass and wristwatch.
   ADVANTAGE - The method enables sharing emotional reactions to content to other users without having to stop watching the content and perform manual actions to input user emotional information in an effective and non-disruptive manner. The method enables utilizing a user reaction distribution service to identify facial expressions of users of client devices while viewing content and provide facial expressions to the client devices of users viewing the content.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computing device
   (2) a non-transitory machine readable medium comprising a set of instructions to perform a method for sharing an emotional reaction of a user.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a network for connecting servers and clients.
   Service (102)
   Server (104)
   Wireless local area network (106)
   Client device (110)
   User (112)
PD US2018027307-A1   25 Jan 2018   H04N-021/81   201809   Pages: 20   English
   US10573048-B2   25 Feb 2020   G06T-013/20   202017      English
UT DIIDW:201807217W
ER

PT P
PN CN104573619-A
TI Human face identification based intelligent large advertisement data analyzing method, involves obtaining human face characteristic value, and carrying out advertising information analyzing process according to obtained statistical data.
AB    NOVELTY - The method involves storing human face image information in a local database of a portable electronic device. Human face characteristic value is obtained. Judgment is made whether the Human face characteristic value is matched with threshold value. Request information is sent to a network server to obtain advertisement information. Multi-user information is collected. Clustering and analyzing processes are carried out on the collected user information to obtain statistical data. An advertising information analyzing process is carried out according to the obtained statistical data.
   USE - Human face identification based intelligent large advertisement data analyzing method.
   ADVANTAGE - The method enables improving advertisement data analyzing efficiency, and effectively performing an intelligent large advertisement data analyzing process to provide guidance for commercial activities.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification based intelligent large advertisement data analyzing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a human face identification based intelligent large advertisement data analyzing system.'(Drawing includes non-English language text)'
PD CN104573619-A   29 Apr 2015   G06K-009/00   201548   Pages: 12   Chinese
UT DIIDW:201539718E
ER

PT P
PN US7757171-B1
TI Media annotation and retrieval system for generating playlist of media item, has intelligent component recognizing positive emotional reaction received from user indicating facial recognition algorithm identified person recognized in scenes.
AB    NOVELTY - The system has a playlist generator (1040) implementing machine learning to produce a playlist (1050) of scenes e.g. crop scene, comprising metadata identifying a person. An intelligent component recognizes emotional reaction of a user (1070) based on a monitored facial expression contained in a received input. A negative emotional reaction received from the user indicates that the facial recognition algorithm that failed to identify the person recognized in the scenes. A positive emotional reaction indicates that the facial recognition algorithm identified the person recognized in the scenes.
   USE - Media annotation and retrieval system for generating a playlist of a media item to facilitate non-linear viewing. Uses include but are not limited to full-length video, video scene, still image and combination of scene and/or image.
   ADVANTAGE - The system enables the user to watch the new playlist of scenes and interact with a scene and request a longer presentation associated with the scene. The user can hear a long-forgotten familiar voice in the new playlist for user perusal, thus enabling the user to enjoy the voice.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a system for authoring and presenting media.
   Authoring and presenting media system (1000)
   Media item (1010)
   Playlist generator (1040)
   Playlist (1050)
   User (1070)
PD US7757171-B1   13 Jul 2010   G06F-003/00   201048   Pages: 30   English
UT DIIDW:2010J45003
ER

PT P
PN WO2008157792-A1; US2008317379-A1; US8213737-B2; US2015063692-A1; US2009003708-A1; US9129381-B2; US9767539-B2; US2018005068-A1; US10157325-B2; US10733472-B2; US2019213434-A1
TI Digital image processing method for detecting and correcting visual imperfections using reference image involves displaying, printing, transmitting or storing modified image or processed version of main image.
AB    NOVELTY - The digital image processing method involves correcting one or more defects or other sub-optimal characteristics in main image based on analysis of reference images, to form a modified image with enhanced main image on a portable, spatial or temporal performance based image capture device e.g. digital still camera, video camera, cell phone, computer. The defects can be motion, red eye, dust or blur defects. The modified image or processed version of main image is displayed, printed, transmitted or stored.
   USE - Digital image processing method for detecting and correcting visual imperfections using a reference image. Can also be used in motion analysis.
   ADVANTAGE - Produces enhanced main image from an original image in real time with spatial economy and performance efficiency, by correcting defects or other sub-optimal characteristics of main image based on an analysis of reference images, thus providing accurate face detection and location results without requiring operation of face detector within main image acquisition chain. Involves only one sensor and one post-processing section to process independently acquired scene images by accessing different portions of sensor array. Ensures favorable speed and memory efficiency by generating preview image by same image capture component, or by subsampling image using software of general processor or dedicated hardware, before displaying or storing the preview image.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a portable, spatial or temporal performance based image capture device e.g. digital still camera, video camera, cell phone, computer.
   DESCRIPTION OF DRAWING(S) - The drawing shows a plan view of the face, eye and/or mouth detections.
   Faces (210,2200)
   Eyes (2140,2160,2240,2260)
   Lips (2180,2280)
PD WO2008157792-A1   24 Dec 2008   G06K-009/00   200903   Pages: 104   English
   US2008317379-A1   25 Dec 2008   G06K-009/40   200915      English
   US8213737-B2   03 Jul 2012   G06K-009/40   201244      English
   US2015063692-A1   05 Mar 2015   G06T-005/00   201518      English
   US2009003708-A1   01 Jan 2009   G06K-009/46   201559      English
   US9129381-B2   08 Sep 2015   G06K-009/00   201559      English
   US9767539-B2   19 Sep 2017   H04N-005/228   201763      English
   US2018005068-A1   04 Jan 2018   G06K-009/40   201804      English
   US10157325-B2   18 Dec 2018   H04N-005/228   201901      English
   US10733472-B2   04 Aug 2020   G06K-009/40   202064      English
   US2019213434-A1   11 Jul 2019   G06K-009/40   202064      English
UT DIIDW:2009A79438
ER

PT P
PN US2007092245-A1; WO2007047719-A2; US7806604-B2; WO2007047719-A3
TI Facial detection and tracking system for use in surveillance system, has narrower field of view cameras covering subset space of wider field of view, and obtaining higher-resolution image of detected objects.
AB    NOVELTY - The system (26) has a wide field of a view (WFOV) camera (28) e.g. tri-band imaging (TBI) camera, for detecting objects located within a wider field of view. Narrower field of view (NFOV) cameras (30, 32) is adapted to cover a subset space of the wider field of view, where the NFOV camera obtains a higher-resolution image of detected objects. The wide field of view camera is operatively coupled to a computer that determines a subset space location of the individual in wider. The wide field of view camera continuously operates in a wide-angle mode.
   USE - Used for detecting and tracking face of an individual that is utilized in surveillance system and security applications such as building facility, room, hallway and security gate area, and for monitoring individual in store, hospital and museum.
   ADVANTAGE - The narrower field of view (NFOV) cameras covers the subset space of the wider field of view, and obtaining the higher-resolution image of detected objects, thus covering entire field of view without a positioning and zoom mechanism, and simultaneously acquiring information for performing facial recognition. The narrower field of view cameras is overlapped slightly to facilitate the detection of moving objects, and simultaneously detecting multiple individuals.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for detecting and tracking an object with a wide field of view.
   DESCRIPTION OF DRAWING(S) - The drawing shows a top diagrammatic view of a facial detection and tracking system.
   Facial detection and tracking system (26)
   Wide field of view (WFOV) camera (28)
   Narrower field of view (NFOV) cameras (30, 32)
   Individual (34)
   Space (36, 50)
   Individual face (56)
PD US2007092245-A1   26 Apr 2007   G03B-017/00   200751   Pages: 16   English
   WO2007047719-A2   26 Apr 2007      200751      English
   US7806604-B2   05 Oct 2010   G03B-017/00   201065      English
   WO2007047719-A3   28 Jun 2007   G06K-009/32   201225      English
UT DIIDW:2007522705
ER

PT P
PN WO2006023046-A1; US2006050933-A1; EP1766555-A1; AU2005278003-A1; IN200700079-P3; KR2007070151-A; CN101027678-A; JP2008504606-W; BR200512354-A; US7697735-B2; EP1766555-B1; DE602005022718-E; KR982197-B1; JP4610614-B2; AU2005278003-B2; CA2571643-C; AU2005278003-B8; AU2011202268-A1; CN101027678-B; AU2011202268-B2; ES2348248-T3
TI Face recognition method for identifying individual, involves calculating similarity score between single image and reference image based on facial feature comparison and iris feature comparison.
AB    NOVELTY - Facial features are located in selected single image of a face (12). Facial features, located in single image, are compared with facial features of a reference image. Iris features in single image are compared with iris characteristic in the reference image. Similarity score between single image and reference image is calculated based on facial feature comparison and iris feature comparison.
   USE - For providing biometric system with unprecented level of accuracy for identifying individual.
   ADVANTAGE - Enables efficient search of large databases using non-invasive biometric that only requires minimal cooperation by individual to be identified. Increases verification accuracy and improves general usability of biometric system due to fusion of voice and face and fingerprint and face. Prevents impostor from using still images since facial feature tracking is used to analyze internal facial movements. Enables discarding of video images that impostor might show to the biometric system since accurate landmark finding is used to determine whether facial features detected all lie in a plane or not.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face recognition apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows the flow chart of a method for recognizing a person from a single digital image using a multi-biometric approach.
   Face (12)
   Semi-orthogonal channels (20,22,24)
   Database (27)
   Fusion module (28)
PD WO2006023046-A1   02 Mar 2006   G06K-009/00   200621   Pages: 20   English
   US2006050933-A1   09 Mar 2006   G06K-009/00   200621      English
   EP1766555-A1   28 Mar 2007   G06K-009/00   200725      English
   AU2005278003-A1   31 May 2007   G06K-009/00   200763      English
   IN200700079-P3   17 Aug 2007   G06K-009/68   200780      English
   KR2007070151-A   03 Jul 2007   G06K-009/00   200805      
   CN101027678-A   29 Aug 2007   G06K-009/00   200806      Chinese
   JP2008504606-W   14 Feb 2008   G06T-007/00   200815   Pages: 13   Japanese
   BR200512354-A   13 May 2008   G06K-009/00   200836      
   US7697735-B2   13 Apr 2010   G06K-009/00   201027      English
   EP1766555-B1   04 Aug 2010   G06K-009/00   201051      English
   DE602005022718-E   16 Sep 2010   G06K-009/00   201061      German
   KR982197-B1   14 Sep 2010   G06K-009/00   201065      
   JP4610614-B2   12 Jan 2011   G06T-007/00   201106   Pages: 11   Japanese
   AU2005278003-B2   24 Feb 2011   G06K-009/00   201116      English
   CA2571643-C   01 Feb 2011   G06K-009/00   201120      English
   AU2011202268-A1   09 Jun 2011   G06K-009/00   201174      English
   CN101027678-B   21 Sep 2011   G06K-009/00   201180      Chinese
   ES2348248-T3   02 Dec 2010   G06K-009/00   201338      Spanish
UT DIIDW:2006203952
ER

PT P
PN US2005147292-A1; US7127087-B2
TI Face recognition method of person, involves employing network ensemble to identify person associated with characterized input image region and face pose of person.
AB    NOVELTY - A neural network ensemble is trained with different stages to combine the outputs of classifiers and to generate output indicating person associated with the characterized input image region and face pose of person. The network ensemble is employed to the identify the person associated with characterized input image region and face pose of person.
   USE - For recognizing face of person in input image.
   ADVANTAGE - Enables identifying the person facing direction reliably and accurately.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) face image recognizing system;
   (2) computer readable medium storing face image recognition program; and
   (3) face recognition network ensemble.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart explaining the face recognition process.
PD US2005147292-A1   07 Jul 2005   G06K-009/00   200553   Pages: 37   English
   US7127087-B2   24 Oct 2006   G06K-009/00   200670      English
UT DIIDW:2005520262
ER

PT P
PN US8861804-B1
TI Method for tagging images, involves identifying highest probability factor among probability factors associated with known persons for multiple untagged faces, and assigning untagged face to known person based on highest probability factor.
AB    NOVELTY - The method (730) involves receiving (810) a batch of images comprising images of faces. A portion of an image in the batch that corresponds to an untagged face is identified (820). A facial recognition algorithm is applied (830) to the image. The facial recognition algorithm takes input from facial recognition models. The matching data comprising possible tags for the untagged face is received. A likelihood for the possible tags to correctly match untagged face is determined. A highest probability factor is identified among probability factors associated with known persons for untagged faces.
   USE - Computer-implemented method for tagging images.
   ADVANTAGE - The highest probability factor is identified among probability factors associated with known persons for multiple untagged faces, thus automatically adjusts the enhancement settings for a cluster of similar images, and thus greatly improves accuracy of the face match suggestions presented to the user.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method for identifying people within an image using logic enhanced facial recognition.
   Method for identifying people within an image using logic enhanced facial recognition (730)
   Receiving a batch of images (810)
   Identifying portion of an image in the batch that corresponds to an untagged face (820)
   Applying facial recognition algorithm (830)
   Presenting possible matches to user (850)
PD US8861804-B1   14 Oct 2014   G06K-009/00   201470   Pages: 20   English
UT DIIDW:2014T61581
ER

PT P
PN WO2010099453-A1; US2010245585-A1; WO2010099453-A4; KR2011131247-A; EP2401865-A1; CN102439972-A; JP2012519422-W; IN201106926-P4; KR2014061472-A; JP5616367-B2; US8902315-B2; JP2015043576-A; US2015085059-A1; US2015133190-A1; KR1511193-B1; CN102439972-B; CN105717989-A; US9699281-B2; EP2401865-A4; JP2017201827-A; US9860352-B2; US2018131791-A1; JP6360601-B2; CN105717989-B; EP2401865-B1; US2021400127-A1
TI Communication apparatus, e.g., battery powered video camera telecommunications headset has processor with software and hardware components which operate any of each video camera, cellular telephone, and standalone, hands-free headset.
AB    NOVELTY - The communications apparatus has housing with attachment assembly for securely retaining housing with user's head, a power source, video camera, cellular telephone, laser range finder, and a standalone, hands-free headset (10) with microphone and earphone/speaker. A processor is coupled to the video camera, cellular telephone and standalone, hands-free headset. The processor has software and hardware components which are configured to operate any of each video camera, cellular telephone, and standalone, hands-free headset via the wired and wireless connections.
   USE - Communication apparatus, e.g., battery powered video camera telecommunications headset.
   ADVANTAGE - Allows the headset to choose among the available network protocols while supporting board and remote operation controls since the headset has built-in intelligence. Provides headset which offers hands-free device operational controls and interfaces including voice command, automated eye-motion and facial recognition for eye-camera control, light and motion sensor for day and night vision, laser rangefinder automated and digital zoom, and removable/rechargeable and rapidly swappable battery power sources for uninterrupted hands-free wireless device operation.
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective view of a single body earpiece and video camera telephone headset with a rectangular camera phone unit worn on the ear with the headset counterbalanced and held onto the ear by a removable/rechargeable earpiece battery unit.
   Hands-free headset (10)
   Earpiece user control (31)
   Start/end button (34)
   Camera phone control (50)
   Camera sensors (60)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The radio area network module is configured for operation with any of cellular, Bluetooth, Wireless Fidelity (Wi-Fi), Worldwide Interoperability for Microwave Access (WiMax), or other Global Positioning System (GPS) protocol.
PD WO2010099453-A1   02 Sep 2010   H04N-007/14   201059   Pages: 90   English
   US2010245585-A1   30 Sep 2010   H04N-005/225   201064      English
   WO2010099453-A4   25 Nov 2010   H04N-007/14   201077      English
   KR2011131247-A   06 Dec 2011      201201      
   EP2401865-A1   04 Jan 2012   H04N-007/14   201203      English
   CN102439972-A   02 May 2012   H04N-007/14   201233      Chinese
   JP2012519422-W   23 Aug 2012   H04M-001/00   201256   Pages: 61   Japanese
   IN201106926-P4   26 Apr 2013   H04N-007/14   201343      English
   KR2014061472-A   21 May 2014   H04N-007/14   201435      
   JP5616367-B2   29 Oct 2014   H04M-001/00   201472   Pages: 40   Japanese
   US8902315-B2   02 Dec 2014   H04N-005/33   201479      English
   JP2015043576-A   05 Mar 2015   H04M-001/00   201518   Pages: 41   Japanese
   US2015085059-A1   26 Mar 2015   H04N-007/14   201522      English
   US2015133190-A1   14 May 2015   H04M-001/02   201532      English
   KR1511193-B1   10 Apr 2015   H04N-007/14   201533      
   CN102439972-B   10 Feb 2016   H04N-007/14   201615      English
   CN105717989-A   29 Jun 2016   G06F-001/16   201647      Chinese
   US9699281-B2   04 Jul 2017   H04N-005/33   201747      English
   EP2401865-A4   11 Dec 2013   H04N-007/14   201771      English
   JP2017201827-A   09 Nov 2017   H04M-001/00   201775   Pages: 40   Japanese
   US9860352-B2   02 Jan 2018      201803      English
   US2018131791-A1   10 May 2018   H04M-001/02   201832      English
   JP6360601-B2   18 Jul 2018   H04M-001/00   201848   Pages: 38   Japanese
   CN105717989-B   21 Feb 2020   G06F-001/16   202019      Chinese
   EP2401865-B1   15 Jul 2020   H04N-007/14   202059      English
   US2021400127-A1   23 Dec 2021   H04M-001/02   202104      English
UT DIIDW:2010L24998
ER

PT P
PN US2009003652-A1; US7620218-B2
TI Face tracking method for use in e.g. preview video involves applying image processing to main image based on information regarding the set of candidate face regions to generate the processed version of the main image.
AB    NOVELTY - A method involves acquiring a full resolution main image and a stream of low resolution reference images. Face regions (301-302) are identified within the reference images. A relative movement is determined between the reference images. Size and location of the face regions are determined within each reference image. Concentrated face detection is applied to a main image portion in a predicted location for candidate face regions. Image processing is applied to main image based on information regarding the set of candidate face regions to generate a processed version of main image.
   USE - Method of tracking a face in a reference image stream using a digital image acquisition device. Uses include but are not limited to a preview video, a pre-exposed image, and a post-exposed image.
   ADVANTAGE - Calculation of a complete highest resolution integral image for every acquired image in an image stream is not needed so that the integral image calculations are reduced in a face tracking system. This minimizes processing overhead for face detection and tracking and allows longer classifier chains to be employed during the frame-to-frame processing interval, so provides higher quality results. The method improves the performance and/or accuracy of real-time face detection and tracking. It provides efficiency while the reduction in computation does not impact the initial detection of faces. It calculates only low resolution integral images from frame to frame. It mitigates the problems of algorithms using a single image for face detection and recognition which have lower probability of performing correctly. It improves processing requirements and/or face detection accuracy. Faster face detection can be employed to more effectively provide similar quality results to running face detection over the whole image with stricter face detection required to positively detect a face.
   DETAILED DESCRIPTION - The candidate face regions have a predicted size as a function of the determined relative movement and the size and location of the face regions within the reference images. The processed versions of the main image or their combination is displayed, stored, or transmitted. INDEPENDENT CLAIMS are included for
   (1) method of detecting faces in a reference image stream using a digital image acquisition device;
   (2) digital image acquisition device for detecting faces in an image stream
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of images processed by the apparatus.
   Face regions (301-302)
PD US2009003652-A1   01 Jan 2009   G06K-009/74   200943      English
   US7620218-B2   17 Nov 2009   G06K-009/00   200976      English
UT DIIDW:2009K92588
ER

PT P
PN EP1768058-A2; JP2007087345-A; JP2007087346-A; US2007122036-A1; KR2007034966-A; CN1940965-A; KR886407-B1; JP4799104-B2; JP4799105-B2; US8542928-B2; US2013322770-A1; EP1768058-A3; EP1768058-B1
TI Information processing apparatus for image recognition has detector to detect feature point of face from image data according to detected position based on which determination unit determines facial expression of face.
AB    NOVELTY - The apparatus has an input unit which inputs the image data of the face, and a predetermined detector detects the position of the specific section of the image of the face. The other detector whose accuracy is greater than the predetermined detector detects the feature point of the face from the image data based on the detected position. A determination unit determines the facial expression of the face based on the detected feature point.
   USE - For image recognition.
   ADVANTAGE - The face image is accurately recognized even in the environment where the sensing conditions dynamically changes due to the variations in the illumination conditions, size and shape of the recognition target object and the rotation of the recognition target object. The position detection of the specific section is robust to the variations in the detection target.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) information processing control method;
   (2) information processing control program; and
   (3) computer-readable medium storing information processing control program.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the information processing apparatus.
PD EP1768058-A2   28 Mar 2007   G06T-000/00   200742   Pages: 63   English
   JP2007087345-A   05 Apr 2007   G06T-001/00   200742   Pages: 32   Japanese
   JP2007087346-A   05 Apr 2007   G06T-001/00   200742   Pages: 32   Japanese
   US2007122036-A1   31 May 2007   G06K-009/46   200742      English
   KR2007034966-A   29 Mar 2007   G06K-009/46   200755      
   CN1940965-A   04 Apr 2007   G06K-009/00   200757      Chinese
   KR886407-B1   02 Mar 2009   G06K-009/46   200924      
   JP4799104-B2   26 Oct 2011   G06T-007/20   201170   Pages: 31   Japanese
   US8542928-B2   24 Sep 2013   G06K-009/46   201363      English
   US2013322770-A1   05 Dec 2013   G06K-009/00   201380      English
   EP1768058-A3   19 Aug 2015   G06K-009/00   201555      English
   EP1768058-B1   01 Aug 2018   G06K-009/00   201852      English
UT DIIDW:2007435736
ER

PT P
PN EP1359536-A2; US2003215115-A1; JP2004005622-A; KR2003084570-A; US7203346-B2; JP4708683-B2; EP1359536-A3; KR493169-B1
TI Face recognition apparatus e.g. linear discriminant analysis based face recognition apparatus, compares face descriptors with registered face descriptors to provided predetermined weights to each facial component image.
AB    NOVELTY - A component division unit (420) divides an input facial image of a person into facial component images with respect to which a generating unit (430) generates face descriptors. An authentication unit (440) authenticates the input facial image by comparing the face descriptors with the registered descriptors stored in a database (450) so that predetermined weights are provided to each facial component image.
   USE - Face recognition apparatus such as linear discriminant analysis (LDA), principal component analysis (PCA) and independent component analysis (ICA) based face recognition apparatuses.
   ADVANTAGE - The use of the authentication unit, preprocesses the facial image core easily so as to express the facial component image easily. Thereby providing improved face recognition apparatus with enhanced efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) face recognition method; and
   (2) computer program for face recognition.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the face recognition apparatus.
   post estimator (410)
   component division unit (420)
   face descriptor generating unit (430)
   authentication unit (440)
   database (450)
PD EP1359536-A2   05 Nov 2003   G06K-009/00   200378   Pages: 20   English
   US2003215115-A1   20 Nov 2003   G06K-009/00   200401      English
   JP2004005622-A   08 Jan 2004   G06T-007/00   200405   Pages: 15   Japanese
   KR2003084570-A   01 Nov 2003   G06K-009/00   200418      
   US7203346-B2   10 Apr 2007   G06K-009/00   200726      English
   JP4708683-B2   22 Jun 2011   G06T-007/00   201141   Pages: 17   Japanese
   EP1359536-A3   23 Mar 2005   G06K-009/00   201731      English
   KR493169-B1   02 Jun 2005   G06K-009/00   200659      
UT DIIDW:2003835874
ER

PT P
PN EP1077399-A2; JP2001125662-A; JP2001084062-A; CN1289953-A; US2004066954-A1; EP1077399-B1; US7010145-B1; DE60025442-E; DE60025442-T2; US7110574-B2; CN100380275-C; CN101231542-A; EP1077399-A3
TI Extension device for providing security function in information processing device e.g. in personal notebook computers, has biological identification acquisition unit.
AB    NOVELTY - The extension device is a card-type structure that fits in the extension bay (24) of the notebook. The device has a capacitance-based fingerprint reading unit (54) on movable tray (53). The tray is urged towards direction X2 by a spring coil when the operation button mechanism (52-1) engages notch (53-6). The user opens shutter (54-1) and places index finger on sensor (54-5) for the computer body to extract and compare fingerprint features.
   USE - For providing security function in information processing device e.g. in personal notebook computers.
   ADVANTAGE - It provides a security function, reduces unauthorized use of the information processing device without sacrificing the portability of the device. It can also be adapted to identify voice or retinal patterns, handwritings, facial images/features.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an information processing device.
   DESCRIPTION OF DRAWING(S) - The figure shows an illustrative drawing of extension device in relation to notebook personal computer.
   Extension Bay (24)
   Operation Button Mechanism (52-1)
   Movable Tray (53)
   Notch (53-6)
   Fingerprint Reading Unit (54)
   Shutter (54-1)
   Sensor (54-5)
PD EP1077399-A2   21 Feb 2001   G06F-001/00   200127   Pages: 61   English
   JP2001125662-A   11 May 2001   G06F-001/00   200133   Pages: 43   Japanese
   JP2001084062-A   30 Mar 2001   G06F-001/16   200134   Pages: 23   Japanese
   CN1289953-A   04 Apr 2001   G06F-001/16   200140      Chinese
   US2004066954-A1   08 Apr 2004   G06K-009/00   200426      English
   EP1077399-B1   11 Jan 2006   G06F-001/00   200604      English
   US7010145-B1   07 Mar 2006   G06K-009/00   200618      English
   DE60025442-E   06 Apr 2006   G06F-001/00   200625      German
   DE60025442-T2   27 Jul 2006   G06F-021/00   200649      German
   US7110574-B2   19 Sep 2006   G06K-009/00   200662      English
   CN100380275-C   09 Apr 2008   G06F-001/16   200845      Chinese
   CN101231542-A   30 Jul 2008   G06F-001/16   200858      Chinese
   EP1077399-A3   02 Oct 2002   G06F-001/00   201740      English
UT DIIDW:2001259445
ER

PT P
PN US7062073-B1
TI Interactive entertainment apparatus e.g. articulated and animated teddy bear, acquires representation of facial characteristic of animate or inanimate object in proximity to toy.
AB    NOVELTY - An acquisition device acquires representation of facial characteristic of object in proximity to toy (27), to produce a relative signal relative. A processor receives signal from device and compares it with stored data, to provide output signal indicating facial recognition of object. The toy provides entertaining interaction with multiple animate or inanimate objects, based on output signal indicating recognition.
   USE - For articulated and animated toy e.g. teddy bear, doll, robot, video game apparatus.
   ADVANTAGE - The facial expression of an individual human user can be recognized by the articulated and animated toy and responses to human user from toy can be tailored in real-time, thus maximizing the challenge and entertainment value of toy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for toy.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the interactive entertainment apparatus.
   video camera (11)
   communication cable (17)
   peaker (26)
   toy (27)
   microphone (29)
PD US7062073-B1   13 Jun 2006   G06K-009/00   200645   Pages: 17   English
UT DIIDW:2006442344
ER

PT P
PN EP1452127-A1; US2004170304-A1; JP2004261598-A; US7280678-B2; EP1452127-B1; DE60324763-E; JP4593942-B2
TI Eye pupil detection apparatus for e.g. detection of drowsiness of truck operator, detects pupils using reflections of lights which are emitted at different illumination angles and which have substantially equal intensity.
AB    NOVELTY - A first light source (103) emits first light at a first illumination angle (110) relative to the axis (107) of a detector (101) which receives reflected light. A second light source (105) emits a second light at a second illumination angle (112) relative to the axis. Pupils of a subject's eyes are detected using the difference between the reflected first and second lights received at the detector.
   USE - For detecting eye pupil in order to e.g. detect drowsiness in operator of automobile, truck, airplane and train. For lie detection, ophthalmology, and eye or facial based identification.
   ADVANTAGE - Pupils/drowsiness can be detected both in the dark and in the presence of background light at various levels, including bright light, and for both stationary and moving subjects.
   DETAILED DESCRIPTION - The second illumination angle is greater than the first illumination angle and the first light and the second light have substantially equal intensity.
   DESCRIPTION OF DRAWING(S) - The figure is a block diagram of the eye pupil detection apparatus.
   Detector (101)
   First light source (103)
   Second light source (105)
   Axis (107)
   First illumination angle (110)
   Second illumination angle (112)
PD EP1452127-A1   01 Sep 2004   A61B-003/10   200460   Pages: 24   English
   US2004170304-A1   02 Sep 2004   G06K-009/00   200460      English
   JP2004261598-A   24 Sep 2004   A61B-003/113   200463   Pages: 25   Japanese
   US7280678-B2   09 Oct 2007   G06K-009/00   200766      English
   EP1452127-B1   19 Nov 2008   A61B-003/10   200878      English
   DE60324763-E   02 Jan 2009   A61B-003/10   200914      German
   JP4593942-B2   08 Dec 2010   A61B-003/113   201082   Pages: 24   Japanese
UT DIIDW:2004617740
ER

PT P
PN US6289107-B1
TI Human extremity measuring apparatus for determining shape and size of foot, has digital camera which receives light in near infrared wavelength reflected from periphery of human extremity placed on transparent plate.
AB    NOVELTY - Optical illuminators (308a-308g) arranged around periphery of a transparent plate (302), project light in near infrared wavelength range onto the periphery of a human extremity placed on a surface of the transparent plate. A digital camera captures the light reflected from the periphery of the human extremity through the transparent plate and stores the digital representation of reflected light.
   USE - For measuring shape and size of human extremity such as foot, hand, neck, head, leg, arm or even face. Also for biometric identification purpose and to determine clothing sizes such as for gloves.
   ADVANTAGE - As light of near infrared wavelength is used for illumination, the image information of the foot is obtained accurately. As the number of optical illuminators required are less, the cost is reduced.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) Object shape information capturing system;
   (b) Human extremity image capture method
   DESCRIPTION OF DRAWING(S) - The figure shows the bottom view of foot placed on transparent heat.
   Transparent plate (302)
   Optical illuminators (308a-308g)
PD US6289107-B1   11 Sep 2001   G06K-009/00   200207   Pages: 34   English
UT DIIDW:2002054345
ER

PT P
PN CA2902093-A1; EP2993619-A1; JP2016051482-A; KR2016026791-A; AU2015218542-A1; CN105389491-A; US2018218139-A1; US10262126-B2; JP6663188-B2; JP2020091892-A; US2020293644-A1; CN105389491-B; CN111984956-A; US2020410076-A1; US2021011986-A1; AU2015218542-B2; AU2021202681-A1; JP6938697-B2; JP2021184265-A; KR2022000893-A; KR2346381-B1; WO2022066955-A1
TI Method for enrolling and authenticating user in authentication system through user mobile computing device, involves comparing authentication movement of mobile device to expected movement of mobile device.
AB    NOVELTY - The method involves capturing enrollment biometric information from first image of the user. The enrollment biometric information is stored on a memory and authentication biometric information is captured. Path parameter data from the movement detecting sensor is recorded. The authentication biometric information is compared (930) to stored enrollment biometric information and authentication movement of mobile device is compared (940) to expected movement of mobile device to determine (950) whether authentication information and movement meets predetermine similarity threshold.
   USE - Method for enrolling and authenticating user in authentication system through user mobile computing device such as camera and movement detecting sensor.
   ADVANTAGE - The authentication system benefits companies and organizations that utilize the system by reducing costs for fraudulent activities and the costs of preventing fraudulent activities. Upon detection of fraud prevents authentication and access to the website or prevent the transaction or account access. The authentication system may be used to login to multiple accounts. The burden of the user having to type in complex passwords onto a small screen of a mobile device is reduced. The accuracy of the facial recognition is improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a facial authentication system utilizing a user's mobile device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating the method of verifying authentication credential in a facial recognition authentication system.
   Step for validating whether the person is being imaged (910)
   Step for comparing enrollment device identification to authentication device identification (920)
   Step for comparing enrollment biometrics to authentication biometrics (930)
   Step for comparing enrollment movement to authentication movement (940)
   Step for determining whether authentication information sufficiently corresponds with enrollment information (950)
PD CA2902093-A1   28 Feb 2016   G06F-021/32   201623      English
   EP2993619-A1   09 Mar 2016   G06K-009/00   201623      English
   JP2016051482-A   11 Apr 2016   G06T-007/00   201626   Pages: 46   English
   KR2016026791-A   09 Mar 2016      201628      English
   AU2015218542-A1   17 Mar 2016   G06F-021/32   201629      English
   CN105389491-A   09 Mar 2016   G06F-021/32   201629      English
   US2018218139-A1   02 Aug 2018   G06F-021/32   201852      English
   US10262126-B2   16 Apr 2019   G06F-021/32   201928      English
   JP6663188-B2   11 Mar 2020   G06T-007/00   202022   Pages: 44   Japanese
   JP2020091892-A   11 Jun 2020   G06T-007/00   202048   Pages: 49   Japanese
   US2020293644-A1   17 Sep 2020   G06F-021/32   202076      English
   CN105389491-B   15 Sep 2020   G06F-021/32   202077      Chinese
   CN111984956-A   24 Nov 2020   G06F-021/32   202098      Chinese
   US2020410076-A1   31 Dec 2020   G06F-021/32   202103      English
   US2021011986-A1   14 Jan 2021   G06F-021/32   202107      English
   AU2015218542-B2   18 Feb 2021   G06F-021/32   202116      English
   AU2021202681-A1   27 May 2021   G06F-021/32   202145      English
   JP6938697-B2   22 Sep 2021   G06T-007/00   202180   Pages: 45   Japanese
   JP2021184265-A   02 Dec 2021   G06F-021/32   202101   Pages: 49   Japanese
   KR2022000893-A   04 Jan 2022   H04W-012/06   202205      
   KR2346381-B1   04 Jan 2022   H04W-012/06   202205      
   WO2022066955-A1   31 Mar 2022      202227      English
UT DIIDW:201614541B
ER

PT P
PN CN104899579-A
TI Human face identification method involves recognizing human face image based on calculated value of similarity of face image samples.
AB    NOVELTY - The method involves extracting (101) a face image to be identified and face image of sample facial feature vectors according to a preset deep learning model. The facial image to be recognized with the value of similarity of face image samples is calculated (102) based on the extracted face feature vector. The human face image is recognized (103) based on calculated value of similarity of face image samples.
   USE - Human face identification method.
   ADVANTAGE - The deep study module cam automatically extract face characteristic from a face image. The accuracy of face recognition is increased.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a human face identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the human face identification method. (Drawing includes non-English language text)
   Step for extracting a face image to be identified and face image of sample facial feature vectors according to a preset deep learning model (101)
   Step for calculating facial image to be recognized with the value of similarity of face image samples based on the extracted face feature vector (102)
   Step for recognizing human face image based on calculated value of similarity of face image samples (103)
PD CN104899579-A   09 Sep 2015   G06K-009/00   201576   Pages: 22   Chinese
UT DIIDW:2015657793
ER

PT P
PN US8965170-B1
TI Computer-implemented method involves displaying program on second device based on determination in second area according to associated time parameter of program.
AB    NOVELTY - The computer-implemented method involves receiving (702) a first signal from a first camera that is associated with a first area. A program is selected from several programs based on user identifier. A time parameter of program that corresponds to detection is associated with user identifier. A second signal is received from a second camera that is associated with a second area. The program on a second device is displayed based on the determination in the second area according to the associated time parameter of the program.
   USE - Computer-implemented method.
   ADVANTAGE - The companion application to the smart television module is operated on a mobile computing device to provide additional information about available programs to a user, to allow the user to control the smart television module. The face of the viewer is compared with a stored image or the face of the viewer is determined using a variety of facial recognition techniques. The image along with the user identifier is generated by the facial recognition module using a facial recognition algorithm, an optical character recognition algorithm. The delay configuration module provides a delay within the content to allow an effective transition from device to device.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for selecting a program for playback based on facial recognition of a viewer.
   DESCRIPTION OF DRAWING(S) - The drawings show a flowchart illustrating computer-implemented method and block diagram of computer-implemented method.
   Step for receiving signal from camera (702)
   Step for identifying face of viewer (704)
   Step for retrieving user identifier (706)
   Step for selecting program based on user identifier (708)
   Step for displaying program (710)
PD US8965170-B1   24 Feb 2015   H04N-005/78   201516   Pages: 21   English
UT DIIDW:201514288H
ER

PT P
PN US8165352-B1
TI Method for reconstructing unknown enrolled biometric target template of face recognition system (FRS), involves approximating coordinates for unknown target template and performing pseudo-inversion of affine transformation.
AB    NOVELTY - The method involves inputting known biometric templates. Match score for known biometric template is converted into dot product distances. Configuration points for dot product distances are determined. Affine transformation (30) approximating the FRS (15) is provided based upon configuration points. Coordinates for configuration point in affine transformation are identified. The coordinates for unknown enrolled biometric target template (41) are approximated, and pseudo-inversion of affine transformation is performed to reconstruct unknown enrolled biometric target template.
   USE - Method for reconstructing unknown enrolled biometric such as face image target template of FRS.
   ADVANTAGE - The reconstructed image template can be used to break-in to the FRS, so that the vulnerabilities in recognition systems can be identified and the security breaches from the recognition system vulnerabilities can be prevented.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the modeling process illustrating the process for reconstructing unknown enrolled biometric target template of FRS.
   FRS (15)
   Modeling process (20)
   Metric distance conversion (26)
   Affine transformation (30)
   Unknown enrolled biometric target template (41)
PD US8165352-B1   24 Apr 2012   G06K-009/00   201231   Pages: 25   English
UT DIIDW:2012E67244
ER

PT P
PN CN102045162-A
TI Three-modal biological features system for identifying identity of card-holders, has central service software module for restoring fingerprints, voice and facial feature data and controlling various service requests to terminal equipment.
AB    NOVELTY - The system has a terminal equipment for collecting personal identification information, static fingerprint feature images, acoustical signals and video facial images. A terminal management software module manages terminal management and registering fingerprints, faces and phonetic features. A terminal service software module compares the fingerprints, voice and facial features. A central service software module restores the fingerprints, the voice and facial feature data and controls various service requests to the terminal equipment.
   USE - Three-modal biological features system for identifying identity of card-holders.
   ADVANTAGE - The system can enable users to obtain a high-quality and high-reliability identification service to identity of card holders.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for controlling a three-modal biological features system for identifying identity of card-holders.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a three-modal biological features system.'(Drawing includes non-English language text)'
PD CN102045162-A   04 May 2011   H04L-009/32   201207   Pages: 21   Chinese
UT DIIDW:2012A81955
ER

PT P
PN US2010008547-A1; WO2010008520-A1; KR2011036934-A; EP2318979-A1; CN102165464-A; JP2011528150-W; US8213689-B2; JP2014146367-A; JP5602135-B2; KR1640268-B1; EP2318979-B1
TI Computer-implemented faces identifying method for recognizing e.g. celebrity, in video on internet, involves creating set of face models from set of face clusters, and correlating one of face models with face model database.
AB    NOVELTY - The method involves generating a set of face tracks from an input video stream, and detecting a face in the input video stream before generation of the face tracks. The face in the input video stream is tracked before generation of the face tracks. A set of key face images is selected for each face track. The face tracks are clustered to generate a set of face clusters, where each face cluster is associated with one of the key face images. A set of face models (121) is created from the face clusters. One of the face models is correlated with a face model database.
   USE - Computer-implemented method for identifying faces in a video to recognize a person e.g. popular people such as celebrity, in the video on internet.
   ADVANTAGE - The method automatically annotates large collections of video with information such as names of persons appearing in the video. The method enables usage of large text and image corpora available on the Internet, to automatically associate names and faces with minimal manual intervention, and to derive a set of face models used for robust recognition of faces in video content, so that the derived models are used for automatic recognition and annotation of video content.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for identifying faces in a video.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a system for identifying faces in a video.
   Face models (121)
   Annotated video (123)
   System interface (130)
   Connection device (131)
   Network (140)
PD US2010008547-A1   14 Jan 2010   G06K-009/00   201008   Pages: 16   English
   WO2010008520-A1   21 Jan 2010   G06K-009/00   201008      English
   KR2011036934-A   12 Apr 2011   G06T-007/00   201128      
   EP2318979-A1   11 May 2011   G06K-009/00   201131      English
   CN102165464-A   24 Aug 2011   G06K-009/00   201161      Chinese
   JP2011528150-W   10 Nov 2011   G06T-007/00   201173   Pages: 20   Japanese
   US8213689-B2   03 Jul 2012   G06K-009/00   201244      English
   JP2014146367-A   14 Aug 2014   G06F-017/30   201455   Pages: 18   Japanese
   JP5602135-B2   08 Oct 2014   G06T-007/00   201466   Pages: 19   Japanese
   KR1640268-B1   15 Jul 2016   G06T-007/00   201651      
   EP2318979-B1   06 Jun 2018   G06K-009/00   201838      English
UT DIIDW:2010A66189
ER

PT P
PN CN101452582-A; CN101452582-B
TI Three-dimensional video effect realizing method, involves recognizing face information in video image, and driving pre-set three-dimensional animation model by face information according to pre-set rule.
AB    NOVELTY - The method involves recognizing face information in a video image, and driving pre-set three-dimensional animation model by the face information according to a pre-set rule, where face information comprises face positioning information, face organ positioning information, face posture information and face expression information. The face information in the video image is recognized via intelligent recognition technique, where the three-dimensional animation model comprises a three-dimensional target model, particle animation model and an illumination model.
   USE - Method for realizing three-dimensional video effect.
   ADVANTAGE - The method drives the three-dimensional animation model by the face information via the pre-set rule according to face information obtained from the video image in real-time. The three-dimensional animation model can be changed in real-time according to the face change, so that real-time interaction between the user and three-dimensional animation model is realized.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for realizing three-dimensional video effect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a three-dimensional video effect realizing method.'(Drawing includes non-English language text)'
PD CN101452582-A   10 Jun 2009   G06T-015/70   200944   Pages: 30   Chinese
   CN101452582-B   18 Sep 2013   G06T-013/40   201378      Chinese
UT DIIDW:2009K70698
ER

PT P
PN US2004179719-A1; US7508961-B2
TI Digital image processing method for detecting human face, involves reconfiguring complementary classifier in distributed face detection system based on selected classifier parameters, and detecting face by face detection system.
AB    NOVELTY - The method involves providing a distributed face detection system with complementary classifiers. Classifier parameters are selected from different parameter generating sources, where one of which is controlled by a human input. The complementary classifiers are reconfigured in the distributed face detection system based on the selected classifier parameters, and faces are detected using the detection system.
   USE - Used in automated photography for a human face detection application e.g. face recognition, image beautification, image scene balancing, image retrieval, security surveillance and person identification, in a digital color image.
   ADVANTAGE - The complementary classifiers are reconfigured in the distributed face detection system based on the selected classifier parameters and the faces are detected using the distributed face detection system, such that the method provides rapid execution, high detection accuracy, and flexibility of detecting the faces in the digital image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a computer storage medium having instructions stored for causing a computer to perform the method of detecting faces in a digital color image
   (B) a digital image processing system for detecting faces in a digital color image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a face detection method.
PD US2004179719-A1   16 Sep 2004   G06K-009/00   200469   Pages: 44   English
   US7508961-B2   24 Mar 2009   G06K-009/00   200922      English
UT DIIDW:2004707444
ER

PT P
PN WO2003052690-A1; JP2003187229-A; EP1460580-A1; US2004197013-A1; CN1552041-A; JP2006338686-A; JP2006344236-A; JP3903783-B2; CN1912890-A; CN1912891-A; CN100367311-C; EP2017770-A2; EP1460580-B1; DE60232365-E; EP2017770-A3; US7593551-B2; CN100492397-C; CN100492398-C; JP4375570-B2; JP4375571-B2; EP2017770-B1; EP1460580-A4
TI Face meta data similarity calculation method for automatic gesture recognition system, involves retrieving face feature and reliability index from objective image to evaluate feature and similarity.
AB    NOVELTY - A face feature is extracted from an objective image by an extracting unit (121) and a reliability index is extracted by an extracting unit (122) to output face meta data. The meta data is estimated using the reliability index by a calculator (141) and accordingly the similarity of the features is calculated by a calculator (142).
   USE - For calculating the face meta data similarity in automatic face and gesture recognition system.
   ADVANTAGE - The precision of face recognition is improved and thereby practical face matching is achieved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for face meta data creation method.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of face meta data similarity calculation system. (Drawing includes non-English language text)
   extracting units (121,122)
   calculators (141,142)
PD WO2003052690-A1   26 Jun 2003   G06T-001/00   200346   Pages: 60   Japanese
   JP2003187229-A   04 Jul 2003   G06T-001/00   200353   Pages: 15   Japanese
   EP1460580-A1   22 Sep 2004   G06T-001/00   200462      English
   US2004197013-A1   07 Oct 2004   G06K-009/00   200466      English
   CN1552041-A   01 Dec 2004   G06T-001/00   200516      Chinese
   JP2006338686-A   14 Dec 2006   G06T-007/00   200701   Pages: 20   Japanese
   JP3903783-B2   11 Apr 2007   G06T-001/00   200726   Pages: 19   Japanese
   CN1912890-A   14 Feb 2007   G06K-009/00   200740      Chinese
   CN100367311-C   06 Feb 2008   G06T-001/00   200833      Chinese
   EP2017770-A2   21 Jan 2009   G06K-009/00   200908      English
   EP1460580-B1   13 May 2009   G06K-009/00   200932      English
   DE60232365-E   25 Jun 2009   G06T-001/00   200942      German
   EP2017770-A3   01 Jul 2009   G06K-009/00   200943      English
   US7593551-B2   22 Sep 2009   G06K-009/00   200962      English
   CN100492397-C   27 May 2009   G06K-009/00   200969      Chinese
   JP4375570-B2   02 Dec 2009   G06T-007/00   200979   Pages: 19   Japanese
   JP4375571-B2   02 Dec 2009   G06T-007/00   200979   Pages: 19   Japanese
   EP2017770-B1   20 Feb 2013   G06K-009/00   201314      English
   EP1460580-A4   13 Jun 2007   G06K-009/00   201747      English
UT DIIDW:2003494098
ER

PT P
PN EP1297782-A1; US2003063801-A1; JP2003187251-A; US7324668-B2
TI Method of performing beauty analysis of subject by receiving image of subject and identifying one or more skin, hair or nail conditions.
AB    NOVELTY - Information on skin, hair and nail conditions is extracted by a processor (510) from the image (508) of a subject and stored. Details of the condition may be quantified and tracked over time. The image may be an image of a subject's face and may be collected and transmitted by any appropriate mechanism, e.g. a web camera (504) communicating over the Internet (512). Conditions may be identified using image-processing techniques, e.g. by segmenting the image and/or tracking wrinkles.
   USE - Feature extraction in beauty analysis.
   ADVANTAGE - Facilitates the analysis of skin, hair and nails.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for performing a beauty analysis of a subject.
   DESCRIPTION OF DRAWING(S) - Figure 1 is a schematic view of the invention.
   Image processor (510)
   Subject image (508)
   Web camera (504)
   Internet (512)
PD EP1297782-A1   02 Apr 2003   A61B-005/00   200332   Pages: 30   English
   US2003063801-A1   03 Apr 2003   G06K-009/46   200343      English
   JP2003187251-A   04 Jul 2003   G06T-007/60   200353   Pages: 74   Japanese
   US7324668-B2   29 Jan 2008   G06K-009/00   200810      English
UT DIIDW:2003335215
ER

PT P
PN WO2017029488-A2; WO2017029488-A3; GB2543893-A; CN107924579-A; EP3335195-A2; US2019035149-A1; US10796480-B2
TI Method for generating image file of personalized three-dimensional head/body model of user, involves providing texture map generation and interpolation and generating image file of personalized three-dimensional head model of user.
AB    NOVELTY - The method involves acquiring a two-dimensional (2D) image of a user's face. Automated face 2D landmark recognition is performed based on the 2D image of the user's face. A three-dimensional (3D) face geometry reconstruction is provided using a shape prior. Texture map generation and interpolation is provided with respect to the 3D face geometry reconstruction to generate a personalized 3D head model of the user. An image file of the personalized 3D head model of the user is generated, where the 2D image of the user's face is acquired through a network communication.
   USE - Method for generating an image file e.g. 3D image file, 2D image file, animation file and personalised sticker set,of a personalized 3D head/body model of a user by using a commercial social network website and a web-app/chatbot plug-in (all claimed).
   ADVANTAGE - The method allows the user to present video or image capture with a live view of a camera feed, and a feedback mechanism for analyzing the live view and providing the user with recommendations on how to improve conditions in order to achieve a high quality end result. The method allows consumers with better experience and confidence, while shopping online, and to significantly increase a proportion of shoppers on a site using virtual fitting room technology.
   DETAILED DESCRIPTION - The 2D image of the user's face is a front image of the user's face. The 2D image of the user's face is a smartphone camera image of the user's face. INDEPENDENT CLAIMS are also included for the following:
   (1) a system for generating an image file of a personalized 3D head/body model of a user
   (2) a computer program product comprising a set of instructions for generating an image file of a personalized 3D head/body model of a user
   (3) a method for facilitating personalised body shape modelling
   (4) an end-to-end method for virtual fitting
   (5) a method for processing a photo containing multiple faces of a collection of people
   (6) a method for reconstructing a user's body shape accurately using a question and survey based user equipment (UI).
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a personalised 3D avatar virtual fitting system.
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The method involves integrating an UI with an application programming interface of a digital tape/string/ultrasonic measurement device with Bluetooth data transfer mechanism.
PD WO2017029488-A2   23 Feb 2017   G06T-015/04   201716   Pages: 128   English
   WO2017029488-A3   30 Mar 2017   G06T-015/04   201724      English
   GB2543893-A   03 May 2017   G06T-017/00   201731      English
   CN107924579-A   17 Apr 2018   G06T-015/04   201829      Chinese
   EP3335195-A2   20 Jun 2018   G06T-015/04   201841      English
   US2019035149-A1   31 Jan 2019   G06T-017/20   201909      English
   US10796480-B2   06 Oct 2020   G06T-017/00   202082      English
UT DIIDW:201713816M
ER

PT P
PN CN102799877-A
TI Human face image screening method, involves obtaining human face and face deflection angle of face image according to screening result to obtain weighted summation and maximum height of face image for human face identification.
AB    NOVELTY - The method involves obtaining a human face image from a video image. A screening result is obtained according to human face size threshold value and a human face position threshold value. Another screening result is obtained according to a preset definition threshold value. A third screening result is obtained according to a preset best angle threshold value and similarity between two face images. A human face and face deflection angle of each face image is obtained according to the third result to obtain a weighted summation and maximum height of the face image for human face identification.
   USE - Human face image screening method.
   ADVANTAGE - The method enables solving problem caused by over-low human face identification error as input image quality, effectively reducing data amount of the human face image so as to effectively screen out a front face image with high quality and improve accuracy and real-time isochronous of the human face.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face image screening system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a human face image screening method.'(Drawing includes non-English language text)'
PD CN102799877-A   28 Nov 2012   G06K-009/00   201320   Pages: 19   Chinese
UT DIIDW:2013D21320
ER

PT P
PN CN101770613-A
TI Social security identity authenticating method, involves reading face template of user, entering complain program for multi-verification, and completing offering of social security fund when audit of staffs is passed.
AB    NOVELTY - The method involves reading a face template of a user from a face template basic data base. Identity authentication is performed by using an identity authenticating module with combined function of face identification and living body detection. Offering of a social security fund is audited and completed when the authentication is succeeded. The social security fund is not offered when the authentication is failed. A complain program is entered for a multi-verification, and the offering of the social security fund is completed when audit of staffs is passed.
   USE - Method for authenticating a social security identity.
   ADVANTAGE - The method improves service quality and work efficiency of social security transaction, so as to effectively prevent the social security fund from losing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a social security identity authenticating method. '(Drawing includes non-English language text)'
PD CN101770613-A   07 Jul 2010   G06Q-010/00   201056   Pages: 16   Chinese
UT DIIDW:2010K15903
ER

PT P
PN EP1669933-A2; JP2006163871-A; US2006188144-A1; CN1787012-A; KR2006064553-A; CN101055646-A; CN101055647-A; CN100468463-C; JP4449723-B2; US7764828-B2; CN101055646-B; CN101055647-B; KR1190686-B1; EP1669933-A3
TI Image processor for generation of three dimensional model of face, comprises three dimensional model generator and extender.
AB    NOVELTY - Three dimensional model generator generates the three dimensional model of the object displayed in the image. An extender extends the lateral region of the three dimensional model along Z-direction as the direction of depth.
   USE - For generating three dimensional model of face from single two dimensional image.
   ADVANTAGE - The three dimensional model is easily obtained. A high accurate feature point is easily obtained from the image, to easily create realistic animations.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) image processing method; and
   (2) computer program comprises program code .
   DESCRIPTION OF DRAWING(S) - The figure illustrates a face model.
PD EP1669933-A2   14 Jun 2006   G06T-017/40   200645   Pages: 81   English
   JP2006163871-A   22 Jun 2006   G06T-001/00   200645   Pages: 66   Japanese
   US2006188144-A1   24 Aug 2006   G06K-009/00   200656      English
   CN1787012-A   14 Jun 2006   G06T-017/00   200665      Chinese
   KR2006064553-A   13 Jun 2006   G06T-015/70   200674      
   CN101055646-A   17 Oct 2007   G06T-017/00   200819      Chinese
   CN101055647-A   17 Oct 2007   G06T-017/00   200819      Chinese
   CN100468463-C   11 Mar 2009   G06T-017/00   200971      Chinese
   JP4449723-B2   14 Apr 2010   G06T-001/00   201027   Pages: 60   Japanese
   US7764828-B2   27 Jul 2010   G06K-009/00   201051      English
   CN101055646-B   23 Jun 2010   G06T-017/00   201058      Chinese
   CN101055647-B   23 Feb 2011   G06T-017/00   201143      Chinese
   KR1190686-B1   12 Oct 2012   G06T-013/40   201270      
   EP1669933-A3   13 Sep 2006   G06T-017/40   201739      English
UT DIIDW:2006435229
ER

PT P
PN CN104021350-A; EP2945098-A1; US2015332439-A1; WO2015172521-A1; KR2015141122-A; CN104021350-B; KR1657231-B1; RU2015100255-A; JP2016532351-W; RU2602985-C2; IN201402856-P2; JP6085721-B2; BR112015000622-A2; EP2945098-B1; MX359781-B; MX2015000193-A1
TI Image processing privacy information hiding method for terminal, involves analyzing privacy information according to message type for picture in hidden process, and determining whether privacy information is text information.
AB    NOVELTY - The method involves analyzing privacy information according to message type for a picture in a hidden process. Determination is made whether the privacy information is text information according to preset of regular expression identification information. Information type is determined corresponding to a regular expression according to a semantic context analysis. Human face information is obtained from a preset human face information library according to the message type. Application program information of the picture is obtained.
   USE - Image processing privacy information hiding method for a terminal (all claimed).
   ADVANTAGE - The method enables solving privacy information hidden problem and improving privacy information hidden effect.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image processing privacy information hiding device
   (2) a terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an image processing privacy information hiding method.'(Drawing includes non-English language text)'
PD CN104021350-A   03 Sep 2014   G06F-021/60   201479   Pages: 19   Chinese
   EP2945098-A1   18 Nov 2015   G06K-009/00   201576      English
   US2015332439-A1   19 Nov 2015   G06T-005/00   201576      English
   WO2015172521-A1   19 Nov 2015   G06F-021/60   201576      Chinese
   KR2015141122-A   17 Dec 2015   G06T-011/40   201602      English
   CN104021350-B   06 Jul 2016   G06F-021/60   201647      Chinese
   KR1657231-B1   13 Sep 2016   G06T-011/40   201663      
   RU2015100255-A   27 Jul 2016   G06K-009/72   201663      Russian
   JP2016532351-W   13 Oct 2016   H04N-001/387   201668   Pages: 29   Japanese
   RU2602985-C2   20 Nov 2016   G06K-009/72   201676      Russian
   IN201402856-P2   26 Aug 2016   G06F-021/60   201702      English
   JP6085721-B2   22 Feb 2017   H04N-001/387   201716   Pages: 21   Japanese
   BR112015000622-A2   27 Jun 2017   G06F-021/60   201755      English
   EP2945098-B1   07 Nov 2018   G06K-009/00   201875      English
   MX359781-B   09 Oct 2018   G06K-009/72   201881      Spanish
   MX2015000193-A1   03 Mar 2016   G06F-021/60   202049      Spanish
UT DIIDW:2014V31416
ER

PT P
PN CN102446327-A; CN102446327-B
TI Human face identification based advertising putting device, has display module switching unit receiving advertisement information, character identifying sub unit calculating image information, and image pickup module obtaining image.
AB    NOVELTY - The device has an image pickup module obtaining an image and converting digital image information to an image identification module. An advertisement switch module is utilized for storing advertisement and receiving the image identifying module to send the digital image information. The advertisement display module is utilized for displaying the advertisement switching module that transmits advertisement information. A display module switching unit receives the advertisement information. A character identifying sub unit calculates character image information.
   USE - Human face identification based advertising putting device.
   ADVANTAGE - The device ensures high advertisement value so as to provide better advertisement effect based on human face identification.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an advertisement putting method based on human face identification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face identification based advertising putting device.
PD CN102446327-A   09 May 2012   G06Q-030/02   201236   Pages: 12   Chinese
   CN102446327-B   23 Mar 2016   G06Q-030/02   201623      English
UT DIIDW:2012F98078
ER

PT P
PN CN102436715-A; CN102436715-B
TI Fatigue driving detecting method, involves identifying identity of driver with video images by using human face recognition algorithm, and locating human face to obtain human face rectangular area.
AB    NOVELTY - The method involves identifying identity of a driver with video images by using a human face recognition algorithm. Haar feature human face identification process is utilized for obtaining human face position information in a video image locating human face to locate an eye and a mouth region to obtain a head. A human face is located to obtain a human face rectangular area. A mouth area is determined according to a grey value.
   USE - Fatigue driving detecting method.
   ADVANTAGE - The method enables improving accuracy and feasibility of judging fatigue driving and intelligently and practicably managing fatigue driving.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a fatigue driving detecting method. '(Drawing includes non-English language text)'
PD CN102436715-A   02 May 2012   G08B-021/06   201234   Pages: 10   Chinese
   CN102436715-B   11 Dec 2013   G08B-021/06   201411      Chinese
UT DIIDW:2012F55489
ER

PT P
PN WO2012020760-A1; EP2604180-A1; US2013188834-A1; JP2012528685-X; US9135708-B2; JP5915981-B2; EP2604180-A4
TI Gaze point detection method for detecting gaze point on monitoring screen of information processing terminal, involves detecting gaze point of subject by calculating intersection of eyes on display apparatus using corrected function.
AB    NOVELTY - The method involves generating subject's (A) face image as bright and dark pupil images. The vector is calculated from corneal-reflex point of subject on plane perpendicular to reference line. The angle of subject's eyes with respect to reference line of cameras (2a,2b) is calculated using function based on vector. The function is corrected so that calculated direction of eyes corresponding to cameras is near. The gaze point (Q) of subject is detected by calculating intersection of eyes on display apparatus (8) using corrected function.
   USE - Gaze point detection method for detecting gaze point on monitoring screen of information processing terminal such as personal computer for monitoring of driver, investigation of degree of interest of goods.
   ADVANTAGE - The automatic correction regarding the function for computing gaze direction can be performed. Thus the highly accurate gaze point detection can be achieved. The burden with respect to the test subject can be reduced. The angle of the eyes can be calculated correctly. The precision of the gaze point detection can be improved. The correction process of function can be simplified.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for gaze point detection apparatus.
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective view of the gaze point detection apparatus.
   Gaze point detection apparatus (1)
   Cameras (2a,2b)
   Display apparatus (8)
   Subject (A)
   Gaze point (Q)
PD WO2012020760-A1   16 Feb 2012   A61B-003/113   201214      Japanese
   EP2604180-A1   19 Jun 2013   A61B-003/113   201340      English
   US2013188834-A1   25 Jul 2013   G06T-007/00   201349      English
   JP2012528685-X   28 Oct 2013   A61B-003/113   201370   Pages: 17   Japanese
   US9135708-B2   15 Sep 2015   G06K-009/00   201561      English
   JP5915981-B2   11 May 2016   A61B-003/113   201633   Pages: 15   English
   EP2604180-A4   05 Jul 2017   A61B-003/113   201747      English
UT DIIDW:2012C11094
ER

PT P
PN US2010328492-A1; WO2011008236-A1; US8154615-B2; EP2449514-A1; CN102473264-A; JP2012532366-W; JP5619156-B2; CN102473264-B
TI Image display mechanism of image display device, has image control mechanism which provides response output according to one of viewers facially expressed and behavioral response to currently displayed image.
AB    NOVELTY - The image display mechanism (12C) has a display control mechanism (14) which selects one image from an image library for display by an image display device. An image control mechanism (16) determines a viewers attentiveness to a currently displayed image and provides a response output according to one of the viewers facially expressed and behavioral response to a currently displayed image.
   USE - Image display mechanism of image display device (claimed). Uses include but are not limited to image display device such as electronic frame, personal computer or computer/terminal, flat panel display or display on personal portable device such as cell phone and personal digital assistant (PDA).
   ADVANTAGE - The facial expression recognition can be utilized together with viewers fixation analysis to more precisely determine viewing intent and emotional reaction of the viewer effectively. The viewer profile of composite three dimensional models is important tool for pose estimation that enables refined three dimensional models and improved viewer identification.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for method for selecting images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic diagram illustrating image display control mechanism.
   Image display mechanism (12C)
   Display control mechanism (14)
   Image control mechanism (16)
   Viewer present output (20A)
   Viewer identification output (20C)
PD US2010328492-A1   30 Dec 2010   H04N-005/76   201106   Pages: 16   English
   WO2011008236-A1   20 Jan 2011   G06Q-030/00   201107      English
   US8154615-B2   10 Apr 2012   H04N-005/228   201225      English
   EP2449514-A1   09 May 2012   G06Q-030/00   201231      English
   CN102473264-A   23 May 2012   G06Q-030/02   201241      Chinese
   JP2012532366-W   13 Dec 2012   G06F-017/30   201282   Pages: 27   Japanese
   JP5619156-B2   05 Nov 2014   G06F-017/30   201475   Pages: 23   Japanese
   CN102473264-B   20 Apr 2016   G06Q-030/02   201630      English
UT DIIDW:2011A11366
ER

PT P
PN CN101404107-A
TI Internet bar monitoring alarm system for public security department i.e. government administration section, has human face recognition host computer for transmitting corresponding alarm information to alarm management server.
AB    NOVELTY - The system has a monitoring vidicon e.g. digital camera, placed in an internet bar. The monitoring vidicon is connected to an alarm management server through a video signal processing module. The monitoring vidicon collects a human face video image, and transmits the collected human face video image to the video signal processing module for performing video image process. A human face recognition host computer transmits the corresponding alarm information to the alarm management server for sending an alarm through a computer network.
   USE - Internet bar monitoring alarm system for a public security department i.e. government administration section (claimed).
   ADVANTAGE - The system automatically judges whether a person is to be looked, and automatically alarms. The system is safe, rapid, stable and reliable in working performance. The system improves the intelligent degree of the internet bar in simple manner. The system provides the intimation for solving a case for the public security department, thus improving the efficiency for handling the public security department.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an internet bar monitoring alarm system.'(Drawing includes non-English language text)'
PD CN101404107-A   08 Apr 2009   G08B-021/00   200930   Pages: 11   Chinese
UT DIIDW:2009H27187
ER

PT P
PN WO2009022631-A1; JP2009064423-A; EP2178045-A1; KR2010047863-A; CN101779218-A; US2012044335-A1; JP2012113747-A; JP5261586-B2; CN101779218-B; HK1141614-A1
TI Makeup simulation system performs predetermined makeup with respect to user's face contained in dynamic image, based on predetermined tracking point from dynamic image.
AB    NOVELTY - A recognition unit (33) recognizes a user's face based on a predetermined tracking point from dynamic image of user's face. A makeup processing unit (35) performs predetermined makeup with respect to the user's face contained in the dynamic image, based on the tracking point, and outputs the image of makeup face to a display unit. A lipstick processing unit (39) performs lipstick processing with respect to the user's face contained in the dynamic image, based on the position of peaks and troughs of upper lip contained in the tracking point.
   USE - Makeup simulation system.
   ADVANTAGE - Makeup with respect to user's face contained in dynamic image can be performed correctly, without increasing the processing burden.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) makeup simulation apparatus;
   (2) makeup simulation method; and
   (3) makeup simulation program.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the makeup simulation system. (Drawing includes non-English language text)
   Shared memory (27)
   Application interface (29)
   Recognition unit (33)
   Makeup processing unit (35)
   Lipstick processing unit (39)
PD WO2009022631-A1   19 Feb 2009   G06T-001/00   200916   Pages: 126   Japanese
   JP2009064423-A   26 Mar 2009   G06T-001/00   200922   Pages: 63   Japanese
   EP2178045-A1   21 Apr 2010   G06T-001/00   201029      English
   KR2010047863-A   10 May 2010   G06T-011/80   201036      
   CN101779218-A   14 Jul 2010   G06T-001/00   201057      Chinese
   US2012044335-A1   23 Feb 2012   H04N-007/18   201215      English
   JP2012113747-A   14 Jun 2012   G06T-001/00   201239   Pages: 68   Japanese
   JP5261586-B2   14 Aug 2013   G06T-001/00   201353   Pages: 65   Japanese
   CN101779218-B   26 Jun 2013   G06T-001/00   201370      Chinese
   HK1141614-A1   11 Oct 2013   G06T-000/00   201376      Chinese
UT DIIDW:2009F13860
ER

PT P
PN CN101339607-A; CN101339607-B
TI Face recognition method, involves inputting features into face recognition model to recognize face, and returning similarity data of each recognition model, and outputting final face recognition result according to returned similarity data.
AB    NOVELTY - The method involves training and acquiring a face recognition model for each user. A face image with and without structural light code is acquired in same time in a setting face gesture range. A face depth image is acquired according to the face image with structural light code. A face grey image is acquired according to the face image without structural light code. Features are input into the recognition model to recognize a face, and similarity data of each recognition model is returned. A final face recognition result is output according to the returned similarity data.
   USE - Face recognition method.
   ADVANTAGE - The method allows decreasing interference of illumination with low cost.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a face recognition model training method
   (2) a face recognition system including a training module.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a face recognition system.'(Drawing includes non-English language text)'
PD CN101339607-A   07 Jan 2009   G06K-009/00   200909   Pages: 47   Chinese
   CN101339607-B   01 Aug 2012   G06K-009/00   201275      Chinese
UT DIIDW:2009E30338
ER

PT P
PN CN101059836-A; CN100452081-C
TI Human eye location and state recognition method, involves setting initial threshold binary pictures, filtering black patch based on eye classifier, and finding optimal projection direction of skin and lip color by fisher linear classifier.
AB    NOVELTY - The method involves setting initial threshold binary pictures based on Otsu method. A black patch is filtered by a detected image based on an eye classifier. An optimal projection direction of a skin and lip color is found by a fisher linear classifier. A symmetry axis passes through center of mouth forming an isosceles triangle with a connecting line of center of the two eyes to locate the symmetry axis of the human face. A binary image is formed, after locating. One of the eyes is segmented and state of the eye is judged based on a black pixel character of the patch of the eyes.
   USE - Method for locating human eye and recognizing state in complicated conditions such as different setting, illumination, turning and deflecting angle, and facial detail.
   ADVANTAGE - The method provides excellent real-time performance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a method of human eye location and state recognition.`(Drawing includes non-English language text)`
PD CN101059836-A   24 Oct 2007   G06K-009/00   200819   Pages: 16   Chinese
   CN100452081-C   14 Jan 2009   G06K-009/00   200966      Chinese
UT DIIDW:2008C51003
ER

PT P
PN WO2007061632-A2; CN101466998-A; US2009238449-A1; US7929751-B2; WO2007061632-A3; CN101466998-B
TI Method for determining absolute coordinates of three-dimensional surface of real object, involves capturing images of reflections of patterns from object surface and generating relative unwrapped phase image from combined images.
AB    NOVELTY - The two phase-shifted illumination patterns each encoding a marker are projected onto the object surface and images of reflections are captured from the object surface. The captured images are combined to generate a phase-wrapped image. A relative unwrapped phase image is generated by unwrapping the phase-wrapped image. An absolute phase image is generated from the relative unwrapped phase image. The absolute coordinates of the object surface are determined from the absolute phase image.
   USE - For determining absolute coordinates of three-dimensional (3D) surface of real object for film-making application, facial recognition application and image generation and related computer graphics application.
   ADVANTAGE - Eliminates the elaborate, cumbersome and time-consuming setup process and eliminates the sensors etc. Ensures the representation of three-dimensional geometric surfaces in real time efficiently and robustly.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) apparatus for determining absolute coordinates of three-dimensional surface of real object;
   (2) method of capturing image of three-dimensional object;
   (3) method of generating representation of virtual three-dimensional object; and
   (4) computer readable medium storing program comprising instructions for generating representation of three-dimensional object.
   DESCRIPTION OF DRAWING(S) - The figure shows a layout of apparatus for determining absolute coordinate three-dimensional surface measurement.
   Signal generator (302)
   Projector (304)
   Camera (306)
   Color encoded signal (800)
   Patterns (802,804,806)
   Objects (812,814)
PD WO2007061632-A2   31 May 2007   G01B-011/25   200772   Pages: 80   English
   CN101466998-A   24 Jun 2009   G01B-011/25   200945      Chinese
   US2009238449-A1   24 Sep 2009   G06K-009/00   200963      English
   US7929751-B2   19 Apr 2011   G06K-009/00   201128      English
   WO2007061632-A3   02 Aug 2007   G01B-011/25   201229      English
   CN101466998-B   16 Sep 2015   G01B-011/25   201575      Chinese
UT DIIDW:2007777095
ER

PT P
PN WO200193746-A2; AU200180441-A; EP1286620-A2; JP2003534864-W; US2005259849-A1; US6996256-B2; IL153268-A; WO200193746-A3
TI Determination of a physiological state of a person for use in face recognition system to detect altered human states like anxiety or fear involves providing a thermal image data of the face.
AB    NOVELTY - Method to detect a physiological state of a person (30) involves providing a thermal image data of the face of a person with a thermal camera (12). The thermal image data is used to determine the physiological state of the person.
   USE - Detect physiological response characteristics representative of one or more altered human states e.g. anxiety, alertness, fear, depression etc...using thermal analysis.
   ADVANTAGE - Symptoms detectable using thermal image data can provide valuable information as to the physiological state of the person.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is for a detection system to determine a physiological state of a person.
   DESCRIPTION OF DRAWING(S) - The drawing shows a physiological state detection system.
   Thermal camera (12)
   Person (30)
PD WO200193746-A2   13 Dec 2001   A61B-000/00   200229   Pages: 31   English
   AU200180441-A   17 Dec 2001   A61B-000/00   200229      English
   EP1286620-A2   05 Mar 2003   A61B-005/00   200319      English
   JP2003534864-W   25 Nov 2003   A61B-005/16   200380   Pages: 36   Japanese
   US2005259849-A1   24 Nov 2005   G06K-009/00   200577      English
   US6996256-B2   07 Feb 2006   G06K-009/00   200611      English
   IL153268-A   04 Jul 2007   A61B-005/00   200761      English
   WO200193746-A3   06 Jun 2002   A61B-005/00   201201      English
UT DIIDW:2002240918
ER

PT P
PN CN105354565-A
TI Location based full convolution network human facial features determining method, involves obtaining human face image, human facial features command and training data, and designing convolution nerve network according to human face image.
AB    NOVELTY - The method involves obtaining a human face image, a human facial features command and a training data. A full convolution nerve network is designed according to the human face image and the human facial features. The full convolution nerve network is trained according to the training data. Five sense organs are located according to the human face image. A human face region for the human face image is determined. A residual background area is removed from the human face region to perform alignment process. A human face key point is determined by performing human face detection algorithm.
   USE - Location based full convolution network human facial features determining method.
   ADVANTAGE - The method enables improving human facial features determining efficiency and five sense organs position determining precision range, and reducing stable human face key point insufficient problems.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a location based full convolution network human facial features determining system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a location based full convolution network human facial features determining method. '(Drawing includes non-English language text)'
PD CN105354565-A   24 Feb 2016   G06K-009/00   201618   Pages: 13   English
UT DIIDW:201614490N
ER

PT P
PN CN104850825-A; CN104850825-B
TI Convolution neural network based face image color value calculating method, involves determining shape characteristic of human face image by image sample, and obtaining image resolution value by characteristics of human face.
AB    NOVELTY - The method involves collecting a different human face image according to an image sample. A key point of a human face is determined by a face image block. Pre convolution nerve network process is performed by a local human face image block. Depth characteristic of a human face image is obtained by the pre convolution nerve network process. Shape characteristic of the human face image is determined by the image sample. Image resolution value is obtained by the characteristics of the human face. The key point is provided with a right edge point, an eye point, a nose tip point and a nozzle point.
   USE - Convolution neural network based face image color value calculating method.
   ADVANTAGE - The method enables calculating convolution neural network based face image color value in an efficient manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a convolution neural network based face image color value calculating method. '(Drawing includes non-English language text)'
PD CN104850825-A   19 Aug 2015   G06K-009/00   201570   Pages: 12   Chinese
   CN104850825-B   27 Apr 2018   G06K-009/00   201830      Chinese
UT DIIDW:201561219Q
ER

PT P
PN CN103593598-A; CN103593598-B
TI Living body detection and recognition-based on-line user authentication method, involves storing face characteristics in server database, and performing re-authentication operation when matching is unsuccessful.
AB    NOVELTY - The method involves filling on-line user information. A 2G identity card number of a user is submitted by a public network to transfer a picture of the user. A human face is photographed. Gray conversion and histogram equalization operations are performed. Face characteristics are extracted. The human face is compared with the picture obtained from the public network. A result output is matched when similarity level exceeds threshold value. The face characteristics are stored in a server database. Re-authentication operation is performed when the matching is unsuccessful.
   USE - Method for authenticating an on-line user based on living body detection and recognition.
   ADVANTAGE - The method enables avoiding forgery of the human face, thus improving system safety performance and authenticating efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a living body detection and recognition-based on-line user authentication system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a living body detection and recognition-based on-line user authentication method.'(Drawing includes non-English language text)'
PD CN103593598-A   19 Feb 2014   G06F-021/32   201426   Pages: 15   Chinese
   CN103593598-B   21 Sep 2016   G06F-021/32   201673      Chinese
UT DIIDW:2014G22867
ER

PT P
PN CN102956050-A
TI Intelligent access control system, has camera module for collecting image information in monitoring space, sensor for monitoring space, and monitoring center connected to network interface module to record face part feature value.
AB    NOVELTY - The system has a first camera module (1) for collecting image information in a monitoring space in a video scene. A second camera module (2) collects image information in a to-be-monitored space in the scene. A sensor (3) monitors the monitoring space. An infrared human face identification processor (4) is provided with a control module, an image processing module, a face locating module, a feature extracting module and a similarity comparing module. A monitoring center (15) is connected to a network interface module (14) to record a face part feature value.
   USE - Intelligent access control system.
   ADVANTAGE - The system improves accuracy and efficiency of identifying and pre-processing a human face image, which avoids large amount of redundant data, reduces power consumption, and improves safety.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an intelligent access control system. '(Drawing includes non-English language text)'
   Camera modules (1, 2)
   Sensor (3)
   Infrared human face identification processor (4)
   Network interface module (14)
   Monitoring center (15)
PD CN102956050-A   06 Mar 2013   G07C-009/00   201350   Pages: 8   Chinese
UT DIIDW:2013J97096
ER

PT P
PN US7602947-B1
TI Security system for motor vehicles e.g. sedan-type vehicle, has facial-recognition system that activates enabling system only when selected characteristics of image signals from camera match with stored signals of authorized drivers.
AB    NOVELTY - The security system has an infrared scanner (12) supported within a taillight housing (10), to scan a portion of the face of a person (20) seated in drivers seat (21). A battery backed-up RAM (BRAM) device coupled to a facial-recognition system stores a set of image signals produced by the camera. The facial-recognition system recognizes the characteristics of iris features and activates an enabling system only when selected characteristics of image signals from the camera match with a set of signals of authorized operators stored in another BRAM.
   USE - Security system for motor vehicles e.g. generic passenger vehicle such as sedan-type automobile. Can also be used in electric vehicles.
   ADVANTAGE - Theft or operation by unauthorized persons is prevented and a record of persons attempting the operation is maintained. No key is required to start the car, when an authorized person is in drivers seat hence a fully auto-starting is possible. The camera is provided with a fixed view of the mirror and the mirror is adjusted by the driver to see the rear window, hence automatic reflection of drivers face is possible without the requirement of motors needed for driving the cameras, reducing the cost. Since the distance along the path is large, adjustments of drivers seat forward or back does not alter the focus of camera. Thus camera is prevented from out-of-focus problems, even without including an auto-focus feature that adds cost to the system. The connection between brake pedal and the housing is utilized to activate the camera, without requiring additional wiring or transmission circuit. The brake pedal is depressed before the car is made operational, improving the safety. By automatically communicating to a receiver through radio, the driver is identified, in case of failure of the security system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for method for making security of powered vehicle.
   DESCRIPTION OF DRAWING(S) - The drawing shows a side view of the security system for motor vehicle.
   Taillight housing (10)
   Infrared scanner (12)
   Person (20)
   Drivers seat (21)
   Receiver (28)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The facial features are recognized in accordance with the industrial standards IEEE.
PD US7602947-B1   13 Oct 2009   G06K-009/00   200968   Pages: 16   English
UT DIIDW:2009P70843
ER

PT P
PN CN1794265-A; CN100397410-C
TI Method and device for distinguishing face expression based on video frequency.
AB    NOVELTY - This invention provides an identification method and a device for countenance based on the video, which applies the ASM profile pick-up algorithm in the pick-up of character vectors and picks up the man-face image based on the position of the eyes to generate a normalized character face from the chin and picks up the most effective character in the character face to identify the countenance, which can eliminate the influence of illumination to make the right and left gray value almost the same with the variance.
PD CN1794265-A   28 Jun 2006   G06K-009/00   200717      Chinese
   CN100397410-C   25 Jun 2008   G06K-009/00   200864      Chinese
UT DIIDW:2007159992
ER

PT P
PN US2004041998-A1; WO2004021254-A2; AU2003262894-A1; US6853444-B2; EP1540574-A2; AU2003262894-A8; CN1695154-A; WO2004021254-A3; EP1540574-A4
TI Optical system for biometric identification system, has mirror for shaping wave front of emitted light and illuminates object, so as to produce strong specular reflection.
AB    NOVELTY - The optical system has a light source (16) e.g. LED, a physical rest for positioning the object e.g. finger (8) to be imaged, and mirror (14) for shaping the wave front of emitted light to produce strong specular reflection and for focussing the image of object. A charge-coupled device camera (44) is positioned to create image of object from the reflected light.
   USE - Optical system for biometric identification device e.g. finger print reader, hand geometry reader, iris imager, retinal camera, voice recognition and face recognition device.
   ADVANTAGE - Produces a high quality, high contrast, uniform and consistent images of surface features of finger or biometric object.
   DESCRIPTION OF DRAWING(S) - The figure shows the cross-sectional view of optical system for fingerprint imaging.
   finger (8)
   aperture (11)
   mirror (14)
   light source (16)
   camera (44)
PD US2004041998-A1   04 Mar 2004   G06K-009/20   200427   Pages: 13   English
   WO2004021254-A2   11 Mar 2004   G06K-000/00   200427      English
   AU2003262894-A1   19 Mar 2004   G06K-009/20   200462      English
   US6853444-B2   08 Feb 2005   G06K-009/74   200511      English
   EP1540574-A2   15 Jun 2005   G06K-009/20   200539      English
   CN1695154-A   09 Nov 2005   G06K-009/20   200656      Chinese
   WO2004021254-A3   08 Jul 2004   G06K-009/20   201213      English
   EP1540574-A4   04 Apr 2007   G06K-009/00   201751      English
UT DIIDW:2004293344
ER

PT P
PN US2003086593-A1; US6826300-B2
TI Image similarity determining method for face recognition system, involves deriving transformation matrix from lower dimensional feature vector and space discriminator to find similarities between image and training sample.
AB    NOVELTY - The method involves calculating an augmented Gabor feature vector from a gray scale image, which is generated from a facial image. A lower dimensional feature vector derived from the augmented vector is processed with a lower dimensional feature space discriminator to derive a transformation matrix. The method then calculates an image feature vector to determine the similarities between the image and training samples.
   USE - Used in face recognizing systems for recognizing the similarities between two images.
   ADVANTAGE - The method derives statistical features from whole images and hence it does not need facial feature point detection. The dimension of the Gabor feature vector derived by this method can be reduced using the enhanced fisher linear discriminant model (EFM) by considering both data compression and recognition performance.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system determining the similarities between images and training samples.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram representing a system for determining similarity between an image and a training sample.
PD US2003086593-A1   08 May 2003   G06K-009/00   200362   Pages: 25   English
   US6826300-B2   30 Nov 2004   G06K-009/00   200479      English
UT DIIDW:2003657934
ER

PT P
PN WO2003034361-A1; CA2359269-A1; EP1444667-A1; AU2002331511-A1; JP2005505871-W; US2005063566-A1; NZ532315-A; CN1568489-A; EP1444667-B1; DE60209760-E; EP1667080-A1; ES2260476-T3; DE60209760-T2; AU2002331511-B2; CN100418112-C; US2009080715-A1
TI Face imaging system for automated identity confirmation including camera unit with range detection system and a camera controller that runs software to detect, track and capture high quality face images.
AB    NOVELTY - Includes a camera unit and a camera controller. The camera unit includes a video camera, a rotatable mirror system for directing images of the security area into the video camera, and a ranging unit for detecting the presence of a target. The ranging unit provides target range data, including distance, angle and width information, to the camera controller.
   USE - For recorded and/or automated identity confirmation.
   ADVANTAGE - Enables rapid capture and recording of high quality face images.
   DETAILED DESCRIPTION - The camera controller includes software for detecting face images of the target, tracking detected face images, and capturing high quality face images. A communication system sends the captured face images to an external controller for face verification, face recognition and database searching.
   DESCRIPTION OF DRAWING(S) - The drawing shows a diagram of the system in use.
PD WO2003034361-A1   24 Apr 2003   G08B-015/00   200333   Pages: 38   English
   CA2359269-A1   17 Apr 2003   A61B-005/117   200338      English
   EP1444667-A1   11 Aug 2004   G08B-015/00   200452      English
   AU2002331511-A1   28 Apr 2003   G08B-015/00   200460      English
   JP2005505871-W   24 Feb 2005   G06T-001/00   200516   Pages: 63   Japanese
   US2005063566-A1   24 Mar 2005   G06K-009/00   200526      English
   NZ532315-A   26 Aug 2005   G08B-015/00   200560      English
   CN1568489-A   19 Jan 2005   G08B-015/00   200572      Chinese
   EP1444667-B1   08 Mar 2006   G08B-015/00   200618      English
   DE60209760-E   04 May 2006   G08B-015/00   200634      German
   EP1667080-A1   07 Jun 2006   G08B-015/00   200638      English
   ES2260476-T3   01 Nov 2006   G08B-015/00   200673      Spanish
   DE60209760-T2   18 Jan 2007   G08B-015/00   200706      German
   AU2002331511-B2   19 Apr 2007   G06T-001/00   200763      English
   CN100418112-C   10 Sep 2008   G08B-015/00   200907      Chinese
   US2009080715-A1   26 Mar 2009   G06K-009/00   200922      English
UT DIIDW:2003355110
ER

PT P
PN CN107239736-A
TI Task concatenated convolutional neural network based human face detection method, involves building cascade multi-level neural network, and utilizing combination of on-line mode and off-line mode to mine negative sample as training sample.
AB    NOVELTY - The method involves building cascade multi-level convolutional neural network by using positive samples and a negative sample of a human face. A part of the human face and the face key point sample are determined as training samples to train the multi-level convolutional neural network to perform face classification. A position of the face region regression and the face key point is obtained by using the multi-level neural network trained to be detected image. A combination of an on-line mode and an off-line mode is utilized to mine a negative sample as the training sample.
   USE - Task concatenated convolutional neural network based human face detection method.
   ADVANTAGE - The method enables utilizing the online mode and offline mode to mine difficult to negative mode of the sample so as to improving the classification ability of the network, thus improving the detection ability and accuracy of the network and ensuring operation speed in an actual product.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a task concatenated convolutional neural network based human face detection device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a task concatenated convolutional neural network based human face detection method. '(Drawing includes non-English language text)'
PD CN107239736-A   10 Oct 2017   G06K-009/00   201776   Pages: 12   Chinese
UT DIIDW:201770555M
ER

PT P
PN US2011222724-A1; US8582807-B2
TI Computer implemented method for classifying camera images, by applying incremental learning to Convolutional Neural Networks (CNNs) and enforcing correspondence constraint such that CNN outputs are consistent and stable for one person.
AB    NOVELTY - The method involves: generating a baseline gender model and an age estimation model using one or more CNNs; capturing correspondences of faces by face tracking; and applying incremental learning to the CNNs and enforcing correspondence constraint such that CNN outputs are consistent and stable for one person.
   USE - Computer implemented method for classifying camera images in video analysis system. Uses include but are not limited to a system for automatic gender and age estimation, a system for face verification and recognition, or a wide-area surveillance system.
   ADVANTAGE - Video analysis system based on statistical learning models maintains its performance when deployed to real-world scenario since training data can cover unfamiliar variations in reality. System maintains output consistent and stable results on face images from same trajectory in videos by using incremental stochastic training. Supervision of correspondences can improve estimation accuracy by large margin.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for classifying camera images.
   DESCRIPTION OF DRAWING(S) - The drawing shows an exemplary architecture of the CNNs where each plane represents a feature map.
PD US2011222724-A1   15 Sep 2011   G06K-009/62   201162   Pages: 9   English
   US8582807-B2   12 Nov 2013   G06K-009/00   201374      English
UT DIIDW:2011L74189
ER

PT P
PN CN101998161-A
TI Method for watching TV program in home, involves acquiring corresponding user right data in remote database depending on identity characteristic, and operating internet protocol TV depending on user right data.
AB    NOVELTY - The method involves establishing and operating a user right database depending on face data of a user. Face identification function is triggered when an internet protocol TV (IPTV) is turned on. An identity is authenticated depending on an identification result. Corresponding user right data in a remote database is acquired at a certain time slice depending on identity characteristic. The IPTV is operated depending on the user right data.
   USE - Method for watching TV program in a home based on face identification.
   ADVANTAGE - The method enables combining face identification and remote network control function together, thus presenting, classifying and limiting the program in real time for users in the home with convenient operation.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a method for watching TV program.'(Drawing includes non-English language text)'
PD CN101998161-A   30 Mar 2011   H04N-021/4415   201132   Pages: 9   Chinese
UT DIIDW:2011E95932
ER

PT P
PN US2005147291-A1; US7142697-B2
TI Computer-implemented face recognition process for identifying person, involves finding face pose data from model image, and inputting face region into neural network ensemble, where ensemble output is interpreted to find person.
AB    NOVELTY - The process involves locating and segmenting a face region belonging to a person. Face pose data is determined, for each face region, from a model image. Each face region, associated with a person to be identified, is prepared and input into a neural network ensemble one at a time. An output of the ensemble is interpreted to identify the person, and the pose data is associated with each input.
   USE - Used for identifying a person depicted in an input image.
   ADVANTAGE - The process is utilized to identify the person as long as the person`s face is visible in the face region. The process also provides pose information which is useful for knowing which way the person is facing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart diagramming an overall face recognition process for identifying a person depicted in an input image and the face pose of each identified person.
PD US2005147291-A1   07 Jul 2005   G06K-009/00   200553   Pages: 34   English
   US7142697-B2   28 Nov 2006   G06K-009/00   200679      English
UT DIIDW:2005520261
ER

PT P
PN US6775397-B1
TI Authorized user identity recognizing method for accessing computer system, involves granting access to system when found facial shape matches profile information of user, where information is updated with shape after each access.
AB    NOVELTY - The method involves obtaining two two-dimensional images of a user and generating a three-dimensional model of the user`s face. A determined facial shape is compared with a profile information of an authorized user stored in a memory. An access to a computer system is granted when the facial shape matches the profile information. The profile information is updated with the facial shape after each occurrence of granting access.
   USE - Used for recognizing an authorized user to grant access to a computer system (claimed) coupled to e.g. local area network, global network e.g. Internet.
   ADVANTAGE - The access to the system is granted when the determined facial shape matches the profile information of the authorized user, thereby indicating recognition and authorization of the user without requiring the user to input a password. The profile information is updated with the facial shape after each occurrence of granting access, thus the stored profile will evolve with the changes in the user appearance, and therefore never deny access if the change in the user appearance is not too far from the average profile stored.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an apparatus for recognizing a user`s identity during an attempt to access a system.
   DESCRIPTION OF DRAWING(S) - DESCRIPTION OF DRAWING - The drawing shows a flow diagram illustrating a method of recognizing an authorized user.
PD US6775397-B1   10 Aug 2004   G06K-009/00   200459   Pages: 12   English
UT DIIDW:2004613236
ER

PT P
PN US2003123712-A1; US6925197-B2
TI Name-face/voice-role association method for name and face/voice recognition systems by storing role and actor names and face model/voice model when particular face model/voice model and role-name association have reached threshold.
AB    NOVELTY - If a particular face model/voice model and role-name association have reached a threshold, then the role name, actor name and particular face model/voice model are stored in a database.
   USE - For name and face/voice recognition systems for consumer queries of databases. For home video.
   ADVANTAGE - Enables users to enter query for name-face-role association while viewing television program, movie or sporting event.
   DETAILED DESCRIPTION - If a closed captioned text accompanies a video sequence, then a text recognition or a speech to text conversion is provided to the video sequence to generate a role-name versus actor-name list. Face boxes/voices are extracted from the video sequence to generate face models/voice models. A predetermined portion of the text is searched for an entry on the role-name versus actor-name list. Video frames are searched for face models/voice models that correspond to the searched text. An equal level of certainty is assigned to each face model/voice model. Lip reading is used to eliminate the searched face models that pronounce a role-name corresponding to the entry on the role-name versus actor-name list. The remaining portion of the text is scanned and the level of certainty for each searched face model/voice model is updated. An INDEPENDENT CLAIM is also included for a system for providing name-face-role association.
   DESCRIPTION OF DRAWING(S) - The figure is a flowchart illustrating a name-to-face algorithm.
PD US2003123712-A1   03 Jul 2003   G06K-009/00   200373   Pages: 9   English
   US6925197-B2   02 Aug 2005   G06K-009/00   200550      English
UT DIIDW:2003777420
ER

PT P
PN SG91841-A1; US6707933-B1; US2004062424-A1; US7266225-B2
TI Face direction generation method for face recognition and gesture detection, involves determining quantitative face direction of face in image dependent upon rotation and tilt of face.
AB    NOVELTY - The rotation of face in image is computed dependent upon a nose axis of face, and the tilt of face in image is computed. The quantitative face direction of face in image is determined dependent upon rotation and tilt of face.
   USE - For recognizing faces from single image and for determining facial gestures.
   ADVANTAGE - Enables fast and easy face recognition by using a single image. Enables active system to be built with the faces only in given position, prior to face recognition, so that the size of the database is reduced and the number of error and the processing time during recognition is also reduced. Provides qualitative and quantitative measure of the face direction and the result is displayed on the face, by drawing the estimated nose axis, hence easily interface with supervised learning system and allows a human to input feedback.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMs are also included for the following:
   (1) A quantitative face direction generation apparatus;
   (2) A computer readable medium storing program for generating quantitative face direction;
   (3) A facial gesture establishment method;
   (4) A facial gesture establishment apparatus;
   (5) A computer readable medium storing program for establishing facial gesture;
   (6) A visual mouse presentation method for detecting quantitative face direction;
   (7) A visual mouse presentation apparatus for detecting quantitative face directions;
   (8) A computer readable medium storing for detecting quantitative face direction;
   (9) A video conference method;
   (10) A video conference apparatus; and
   (11) A computer readable medium storing program for video conferencing.
   DESCRIPTION OF DRAWING(S) - The figure shows a symbolic diagram of correlation window corresponding to segment (x,theta) for two images, being a frontal case and a right rotated case at a face.
PD SG91841-A1   15 Oct 2002   G06K-009/36   200348   Pages: 40   English
   US6707933-B1   16 Mar 2004   G06K-009/00   200420      English
   US2004062424-A1   01 Apr 2004   G06K-009/00   200425      English
   US7266225-B2   04 Sep 2007   G06K-009/00   200758      English
UT DIIDW:2003511741
ER

PT P
PN US8441548-B1; DE102013009901-A1; GB2517562-A; GB2517562-B
TI Method for assessing quality of human facial images in facial recognition application, involves classifying quality of image by classifying that image has quality same as that of previously classified image related to smallest difference.
AB    NOVELTY - The method involves computing a difference between a tonal distribution of a human facial image and a tonal distribution of each of multiple previously classified images. A smallest computed difference between the tonal distribution of an image and the tonal distribution of the previously classified image is determined based on the computed differences. A quality of the image is classified by a mobile computing device (200) e.g. mobile phone, by classifying that the image has a quality same as that of the previously classified image related to the smallest computed difference.
   USE - Method for assessing quality of human facial images captured by a camera e.g. front-facing camera, rear-facing camera, still camera and/or video camera, of a mobile computing device (claimed) e.g. mobile phone or smartphone, personal digital assistant (PDA) and tablet computer, in a facial recognition application. Can also be used for netbook computer, laptop computer, set-top box, TV, watch and desktop computer.
   ADVANTAGE - The method enables reducing a frequency of facial recognition authentication failures by identifying image captures of low quality, alerting a user and refraining from performing facial recognition authentication or completing enrollment until multiple images of satisfactory quality have been captured.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer-readable storage device storing a program comprising a set of instructions for assessing quality of facial images
   (2) a mobile computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a mobile computing device for assessing a quality of human facial images.
   Mobile computing device (200)
   Processors (202)
   Front-facing camera (214)
   Picture quality module (218)
   Tonal distribution module (220)
PD US8441548-B1   14 May 2013   H04N-005/228   201334   Pages: 17   English
   DE102013009901-A1   19 Dec 2013   G06T-001/00   201401      German
   GB2517562-A   25 Feb 2015   G06K-009/00   201517      English
   GB2517562-B   01 Jul 2015   G06K-009/00   201543      English
UT DIIDW:2013H17029
ER

PT P
PN CN102004908-A; CN102004908-B
TI Self-adaptive face recognition method involves updating face characteristic template in information library, when preset self-adaptive demand is satisfied with face characteristics.
AB    NOVELTY - The method involves logging the basic face information of the user and acquiring multiple face images of the user. The face information of the user and the face characteristic template is conserved to information library. The face characteristic is extracted from the acquired face image, based on the comparison with the face characteristic template in the information library. The face characteristic template in the information library is update, when the preset self-adaptive demand is satisfied with the face characteristics.
   USE - Self-adaptive face recognition method.
   ADVANTAGE - The influence of the face recognition effect can be eliminated, so that the change in face characteristic of the user can be detected. Hence the recognition speed and accuracy of the face recognition device can be improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for self-adaptive face recognition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flowchart illustrating the self-adaptive face recognition process. (Drawing includes non-English language text)
PD CN102004908-A   06 Apr 2011   G06K-009/00   201133   Pages: 15   Chinese
   CN102004908-B   17 Oct 2012   G06K-009/00   201301      Chinese
UT DIIDW:2011F15860
ER

PT P
PN CN101833624-A; CN101833624-B
TI Information machine, has face identifying module comparing identified face image information with existing face image library in machine, and user permission processing module performing user permission management or distribution.
AB    NOVELTY - The machine has a face detecting module for performing face collection and for processing on images short by a camera based on a face detection algorithm, so as to extract face image information from the images. A face identifying module compares the identified face image information with existing face image library in the machine, so as to determine attribute of the face information. A user permission processing module performs user permission management or distribution based on the determined face information.
   USE - Information machine.
   ADVANTAGE - The machine enables users with permit permission to use the machine without the need for complex verification process. The user permission processing module effectively avoids machine operations according to security, secrete functions and information by the common users, so as to protect security of the machine.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a permission control method for an information machine.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a permission control method for an information machine. '(Drawing includes non-English language text)'
PD CN101833624-A   15 Sep 2010   G06F-021/00   201104   Pages: 12   Chinese
   CN101833624-B   10 Dec 2014   G06F-021/32   201510      Chinese
UT DIIDW:2010N14128
ER

PT P
PN WO2009056995-A1; US2009116684-A1; EP2203869-A1; CN101836219-A; EP2203869-B1; US8094891-B2
TI Music playlist generating method for e.g. mobile phone, involves capturing image of user of consumer device, performing facial expression recognition of user based on image, and selecting song based on facial expression of user.
AB    NOVELTY - The method involves playing a song on a consumer device e.g. mobile phone, and capturing an image of a user of the device. Facial expression recognition of the user is performed based on the image, and another song is selected based on the facial expression of the user. The image of the user is automatically captured based on the playing of the former song, and a mood category of the user is determined. Multiple musical characteristics of the former song are identified. An artist or genre of the former song is determined.
   USE - Method for generating music playlist based on facial expression in a consumer device. Uses include but are not limited to single functional device, multi-functional device, portable device e.g. mobile phone, personal digital assistant (PDA), handheld computer, wireless telephone and audio/video player i.e. MPEG-1 Audio Layer 3 (MP3) player, and stationary device e.g. audio/video system, game system, digital media player (DMP) and digital audio player (DAP) that are utilized in automobile and airplane.
   ADVANTAGE - The method allows the user to automatically update an active playlist based on the facial expression, so that the user can listen to the music that closely matches the user's mood, a new/different facial expression is correlated to a particular song, and a potential playlist is created based on the new/different facial expression.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device comprising a processor that executes a set of instructions for capturing an image of a user
   (2) a computer-readable medium having a set of instructions for performing a method for capturing an image of a user.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for generating music playlist based on facial expression.
PD WO2009056995-A1   07 May 2009   G06K-009/00   200932   Pages: 31   English
   US2009116684-A1   07 May 2009   G06K-009/00   200932      English
   EP2203869-A1   07 Jul 2010   G06K-009/00   201044      English
   CN101836219-A   15 Sep 2010   G06K-009/00   201068      Chinese
   EP2203869-B1   08 Jun 2011   G06K-009/00   201137      English
   US8094891-B2   10 Jan 2012   G06K-009/00   201205      English
UT DIIDW:2009H95074
ER

PT P
PN US2008118156-A1; JP2008131405-A; CN101188677-A; JP4264663-B2; CN100556078-C; US8385607-B2
TI Imaging apparatus e.g. digital still camera, digital video camera, has display unit that displays image overlaid by overlaying unit based on face position marker generated by face position marker generating unit and input image.
AB    NOVELTY - A display unit displays an image overlaid by an overlaying unit. The overlaying unit overlays the face position marker generated by a face position marker generating unit and the image input by an image input unit (220). A face detector (200) detects a face from the image input by the image input unit. The face position marker generating unit generates the face position marker indicating the position of the face detected by the face detector, in the image input by the image input unit.
   USE - Imaging apparatus e.g. digital still camera, digital video camera.
   ADVANTAGE - Displays position of face appearing in image. Improves performance of imaging apparatus. Ensures quick capture of image. Ensures easy recognition of face detected from image together with variety of information related to face.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) an image processing apparatus; and
   (2) an image processing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of a face detector.
   Face detector (200)
   Image input unit (220)
   Standard data memory (250)
   Determiner (260)
   Image output unit (280)
PD US2008118156-A1   22 May 2008   G06K-009/46   200837   Pages: 35   English
   JP2008131405-A   05 Jun 2008   H04N-005/225   200837   Pages: 37   Japanese
   CN101188677-A   28 May 2008   H04N-005/225   200853      Chinese
   JP4264663-B2   20 May 2009   H04N-005/225   200933   Pages: 36   Japanese
   CN100556078-C   28 Oct 2009   H04N-005/225   201001      Chinese
   US8385607-B2   26 Feb 2013   G06K-009/00   201315      English
UT DIIDW:2008F84661
ER

PT P
PN EP1903509-A1; JP2008071158-A; CN101145199-A; US2008226136-A1; KR2008025340-A; KR918286-B1; JP4786483-B2; CN101145199-B; US8184866-B2
TI Biometric authentication device for identifying biometric characteristic data of e.g. finger print, has processing unit comparing extracted body image capture state data with body image capture state data registered for user.
AB    NOVELTY - The device has a storage unit storing a biometric characteristic data and body image capture state data for a user. A processing unit extracts the body image capture state data from images of a body captured by the image capture unit, compares the extracted body image capture state data with the body image capture state data registered for the user, displays a guidance screen on the display unit according to the comparison result, and verifies the biometric characteristic data registered for the user against the biometric characteristic data detected from the body.
   USE - Biometric authentication device for identifying biometric characteristic data of a portion e.g. fingerprint, toeprint, retina of eye, facial feature and blood vessel, in finger, palm, and back of hand (claimed), of a body to perform authentication of individual such as man, woman, older and younger person.
   ADVANTAGE - The device prevents verification failures, improves the reliability of biometric authentication, and the efficiency of verification. The device performs individual authentication or identification rapidly, and enables guidance of the body according to the individual user.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a living body guidance control method for a biometric authentication device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic representation explaining body guidance control processing at the time of verification.
   Captured image (CG)
   Hand silhouette (CS)
PD EP1903509-A1   26 Mar 2008   G07C-009/00   200826   Pages: 35   English
   JP2008071158-A   27 Mar 2008   G06T-001/00   200833   Pages: 24   Japanese
   CN101145199-A   19 Mar 2008   G06K-009/00   200841      Chinese
   US2008226136-A1   18 Sep 2008   G06K-009/00   200863      English
   KR2008025340-A   20 Mar 2008   G06K-009/00   200864      
   KR918286-B1   18 Sep 2009   G06K-009/00   200965      
   JP4786483-B2   05 Oct 2011   G06T-001/00   201165   Pages: 24   Japanese
   CN101145199-B   13 Apr 2011   G06K-009/00   201167      Chinese
   US8184866-B2   22 May 2012   G06K-009/00   201235      English
UT DIIDW:2008D60011
ER

PT P
PN US2007058839-A1; KR688398-B1; US7358972-B2
TI Motion capture system for capturing movement of real object, has motion capture volume adapted to contain actor having body markers defining body points, facial markers, and facial points.
AB    NOVELTY - The system has a motion capture volume adapted to contain an actor having body markers defining body points, facial markers, and facial points. A set of motion capture cameras are arranged around a periphery of the motion capture volume. The motion capture cameras are arranged such that laterally exposed surfaces of an actor are within a field of view of the set of motion capture cameras at all times. A motion capture processor (12) is coupled to the set of motion capture cameras to produce a digital representation reflecting combined facial and body motion of the actor.
   USE - Used for capturing movement of a real object and mapping the object onto a computer generated object, for producing a motion picture and video game to create a digital representation of a person that is used as source data to create a computer graphics (CG) animation, and for tracking motion of facial features of an actor to create a representation of the actor`s facial motion and expression e.g. laughing, crying, and smiling.
   ADVANTAGE - The utilization of the motion capture volume allows the system to enable body and facial motion to be captured simultaneously within a volume that can accommodate plural actors, and audio recording simultaneously with body and facial motion capture.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block representation of a motion capture system.
   Motion capture system (10)
   Facial motion cameras (141 - 14n)
   Motion cameras (161 -16n)
   Computer workstations (181 - 18n)
   Microphones (241 - 24n)
PD US2007058839-A1   15 Mar 2007   G06K-009/00   200741   Pages: 14   English
   KR688398-B1   02 Mar 2007   G06T-015/70   200820      
   US7358972-B2   15 Apr 2008   G06T-015/70   200828      English
UT DIIDW:2007431944
ER

PT P
PN EP1347551-A1; US2003177676-A1; CN1447273-A; JP2004038147-A; US6718674-B2; US2004231215-A1; US7007422-B2; US2006112603-A1; US7353629-B2; US2008138573-A1; CN100511264-C; JP2010271738-A; JP4614628-B2; JP5222332-B2
TI Identification labeling, e.g. for apparatus for computer network, has extending fingers that serve as label stop during insertion of label into cover having thickness same as depth of recess.
AB    NOVELTY - A label cover (10) provided with compartments is inserted within a recess of a face plate. The cover has a thickness same as the depth of the recess. A pair of transversely extending fingers (30) is provided in the label cover, which serves as a label stop during insertion of the label (20) into the cover.
   USE - For identification labeling of network ports for communication system and computer network.
   ADVANTAGE - The label cover can accommodate different non- adhesive identification labels. Hence, provides an improved and versatile identification labeling system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for label cover.
   DESCRIPTION OF DRAWING(S) - The figure shows a perspective view of the multi-segment label cover.
   multi-segment label cover (10)
   label (20)
   transversely extending fingers (30)
   protective shield (40)
   chamfered corners (90)
PD EP1347551-A1   24 Sep 2003   H02G-003/14   200371   Pages: 20   English
   US2003177676-A1   25 Sep 2003   G09F-003/10   200371      English
   CN1447273-A   08 Oct 2003   G06K-009/00   200403      Chinese
   JP2004038147-A   05 Feb 2004   G09F-003/18   200411   Pages: 46   Japanese
   US6718674-B2   13 Apr 2004   G09F-003/18   200425      English
   US2004231215-A1   25 Nov 2004   G09F-003/20   200478      English
   US7007422-B2   07 Mar 2006   G09F-003/18   200618      English
   US2006112603-A1   01 Jun 2006   G09F-003/18   200637      English
   US7353629-B2   08 Apr 2008   G09F-003/10   200826      English
   US2008138573-A1   12 Jun 2008   B32B-003/10   200841      English
   CN100511264-C   08 Jul 2009   G06K-009/00   200982      Chinese
   JP2010271738-A   02 Dec 2010   G09F-007/08   201079   Pages: 22   Japanese
   JP4614628-B2   19 Jan 2011   G09F-003/18   201106   Pages: 17   Japanese
   JP5222332-B2   26 Jun 2013   G09F-003/18   201343   Pages: 18   Japanese
UT DIIDW:2003749747
ER

PT P
PN TW413795-A; US6580810-B1
TI An image processing method of 3-D head motion with three face feature points - by inferring 3-D motion from 2-D images.
AB    NOVELTY - The invention relates to an image processing method of 3D head motion with three face feature points, which consists the following steps. Provide a first processing device with a user's original image and capture the user's first image and provide to a second processing device. Select three face feature points in the second processing device from the first image to form a 3-D feature triangle. Catch the user's continuous images for when the user is having a head motion and provide to the second processing device. Track those three face feature points in accordance with those continuous images to form a series of real 2-D feature triangles. And then apply the steepest downhill iteration method to rotate and displace the 3-D feature triangles freely and choose the geometric transformation between two consecutive real 2-D feature triangles with an acceptable deviation. Repeat these mentioned steps till the last one of the continuous images to form the geometry transformations in accordance with the continuous images. Send the geometry transformations to the first processing device to make user's original image generate the corresponding head motion.
PD TW413795-A   01 Dec 2000   G06T-017/40   200126      Chinese
   US6580810-B1   17 Jun 2003   G06K-009/00   200341      English
UT DIIDW:2001255975
ER

PT P
PN DE3614548-A; US4736441-A; DE3614548-C; US4783825-A; IT1191762-B
TI Letter scanning device for automatic postal handling - allows selective masking of marked areas of letter with evaluation of orientation.
AB       The device provides an image signal representing the visual image of the letter, evaluated to detect a marking which may prevent correct assessment of the orientation or alignment of the letter. Upon detection of this marking a masking signal for masking the corresponding area of the letter is pref. supplied, before evaluation of the orientation etc.
   The masking signals are used to mask the edge of the letter when an edge marking is detected by gating the stored image signals representing the letter image.
   USE -   For automatic handling of non standardised letters.
PD DE3614548-A   30 Oct 1986   B07C-001/20   198645   Pages: 40   German
   US4736441-A   05 Apr 1988   G06K-009/20   198816      English
   DE3614548-C   09 Jun 1988   B07C-001/20   198823      German
   US4783825-A   08 Nov 1988   G06K-009/00   198847      English
   IT1191762-B   23 Mar 1988   G11C-000/00   199101      Italian
UT DIIDW:1986292715
ER

PT P
PN CN105426850-A; CN105426850-B
TI Human face recognition based relevant information push device, has processor provided with human face identification unit and relevant information retrieval, and display screen for displaying relevant information.
AB    NOVELTY - The device has a processor provided with a human face identification unit and a relevant information retrieval unit. An imaging device detects image information. The human face recognition unit detects human face information and attribute value based the detected image information of the imaging device. The relevant information searching unit performs a searching process to obtain the human face information and the attribute value corresponding to relevant information. A display screen displays the relevant information. An external connecting device is provided with USB device.
   USE - Human face recognition based relevant information push device.
   ADVANTAGE - The device provides more accurate and more personal characteristics of products to different users, and detects the image information by an image pickup device and transmits to the human face identification unit based on human face identification method, and detects the human face information and the attribute value using the relevant information searching unit based on internal and external resources.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face recognition based relevant information pushing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face recognition based relevant information push device. '(Drawing includes non-English language text)'
PD CN105426850-A   23 Mar 2016   G06K-009/00   201626      Chinese
   CN105426850-B   31 Aug 2021   G06K-009/00   202175      Chinese
UT DIIDW:201619499D
ER

PT P
PN US9147117-B1; WO2015191896-A1; EP3155549-A1; SG11201610318-A1; CN106575327-A; IN201617045182-A; EP3155549-A4; HK1236655-A0; CN106575327-B; SG11201610318-B; CN111008592-A; SG10201913318-A1; SG10201913317-A1; IL249414-A; IL277490-A; SG10201913317-B; IN202118014451-A
TI Computer-implemented method for calculating authenticity score for user, involves combining calculated user score, calculated liveliness check score, and calculated facial confirmation score to generate authenticity score for user.
AB    NOVELTY - The method involves calculating a liveliness check score in response to analysis of image data. A facial confirmation score is calculated by facial evaluation system (102). A facial vector map is generated based on image data. An authorization score for the user (106) in response to calculated liveliness check score and calculated facial confirmation score are calculated. The user score is calculated in response to portion of profile data. The user score, liveliness check score, and the facial confirmation score are combined to generate an authenticity score for the user.
   USE - Computer-implemented method for calculating authenticity score for user (claimed).
   ADVANTAGE - The losses are reduced to financial institutions, businesses, individuals, and other entities. The consumer's experience can be improved by significantly enhancing the authentication process and reducing false positive friction. The privacy of data is protected and to enhance their reputations in various online environments. The fraud losses, user experience friction, and fraud remediation costs are reduced. The authenticity and integrity of plastic card/mobile transactions in real-time is verified. The security and productivity is increased while decreasing cost, downtime and repetitive tasks.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a computer-implemented system for calculating an authenticity score for a user; and
   (2) a non-transitory computer-readable medium storing program for calculating an authenticity score for a user.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the facial evaluation system.
   Facial evaluation system (102)
   Facial recognition application (104)
   User (106)
   Client system (108)
   Data storage media (110)
PD US9147117-B1   29 Sep 2015   G06K-009/62   201568   Pages: 24   English
   WO2015191896-A1   17 Dec 2015   G06F-021/32   201601      English
   EP3155549-A1   19 Apr 2017   G06F-021/32   201727      English
   SG11201610318-A1   27 Jan 2017   G06F-021/32   201728      English
   CN106575327-A   19 Apr 2017   G06F-021/32   201729      Chinese
   IN201617045182-A   28 Apr 2017   G06F-021/32   201733      English
   EP3155549-A4   17 Jan 2018   G06F-021/32   201806      English
   HK1236655-A0   29 Mar 2018   G06F-000/00   201823      English
   CN106575327-B   27 Dec 2019   G06F-021/32   202002      Chinese
   SG11201610318-B   22 Jan 2020   G06K-009/62   202027      English
   CN111008592-A   14 Apr 2020   G06K-009/00   202034      Chinese
   SG10201913318-A1   27 Feb 2020      202039      English
   IL249414-A   30 Sep 2020   G06F-021/32   202084      English
   IL277490-A   29 Apr 2021   G06K-009/00   202143      English
   SG10201913317-B   27 Jul 2021      202181      English
   IN202118014451-A   04 Feb 2022   G06K-009/00   202216      English
UT DIIDW:2015589470
ER

PT P
PN CN104063150-A
TI Method for implementing scene recognition mode of mobile terminal i.e. mobile phone, through human face, involves obtaining interface property, and performing corresponding scene mode entering process based on different age stages.
AB    NOVELTY - The method involves setting different scene modes in different age stages of a mobile terminal. Operation interface property is obtained corresponding to the different scene modes. Camera information is obtained by a current user. A human face identification process is performed to recognize face characteristic. A corresponding scene mode entering process is performed based on the different age stages. A young scene mode is restored corresponding to the different scene modes. Image information is obtained by the mobile terminal.
   USE - Method for implementing a scene recognition mode of a mobile terminal i.e. mobile phone, through a human face (claimed).
   ADVANTAGE - The method enables setting a camera through the mobile terminal based on a human face identification algorithm to identify a mobile phone by a user.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a mobile terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for implementing a scene recognition mode of a mobile terminal through a human face.'(Drawing includes non-English language text)'
PD CN104063150-A   24 Sep 2014   G06F-003/0484   201501   Pages: 12   Chinese
UT DIIDW:2014W50851
ER

PT P
PN CN103488293-A; CN103488293-B
TI Human-computer interaction-based facial expression recognition system for monitoring psychological disorder of patient, has storage module to obtain psychological state and store different expression type and provide to user query.
AB    NOVELTY - The system has a statistical image collecting module (100) to collect and transfer face image to image processing module (200) that processes human face image. An expression identifying module (300) automatically identifies human face expression and expression type by processing of human face image. The interaction module (400) sets the human-computer interaction mode corresponding to response of user. The storage module (500) analysis human face expression to obtain corresponding psychological state and stores different expression type and provides to user query.
   USE - Human-computer interaction-based facial expression recognition system for monitoring psychological disorder of patient.
   ADVANTAGE - The human-computer interaction system is simple and effective and the real-time psychological disease can be monitored safely.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for human-computer interaction-based facial expression recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of facial expression recognition-based human-computer interaction system. (Drawing includes non-English language text)
   Statistical image collecting module (100)
   Image processing module (200)
   Expression identifying module (300)
   Interaction module (400)
   Storage module (500)
PD CN103488293-A   01 Jan 2014   G06F-003/01   201417   Pages: 12   Chinese
   CN103488293-B   30 Nov 2016   G06F-003/01   201681      Chinese
UT DIIDW:2014E00909
ER

PT P
PN CN102749991-A; CN102749991-B
TI Non-touch type free space sight tracking method for man-machine interaction, involves testing rotation center of eyeball, and establishing relation between visual field and pupil coordinate systems to obtain real human eye multi-frame video.
AB    NOVELTY - The method involves establishing a human face classifier to detect a human face to locate eye area. A pupil center part is located by realizing human eye area based on image grey scale projection to simplify pupil region imaging process. Biological characteristic information is extracted from a human eye position collected image. An eye information movement model is established to test rotation center of an eyeball to obtain visual perception. Conversion relation between a visual field coordinate system and a pupil coordinate system is established to obtain real human eye multi-frame video.
   USE - Non-touch type free space sight tracking method for man-machine interaction. Uses include but are not limited to disabled people assisting field, aerospace field, sports and virtual reality games.
   ADVANTAGE - The method enables improving man-machine interaction process and application range.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a non-touch type man-machine interaction free space sight tracking method.'(Drawing includes non-English language text)'
PD CN102749991-A   24 Oct 2012   G06F-003/01   201313   Pages: 25   Chinese
   CN102749991-B   27 Apr 2016   G06F-003/01   201631      English
UT DIIDW:2013B56390
ER

PT P
PN CN102360421-A; CN102360421-B
TI Human face identifying method, involves receiving to-be-identified video flow by video collecting device to identify to-be-identified human face, and generating face recognition result in to-be-identified video flow.
AB    NOVELTY - The method involves receiving to-be-identified video flow by a video collecting device to identify a to-be-identified human face. A key characteristic point is located to a frame face image. A key human face image is provided with the face image. The key characteristic point is provided with the human face image. A key feature point is provided with the human face image and matched with a feature point in a human face image database. A face recognition result is generated in the to-be-identified video flow. The to-be-identified video stream is arranged in an image block.
   USE - Human face identifying method.
   ADVANTAGE - The method enables eliminating video collecting environment identifying deviation.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification system comprising a video receiving module.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face identifying method.'(Drawing includes non-English language text)'
PD CN102360421-A   22 Feb 2012   G06K-009/00   201219      Chinese
   CN102360421-B   28 May 2014   G06K-009/00   201449      Chinese
UT DIIDW:2012C91187
ER

PT P
PN WO2009094142-A1; US2009192967-A1; EP2245580-A1; JP2011514575-W; US7953690-B2; JP5383705-B2
TI Method for categorizing relationship of individuals from collection of photos involves inferring social relationships between individuals based on set of rules using occurrences, co-occurrences and ages of identified individuals.
AB    NOVELTY - The method involves searching photo collection to identify individuals in the photo collection, and determining the gender and age range of identified individuals. The social relationships between individuals are inferred based on the set of rules using the occurrences, co-occurrences and ages of the identified individuals. The relationships include parent, child, spouse, other relatives, child's friends and acquaintances.
   USE - Method for categorizing relationship of individuals from collection of photos taken in multiple sets of events.
   ADVANTAGE - Performs artificial intelligence and machine learning to discover social relationships from personal photo collections. Improves overall accuracy by inferring social relationships between individuals based on set of rules using occurrences, co-occurrences and ages of identified individuals. Infers social relationship of the persons in personal image collection to allow the computing device to organize or search the collection of images for the inferred social relationship. Infers social relationship of persons in incremental manner such that new images, new individuals, and new relationships can be properly handled. Enables to track the evolution of individuals in terms of changing appearances and social relationships in terms of expansion, e.g., new family members and new friends.
   DETAILED DESCRIPTION - The method involves searching photo collection to identify individuals in the photo collection, and determining the gender and age range of identified individuals. The social relationships between individuals are inferred based on the set of rules using the occurrences, co-occurrences and ages of the identified individuals. The relationships include parent, child, spouse, other relatives, child's friends and acquaintances. The rules include hard and soft rules including parents are older than their children, spouses are of opposite gender, and parents and grandparents appear with their children or grand children. Other rules are friend and relatives are clustered across the photos, acquaintance appears in less images than relatives and friends, and people with closer relationship appear closer in photos than those who do not have close relationship.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flowchart of the operations in categorizing the relationship of individuals from the collection of photos taken in multiple sets of events.
   Face detection/recognition step (24)
   Evidence extraction step (26)
   Identified person and associated evidence storing step (28)
   Social relationship inferring step (30)
   Image collection searching/organizing step (32)
PD WO2009094142-A1   30 Jul 2009      200952   Pages: 25   English
   US2009192967-A1   30 Jul 2009   G06N-005/02   200952      English
   EP2245580-A1   03 Nov 2010   G06N-001/00   201072      English
   JP2011514575-W   06 May 2011   G06F-017/30   201132   Pages: 15   Japanese
   US7953690-B2   31 May 2011   G06N-005/02   201135      English
   JP5383705-B2   08 Jan 2014   G06F-017/30   201404   Pages: 11   Japanese
UT DIIDW:2009M03430
ER

PT P
PN CN101101752-A; CN101101752-B
TI Monosyllabic language lip-reading recognition system, has video decoding module conveying input face video signal into frame image sequence, and lip language recognition module performing recognition to image sequence.
AB    NOVELTY - The system has a video decoding module (10) conveying an input face video signal into a frame image sequence for transmission to a lip positioning unit. A lip positioning module (20) finds positions face of people from the image sequence. A character extracting module (40) extracts and describes low and high level video characters. A model establishing module (60) obtains a character vector of a monosyllabic lip motion image sequence. A lip language recognition module (70) performs recognition to the image sequence and combines a monosyllabic spell character with an output of final users.
   USE - Monosyllabic language lip-reading recognition system.
   ADVANTAGE - The language base content of the system is rich and can be easily extended. The system avoids the audio data to perform auxiliary recognition. The system satisfies the requirement of talking content recognition under voiceless condition. The lip motion dividing part of the system performs machine intelligence division with monosyllabic as recognition target. The practicability of the system is higher and the nicety rate of recognition is greatly improved.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a monosyllabic language lip-reading recognition system.'(Drawing includes non-English language text)'
   Video decoding module (10)
   Lip positioning module (20)
   Lip motion division module (30)
   Character extracting module (40)
   Language base (50)
   Model establishing module (60)
   Lip language recognition module (70)
PD CN101101752-A   09 Jan 2008   G10L-015/24   200828   Pages: 28   Chinese
   CN101101752-B   01 Dec 2010   G10L-015/24   201113      Chinese
UT DIIDW:2008D87208
ER

PT P
PN US2006245639-A1; US7415152-B2
TI Object`s e.g. face, 3D image generating method for e.g. cell phone, involves identifying which 2D image of corpus best matches 2D image of target face, and generating 2D images dynamically from 3D image using 3D model/parameters.
AB    NOVELTY - The method involves generating 2D images of faces at different image conditions from 3D images. A set of parameters that maps feature points of the 2D image to the feature points of the 2D image of a standard face is calculated by a recognition system. The 2D images are texture mapped to the 3D image and points occluded in the input 2D image are interpolated by the system. The system identifies which 2D image of the corpus best matches the 2D image of a target face and generates the 2D images for various image conditions dynamically from the 3D image using a 3D model and the parameters.
   USE - Used in a computer system (claimed) such as a cell phone, personal digital assistant, smart phone, personal computer, programmable consumer electronics, digital camera, server computer, hand-held or laptop device, multiprocessor system, microprocessor-based system, network PCs, minicomputer and mainframe computer, for generating a 3D image of an object e.g. face, vehicle, and armament, in a security and automatic face recognization application for identifying a person of interest or confirming an identity of a person seeking access to a resource and processing of digital photograph application for identifying a people within each photograph.
   ADVANTAGE - The system identifies which 2D image of the corpus best matches the 2D image of the target face and generate the 2D images of the face for the various image conditions dynamically from the 3D image using the 3D model and the parameters, thus automatically generating the 2D images of the faces under various image conditions and using the 2D images to recognize target face in the 2D image.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer-readable medium containing instructions for performing a 3D image generating method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a 3D image generating method.
PD US2006245639-A1   02 Nov 2006   G06K-009/00   200707   Pages: 12   English
   US7415152-B2   19 Aug 2008   G06K-009/00   200857      English
UT DIIDW:2007070522
ER

PT P
PN US2006078172-A1; US7436988-B2
TI Automatic face authentication/recognition method used in law enforcement, involves computing symmetry profile by intersecting bilateral symmetry plane/facial surface to determine compact representation of face, using face/cheek profiles.
AB    NOVELTY - Bilateral symmetry plane of face surface is extracted from three-dimensional (3D) facial mesh. Symmetry profile is computed by intersecting symmetry plane/facial surface, to extract three points of nose determining face intrinsic coordinate system (FICS). Compact representation of face surface is formed by symmetry, forehead and cheek profiles. Face authentication/recognition is performed by comparing compact representations by FICS.
   USE - For performing three-dimensional (3D) face authentication and recognition for use in application such as law enforcement, security access control for building, room or transaction at automatic teller machine (ATM), and man-machine interaction.
   ADVANTAGE - Performs authentication and recognition using 3D facial data in more accurate and faster manner, with less computational complexity.
   DESCRIPTION OF DRAWING(S) - The figure shows a flow diagram explaining the operation of authenticating facial image or scan of individual.
PD US2006078172-A1   13 Apr 2006   G06K-009/00   200630   Pages: 21   English
   US7436988-B2   14 Oct 2008   G06K-009/00   200869      English
UT DIIDW:2006292193
ER

PT P
PN US2005169511-A1; WO2005076229-A1
TI Document e.g. currency bill, processing system e.g. color printer, for use in financial institution, has image scanner capturing visual and ultraviolet pictorial images on pixel-by-pixel basis.
AB    NOVELTY - The system comprises an image scanner (14) for capturing visual and ultraviolet pictorial images on a pixel-by-pixel basis from a face of a financial document. A processor (18) is provided for controlling the speed of a transport mechanism to collect the pictorial images from the scanner. The processor compares the captured pictorial images with master pictorial images stored in a memory (21).
   USE - Used in a financial institution for processing a currency document e.g. currency bill and substitute currency media e.g. casino ticket such as EZ pay ticket or quicket, or promotional media such as Disney dollar or toys R Us Geoffrey Dollar, retailer coupon, gift certificate, gift card, food stamp and barcode ticket, and non-currency document e.g. check, deposit slip, withdrawal slip, coupon and loan payment document, food stamp, cash ticket, loan application, credit card application, student loan application, accounting invoice, debit form and account transfer form.
   ADVANTAGE - The scanner captures visual and ultraviolet pictorial images on a pixel-by-pixel basis, thus accurately identifying and authenticating the documents.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for document processing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an identification device.
   Input receptacle (10)
   Transport mechanism (12)
   Image scanner (14)
   Scanner (18)
   Memory (21)
PD US2005169511-A1   04 Aug 2005   G06K-009/00   200557   Pages: 12   English
   WO2005076229-A1   18 Aug 2005   G07D-007/12   200557      English
UT DIIDW:2005562609
ER

PT P
PN US2003165270-A1; US7003139-B2
TI Affective image information determining method for imaging systems e.g. digital cameras, involves displaying digital image and monitoring facial expression of user to determine affective information for digital image.
AB    NOVELTY - The method involves displaying a digital image for the user's view. The facial expression of the user is monitored when the user views the digital image. Affective information for the digital image is determined using the facial expression of the user. A video camera is provided to monitor the facial expression of the user.
   USE - Used for determining affective image information in imaging systems e.g. digital cameras.
   ADVANTAGE - The method provides unique personalized affective information associated with the digital images for future usage e.g. retrieval, communication, sharing, advertising and marketing. The method continuously updates the existing affective information. The history of the users reaction to a given image can be used to analyze the change in persons reaction, which is used for therapeutic, diagnostic, or retrospective purposes.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for providing affective information for images in an imaging system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram showing the system for providing affective information based on the analysis of skin conductance.
PD US2003165270-A1   04 Sep 2003   G06K-009/00   200379   Pages: 16   English
   US7003139-B2   21 Feb 2006   G06K-009/00   200615      English
UT DIIDW:2003851960
ER

PT P
PN JP11175718-A; US6608914-B1; US2003194113-A1; US6697505-B2; JP3621245-B2
TI Recognition apparatus for person identification - has updating component which performs automatic updating of recognition information of person in holder, using computed value or extracted value of recognition information characteristics.
AB       NOVELTY - An updating component performs automatic updating of recognition information of a person registered in a holder (18), using the computed value or extracted value of recognition information characteristics. A recognition component (16) identifies the person via registration information stored in the holder. DETAILED DESCRIPTION - An updating information generator produces new registration recognition information corresponding to the secular change caused by aging person.
   USE -   For person identification.
   ADVANTAGE -   Maintains stable recognition rate and reduces influence of secular change in person's face by updating process. Increases efficiency of recognition even if large change happened, e.g. existence of spectacles and a decoration is added to the face, by comparing multiple data with recognition information stored in database. prevents reduction of recognition rate by automatic updating using mechanism. DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the recognition apparatus. (16) Recognition component; (18) Holder.
PD JP11175718-A   02 Jul 1999   G06T-007/00   199937   Pages: 10   Japanese
   US6608914-B1   19 Aug 2003   G06K-009/00   200356      English
   US2003194113-A1   16 Oct 2003   G06K-009/00   200369      English
   US6697505-B2   24 Feb 2004   G06K-009/00   200415      English
   JP3621245-B2   16 Feb 2005   G06T-007/00   200513   Pages: 16   Japanese
UT DIIDW:1999434788
ER

PT P
PN WO2011017557-A1; US2011125735-A1; CA2771094-A1; AU2010279333-A1; EP2462520-A1; KR2012058538-A; CN102625937-A; JP2013501975-W; AU2010279333-B2; AU2013205924-A1; AU2013245488-A1; CN102625937-B; US2014164406-A1; EP2462520-B1; JP2015064901-A; US9135277-B2; AU2016201546-A1; AU2013245488-B2; JP5933677-B2; JP2016139424-A; KR2016092045-A; AU2016201546-B2; KR1667346-B1; AU2017200336-A1; KR1725885-B1; JP6148367-B2; BR112012002815-A2; US10031927-B2; US2019012334-A1; US10534808-B2; CA3068761-A1; CA2771094-C; BR112012002815-B1
TI Computer-implemented processing method for processing visual query involves sending one of multiple search results to client system after receiving search results from one or more parallel search systems.
AB    NOVELTY - The computer-implemented processing method involves sending one of the multiple search results to a client system (102) after receiving the search results from one or more parallel search systems (112A-112N). The visual query is sent to the parallel search systems for simultaneous processing. One of the search systems implements a visual query search process. The visual query search processes include optical character recognition (OCR), facial recognition and a first query-by-image process.
   USE - Computer-implemented processing method for processing visual query.
   ADVANTAGE - Enables visual querying of photograph, screen shot, scanned image or a video frame through mobile device or desktop device.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a search engine system; and
   (2) a computer-readable storage medium.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of a computer network.
   Computer network (100)
   Client system (102)
   Communication network (104)
   Visual query server system (106)
   Search systems (112A-112N)
PD WO2011017557-A1   10 Feb 2011   G06F-017/30   201113      English
   US2011125735-A1   26 May 2011   G06F-017/30   201135      English
   CA2771094-A1   10 Feb 2011   G06F-017/30   201226      English
   AU2010279333-A1   15 Mar 2012   G06F-017/30   201230      English
   EP2462520-A1   13 Jun 2012   G06F-017/30   201238      English
   KR2012058538-A   07 Jun 2012      201240      
   CN102625937-A   01 Aug 2012   G06F-017/30   201267      Chinese
   JP2013501975-W   17 Jan 2013   G06F-017/30   201306   Pages: 37   Japanese
   AU2013205924-A1   06 Jun 2013   G06F-017/30   201371      English
   AU2013245488-A1   07 Nov 2013   G06F-017/30   201374      English
   CN102625937-B   12 Feb 2014   G06F-017/30   201426      Chinese
   US2014164406-A1   12 Jun 2014   G06F-017/30   201438      English
   EP2462520-B1   02 Jul 2014   G06F-017/30   201444      English
   JP2015064901-A   09 Apr 2015   G06F-017/30   201526   Pages: 39   Japanese
   US9135277-B2   15 Sep 2015   G06F-017/30   201561      English
   AU2016201546-A1   31 Mar 2016   G06F-017/30   201634      English
   AU2013245488-B2   21 Apr 2016   G06F-017/30   201636      English
   JP5933677-B2   15 Jun 2016   G06F-017/30   201640   Pages: 35   Japanese
   JP2016139424-A   04 Aug 2016   G06F-017/30   201652   Pages: 36   Japanese
   KR2016092045-A   03 Aug 2016   G06F-017/30   201655      
   AU2016201546-B2   20 Oct 2016   G06F-017/30   201670      English
   KR1667346-B1   18 Oct 2016   G06F-017/30   201672      
   AU2017200336-A1   02 Feb 2017   G06F-017/30   201712      English
   KR1725885-B1   11 Apr 2017   G06F-017/30   201729      
   BR112012002815-A2   05 Sep 2017   G06F-017/30   201775      English
   US10031927-B2   24 Jul 2018   G06K-009/00   201850      English
   US2019012334-A1   10 Jan 2019   G06F-017/30   201904      English
   US10534808-B2   14 Jan 2020      202006      English
   CA3068761-A1   10 Feb 2011   G06F-017/30   202016      English
   CA2771094-C   24 Mar 2020   G06F-016/50   202029      English
   BR112012002815-B1   09 Jun 2020   G06F-017/30   202055      English
UT DIIDW:2011B51753
ER

PT P
PN CN101393599-A; CN101393599-B
TI Game characters control method, involves analyzing expression and movement information of eyes, lips and eyebrows by utilizing face feature point positions, and applying face expression information to control game characters.
AB    NOVELTY - The method involves learning target samples of faces by a statistical method to obtain target mode of player faces. Video images of the players are collected by an image input device, where the video images of the players are pre-processed to produce pre-processed images. Faces are aligned in a face area by a face feature point location algorithm to obtain positions of face feature points. Expression and movement information of eyes, lips and eyebrows are analyzed by utilizing the face feature point positions, and the face expression information is applied to control game characters.
   USE - Method for controlling game characters based on face expression.
   ADVANTAGE - The game characters can be controlled by the face expression of players, which extends the interaction mode of the game. The method easily and effectively achieves video detection method and operates with good robustness in real time, and players can interactively operate in natural, intelligent and new modes, thus increasing interactivity and immersion of the game.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a game characters control method.'(Drawing includes non-English language text)'
PD CN101393599-A   25 Mar 2009   G06K-009/00   200926   Pages: 16   Chinese
   CN101393599-B   08 Feb 2012   G06K-009/00   201218      Chinese
UT DIIDW:2009G76894
ER

PT P
PN US2008275830-A1; US8126220-B2
TI Method for annotating audio visual data, involves generating annotation of audio-video stimulus based on emotional response of audience.
AB    NOVELTY - Several facial expressions of an audience is detected based on a stimulus such as audio-video stimulus. An emotional response to the stimulus is detected based on the facial expressions. An annotation of the stimulus is generated based on the determined emotional response.
   USE - Method for annotating audio visual data for measuring strength of audience response to stimulus such as movie.
   ADVANTAGE - Annotations are created in real time, and can be indexed, rated and searched in an easy manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows an explanatory view of the annotation generation module.
   Annotation generation module (400)
   Face detection logic (430)
   Facial analysis logic (440)
   Mood analysis logic (450)
   Annotation generation logic (460)
PD US2008275830-A1   06 Nov 2008   G06F-015/18   200877   Pages: 8   English
   US8126220-B2   28 Feb 2012   G06K-009/00   201216      English
UT DIIDW:2008N20307
ER

PT P
PN US2007110285-A1; US7801335-B2
TI Living human eye detecting method for performing iris-based recognition, involves determining whether portion of image includes representation of other image on screen reflected by curved surface consistent with human eye.
AB    NOVELTY - The method involves presenting an image on a computer screen, where the computer screen is oriented to face a user. A camera is positioned proximate to the computer screen and is oriented to face the user, so that light emitted by the computer screen as the image is reflected by the user and captured by the camera. Another image is obtained through the camera, and a determination is made to whether a portion of the latter image includes a representation of the former image on the computer screen reflected by a curved surface consistent with a human eye.
   USE - Used for detecting a living human eye to perform an iris-based recognition. Can also be used for uniquely identifying a person.
   ADVANTAGE - The method deters spoofing with a photograph and optimizes performance for specific applications. The method reduces the cost, size and acquisition time for detecting a living human eye.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method of detecting a human iris.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a method for detecting a human eye.
PD US2007110285-A1   17 May 2007   G06K-009/00   200754   Pages: 13   English
   US7801335-B2   21 Sep 2010   G06K-009/00   201062      English
UT DIIDW:2007557984
ER

PT P
PN EP1413972-A1; WO2004038646-A1; AU2003274009-A1; US2006153429-A1; DE50212936-G; EP1413972-B1; US7715596-B2
TI Checking recorded images of persons, especially for suitability for identification, involves segmenting into background and head or face regions, analyzing and comparing characteristic values.
AB    NOVELTY - The method involves segmenting the image into a background region and a head region or a face region, analyzing the head region or face region to determine at least one characteristic value and comparing the at least one characteristic value with at least one predefined threshold characteristic value. The segmenting step includes carrying out an object recognition process.
   USE - For checking recorded images of persons.
   ADVANTAGE - Overcomes certain disadvantages and imponderables in quality control of personal images and special portrait images, e.g. that quality criteria are only vaguely defined and only checked by visual inspection.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) a method of correction of digitized personal images
   (b) a data processing device for implementing the inventive method
   (c) and a system for quality checking and correction of digitized personal images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic flow diagram of an inventive method
   image digitization/transmission (1)
   checking image versus global criteria (2)
   egmenting (3)
   checking image versus local criteria (4)
   image correction (5)
   ignal results and/or data export (6)
PD EP1413972-A1   28 Apr 2004   G06K-009/00   200434   Pages: 14   English
   WO2004038646-A1   06 May 2004   G06K-009/00   200434      German
   AU2003274009-A1   13 May 2004   G06K-009/00   200468      English
   US2006153429-A1   13 Jul 2006   G06K-009/00   200646      English
   DE50212936-G   04 Dec 2008   G06K-009/00   200882      German
   EP1413972-B1   22 Oct 2008   G06K-009/00   200922      German
   US7715596-B2   11 May 2010   G06K-009/00   201032      English
UT DIIDW:2004358494
ER

PT P
PN EP910986-A1; WO9921479-A1; AU9896351-A; EP1024748-A1; CN1276710-A; KR2001031389-A; JP2001520904-W; AU743382-B; US2002164054-A1; US6687389-B2; EP1024748-B1; DE69829496-E; ES2241168-T3; DE69829496-T2; CN1221212-C; CA2306876-C
TI Imaging apparatus suitable for capturing image of one or more of user's facial features e.g. iris.
AB    NOVELTY - The apparatus includes a spherical mirror (45) which presents a concave surface to the user and is disposed between the user and the camera (47). The user is able to conclude that he is close enough to the camera (47) for successful image capture when the reflection he sees in the mirror (45) is the right way up.
   USE - As part of e.g. iris recognition apparatus.
   ADVANTAGE - User is able to adopt sufficiently accurate preferred position in relation to apparatus in a simple manner. Apparatus is relatively inexpensive.
   DETAILED DESCRIPTION - Markings may be provided on the front surface of the mirror to aid the user in placing his eye within predetermined range of distances from the camera (47). An INDEPENDENT CLAIM is included for an identification apparatus.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic illustration partly in cross section, of the arrangement of components within an iris capture part of the data sequence generation unit.
   Mirror (45)
   Camera (47)
PD EP910986-A1   28 Apr 1999   A61B-005/117   199922   Pages: 15   English
   WO9921479-A1   06 May 1999   A61B-005/117   199925      English
   AU9896351-A   17 May 1999   A61B-005/117   199939      English
   EP1024748-A1   09 Aug 2000   A61B-005/117   200039      English
   CN1276710-A   13 Dec 2000   A61B-005/117   200118      Chinese
   KR2001031389-A   16 Apr 2001   A61B-005/117   200163      
   JP2001520904-W   06 Nov 2001   A61B-005/117   200203   Pages: 25   Japanese
   AU743382-B   24 Jan 2002   A61B-005/117   200221      English
   US2002164054-A1   07 Nov 2002   G06K-009/00   200275      English
   US6687389-B2   03 Feb 2004   G06K-009/00   200413      English
   EP1024748-B1   23 Mar 2005   A61B-005/117   200523      English
   DE69829496-E   28 Apr 2005   A61B-005/117   200530      German
   ES2241168-T3   16 Oct 2005   A61B-005/117   200571      Spanish
   DE69829496-T2   13 Apr 2006   A61B-005/117   200626      German
   CN1221212-C   05 Oct 2005   A61B-005/117   200655      Chinese
   CA2306876-C   25 Sep 2007   A61B-005/117   200765      English
UT DIIDW:1999256372
ER

PT P
PN EP838737-A1; FR2755269-A1; JP10132968-A; KR98032629-A; SG71049-A1; US6184871-B1; EP838737-B1; DE69709522-E; CN1181561-A; CN1160656-C; KR522219-B1
TI System for identification of a manual action on a touch sensitive surface - comprises assembly of detectors each capable of activation by the finger, etc..
AB       The device for detection of a finger pressed on a surface, which is comprised of an assembly of detectors (41), each being capable of activation by the said finger which causes a change in an electric quantity of the detectors (41). The detectors are laid out together so that they correspond to surface zones. Detection means (42) are provided to determine which detector (41) amongst an array of detectors which are simultaneously activated has the greatest variation in electrical quantity. Means for conversion of the electrical quantity of the detectors to a signal the frequency of which is proportional to the input electrical quantity.
   USE/ADVANTAGE -   The device is used in wrist watch with means for detection of characters pressed or drawn on the watch face using a finger or stylus. The character drawn or area pressed is detected by production of electrical signals using, for example, photoelectric or capacitative effects. Equally the character detection can be used for other similar screen based devices. The device provides improved detection of area pressed or character drawn is made possible by improved selection of detector with highest change in electrical quantity being measured.
PD EP838737-A1   29 Apr 1998   G04G-001/00   199826   Pages: 9   French
   FR2755269-A1   30 Apr 1998   G06K-011/06   199826      French
   JP10132968-A   22 May 1998   G04G-001/00   199831   Pages: 6   Japanese
   KR98032629-A   25 Jul 1998   G06K-009/00   199932      
   SG71049-A1   21 Mar 2000   G04G-001/00   200022      English
   US6184871-B1   06 Feb 2001   G09G-005/00   200109      English
   EP838737-B1   09 Jan 2002   G04G-001/00   200211      French
   DE69709522-E   14 Feb 2002   G04G-001/00   200220      German
   CN1181561-A   13 May 1998   G06K-009/18   200238      Chinese
   CN1160656-C   04 Aug 2004   G06K-009/18   200612      Chinese
   KR522219-B1   12 Jan 2006   G06K-009/00   200682      
UT DIIDW:1998288483
ER

PT P
PN US8515139-B1
TI Method for detecting facial features of images, involves capturing image including face of user and calculating and analyzing face template of face of user to determine whether face includes removable or non-removable facial features.
AB    NOVELTY - The method (500) involves capturing (502) an image including a face of a user. A face template of the face of the user is calculated (504) in the image. The face template is analyzed (506) to determine (508,512) whether the face includes a removable facial features or a non-removable facial features. A notification is outputted (510) for the user to remove the removable facial features when the face includes the removable facial features. A similarity score threshold is adjusted (514) to another similarity score threshold when the face includes the non-removable facial features.
   USE - Method for detecting facial features of images captured for use in facial recognition.
   ADVANTAGE - The face template is analyzed to determine whether the face includes removable facial features or non-removable facial features which ensures that the facial recognition results are improved by preventing potential enrollment templates including removable facial features and non-removable facial features from being stored as enrollment templates. A facial recognition failure is prevented by analyzing a face template calculated from a captured image for removable facial features and non-removable facial features and a user is authenticated before facial recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computing device comprises a processor and a camera operable by the processor to capture an image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a process that is performed by a computing device to analyze a face image and detect facial features that decreases a level of distinctiveness between two faces during facial recognition.
   Facial features detecting method (500)
   Capturing an image including a face of a user (502)
   Calculating face template of the face of the user (504)
   Analyzing face template (506)
   Determining whether the face includes a removable facial features or a non-removable facial features (508,512)
   Outputting a notification for the user to remove the removable facial features when the face includes the removable facial features (510)
   Adjusting similarity score threshold to another similarity score threshold when the face includes the non-removable facial features (514)
PD US8515139-B1   20 Aug 2013   G06K-009/00   201358   Pages: 21   English
UT DIIDW:2013M47127
ER

PT P
PN CN101404060-A; CN101404060-B
TI Face recognizing method, involves placing positive near infrared light on near infrared camera, and extracting Gabor features of visible light face picture and near infrared face picture.
AB    NOVELTY - The method involves placing a positive near infrared light on a near infrared camera, where the positive near infrared light is provided with LEDs with wavelength of 850 nanometers. The LEDs and the infrared camera are installed at a same time when the positive light and the infrared camera are combined. The infrared camera is connected in a same picture collecting card through a picture collecting software. Backgrounds of a visible light face picture and a near infrared face picture are removed. Gabor features of the visible light face picture and the near infrared face picture are extracted.
   USE - Method for recognizing a face.
   ADVANTAGE - The method provides high accuracy with better robustness to the light face identification, and utilizes less feature numbers and fast sorting speed.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a face recognizing method.'(Drawing includes non-English language text)'
PD CN101404060-A   08 Apr 2009   G06K-009/00   200930   Pages: 24   Chinese
   CN101404060-B   30 Jun 2010   G06K-009/00   201058      Chinese
UT DIIDW:2009H27221
ER

PT P
PN CN1769107-A; CN100341732-C
TI Automobile anti-theft method based on human face identification technology.
AB    NOVELTY - The invention discloses a vehicle anti-thief method based on the face-identification technique, which comprises following steps: first collecting data; then combining the motion visual information to process face identification; at last, controlling the identified result. After the identification, the identified information is connected to the control bus of vehicle power system, while the identified result is illegal and the vehicle is not started, controlling the motor of vehicle to avoid the start, and while the vehicle is moving, the main control system sends the control signal to the speed control system to limit its mph and combine the vehicle-carrying GPS positioning system to send the alarm information of real-time position of vehicle, which can apply the tracing and catching on the thief.
PD CN1769107-A   10 May 2006   B60R-025/00   200663      Chinese
   CN100341732-C   10 Oct 2007   B60R-025/00   200829      Chinese
UT DIIDW:2006603187
ER

PT P
PN WO2005038700-A1; GB2408615-A; EP1673714-A1; GB2408615-B; GB2428325-A; JP2007508609-W; US2007122007-A1; SG131934-A1; GB2428325-B; KR2007003759-A; US7689043-B2; JP4860472-B2; KR1149931-B1; SG123829-A1; SG123829-B; SG131934-B
TI Image recognition method for identification and security purposes, involves generating image key from image set represented in transformed data space, and comparing image key with previously stored image key of known image.
AB    NOVELTY - The processed images are combined in the image set, and the data space occupied by processed images is transformed. An image key representative of the image, is generated from the image set represented in the transformed data space. The image key is compared with previously stored image key of a known image.
   USE - For recognition of natural and synthetic three-dimensional images, for identification of geographical locations, and for identification and security purposes in commercial and industrial applications.
   ADVANTAGE - A high level of accuracy can be achieved when performing recognition on a large database of 3D face models captured under different conditions.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for image recognition apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart explaining the image recognition procedure.
PD WO2005038700-A1   28 Apr 2005   G06K-009/62   200541   Pages: 56   English
   GB2408615-A   01 Jun 2005   G06K-009/52   200541      English
   EP1673714-A1   28 Jun 2006   G06K-009/62   200643      English
   GB2408615-B   13 Dec 2006   G06K-009/00   200682      English
   GB2428325-A   24 Jan 2007   G06K-009/52   200708      English
   JP2007508609-W   05 Apr 2007   G06T-007/00   200726   Pages: 38   Japanese
   US2007122007-A1   31 May 2007   G06K-009/00   200736      English
   SG131934-A1   28 May 2007      200737      English
   GB2428325-B   01 Aug 2007   G06K-009/52   200753      English
   KR2007003759-A   05 Jan 2007   G06K-009/00   200755      
   US7689043-B2   30 Mar 2010   G06K-009/00   201023      English
   JP4860472-B2   25 Jan 2012   G06T-007/00   201208   Pages: 32   Japanese
   KR1149931-B1   30 May 2012   G06K-009/00   201238      
   SG123829-A1   30 Aug 2006   G06K-000/00   201406      English
   SG131934-B   30 May 2008   G06K-009/52   201408      English
UT DIIDW:2005404671
ER

PT P
PN US2003142849-A1; US6831993-B2
TI Security system for movable vehicle e.g. sedan-type car, has electronic facial recognition system that produces select control signals if signals representing select characteristics of scanned face match member of set of stored signals.
AB    NOVELTY - An electronic facial recognition system, coupled to an electronic storage device and to the output of a camera (12), produces select control signals if signals representing select characteristics of a scanned face match a member of the set of stored signals. The electronic facial recognition system has an output coupled to the enabling system of a powered vehicle (11).
   USE - For movable vehicle e.g. sedan-type car.
   ADVANTAGE - Improves antitheft property of vehicle. Prevents unauthorized person from operating vehicle based on electronic facial recognition. Ensures secure keyless operation of vehicle. Does not require modification to vehicle when installing security system.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) a movable vehicle securing apparatus;
   (b) a movable vehicle;
   (c) a moving vehicle securing method; and
   (d) a security system fitting method.
   DESCRIPTION OF DRAWING(S) - The figure shows the outline side view of a moving vehicle.
   Powered vehicle (11)
   Camera (12)
   Support beam (15)
   Dashboard (17)
   Ignition switch (22)
PD US2003142849-A1   31 Jul 2003   G06K-009/00   200371   Pages: 17   English
   US6831993-B2   14 Dec 2004   G06K-009/00   200503      English
UT DIIDW:2003755499
ER

PT P
PN EP1260935-A2; US2002191818-A1; JP2003044853-A; EP1260935-B1; DE60213032-E; US7123754-B2; DE60213032-T2; EP1260935-A3
TI Face detector has face and non-face identification unit which judges whether or not each extracted partial image contains facial image with reference to learning dictionary.
AB    NOVELTY - An edge image extraction unit (7) take-out edge image from the subject image. A partial image extraction unit (8) take-out partial images containing facial images from the subject image based on edge image. A face and non-face identification unit (9) judges whether or not each extracted partial image contains facial image with reference to a learning dictionary (10).
   USE - Face detector.
   ADVANTAGE - Can be handled conveniently and can automatically detect the position and approximate size of the person's face.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) a partial image extraction device;
   (b) and a face pose detection method.
   DESCRIPTION OF DRAWING(S) - The figure is a block diagram of a face pose detector.
   Edge image extraction unit (7)
   Partial image extraction unit (8)
   Face and non-face identification unit (9)
   Learning dictionary (10)
PD EP1260935-A2   27 Nov 2002   G06K-009/00   200307   Pages: 16   English
   US2002191818-A1   19 Dec 2002   G06K-009/00   200307      English
   JP2003044853-A   14 Feb 2003   G06T-007/00   200322   Pages: 9   Japanese
   EP1260935-B1   12 Jul 2006   G06K-009/00   200652      English
   DE60213032-E   24 Aug 2006   G06K-009/00   200657      German
   US7123754-B2   17 Oct 2006   G06K-009/00   200668      English
   EP1260935-A3   14 Jan 2004   G06K-009/00   201732      English
UT DIIDW:2003069410
ER

PT P
PN US2002105530-A1; EP1231569-A1; JP2003030276-A; US7016824-B2; US7016824-C1
TI Online eyeglass selling method using interactive internet based transactions, involves placing 3D representation of eyeglass image on default position of 3D face model based on face model characteristics.
AB    NOVELTY - An interactive platform is displayed on the client devices (114-1 - 114-n) for receiving three-dimensional (3D) face model. The characteristics of the received model is determined with respect to 3D reference frame. The 3D representation of an eyeglass image is retrieved and is placed on default position on the face model corresponding to the characteristics of the received face model.
   USE - For online selling of eyeglass using interactive internet based transactions.
   ADVANTAGE - Permits user to try or test selected item, visually due to interactive 3D presentation, thereby marketing of goods or services is promoted and customer service efficiency is improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) Eyeglass selling system; and
   (2) Software product for online eyeglass selling.
   DESCRIPTION OF DRAWING(S) - The figure shows an outline of the eyeglass selling system.
   Client devices (114-1 - 114-n)
PD US2002105530-A1   08 Aug 2002   G09G-005/00   200270   Pages: 26   English
   EP1231569-A1   14 Aug 2002   G06T-017/40   200270      English
   JP2003030276-A   31 Jan 2003   G06F-017/50   200319   Pages: 75   Japanese
   US7016824-B2   21 Mar 2006   G06G-007/48   200621      English
   US7016824-C1   29 Jul 2014   G06G-007/48   201450      English
UT DIIDW:2002656732
ER

PT P
PN JP3203399-A; US5140643-A
TI Electronic component mounter - has mirror which enters image light representing shape of part vertical to optic axis of shape recognition device NoAbstract Dwg 1/12.
AB    (JP3203399-A)      The plasma treatment appts. used for plasma dry etching or plasma CVD has at least one pole of an upper pole (3) and a lower pole (2) and the gas injecting port of a gas introducing device (8) which are disposed shiftably against facing direction of the poles.
   ADVANTAGE -   The distance between the poles is arbitrarily decided, and optimum distance is presented for various step dimensions. @(5pp Dwg.No.1/4)@
PD JP3203399-A   05 Sep 1991      199142      Japanese
   US5140643-A   18 Aug 1992      199236   Pages: 20   English
UT DIIDW:1991306129
ER

PT P
PN EP338376-A; DE3813725-A; DE3813725-C; US5025477-A; EP338376-B1; DE58907922-G; ES2055755-T3
TI Optical mask scanning and automatic return of rejects - using video camera linked to central display console for identification of articles not recognised from scans.
AB       At each data acquisition station (10) the articles are transferred from an entry conveyor (14) to an exit conveyor (16) on which each article breaches an optical barrier (18). The resulting signal triggers a scanning arrangement of mirrors (20) or windows on the facing surfaces of walls (22,24) between which the conveyor (16) passes.
   Scanning is terminated when a second optical barrier (26) is breached. Unidentified articles are returned automatically to the entry barrier (18) for rescanning. After a predetermined number of unsuccessful attempts, an image of the unidentifiable article is sent from the camera (28) to the central station (12).
PD EP338376-A   25 Oct 1989      198943      German
   DE3813725-A   09 Nov 1989      198946      German
   DE3813725-C   17 Jan 1991      199103      German
   US5025477-A   18 Jun 1991      199127      English
   EP338376-B1   22 Jun 1994   G06K-007/10   199424   Pages: 18   German
   DE58907922-G   28 Jul 1994   G06K-007/10   199429      German
   ES2055755-T3   01 Sep 1994   G06K-007/10   199436      Spanish
UT DIIDW:1989310845
ER

PT P
PN CN106851216-A; CN106851216-B
TI Face and speech recognition based class action monitoring system, has main control processor for processing voice information to extract facial expression characteristic and behavior characteristics of students to calculate teaching score.
AB    NOVELTY - The system has a decoder connected with an omni-directional rotation camera and an image divider. The image divider collects video data information of classroom students and a teacher. A voice information collecting system is provided with a student desk that is connected with a recording device platform. The recording device platform collects voice information of the teacher. A main control processor process the voice information to extract facial expression characteristic and behavior characteristics of the students to calculate a teaching score.
   USE - Face and speech recognition based class action monitoring system.
   ADVANTAGE - The system has high teaching quality, and observes classroom behavior of the students so as to improve monitoring accuracy and evaluation objectivity.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face and speech recognition based class action monitoring method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a face and speech recognition based class action monitoring system. '(Drawing includes non-English language text)'
PD CN106851216-A   13 Jun 2017   H04N-007/18   201756   Pages: 10   Chinese
   CN106851216-B   28 May 2019   H04N-007/18   201941      Chinese
UT DIIDW:201742798K
ER

PT P
PN US9424461-B1
TI Computing device for recognition for two-dimensional and three-dimensional object and/or matching images of object, has memory including instructions that, when executed by processor for identifying respective type of object in image.
AB    NOVELTY - The device has a memory including instructions that, when executed by a processor for causing the computing device to generate (506) a compact combined visual feature vector by using a selected subset of visual feature vectors, compare (508) the compact combined visual feature vector to vectors of a set of stored vectors, determine a matching stored vector with a respective similarity score that meets a matching threshold, and identify respective type of an object represented in the image based on the matching stored vector.
   USE - Computing device for recognition for two-dimensional and three-dimensional object and/or matching images of the two-dimensional and three-dimensional object, by using an electronic device over a network. Uses include but are not limited to a mobile phone, a tablet computer, a wearable computer, a notebook computer, an electronic book reader, personal data assistant, cellular phone, and a video gaming console for recognition for monuments, buildings, consumer products, and food items and facial recognition applications over a local area network, a wide-area network, a virtual private network, Internet, an intranet, an extranet, a public switched telephone network, and an infrared network.
   ADVANTAGE - The device performs online object recognition process by extracting visual features of the object on the user's device, selecting and converting the visual features into a compact feature vector on the device, and providing the feature vector to the image analysis service on a remote server. The image analysis service can attempt to match the device model to one of the catalog simplified models of the set of simplifying models by comparing the device model against articulation configurations of each catalog simplified model so as to produce saliency information describing the quality of each match. The device guides the user to capture images with the proper orientation so as to improve the accuracy and speed of matching and/or identification process.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for recognition for two-dimensional and three-dimensional object and/or matching images of the two-dimensional and three-dimensional object.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a process for locating matches for an object in captured image information that can be utilized.
   Method for recognition for two-dimensional and three-dimensional object and/or matching images of the two-dimensional and three-dimensional object (500)
   Step for capturing image data (502)
   Step for analyzing the image data to detect visual feature vectors (504)
   Step for generating a compact combined visual feature vector (506)
   Step for comparing the compact combined visual feature vector to stored vectors of a set of stored vectors (508)
PD US9424461-B1   23 Aug 2016   G06K-009/00   201656   Pages: 28   English
UT DIIDW:201651175D
ER

PT P
PN CN105700682-A
TI Vision and voice based intelligent sex emotion identification and detection system, has voice module for identifying human voice emotion, and individual intelligent voice interaction system for transmitting emotion recognition result.
AB    NOVELTY - The system has a mood and sex identification module for determining emotion recognition and personnel of human face image. A voice module is utilized for identifying human voice emotion. A merge module is utilized for determining sex identification result based on an individual intelligent voice interaction system. The individual intelligent voice interaction system is utilized for transmitting emotion recognition result. A collecting module is provided with an image collecting device and a voice collecting device. The voice interaction system is provided with an information unit.
   USE - Vision and voice based intelligent sex emotion identification and detection system.
   ADVANTAGE - The system improves information service accuracy, driving experience and driving safety in an effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a vision and voice based intelligent sex emotion identification and detection method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a vision and voice based intelligent sex emotion identification and detection system. '(Drawing includes non-English language text)'
PD CN105700682-A   22 Jun 2016   G06F-003/01   201648   Pages: 17   Chinese
UT DIIDW:201641915C
ER

PT P
PN CN105023010-A; CN105023010-B
TI Human face detecting method, involves obtaining depth information value, and judging whether human face image is matched with recognition target according to depth information value.
AB    NOVELTY - The method involves obtaining two individual web images according to recognition target (S1). A human face classifier location is determined (S2) according to the individual web images. A human face image is obtained (S3). A characteristic point location of the human face image is determined (S4). A three-dimensional characteristic point of the human face image is determined (S5). A depth information value is obtained. A judgment is made (S6) to check whether the human face image is matched with the recognition target according to the depth information value.
   USE - Human face detecting method.
   ADVANTAGE - The method enables improving safety performance, matching speed, attitude adaptability and user experience degree and realizing human face detecting process with wide range of applications.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face detecting system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face detecting method. '(Drawing includes non-English language text)'
   Step for obtaining two individual web images according to recognition target (S1)
   Step for determining human face classifier location according to individual web images (S2)
   Step for obtaining human face image (S3)
   Step for determining characteristic point location of human face image (S4)
   Step for determining three-dimensional characteristic point of human face image (S5)
   Step for judging whether human face image is matched with recognition target according to depth information value (S6)
PD CN105023010-A   04 Nov 2015   G06K-009/00   201578   Pages: 15   Chinese
   CN105023010-B   06 Nov 2018   G06K-009/00   201878      Chinese
UT DIIDW:201572420X
ER

PT P
PN WO2015063032-A1; US2016250006-A1; EP3062733-A1; US11045293-B2
TI Method for placing and using design guides in digital environment for use in dental treatment, involves aligning digital two dimension image and three dimensional representation and generating design guide according to facial features.
AB    NOVELTY - The method involves obtaining digital three dimensional (3D) representation (5) of dental setup of patient. Digital two dimensional (2D) image (1) of portion of face of the patient are obtained. The digital 2D image and digital 3D representation are aligned. Design guide is generated according to facial features in the digital 2D image. The design guide is applied to the digital 3D representation.
   USE - Method for placing and using design guides in digital environment for use in dental treatment.
   ADVANTAGE - The use of design guide as threshold prevents design from passing or coming within preset boundaries of that threshold and facilitates designing canines by manipulating digital restoration representing canine and this, provides an image that is very close to true scale and enables the user to do measurements with the guide measurement on the image that are reliable. The practitioner or other user are enabled to use the digital image in the digital design of digital restoration on the digital 3D representation, since esthetics of digital restoration are easily checked using image view. The alignment of digital 3D representation and design guide(s) together enables showing the overlaid features in a common view. Thus, system efficiency is improved and cost is reduced.
   DESCRIPTION OF DRAWING(S) - The drawing shows a front view of a 2D image of a face of a patient.
   Two dimensional image (1)
   Three dimensional representation (5)
   Reference points on visible teeth (16-19)
   Canine guidelines (26,27)
   Alar sidewalls (30,31)
PD WO2015063032-A1   07 May 2015   A61C-009/00   201532      English
   US2016250006-A1   01 Sep 2016   A61C-013/00   201658      English
   EP3062733-A1   07 Sep 2016   A61C-009/00   201659      English
   US11045293-B2   29 Jun 2021   A61C-013/34   202153      English
UT DIIDW:201528325P
ER

PT P
PN CN103605965-A
TI Multi-posture human face identification method, involves identifying human face image, and obtaining human face identify result by processing human face identify process on corrected human face image.
AB    NOVELTY - The method involves identifying a human face image. A key point in the human face image is located by a key point positioning unit. A coordinate of the located key point is determined to determine deflection angle of the human face image. A human face posture correction process is performed in the human face image according to a preset human face rotation conversion rule based on deflection angle of the human face image. Human face identify result is obtained by processing a human face identify process on the corrected human face image.
   USE - Multi-posture human face identification method.
   ADVANTAGE - The method enables reducing identification time of the multi-pose human face.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multi-posture human face identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-posture human face identification method.'(Drawing includes non-English language text)'
PD CN103605965-A   26 Feb 2014   G06K-009/00   201429   Pages: 28   Chinese
UT DIIDW:2014H18001
ER

PT P
PN CN103020579-A; CN103020579-B
TI Human face image wearing glasses frame removing method, involves detecting positions of eyes of human face image wearing glasses, and obtaining and removing image of eye area and human face image and glass frame.
AB    NOVELTY - The method involves detecting positions of eyes of human face image wearing glasses. An estimated eye region is extracted to obtain a first image. A position of a glass frame is determined. A human face skin color model of the image is established. A pixel point of the frame is obtained. A second image of an eye area is obtained and removed from a human face image and the glass frame. Mathematical morphology operation is performed. The frame is placed with a convolution filter operation unit and matched with a third image. A preset eye module is fixed to a mask eyeball and a conversion module.
   USE - Human face image wearing glasses frame removing method.
   ADVANTAGE - The method enables realizing glass frame removing process so as to increase human face recognition rate.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a human face image wearing glasses frame removing device
   (2) a human face image wearing glasses frame recognition method
   (3) a human face image wearing glasses frame recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face image wearing glasses frame removing method.'(Drawing includes non-English language text)'
PD CN103020579-A   03 Apr 2013   G06K-009/00   201363   Pages: 22   Chinese
   CN103020579-B   25 Nov 2015   G06K-009/00   201602      English
UT DIIDW:2013P10135
ER

PT P
PN WO2011045422-A1
TI Method for measuring emotional probabilities of facial image of consumer during online advertisement presentation, involves generating measurable description comprising facial features based on model generated based on image.
AB    NOVELTY - The facial image of a person is received in an image receiving unit. A model is built based on the representation from the image. A feature description of model based representation is extracted. The measurable description of the image comprising facial features is generated based on movements, presence of features, and visual appearance found in the model. The computed facial features predicted classification probabilities from the image are output based on the measurable description of image.
   USE - Method for measuring emotional probabilities of facial image of consumer during online advertisement presentation.
   ADVANTAGE - The emotional probabilities of facial image can be measured accurately in less time and less cost without the need for special equipment except a standard home computer.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for system for measuring emotional probabilities of facial image of consumer.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart explaining the steps conducted on online survey to measure the emotional probabilities.
   Step for survey generation (100)
   Step for conducting survey (200)
   Step for predicting emotion (300)
   Step for survey data extraction (400)
   Step for analyzing survey data (500)
PD WO2011045422-A1   21 Apr 2011   G06K-009/00   201129   Pages: 36   English
UT DIIDW:2011E40792
ER

PT P
PN US2008205712-A1; WO2008104549-A2; WO2008104549-A3; EP2115662-A2; JP2010520521-W; EP2115662-B1; DE602008001607-E; JP5049356-B2; US8509561-B2
TI Human face characteristic determining method for scene captured in digital image, involves applying face recognition program to correct face image including determined characteristic, and editing and displaying corrected face image.
AB    NOVELTY - The method involves determining an initial location of a human face in a scene captured in a digital image. A fit of the model is obtained to the face by adjusting multiple individual values of multiple model components of the linear texture model. A characteristic of the face is determined based on the obtained fit of the model to the face in the scene captured in a digital image. A face recognition program is applied to correct the face image including the determined characteristic. The corrected face image is edited and displayed.
   USE - Method for determining a characteristic of a human face within a scene captured in a digital image.
   ADVANTAGE - The face recognition program is electronically stored, transmitted and applied to correct the face image including the determined characteristic and the corrected face image is edited and displayed, thus effectively determining the characteristic of a human face.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for adjusting a characteristic of a face or certain other object within a scene captured in a digital image
   (2) a method for constructing a linear texture model of a class of objects
   (3) a face illumination normalization method for acquiring a digital image
   (4) a face detection method for acquiring a digital image
   (5) a digital image acquisition device including an optoelectronic system for acquiring a digital image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a screen shot of different individuals at constant frontal illumination based on which a shape model and a texture model are built.
PD US2008205712-A1   28 Aug 2008   G06K-009/00   200860   Pages: 20   English
   WO2008104549-A2   04 Sep 2008   G06K-009/00   200860      English
   WO2008104549-A3   30 Oct 2008   G06K-009/00   200874      English
   EP2115662-A2   11 Nov 2009   G06K-009/00   200975      English
   JP2010520521-W   10 Jun 2010   G06T-001/00   201038   Pages: 29   Japanese
   EP2115662-B1   23 Jun 2010   G06K-009/00   201041      English
   DE602008001607-E   05 Aug 2010   G06K-009/00   201054      German
   JP5049356-B2   17 Oct 2012   G06T-007/00   201271   Pages: 26   Japanese
   US8509561-B2   13 Aug 2013   G06K-009/40   201355      English
UT DIIDW:2008K17686
ER

PT P
PN WO2004055735-A1; JP2004199200-A; AU2003289116-A1; JP2005174179-A; US2006204053-A1; JP4266798-B2; JP4298283-B2; US7577297-B2
TI Image pattern identification method involves extracting primary feature of input data and analyzing distribution of extracted feature.
AB    NOVELTY - The primary feature of input image data is extracted and distribution of the extracted feature is analyzed, based on which the secondary feature of the data is obtained.
   USE - For identifying image pattern.
   ADVANTAGE - Reliable pattern identification is performed irrespective of the fluctuation of the input pattern, at reduced processing cost.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) image pattern identification device; and
   (2) image pattern identification program.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of pattern identification device. (Drawing includes non-English language text).
   image input unit (21)
   initial feature extraction unit (22)
   local feature extraction unit (23)
   partial feature extraction unit (24)
   face extraction unit (26)
PD WO2004055735-A1   01 Jul 2004   G06T-007/00   200448   Pages: 136   Japanese
   JP2004199200-A   15 Jul 2004   G06T-007/00   200448   Pages: 33   Japanese
   AU2003289116-A1   09 Jul 2004   G06T-007/00   200474      English
   JP2005174179-A   30 Jun 2005   G06T-007/00   200543   Pages: 25   Japanese
   US2006204053-A1   14 Sep 2006   G06K-009/00   200661      English
   JP4266798-B2   20 May 2009   G06T-007/00   200933   Pages: 24   Japanese
   JP4298283-B2   15 Jul 2009   G06T-007/00   200946   Pages: 29   Japanese
   US7577297-B2   18 Aug 2009   G06K-009/46   200955      English
UT DIIDW:2004507522
ER

PT P
PN EP1246123-A2; CA2378286-A1; US2002186880-A1; JP2002312788-A; US6888960-B2; EP1246123-B1; DE60232697-E; EP1246123-A3
TI Viewpoint-independent spatial covariance matrix calculation method for image recognition e.g. face recognition, involves diagonalizing calculated spatial covariance matrix, based on quantized vectors.
AB    NOVELTY - The matrix element of a spatial covariance matrix for a three-dimensional object is calculated for an arbitrary predetermined distribution of illumination conditions. The set of vectors from different points on the surface of the object are quantized. The spatial covariance matrix is diagonalized based on quantized vectors, using which viewpoint-dependent illumination subspace is calculated.
   USE - For image recognition e.g. face recognition.
   ADVANTAGE - The complexity in computing the illumination subspace of the image is reduced. Hence, accurate and efficient computation of the low dimensional linear illumination subspace is achieved. The time and memory space is reduced.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) Albedo and normal vector calculation method;and
   (2) Viewpoint-dependent illumination subspace calculation method.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart explaining the viewpoint-independent spatial covariance matrix calculation method.
PD EP1246123-A2   02 Oct 2002   G06K-009/62   200278   Pages: 17   English
   CA2378286-A1   28 Sep 2002   G06K-009/62   200282      English
   US2002186880-A1   12 Dec 2002   G06K-009/00   200301      English
   JP2002312788-A   25 Oct 2002   G06T-007/00   200303   Pages: 14   Japanese
   US6888960-B2   03 May 2005   G06K-009/00   200530      English
   EP1246123-B1   24 Jun 2009   G06K-009/62   200942      English
   DE60232697-E   06 Aug 2009   G06K-009/62   200951      German
   EP1246123-A3   27 Nov 2002   G06K-009/62   201733      English
UT DIIDW:2002715012
ER

PT P
PN US2001028731-A1; US6400828-B2
TI Hidden data determination in image analysis, involves applying model explicitly representing coupling between aligned observed data and hidden data, to unlabeled aligned data set.
AB    NOVELTY - The unlabeled set of unaligned observed data are analyzed by a model representing aligned observed data. Another model which explicitly represents coupling between aligned observed data and hidden data, is applied to unlabeled set of the aligned data, to determine the hidden data for that unlabeled set of the aligned data.
   USE - For determining continuous valued hidden data such as control point data from observable data e.g. unmarked image data, in data analysis including speech analysis, image analysis, video analysis and also in applications including graphics, expression and gesture recognition, image morphing, gesture based manipulation of video imagery, image segmentation and recomposition, face recognition, image compositing, gait recognition, body tracking, image encoding e.g. compression, pose estimation and guiding robot arm control.
   ADVANTAGE - The hidden data including control point data on the unmarked images, are automatically estimated using a model representing coupling between the images and the hidden data.
   DESCRIPTION OF DRAWING(S) - The figure shows an image of a person's lip, in which control points on salient image portions are identified.
PD US2001028731-A1   11 Oct 2001   G06K-009/00   200227   Pages: 17   English
   US6400828-B2   04 Jun 2002   G06K-009/00   200242      English
UT DIIDW:2002215454
ER

PT P
PN WO200163386-A1; AU200133963-A; GB2365724-A; EP1259867-A1; GB2365724-B; CN1419664-A; US2003195935-A1; CN1256633-C; US7346779-B2; IN200200784-P1
TI Method for securing electronic document such as e-mail communications by attaching biometric characteristic and electronic document to form biometric characteristic-document combination and encrypting them.
AB    NOVELTY - The method involves attaching a biometric characteristic and an electronic document to form a biometric characteristic-document combination and encrypting the biometric characteristic-document combination.
   USE - For securing and authenticating electronic documents, in particular e-mail communications.
   ADVANTAGE - Provides a more convenient and secure authenticating electronic documents.
   DETAILED DESCRIPTION - Two personal computers (30) are connected via a modem (32) to the Internet (36). Each PC has a processor that is connected to a biometric reader (40) and a memory. Readings taken by the sensor (40) can be passed via the processor and stored in the memory. As biometric data signature, the pressure distribution when the user signs his signature, finger length, a fingerprint, DNA, iris characteristics, retina characteristics, facial tomography, facial shape, ear prints, typing speed, typing pressure and voice recognition may be used.
   INDEPENDENT CLAIMS are included for:
   (a) a system for securing an electronic document
   (b) a user terminal
   (c) a computer program on a data carrier or a computer readable medium
   DESCRIPTION OF DRAWING(S) - The drawing shows Internet based system for authenticating electronic document according to the present invention.
   personal computers (30)
   modem (32)
   Internet (36)
   biometric reader (40)
PD WO200163386-A1   30 Aug 2001   G06F-001/00   200160   Pages: 43   English
   AU200133963-A   03 Sep 2001   G06F-001/00   200202      English
   GB2365724-A   20 Feb 2002   H04L-009/32   200213      English
   EP1259867-A1   27 Nov 2002   G06F-001/00   200302      English
   CN1419664-A   21 May 2003   G06F-001/00   200355      Chinese
   US2003195935-A1   16 Oct 2003   G06F-015/16   200369      English
   US7346779-B2   18 Mar 2008   H04K-001/00   200825      English
   IN200200784-P1   26 Mar 2010   G06F-001/00   201117      English
UT DIIDW:2001541718
ER

PT P
PN US6195452-B1
TI Negotiable instrument authentication for e.g. check for financial institution, involves comparing ID number sequence with verify coded number on front face of check using mathematics relationship.
AB    NOVELTY - A numerical identification number (80a) is placed on the front face of a check (10). Verity code numbers are marked on separate blocks (90a-90j) placed on the front face of the check. The numerical identification number is compared with the verity code numbers using mathematical relationship which includes verity code numbers as terms, to obtain resultant unadded sequence of integers (R).
   USE - For detecting authenticity of negotiable instruments such as check and any type of document for financial institutions.
   ADVANTAGE - Detection of authenticity of check when it is instantly presented, is improved. Forged and unauthorized signatures on the document are detected reliably.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for negotiable instrument production authenticating method.
   DESCRIPTION OF DRAWING(S) - The figure shows the front elevation of the check.
   Cheque (10)
   Identification number (80a)
   Blocks (90a-90j)
PD US6195452-B1   27 Feb 2001   G06K-009/00   200124   Pages: 10   English
UT DIIDW:2001234451
ER

PT P
PN CN106203395-A; CN106203395-B
TI Multi-task learning depth based human face attribute identifying method, involves placing convolutional neural network model for feed-forward calculation operation of attributes to obtain face identification result.
AB    NOVELTY - The method involves preparing an image data set comprising a human face attribute label. Human face position in the image is obtained. The detected human face key point is divided by a face alignment method. Average human face image training set is calculated. A deep convolutional neural network is constructed with multiple tasks. A convolution neural network model is obtained. The convolutional neural network model is placed for feed-forward calculation operation of attributes to obtain a face identification result.
   USE - Multi-task learning depth based human face attribute identifying method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a multi-task learning depth based human face attribute identifying method. '(Drawing includes non-English language text)'
PD CN106203395-A   07 Dec 2016   G06K-009/00   201705   Pages: 11   Chinese
   CN106203395-B   14 Jan 2020   G06K-009/00   202007      Chinese
UT DIIDW:2016781917
ER

PT P
PN CN105956572-A
TI Convolution nerve network based living body human face detecting method, involves calculating collective video segment probability value of human face image to obtain video classification result.
AB    NOVELTY - The method involves acquiring video by a video camera after dividing a frame sample. Sample image normalization is carried out. A normalization characteristic extraction image processing function is carried out by using a convolution nerve network. A convolution nerve network structure is designed. Human face image characteristic extraction and probability distribution true and false determination are carried out. A collective video segment probability value of a human face image is calculated to obtain video classification result.
   USE - Convolution nerve network based living body human face detecting method.
   ADVANTAGE - The method enables avoiding human face identification problem, simplifying living body human face detecting operation by using a human face photo, video and 3D models and carrying out true and fake human face recognition operation by using a human face recognition system in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a convolution nerve network based living body human face detecting method. '(Drawing includes non-English language text)'
PD CN105956572-A   21 Sep 2016   G06K-009/00   201669   Pages: 9   Chinese
UT DIIDW:201661507Y
ER

PT P
PN CN104008320-A; WO2015176386-A1; US2016085950-A1; EP3154309-A1; EP3154309-A4
TI Human face identification based mobile terminal user authority mode controlling method, involves identifying human face of current user from human face data by right module through human face identification process.
AB    NOVELTY - The method involves identifying human face of a current user from human face data by a right module through a human face identification process. Preset time of a mobile terminal is determined. Authority level of the mobile terminal is detected. The mobile terminal is directly started. Camera shooting process is started. Human face information of the user is extracted. The human face information of the user is transmitted. The human face information is processed by a time manager. Human face authority is prompted.
   USE - Human face identification based mobile terminal user authority mode controlling method.
   ADVANTAGE - The method enables reducing error rate of the mobile terminal user authority mode controlling operation.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification based mobile terminal user authority mode controlling system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face identification based mobile terminal user authority mode controlling method.'(Drawing includes non-English language text)'
PD CN104008320-A   27 Aug 2014   G06F-021/32   201477   Pages: 10   Chinese
   WO2015176386-A1   26 Nov 2015   H04W-088/02   201578      Chinese
   US2016085950-A1   24 Mar 2016   G06F-021/32   201622      English
   EP3154309-A1   12 Apr 2017   H04W-088/02   201726      English
   EP3154309-A4   11 Apr 2018   H04W-088/02   201826      English
UT DIIDW:2014V00611
ER

PT P
PN CN103093490-A; WO2014117446-A1; US2015035825-A1; CN103093490-B; US9361723-B2
TI Single video camera based real-time human face animation method, involves tracking three-dimensional positions of facial feature points, and determining expression parameters of user to drive facial animation of cartoon character.
AB    NOVELTY - The method involves tracking three-dimensional (3D) positions of facial feature points in real time by utilizing a single video camera. A face posture and facial expression of a user are determined according to the 3D positions. The single video camera is connected with a terminal. Characteristic points of an image are determined. An expression fusion model is obtained. Expression parameters of the user are determined to drive facial animation of a cartoon character.
   USE - Single video camera based real-time human face animation method.
   ADVANTAGE - The method enables providing the single video camera to accurately process different wide-angle facial rotations in a convenient manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a photograph of a human face.
PD CN103093490-A   08 May 2013   G06T-013/40   201366   Pages: 15   Chinese
   WO2014117446-A1   07 Aug 2014   G06T-013/40   201452      Chinese
   US2015035825-A1   05 Feb 2015   G06T-013/40   201510      English
   CN103093490-B   26 Aug 2015   G06T-013/40   201571      Chinese
   US9361723-B2   07 Jun 2016   G06T-015/00   201638      English
UT DIIDW:2013Q56017
ER

PT P
PN US2011135166-A1; US8194938-B2
TI Robust recognition-by-parts face authentication system has identification authentication module to match query exemplar-based portions against gallery of all enrolled training exemplar-based portions.
AB    NOVELTY - An enrollment patch extractor (120) extracts a multitude of training patches at different scales for each center position of training image (110). An enrollment data fusion module (135) enrolls the training exemplar-based portions (140). A query module (155) has a query patch extractor (160) to extract a multitude of query patches at different scales for each center position of a query image (150). An identification (ID) authentication module (180) matches the query exemplar-based portions (175) against a gallery of all enrolled training exemplar-based portions.
   USE - Robust recognition-by-parts face authentication system for use in surveillance and security fields. Can also be used in fields of defense, health care, human-computer interaction, image retrieval and data mining, industrial, personal robotics, telecommunication and digital libraries, smart environment, and transportation.
   ADVANTAGE - The robust face authentication system robustly authenticates facial images, by comparing and authenticating the test image with training image.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for physical and tangible computer readable medium storing robust recognition-by-parts face authentication program.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of the robust face authentication system.
   Training image (110)
   Enrollment module (115)
   Enrollment patch extractor (120)
   Enrollment data fusion module (135)
   Training exemplar-based portion (140)
   Query image (150)
   Query module (155)
   Query patch extractor (160)
   Query exemplar-based portion (175)
   ID authentication module (180)
PD US2011135166-A1   09 Jun 2011   G06K-009/00   201139   Pages: 32   English
   US8194938-B2   05 Jun 2012   G06K-009/00   201237      English
UT DIIDW:2011G39976
ER

PT P
PN CN101635743-A; CN101635743-B
TI Mobile communication terminal holder's identity determining system for e.g. personnel monitoring, has positioning communication terminal provided for acquiring human biometric feature in real-time to complete feature processing.
AB    NOVELTY - The system has a control terminal provided with a biometric input device, a remote network, a mobile communication network and a positioning communication terminal with a biometric acquisition device. An application server, a feature identification authentication server and the positioning communication terminal are provided in a local area network or integrally connected with the mobile communication network via the remote network. The positioning communication terminal is provided for acquiring human biometric feature i.e. identity feature, in real-time to complete feature processing.
   USE - System for determining identity of a mobile communication terminal holder by using biometric authentication result. Uses include but are not limited to personnel monitoring, positioning and monitoring correction object in community correction, public security patroller, bank and mobile customer service.
   ADVANTAGE - The system is suitable for personnel monitoring and positioning functions and application using the mobile terminal service such as monitoring correction object in community correction, public security patroller, and bank and mobile customer service. The system can effectively determine the identity of mobile communication terminal holder, and achieves high authentication rate and low error rate.
   DETAILED DESCRIPTION - The human biometric feature comprises finger print, facial form, voice finger, hand, voice finger, iris, retina, pulse, auricle and signature and key press force. An INDEPENDENT CLAIM is also included for a method for determining identity of a mobile communication terminal holder by biometric authentication result.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a mobile communication terminal holder's identity determining system.'(Drawing includes non-English language text)'
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The system utilizes a mobile communication data channel i.e. mobile communication data channel conforming to global system for mobile communication (GSM) general packet radio service (GPRS)/enhanced data rates for global system for mobile communication evolution (EDGE), code division multiple access (CDMA) 1X/evolution data only (EVDO), time division-synchronous code division multiple access (TD-SCDMA) R4/high-speed downlink packet access (HSDPA), long term evolution (LTE). The system performs voice transmission by a standard network communication protocol, and transmission voice compression code selects G711 code confirmed by International Telecommunication Union (ITU).
PD CN101635743-A   27 Jan 2010   H04L-029/08   201014   Pages: 20   Chinese
   CN101635743-B   26 Nov 2014   H04L-029/08   201506      Chinese
UT DIIDW:2010B54802
ER

PT P
PN US2005008199-A1; US7643671-B2
TI Facial processing method for identifying individual from face image, involves combining two dimensional facial images and standard 3D facial images to create 3D facial images, and modifying intermediate 3D facial images.
AB    NOVELTY - The method involves receiving two dimensional facial images and combining the two dimensional facial images and standard three dimensional facial images to create three dimensional facial images. The intermediate two dimensional facial images are rendered based upon the intermediate three dimensional facial images. The intermediate three dimensional facial images are modified.
   USE - Used for identifying an individual from image of face.
   ADVANTAGE - The method allows modification of image to correct for difference in lighting or pose, in identification systems or authentication systems. The method utilizes shape information for making facial image comparisons without the need for a three dimensional camera.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for identifying in individual.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a process for determining facial recognition.
   Image acquisition (20)
   Light correction (30)
   Facial recognition (40)
PD US2005008199-A1   13 Jan 2005   G06K-009/00   200511   Pages: 10   English
   US7643671-B2   05 Jan 2010   G06K-009/00   201003      English
UT DIIDW:2005099860
ER

PT P
PN US2004190775-A1; US7853085-B2
TI Multi-featured object identification method e.g. for person's face, involves comparing two-dimensional projection source with three-dimensional projection of candidate, to determine whether candidate corresponds to the source.
AB    NOVELTY - The method involves detecting two-dimensional (2D) projection source in an image, and performing viewpoint-invariant search of 3D representation of candidate, to locate the 3D representation of candidate, having 2D projection that closely resembles the 2D projection source. The 2D projection source is then compared with located 3D projection of candidate, to determine whether the candidate corresponds to the source.
   USE - For identifying multi-featured object such as face of person, animal, plants, buildings, corresponding to two-dimensional projection of source multi-featured object.
   ADVANTAGE - Enables locating the 3D object from source 2D image, under varying lighting conditions and source viewpoints.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for multi-featured object identification system.
   DESCRIPTION OF DRAWING(S) - The figure shows an explanatory drawing of the multi-featured object identification method.
PD US2004190775-A1   30 Sep 2004   G06K-009/46   200469   Pages: 18   English
   US7853085-B2   14 Dec 2010   G06K-009/62   201101      English
UT DIIDW:2004708434
ER

PT P
PN JP2003344891-A
TI Camera e.g. electronic still camera sets imaging mode based on output of object face recognition unit and detector detecting state of object, automatically.
AB    NOVELTY - A face recognition unit recognizes whether a photographed object contains a face or not, based on output of an area sensor (13) which photographs the object. The imaging mode of the camera is set automatically, based on the output of the recognition unit and a detector which detects state of the object.
   USE - Camera e.g. electronic still camera and video camera.
   ADVANTAGE - The camera having favorable imaging characteristics is achieved by automatically setting imaging mode of the camera.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the camera.
   focus detector (6)
   photometry sensor (10)
   area sensor (13)
   blurring detection sensor (16)
   focal distance detector (37)
PD JP2003344891-A   03 Dec 2003   G03B-007/08   200402   Pages: 11   Japanese
UT DIIDW:2004017763
ER

PT P
PN EP1139269-A2; JP2001283222-A; JP3528915-B2; US6956569-B1; EP1139269-B1; DE60036780-E; DE60036780-T2; EP1139269-A3
TI Two-dimensional image and three-dimensional candidate models matching method for human face recognition, involves computing histogram-like table with brightness coefficient corresponding to 3D candidate models.
AB    NOVELTY - A histogram-like table with brightness coefficient that depends on surface normal of three-dimensional candidate models (10), is computed based on the corresponding value in a two-dimensional image (50). The three-dimensional models are successively rendered in the determined position and orientation of the two-dimensional image using surface normals and the tables. The two-dimensional image is compared with each rendered model.
   USE - For matching two-dimensional image and three-dimensional candidate models for recognizing various types of objects and human faces.
   ADVANTAGE - Computational cost is reduced, since complex and costly computations such as eigen vector determination and solving for light sources are not required. The matching is simpler, as single process of building a histogram-like data structure is only required. The straight forward nature of matching avoids unstable solutions. The matching is robust against errors and is extremely fast, as iterative algorithms are not required.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) Computer program product;
   (b) Program storage device that stores image matching program
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic representation of image matching method.
   Three-dimensional candidate models (10)
   Two-dimensional image (50)
PD EP1139269-A2   04 Oct 2001   G06K-009/00   200217   Pages: 23   English
   JP2001283222-A   12 Oct 2001   G06T-007/00   200217   Pages: 17   Japanese
   JP3528915-B2   24 May 2004   G06T-007/00   200434   Pages: 17   Japanese
   US6956569-B1   18 Oct 2005   G06T-017/00   200568      English
   EP1139269-B1   17 Oct 2007   G06K-009/00   200770      English
   DE60036780-E   29 Nov 2007   G06K-009/00   200780      German
   EP1139269-A3   08 Jan 2003   G06K-009/00   201731      English
UT DIIDW:2002123997
ER

PT P
PN JP11219421-A; US2002126879-A1; US6504944-B2; US2003048930-A1; US6690815-B2; JP3688879-B2
TI Image shape recognition device - extracts image data of mouth cavity portion from image of human face, based on which shape of mouth and lips is recognized.
AB       NOVELTY - Image acquisition unit (1) acquires the image of human face. Mouth cavity portion extractor (2) extracts image data of the mouth cavity portion from acquired image. Image recognition unit (3) recognizes the shape of mouth and lips, based on extracted image. DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for image recognition procedure.
   USE -   For recognizing shape of mouth, lips in human face image.
   ADVANTAGE -   Image is recognized in high speed with high precision. DESCRIPTION OF DRAWING(S) - The figure shows the schematic diagram of image recognition device. (1) Image acquisition unit; (2) Mouth cavity portion extractor; (3) Image recognition unit.
PD JP11219421-A   10 Aug 1999   G06T-001/00   199942   Pages: 20   Japanese
   US2002126879-A1   12 Sep 2002   G06K-009/00   200262      English
   US6504944-B2   07 Jan 2003   G06K-009/00   200306      English
   US2003048930-A1   13 Mar 2003   G06K-009/00   200321      English
   US6690815-B2   10 Feb 2004   G06K-009/00   200413      English
   JP3688879-B2   31 Aug 2005   G06T-001/00   200558   Pages: 31   Japanese
UT DIIDW:1999504009
ER

PT P
PN CN103914689-A; WO2015154516-A1; CN103914689-B
TI Human face identification based picture cutting method, involves identifying human face in standard picture, obtaining human face identification result, and performing picture cutting process to obtain target cutting size of picture.
AB    NOVELTY - The method involves identifying a human face in a standard picture. A human face identification result is obtained. Picture cutting process is performed to obtain target cutting size of the standard picture. A picture main body is established. A picture cutting mode is determined. Picture minimum length is calculated. Target cut size proportion process is performed to obtain size ratio of the standard picture. A human picture is contained with face number information and human position information.
   USE - Human face identification based picture cutting method.
   ADVANTAGE - The method enables increasing picture cutting effect, solving picture identification or content identification algorithm problems caused by picture cutting effect and lifting reading experience for a website picture.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification based picture cutting device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face identification based picture cutting method.'(Drawing includes non-English language text)'
PD CN103914689-A   09 Jul 2014   G06K-009/00   201465   Pages: 15   Chinese
   WO2015154516-A1   15 Oct 2015   G06K-009/00   201569      Chinese
   CN103914689-B   15 Mar 2017   G06K-009/00   201722      Chinese
UT DIIDW:2014R98632
ER

PT P
PN US2013281206-A1; WO2013186636-A2; WO2013186636-A3; AU2013276223-A1; CA2876130-A1; US2015126279-A1; CN104919507-A; US9558612-B2; US9858754-B2; CN104919507-B; AU2013276223-B2; CN109003398-A
TI Method for augmented reality gaming, involves controlling camera on mobile device using reality interaction system, determining whether image tags are on gaming machine cabinet, and producing virtual rendering of gaming machine cabinet.
AB    NOVELTY - The method involves controlling a camera on a mobile device using an augmented reality interaction system. A user is enabled to capture a live camera image of a gaming machine cabinet by the camera. The image tags are on the gaming machine cabinet in the live camera image is determined. A virtual rendering of the gaming machine cabinet is produced in response to identifying images tags. A virtual depth rendering of the virtual three dimensional (3D) components is compared to a virtual depth rendering of the virtual gaming machine cabinet.
   USE - Method for augmented reality gaming, particularly for enabling an augmented reality interaction system and a mobile device such as smart phone, tablet to overlay a virtual three dimensional component over a physical three dimensional component with which the virtual three dimensional components interact. Uses include but are not limited to wagering games, gaming machines and networked gaming systems.
   ADVANTAGE - The method involves controlling a camera on a mobile device using an augmented reality interaction system, thus improves the maintenance experience with augmented reality effects. A loyalty points program is implemented that enables players to accumulate points to be redeemed for larger awards. The winning amount won by the player as represented on the screen and extracted and validated against the known pay table to ensure the winning amount has not been faked. The player registration module generally requires some identification data to ensure that players do not register multiple accounts, and thus prevent frauds.
   DETAILED DESCRIPTION - The elements of the virtual 3D components that has a lower depth value than elements of the virtual gaming machine cabinet are subtracted at corresponding spatial positions. The virtual 3D components without the subtracted elements are overlaid onto a live camera image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a method for rendering enhanced augmentation using depth masking.
PD US2013281206-A1   24 Oct 2013   G07F-017/32   201376   Pages: 115   English
   WO2013186636-A2   19 Dec 2013      201401      English
   WO2013186636-A3   10 Apr 2014   G06T-017/00   201426      English
   AU2013276223-A1   22 Jan 2015   G06T-017/00   201518      English
   CA2876130-A1   19 Dec 2013   G06T-017/00   201521      English
   US2015126279-A1   07 May 2015   G07F-017/32   201532      English
   CN104919507-A   16 Sep 2015   G07F-017/32   201602      English
   US9558612-B2   31 Jan 2017   G07F-017/32   201710      English
   US9858754-B2   02 Jan 2018   G07F-017/32   201818      English
   CN104919507-B   26 Jun 2018   G07F-017/32   201845      Chinese
   AU2013276223-B2   30 Aug 2018   G06T-017/00   201858      English
   CN109003398-A   14 Dec 2018   G07F-017/32   201902      Chinese
UT DIIDW:2013S64167
ER

PT P
PN US2012106806-A1; CN102542249-A; US8494231-B2; CN102542249-B; HK1171846-A1
TI Method of recognizing face in video content provided by personalized video service in computing environment, involves generating metadata associating video frame and face with face identification data.
AB    NOVELTY - The face detection data is received in response to the detection (704) of face in an input video frame. The face detection data is matched (712) against face identification data maintained in a face gallery to recognize the face in the input video frame. The metadata associating video frame and face with the face identification data is generated after face recognition.
   USE - Method of recognizing face in video content provided by personalized video service in computing environment.
   ADVANTAGE - The metadata is used to efficiently identify a person in the video with high precision and recall performance. The similar faces in the gallery are filtered out for eliminating highly repetitive data. The face tracking precision is improved by discovering more faces in the video sequence with higher confidence and reducing false detections in the video sequence.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) face recognition system; and
   (2) computer-readable media storing computer executable instructions executed for face recognition.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram explaining the steps performed by the face recognition pipeline.
   Video separation step (702)
   Face detection step (704)
   Face filtering step (706)
   Gallery generation step (707)
   Matching step (712)
   Metadata saving step (714)
PD US2012106806-A1   03 May 2012   G06K-009/00   201231   Pages: 14   English
   CN102542249-A   04 Jul 2012   G06K-009/00   201251      Chinese
   US8494231-B2   23 Jul 2013   G06K-009/00   201348      English
   CN102542249-B   17 Jun 2015   G06K-009/00   201555      Chinese
   HK1171846-A1   08 Jan 2016   G06K-000/00   201617      Chinese
UT DIIDW:2012F08377
ER

PT P
PN CN102271241-A
TI Facial expression/human action recognition decoded information image wireless network communication method, involves utilizing characteristic parameter for controlling face model of local at receiving end to reproduce expression by model.
AB    NOVELTY - The method involves collecting an image at a transmitting end, and determining a face area in the image. A facial expression characteristic parameter is extracted in the face area. The extracted facial expression characteristic parameter is compared to a receiving end. The facial expression characteristic parameter is utilized for controlling a face model of local at the receiving end to reproduce the facial expression by the face model. A face area image is pre-processed to enhance display of facial expression. Multiple motion features of a probe are formed in the face model.
   USE - Facial expression/human action recognition decoded information image wireless network communication method.
   ADVANTAGE - The method enables enhancing user experience and reducing a video carrier network transmission rate to realize effective video communication and to limit network bandwidth and capacity of a wireless communication network.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an image mode identification based communication system comprising a collecting module.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a facial expression/human action recognition decoded information image wireless network communication method.'(Drawing includes non-English language text)'
PD CN102271241-A   07 Dec 2011   H04N-007/14   201203   Pages: 14   Chinese
UT DIIDW:2011Q89097
ER

PT P
PN WO2010131561-A1; JP2010268052-A; US2012007997-A1; EP2430827-A1; CN102422630-A; IN201109157-P4; RU2011150281-A; JP5478935-B2; JP2014131320-A; RU2525034-C2; JP5793210-B2; RU2014119389-A; US9201288-B2; CN102422630-B; US2016044268-A1; RU2585235-C2; EP2430827-B1; EP3110135-A1; US9591246-B2; EP2430827-A4; EP3110135-B1; IN304226-B
TI Image pickup apparatus e.g. electronic camera, has CPU that performs blur correction on acquired image based on distance information included in image.
AB    NOVELTY - A CPU controls the display unit to display the image acquired by a complementary metal oxide semiconductor (CMOS) sensor. The CPU performs the blur correction on the acquired image based on the distance information included in the image. The display unit displays the blur-corrected image where multiple distances in the acquired image are focused.
   USE - Image pickup apparatus e.g. electronic camera.
   ADVANTAGE - A blur corrected image where multiple distances in the acquired image are focused can be displayed as a confirmation image, so that the photographer can easily confirm the photographed image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating the operation of the image pickup apparatus.
   Step for detecting initial state (S102)
   Step for driving CMOS sensor (S104)
   Step for face recognition (S106)
   Step for determining focus detection position (S111)
   Step for generating object distance map (S200)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The image is compressed in accordance with JPEG standard.
PD WO2010131561-A1   18 Nov 2010   H04N-005/232   201078   Pages: 71   English
   JP2010268052-A   25 Nov 2010   H04N-005/232   201078   Pages: 23   Japanese
   US2012007997-A1   12 Jan 2012   H04N-005/228   201205      English
   EP2430827-A1   21 Mar 2012   H04N-005/232   201221      English
   CN102422630-A   18 Apr 2012   H04N-005/232   201230      Chinese
   IN201109157-P4   15 Mar 2013   H04N-005/232   201337      English
   RU2011150281-A   20 Jun 2013   H04N-005/232   201360      Russian
   JP5478935-B2   23 Apr 2014   H04N-005/232   201427   Pages: 22   Japanese
   JP2014131320-A   10 Jul 2014   H04N-005/232   201445   Pages: 26   Japanese
   RU2525034-C2   10 Aug 2014   H04N-005/232   201452      Russian
   JP5793210-B2   14 Oct 2015   H04N-005/232   201567   Pages: 24   Japanese
   RU2014119389-A   27 Sep 2015   H04N-005/232   201570      Russian
   US9201288-B2   01 Dec 2015   G03B-013/36   201579      English
   CN102422630-B   25 Nov 2015   H04N-005/232   201603      English
   US2016044268-A1   11 Feb 2016   H04N-005/3745   201612      English
   RU2585235-C2   27 May 2016   H04N-005/232   201646      Russian
   EP2430827-B1   24 Aug 2016   H04N-005/232   201656      English
   EP3110135-A1   28 Dec 2016   H04N-005/232   201702      English
   US9591246-B2   07 Mar 2017   H04N-005/3745   201719      English
   EP2430827-A4   19 Jun 2013   H04N-005/232   201771      English
   EP3110135-B1   14 Nov 2018   H04N-005/232   201876      English
   IN304226-B   14 Dec 2018   H04N-005/232   201901      English
UT DIIDW:2010P28898
ER

PT P
PN US2008130960-A1; WO2008067475-A1; IN200901107-P3; AU2007325117-A1; KR2009088424-A; EP2100260-A1; CA2671091-A1; CN101601053-A; JP2010511938-W; US8085995-B2; EP2100260-B1; JP5037627-B2; AU2007325117-B2; CN101601053-B; BR200719735-A2; KR1387147-B1; CA2671091-C; EP2100260-A4; IN295780-B; BR200719735-B1
TI Method for identifying image involves performing face detection on retrieved images to detect faces using face detection algorithm for identifying representative face image.
AB    NOVELTY - The method involves identifying (405) named entity and retrieving (410) images associated with the identified named entity. The face detection is performed (415) on the retrieved images to detect faces using face detection algorithm for identifying (420) representative face image. The additional images representing the named entity are identified (425) using the representative face image.
   USE - Method for identifying images collected from web documents, videos and data bases.
   ADVANTAGE - The images can be identified accurately using the image identification system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for image identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart explaining the method for identifying image.
   Step for identifying named entity (405)
   Step for retrieving images associated with named entity (410)
   Step for performing face detection (415)
   Step for identifying representative face image (420)
   Step for identifying additional images (425)
PD US2008130960-A1   05 Jun 2008   G06K-009/62   200841   Pages: 12   English
   WO2008067475-A1   05 Jun 2008   G06K-009/68   200841      English
   IN200901107-P3   19 Jun 2009   G06K-009/68   200951      English
   AU2007325117-A1   02 Jul 2009   G06K-009/68   200954      English
   KR2009088424-A   19 Aug 2009   G06K-009/68   200957      
   EP2100260-A1   16 Sep 2009   G06K-009/00   200961      English
   CA2671091-A1   05 Jun 2008   G06K-009/68   200966      English
   JP2010511938-W   15 Apr 2010   G06T-001/00   201026   Pages: 21   Japanese
   US8085995-B2   27 Dec 2011   G06K-009/62   201203      English
   EP2100260-B1   23 May 2012   G06K-009/00   201235      English
   JP5037627-B2   03 Oct 2012   G06T-001/00   201266   Pages: 16   Japanese
   AU2007325117-B2   11 Oct 2012   G06K-009/68   201273      English
   CN101601053-B   14 Nov 2012   G06F-017/00   201313      Chinese
   BR200719735-A2   10 Dec 2013   G06K-009/68   201427      
   KR1387147-B1   21 Apr 2014   G06K-009/68   201430      
   CA2671091-C   24 Nov 2015   G06K-009/68   201609      English
   EP2100260-A4   17 Feb 2010   G06K-009/00   201748      English
   IN295780-B   16 Apr 2018   G06K-009/68   201830      English
   BR200719735-B1   23 Jul 2019   G06K-009/68   201959      English
UT DIIDW:2008G53170
ER

PT P
PN CN1971630-A; CN100468467-C
TI Access control device and check on work attendance tool based on human face identification technique.
AB    NOVELTY - A gate inhibition and work attendance device based on face discrimination technique are disclosed those include: cam used to read the face images, embedded system used to transmit the video information, input device used to input the identification information and computer used to distinguish the images and manage the work attendance. Said cam is connected with the embedded system, said embedded system is connected with the input device, and said embedded system is connected with the computer data. The gate inhibition and work attendance device also include mirror face used to confirm the sideposition of face, the cam is mounted on the top of mirror face, the optimal visual range of the cam is matched with the mirror face; said computer includes: image acquisition module, face image library, face detecting position module, image preprocessing module, face exercising module and face discrimination module. The invention provides the gate inhibition and work attendance device based on face discrimination technique with characterized that high discrimination success rate, easy to realize, high-speed to discriminate and judge, convenient to install and low implement cost.
PD CN1971630-A   30 May 2007   G07C-009/00   200763      Chinese
   CN100468467-C   11 Mar 2009   G07C-009/00   200966      Chinese
UT DIIDW:2007666466
ER

PT P
PN US2006204055-A1; US7574016-B2
TI Digital image processing method for digital camera, involves adjusting values of parameters within digitally-detected image.
AB    NOVELTY - A group of pixels corresponding to an image of a face are identified within a digital image. The default values of parameters of the digital image are determined. The values of the parameters within the digitally-detected image are adjusted based on the analysis of digital image and default values.
   USE - For digital camera.
   ADVANTAGE - The processing is automatically performed or suggested based on information related to important regions such as face in the image and reliable assistance is provided to the photographer in post processing stage.
   DETAILED DESCRIPTION - AN INDEPENDENT CLAIM is also included for one or more processor readable storage devices having processor readable code embodied thereon.
   DESCRIPTION OF DRAWING(S) - The figure shows a flow diagram explaining the digital image processing method.
PD US2006204055-A1   14 Sep 2006   G06K-009/00   200675   Pages: 43   English
   US7574016-B2   11 Aug 2009   G06K-009/00   200953      English
UT DIIDW:2006724040
ER

PT P
PN CN1794264-A; CN100361138-C
TI Method and system of real time detecting and continuous tracing human face in video frequency sequence.
AB    NOVELTY - This invention puts forward a method and a system for real time testing and continuous tracking of man-face in a video sequence including the following steps: carrying out a test algorithm to a man-face input video image, then applying a rough and fine two stages of test algorithm to verify the tested face, applying an object following algorithm to follow the verified one and verifying it by verifying the followed regions, which realizes a real time test to a positive and vertical man-face by a test method based on Ada Boost statistic hierarchical sorter and Mean shift and a square pattern property.
PD CN1794264-A   28 Jun 2006   G06K-009/00   200711      Chinese
   CN100361138-C   09 Jan 2008   G06K-009/00   200833      Chinese
UT DIIDW:2007102946
ER

PT P
PN US2004001647-A1; US7657079-B2
TI Virtual reality rendering system for determining position of object in three-dimensional virtual environment, determines pose of object based on actual and estimated locations of landmarks in object's image.
AB    NOVELTY - A landmark recognition device (130) recognizes a set of predetermined landmarks in an object's image captured by a camera (100). A location detection device (135) determines the actual location of the predetermined landmarks, and estimates the positions of the other remaining landmarks. A pose calculator (140) determines the pose of the object based on the output of the location detection device.
   USE - For determining position of object such as user's face in three-dimensional (3D) virtual environment defined by users personal digital assistant (PDA) e.g. for analyzing different diseased skin condition and to view virtual head of person suffering from disease such as psoriasis, in medical applications, for viewing virtual representation of damaged car in order to estimate repairing cost of damaged car, and for playing virtual reality games.
   ADVANTAGE - As camera captures the image of user with his current posture, the user analyzes the captured image in 3D virtual environment without any discomfort. Even if there is a certain degree of error, the rendered 3D image is quickly updated while avoiding production of swimming effect that is inherent when the pose cannot be quickly calculated, thereby resulting in minimization of the error.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) position detection apparatus; and
   (2) position detection method; and
   (3) article comprising storage medium for storing position detection program.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic view of the virtual reality rendering system.
   camera (100)
   landmark recognition device (130)
   location detection device (135)
   pose calculator (140)
   memory (150)
PD US2004001647-A1   01 Jan 2004   G06K-009/36   200413   Pages: 18   English
   US7657079-B2   02 Feb 2010   G06K-009/00   201010      English
UT DIIDW:2004131611
ER

PT P
PN WO200165486-A1; EP1177525-A1; KR2002021789-A; JP2003525504-W; US6792144-B1
TI Digital image processing system in automatic/surveillance system application, has feature extractor that identifies features of object in accordance with model of object.
AB    NOVELTY - The system (10) has an object determinators (11) that determines whether an object is present in right and left frames (14,15). A feature extractor (13) coupled to one of the object determinator identifies features of the object in accordance with a model (12).
   USE - For processing digital images such as human faces for application in automated/surveillance systems, monitoring systems, human interfaces to computers, television and video signal analysis.
   ADVANTAGE - Since a front-end modeling of an object is applied to the determined object, the processing time of feature determination is reduced and appropriate positions of one or more specific features of the object is determined quickly. Reduces the amount of memory required to store information related to features of the object and overall image, and hence compactness of the system is improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) Object feature determination method;
   (b) Computer readable memory medium including code for processing image data
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of feature extraction system.
   System (10)
   Object determinators (11)
   Model (12)
   Feature extractor (13)
   Right and left frames (14,15)
PD WO200165486-A1   07 Sep 2001   G06T-007/00   200158   Pages: 18   English
   EP1177525-A1   06 Feb 2002   G06T-007/00   200218      English
   KR2002021789-A   22 Mar 2002   H04N-007/18   200264      
   JP2003525504-W   26 Aug 2003   G06T-007/00   200357   Pages: 21   Japanese
   US6792144-B1   14 Sep 2004   G06K-009/00   200460      English
UT DIIDW:2001530158
ER

PT P
PN EP849707-A2; JP10232950-A; JP10240919-A; JP10240920-A; JP10240921-A; US6661906-B1; EP849707-B1; DE69734640-E; DE69734640-T2; EP849707-A3
TI Image generating apparatus for e.g. face of person - in which image component data is selected based on character quantity of image component extracted from entered image data, and selected-and-entered class to create image.
AB       The image creation apparatus includes a face component library (11) which includes image libraries (11a-11g) of the respective face components e.g. eyes, noses and mouths etc., each of which stores a number of image groups of the face components having different image elements in each of the image classes.
   An eye image library stores an image group composed by a number of eye images having different image elements about slant and roundness for each image class. The image elements are an item for measurement about character quantities extracted from the image data. A rule library (12) includes a select rule library, a deform rule library and an arrange rule library which store rule groups consisting of rules for each face component about each image class.
   USE -   Creating several types of images having different styles and expressions based on entered single image data e.g. creating face of person for newspapers and magazines etc.
   ADVANTAGE -   Enables expression of composite image in three dimensions without increasing number of images.
PD EP849707-A2   24 Jun 1998   G06T-007/00   199829   Pages: 33   English
   JP10232950-A   02 Sep 1998   G06T-011/80   199845   Pages: 12   Japanese
   JP10240919-A   11 Sep 1998   G06T-001/00   199847   Pages: 11   Japanese
   JP10240920-A   11 Sep 1998   G06T-001/00   199847   Pages: 11   Japanese
   JP10240921-A   11 Sep 1998   G06T-001/00   199847   Pages: 12   Japanese
   US6661906-B1   09 Dec 2003   G06K-009/00   200381      English
   EP849707-B1   16 Nov 2005   G06T-007/00   200579      English
   DE69734640-E   22 Dec 2005   G06T-007/00   200603      German
   DE69734640-T2   10 Aug 2006   G06T-007/00   200654      German
   EP849707-A3   26 Jul 2000   G06T-007/00   201735      English
UT DIIDW:1998324950
ER

PT P
PN CN106503687-A; CN106503687-B
TI Fusing-angle characteristic and monitored video character identification system, has object detection module for collecting human image of single frame, and identity characteristic vector of identity matching result obtained by identity tag.
AB    NOVELTY - The system has an object detection module for collecting a human image of a single frame. A support vector machine (SVM) is connected with a target detection module. The target detecting module transmits a monitoring video and a single-frame image. An angle of a human face image is about 90-270 degrees. A human face image is stored in a multi-angle face database. The human face image is classified based on an angle value. The human face image is identified by a neural network model. Identity characteristic vector of identity matching result is obtained by an identity tag.
   USE - Fusing-angle characteristic and monitored video character identification system.
   ADVANTAGE - The system can monitor video information of face characteristics and multiple angles so as to improve video and monitoring and identification accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a fusing-angle characteristic and monitored video character identification method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a fusing-angle characteristic and monitored video character identification system. '(Drawing includes non-English language text)'
PD CN106503687-A   15 Mar 2017   G06K-009/00   201723   Pages: 12   Chinese
   CN106503687-B   05 Apr 2019   G06K-009/00   201927      Chinese
UT DIIDW:201719039Y
ER

PT P
PN CN105608450-A; CN105608450-B
TI Depth or quality convolution nerve network based human face identification method, involves determining cross-near infrared region, and determining high similarity score to identify human face image.
AB    NOVELTY - The method involves determining a cross-near infrared region. A wheel or quality trained human face identification convolution nerve network model is established. Standard correct rate is evaluated to identify true positive rate of heterogeneous. A near infrared or visible light image and a visible light image of a human face recognition system are compared. High similarity score is determined to identify a human face image. Gray level image process is performed according to preset size.
   USE - Depth or quality convolution nerve network based human face identification method.
   ADVANTAGE - The method enables improving lifting medium or human face identification accuracy rate, and reducing small scale data of convolution nerve network training in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a depth or quality convolution nerve network based human face identification method. '(Drawing includes non-English language text)'
PD CN105608450-A   25 May 2016   G06K-009/00   201638   Pages: 13   English
   CN105608450-B   27 Nov 2018   G06K-009/00   201882      Chinese
UT DIIDW:201632577K
ER

PT P
PN EP2869239-A2; US2015125049-A1; WO2015066628-A1; EP2869239-A3; CA2928932-A1; AU2014341919-A1; KR2016083900-A; CN105874474-A; IN201647014965-A; JP2017501514-W; BR112016010093-A2; MX2016005868-A1; US10095917-B2; MX362983-B; US2019171868-A1; JP6594329-B2; AU2014341919-B2; CN105874474-B; IL245320-A; MX2019002400-A1; US11210503-B2
TI Computer-implemented method for representing facial images using deep learning, involves classifying identity of two-dimensional (2D) face image based on provision of three-dimensional (3D) aligned face image to deep neural network.
AB    NOVELTY - The method (700) involves generating a 3D-aligned face image from a 2D face image by a computing system. An identity of the 2D face image is classified (710) based on provision of the 3D-aligned face image to a deep neural network (DNN) by computing system, the identity of the 2D face image comprising a feature vector. A set of fiducial points of the 2D face image is detected (706). The 2D face image is warped (708) to a 3D shape using the set of fiducial points. A set of anchor points is placed onto the 3D shape, each anchor point corresponding to a fiducial point of the 2D face image.
   USE - Computer-implemented method for representing facial images using deep learning.
   ADVANTAGE - The normalization ensures the face classification module to be robust to illumination changes. The cross-entropy loss for each training sample is minimized in order to maximize the probability of the correct class. The unrestricted protocol provides the operator with knowledge about the identities in the training sets, hence enabling the generation of many more training pairs to be added to the training set. The system provides an accuracy of 91.4% which reduces the error of the existing best methods by more than 50%. The accuracy is improved to 92.5% subsequent to correcting 100 wrong labels for video pairs in the yttrium-iron garnet (YiG) tuned filter (YTF) dataset.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a non-transitory computer-readable storage medium including instructions for representing facial images using deep learning; and
   (2) a system for representing facial images using deep learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method of generating a face representation for 2D face image.
   Method of generating a face representation for 2D face image (700)
   Step for identifying a face portion of the 2D image (702)
   Step for detecting the fiducial points of the 2D face image (706)
   Step for warping the 2D face image to the 3D shape (708)
   Step for classifying the identity of the 2D image (710)
PD EP2869239-A2   06 May 2015   G06K-009/00   201532   Pages: 30   English
   US2015125049-A1   07 May 2015   G06K-009/00   201533      English
   WO2015066628-A1   07 May 2015   G06K-009/46   201533      English
   EP2869239-A3   19 Aug 2015   G06K-009/00   201555      English
   CA2928932-A1   07 May 2015   G06K-009/46   201633      English
   AU2014341919-A1   19 May 2016   G06K-009/46   201640      English
   KR2016083900-A   12 Jul 2016   G06K-009/00   201649      
   CN105874474-A   17 Aug 2016   G06K-009/46   201658      Chinese
   IN201647014965-A   01 Jul 2016   G06K-009/46   201675      English
   JP2017501514-W   12 Jan 2017   G06T-001/00   201706   Pages: 40   Japanese
   BR112016010093-A2   01 Aug 2017   G06K-009/46   201774      English
   MX2016005868-A1   20 Jan 2017   G06K-009/46   201812      Spanish
   US10095917-B2   09 Oct 2018   G06K-009/00   201867      English
   MX362983-B   28 Feb 2019   G06K-009/46   201923      Spanish
   US2019171868-A1   06 Jun 2019   G06K-009/00   201942      English
   JP6594329-B2   23 Oct 2019   G06T-001/00   201983   Pages: 34   Japanese
   CN105874474-B   18 Feb 2020   G06K-009/46   202017      Chinese
   IL245320-A   27 Feb 2020   G06K-009/46   202020      English
   MX2019002400-A1   15 Jul 2019   G06T-003/00   202034      Spanish
   US11210503-B2   28 Dec 2021   G06K-009/00   202104      English
UT DIIDW:201528170M
ER

PT P
PN CN104598869-A
TI Method for pushing intelligent advertisement of human face recognition device, involves obtaining personalized media information, obtaining displaying advertisement information in window, and determining facial expression of user.
AB    NOVELTY - The method involves detecting a human face by a portable electronic device. Human face image information is stored. A human face detection application is started for detecting the human face in a display screen surface. A human face active region track is identified. The human face is extracted. An image threshold value is determined. A regulating signal is received for getting related data. Personalized media information is obtained. Displaying advertisement information is obtained in a window. Facial expression of a user is determined.
   USE - Method for pushing intelligent advertisement of a human face recognition device.
   ADVANTAGE - The method enables reducing pushing time.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method pushing intelligent advertisement of a human face recognition device. '(Drawing includes non-English language text)'
PD CN104598869-A   06 May 2015   G06K-009/00   201551   Pages: 10   Chinese
UT DIIDW:201541827F
ER

PT P
PN CN104376250-A
TI Real human living body identity authentication method, involves marking identity authentication as successful when operator is real human, and marking identity verification to be failed when operator is not real human.
AB    NOVELTY - The method involves collecting operator reading random preset time content of video information and audio information according to a human face image. Lip language semantics and preset content are compared to obtain random or/and the audio information according to a voice message and voice recognition in a database. Identity authentication is marked as successful when an operator is a real human. Identity verification is failed when the operator is not real human.
   USE - Real human living body identity authentication method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a real human living body identity authentication method. '(Drawing includes non-English language text)'
PD CN104376250-A   25 Feb 2015   G06F-021/32   201529   Pages: 10   Chinese
UT DIIDW:2015240904
ER

PT P
PN CN103632132-A; CN103632132-B
TI Skin color segmentation and template matching based human face detection and recognization method, involves performing rear human face skin color segmentation of human face images by utilizing discrete orthogonal wavelet conversion.
AB    NOVELTY - The method involves performing rear human face skin color segmentation of human face images by utilizing two-dimensional multi-scale discrete orthogonal wavelet conversion for the human face images. The human face images are collected by non-uniformity illumination compensation process opening and closing operation treatment. Human face detection is performed. Logarithm domain of the human face images is obtained by utilizing logarithm conversion. A low frequency component is obtained by taking multiple two-dimensional multi-orthogonal wavelet conversion.
   USE - Skin color segmentation and template matching based human face detection and recognization method.
   ADVANTAGE - The method enables detecting human face in a quick manner and ensuring strong practicability human face detection and identification and strong popularization and application values.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a skin color segmentation and template matching based human face detection and recognization method.'(Drawing includes non-English language text)'
PD CN103632132-A   12 Mar 2014   G06K-009/00   201429   Pages: 32   Chinese
   CN103632132-B   15 Feb 2017   G06K-009/00   201716      Chinese
UT DIIDW:2014H64045
ER

PT P
PN US2012120283-A1; WO2012062893-A2; WO2012062893-A3; EP2548154-A2; CN103052960-A; JP2013242571-A; KR2013139243-A; US8648959-B2; JP2014505915-W; JP5530568-B2; EP2548154-B1; EP3007104-A1; JP6081297-B2; EP3007104-B1; EP3236391-A1; CN103052960-B; KR1870853-B1; EP3236391-B1
TI Object-based auto-focus method of digital image acquisition device, involves combining image and applying object recognition program to combined image to edit and display corrected object image.
AB    NOVELTY - The method involves acquiring digital image with data corresponding faces. The focus condition classifier programs are applied to the object data. The object data as corresponding to object within the digital image is identified. The out of focus condition of the object is corrected based on the determination to generate a corrected object image appearing to be sharply focused. The image is combined and object recognition program is applied to the combined image to edit and display the corrected object image.
   USE - Object-based auto-focus method of digital image acquisition device (claimed) e.g. digital camera, camera-phone and video camera.
   ADVANTAGE - The object detection can be performed effectively. The performance of image acquisition device is improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) object detection method;
   (2) non-transitory processor-readable medium storing program for object detection; and
   (3) digital image acquisition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a front view of display screen of digital image acquisition device.
   Face regions (301,302)
PD US2012120283-A1   17 May 2012   H04N-005/217   201235   Pages: 32   English
   WO2012062893-A2   18 May 2012   G06K-009/00   201235      English
   WO2012062893-A3   12 Jul 2012   G06K-009/00   201246      English
   EP2548154-A2   23 Jan 2013   G06K-009/00   201308      English
   CN103052960-A   17 Apr 2013   G06K-009/00   201364      Chinese
   JP2013242571-A   05 Dec 2013   G02B-007/28   201380   Pages: 14   Japanese
   KR2013139243-A   20 Dec 2013   G06K-009/46   201403      
   US8648959-B2   11 Feb 2014   H04N-005/232   201412      English
   JP2014505915-W   06 Mar 2014   G06T-001/00   201417   Pages: 23   Japanese
   JP5530568-B2   25 Jun 2014   G06T-001/00   201442   Pages: 11   Japanese
   EP2548154-B1   21 Oct 2015   G06K-009/00   201570      English
   EP3007104-A1   13 Apr 2016   G06K-009/00   201627      English
   JP6081297-B2   15 Feb 2017   G02B-007/28   201714   Pages: 13   Japanese
   EP3007104-B1   19 Jul 2017   G06K-009/00   201749      English
   EP3236391-A1   25 Oct 2017   G06K-009/00   201771      English
   CN103052960-B   24 Oct 2017   G06K-009/00   201774      Chinese
   KR1870853-B1   25 Jun 2018   G06T-007/00   201844      
   EP3236391-B1   09 Oct 2019   G06K-009/00   201978      English
UT DIIDW:2012F72047
ER

PT P
PN CN102298443-A; CN102298443-B
TI Intelligent home furnishing voice control system combining video channel, has gesture detecting module, human face detecting module, identifying system module, order judging module and instruction confirmation module.
AB    NOVELTY - The channel has a gesture detecting module, a human face detecting module, an identifying system module, an order judging module and an instruction confirmation module, where the identification system module is provided with a lip part and a sound part. The lip part is provided with a human face tracking positioning module, a lip video input module, and a lip recognition module. The voice part is provided with a voice input module and a voice identification module.
   USE - Intelligent home furnishing voice control system combining a video channel.
   ADVANTAGE - The channel enables controlling gesture and detecting face and lip information by supplying the voice information identified, thus improving accuracy of control instruction, and increasing utilization feasibility voice control system for fuzzy control of household. The channel has strong anti-interference ability and wide range application, and reduces noise in a family environment.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an identification method for intelligent home furnishing voice control system combining video channel.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating operations performed by an intelligent home furnishing voice control system combining video channel. '(Drawing includes non-English language text)'
PD CN102298443-A   28 Dec 2011   G06F-003/01   201208   Pages: 13   Chinese
   CN102298443-B   25 Sep 2013   G06F-003/01   201378      Chinese
UT DIIDW:2012A75378
ER

PT P
PN US2011224509-A1; WO2011111011-A1; CA2791348-A1; EP2545485-A1; KR2013005288-A; CN102812471-A; US8401875-B2; JP2013534652-W; HK1179702-A0; IN201202376-P3; RU2012137172-A; JP5659246-B2; IL221765-A; RU2558617-C2; CN102812471-B; KR1720790-B1; EP2545485-A4
TI Managing apparatus for personal and secured data and documentation files has authentication unit by which access to user's stored personal data and documentation files is enabled after positive authentication.
AB    NOVELTY - The managing apparatus has sensor module with biometric sensors for reading personal biological identification parameters, and a processing module which processes the personal biological identification parameters. An authentication unit authenticates the identity of the user by comparing the user's personal identification parameters read by the sensor module with a pre-recorded set of personal biological identification parameters. The access to the user's stored personal data and documentation files is enabled by the authentication unit.
   USE - Managing apparatus for personal and secured data and documentation files.
   ADVANTAGE - Ensures that the encryption process will be highly immune to potential hackers trials to break the encryption code and read the personal highly secured data of the apparatus since the encryption keys required to encrypt and decipher the secured data in sub-module are stored in a separate special memory partition located in the other memory module. Saves battery power consumption since the sensor deactivates when accomplished a satisfactory result of grabbing the face image of the user.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a managing method for personal and secured data and documentation files; and
   (2) a managing system for personal and secured data and documentation files.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a highly secured personal data dedicated communication and management system.
   Personal data dedicated communication and management system (300)
   Cellular infrastructure network (310)
   Switches (320,325)
   Multiplexer sub-module (330)
   Communication system server (345)
PD US2011224509-A1   15 Sep 2011   A61B-005/00   201163   Pages: 35   English
   WO2011111011-A1   15 Sep 2011   G06F-021/00   201163      English
   CA2791348-A1   15 Sep 2011   G06F-021/00   201274      English
   EP2545485-A1   16 Jan 2013   G06F-021/00   201306      English
   KR2013005288-A   15 Jan 2013   G06F-021/00   201308      
   CN102812471-A   05 Dec 2012   G06F-021/00   201320      Chinese
   US8401875-B2   19 Mar 2013   G06Q-010/00   201321      English
   JP2013534652-W   05 Sep 2013   G06F-021/62   201358   Pages: 52   Japanese
   HK1179702-A0   04 Oct 2013   G06F-000/00   201376      English
   IN201202376-P3   20 Dec 2013   G06F-021/00   201402      English
   RU2012137172-A   20 Apr 2014   G06F-021/00   201434      Russian
   JP5659246-B2   28 Jan 2015   G06F-021/62   201509   Pages: 51   Japanese
   IL221765-A   31 Oct 2012   G06F-021/00   201518      English
   RU2558617-C2   10 Aug 2015   G06F-021/32   201555      Russian
   CN102812471-B   22 Jun 2016   G06F-021/62   201646      Chinese
   KR1720790-B1   29 Mar 2017   G06F-021/62   201726      
   EP2545485-A4   05 Mar 2014   G06F-021/32   201770      English
UT DIIDW:2011L73205
ER

PT P
PN US2008219517-A1; WO2008107112-A2; WO2008107112-A3; KR2009118095-A; EP2137670-A2; JP2010520542-W; KR1129405-B1; JP5174045-B2; US8503800-B2; EP2137670-B1
TI Face illumination normalization method for image processing apparatus e.g. digital still camera, involves electronically storing, transmitting and applying face recognition program to corrected face image that is displayed.
AB    NOVELTY - The method involves acquiring a digital image including data corresponding to a face, and applying uneven illumination classifier programs to the face data. An uneven illumination condition for the face is determined as a result of application of uneven illumination classifier programs. The uneven illumination condition of the face is corrected to generate a corrected face, and a face recognition program is electronically stored, transmitted and applied to the corrected face image that is displayed.
   USE - Method for normalizing face illumination for recognizing a face in an image processing apparatus such as a digital still camera, video camera, cell phone and a hand held computer.
   ADVANTAGE - The method enables the apparatus to correct the uneven illumination condition of the face to generate a corrected face image appearing to have more uniform illumination, thus effectively normalizing illumination on faces.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for detecting a face
   (2) a digital image acquisition device including an optoelectronic system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a face illumination normalization method.
PD US2008219517-A1   11 Sep 2008   G06K-009/00   200865   Pages: 29   English
   WO2008107112-A2   12 Sep 2008   G06K-009/00   200865      English
   WO2008107112-A3   08 Jan 2009   G06K-009/00   200906      English
   KR2009118095-A   17 Nov 2009   G06K-009/00   200978      
   EP2137670-A2   30 Dec 2009   G06K-009/00   201003      English
   JP2010520542-W   10 Jun 2010   G06T-007/00   201038   Pages: 35   Japanese
   KR1129405-B1   26 Mar 2012   G06K-009/00   201226      
   JP5174045-B2   03 Apr 2013   G06T-007/00   201324   Pages: 27   Japanese
   US8503800-B2   06 Aug 2013   G06K-009/68   201352      English
   EP2137670-B1   11 Dec 2013   G06K-009/00   201381      English
UT DIIDW:2008L11097
ER

PT P
PN WO2007137047-A2; US2008152217-A1; WO2007137047-A3; GB2453263-A; US7805386-B2; US2010316283-A1; US8041651-B2
TI Encoded output signal generation method for e.g. neuron modeling uses recursive loop between two processing elements, training elements to store reciprocal signal pairs, and outputting encoded signal when elements receive input signal units.
AB    NOVELTY - The input signal is input into a processor that executes processing elements. A recursive loop is formed between a pair of processing elements. The output of each pair is fed back into the input of the other and the pair receives an element input including a subset of discrete units of the input. Pairs of processing elements are trained to store reciprocal signal pairs such that the reciprocal signal pair is locked within the pair of processing elements. When the pair of processing elements receives the input signal units, it forces the two processing elements to generate an encoded output.
   USE - For generating an encoded output signal from an input signal having a structure including discrete signal units to model a neuron or a neurotransmitter. It is used in image and signal analysis and for data storage. It can be used for speech recognition, pen stroke analysis, two-dimensional image encoding, image encoding, handwriting recognition, three-dimensional shape analysis, three-dimensional face recognition, hand gesture or sign language recognition, industrial inspection, and noise removal.
   ADVANTAGE - The encoded output signal generation method enables physiologically realistic modeling of neurons and neurotransmitters. It reduces noise and creates stability in the recursive connections. It reduces computational errors and guarantees that the outputs always represent valid images. It is accurate and represents images with very few neurons. It is also consistent with the known lateral inhibition in the pyramidal cells.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) method for extracting knowledge from an input signal;
   (2) system for encoding or decoding an input signal
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a Set-Reset Manifold Association Processor.
PD WO2007137047-A2   29 Nov 2007   G06K-009/62   200802   Pages: 95   English
   US2008152217-A1   26 Jun 2008   G06K-009/62   200844      English
   WO2007137047-A3   14 Aug 2008   G06F-017/00   200856      English
   GB2453263-A   01 Apr 2009   G06N-005/02   200924      English
   US7805386-B2   28 Sep 2010   G06F-015/18   201064      English
   US2010316283-A1   16 Dec 2010   G06K-009/62   201082      English
   US8041651-B2   18 Oct 2011   G06F-015/18   201168      English
UT DIIDW:2008A34491
ER

PT P
PN EP1635307-A1; JP2006079382-A; CN1746929-A; CN101833830-A; JP2010244570-A; JP4686153-B2; CN101833830-B
TI Information processing apparatus e.g. automated teller machine, involves determining whether user is suspicious person registered in advance from result of face identification, and accordingly halting transaction.
AB    NOVELTY - A control unit performs face identification, based on the detected face area in the captured image, and facial information registered in advance in a storage unit. The face of user is registered in database, the threshold of validation of entered bill is controlled, and/or the transaction is halted, if the user is determined as a suspicious person registered in advance from the face identification result.
   USE - E.g. automated teller machine (ATM).
   ADVANTAGE - Prevents safety reduction by suspicious person during ATM use. Detects criminals where face images are registered in advance, persons who use several cards, criminals who peep at other's personal identification number and illegal withdrawals by persons who peeped at other's personal identification.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart explaining the process of detecting suspicious person.
PD EP1635307-A1   15 Mar 2006   G07F-019/00   200624   Pages: 26   English
   JP2006079382-A   23 Mar 2006   G06Q-040/00   200624   Pages: 17   Japanese
   CN1746929-A   15 Mar 2006   G07F-019/00   200644      Chinese
   CN101833830-A   15 Sep 2010   G07F-019/00   201067      Chinese
   JP2010244570-A   28 Oct 2010   G07D-009/00   201071   Pages: 16   Japanese
   JP4686153-B2   18 May 2011   G06Q-040/00   201133   Pages: 16   Japanese
   CN101833830-B   24 Apr 2013   G07F-019/00   201364      Chinese
UT DIIDW:2006223952
ER

PT P
PN CN105869248-A; CN105869248-B
TI Human face identification based access cloud management system, has cloud platform communicated with human face recognition management device, where front end of human face recognition management device is communicated with door lock.
AB    NOVELTY - The system has a cloud platform communicated with a managing user intelligent terminal device, a property intelligent terminal device and a human face recognition management device. A front end of the human face recognition management device is communicated with a door lock. The user intelligent terminal device is provided with an intelligent mobile phone, a flat computer and a desktop computer. The property intelligent terminal device is provided with the intelligent mobile phone, the flat computer and the desktop computer.
   USE - Human face identification based access cloud management system.
   ADVANTAGE - The system is simple to operate, and verifies personal identity information of a user, generates alarm when a human face identification process is non-prohibited, quickly generates or records inlet information of a visitor, ensures better safety management effect and reduces work amount of security personnel. The system has high operational speed.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification based access cloud management method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a human face identification based access cloud management method. '(Drawing includes non-English language text)'
PD CN105869248-A   17 Aug 2016   G07C-009/00   201663   Pages: 12   Chinese
   CN105869248-B   25 Sep 2018   G07C-009/00   201866      Chinese
UT DIIDW:201652961F
ER

PT P
PN WO2015078018-A1; WO2015078018-A8; KR2016083127-A; CN105849747-A; EP3074918-A1; JP2016538656-W; US9530047-B1; US2016379044-A1; IN201617021371-A; JP6127214-B2; HK1223439-A0; HK1223718-A0; EP3074918-A4; CN105849747-B; EP3074918-B1; HK1223439-A1
TI Method for performing face image recognition, involves generating face region pairs of face images, and recognizing whether face images belong to same identity or not based on identity relational features of the face images.
AB    NOVELTY - The method involves generating (S201) multiple face region pairs of face images to be compared and recognized. Multiple feature modes are formed (S202) by exchanging two face regions of each face region pair and each face region of each face region pair is horizontally flipped. Multiple feature modes are received (S203). Multiple identity relational features are extracted (S204) from input maps to form multiple output maps. The face images belong to the same identity or not is recognized (S205) based on the identity relational features of the face images.
   USE - Method for performing face image recognition.
   ADVANTAGE - The face images belong to the same identity or not is recognized based on the identity relational features of the face images which ensures that the face image recognition is performed in an efficient manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a system for face image recognition; and
   (2) a convolutional neural networks for extracting identity relational features for a face image recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method for face image recognition.
   Generating multiple face region pairs of face images to be compared and recognized (S201)
   Forming feature modes by exchanging two face regions of each face region pair (S202)
   Receiving feature modes (S203)
   Extracting identity relational features from input maps to form multiple output maps (S204)
   Recognizing whether face images belong to the same identity or not (S205)
PD WO2015078018-A1   04 Jun 2015   G06K-009/00   201539   Pages: 21   English
   WO2015078018-A8   07 Jul 2016   G06K-009/00   201646      English
   KR2016083127-A   11 Jul 2016   G06K-009/46   201649      
   CN105849747-A   10 Aug 2016   G06K-009/00   201658      Chinese
   EP3074918-A1   05 Oct 2016   G06K-009/00   201666      English
   JP2016538656-W   08 Dec 2016   G06T-007/00   201681   Pages: 14   Japanese
   US9530047-B1   27 Dec 2016   G06K-009/00   201702      English
   US2016379044-A1   29 Dec 2016   G06K-009/00   201703      English
   IN201617021371-A   31 Aug 2016   G06K-009/00   201719      English
   JP6127214-B2   10 May 2017   G06T-007/00   201733   Pages: 11   Japanese
   HK1223439-A0   28 Jul 2017   G06K-000/00   201755      English
   HK1223718-A0   04 Aug 2017   G06K-000/00   201756      English
   EP3074918-A4   27 Sep 2017   G06K-009/00   201766      English
   CN105849747-B   17 Aug 2018   G06K-009/00   201858      Chinese
   EP3074918-B1   03 Apr 2019   G06K-009/00   201925      English
   HK1223439-A1   05 Jul 2019   G06K-000/00   201953      English
UT DIIDW:201532337H
ER

PT P
PN US2009087042-A1; US7809162-B2
TI Digital image processing method for digital camera, involves determining default values of one or more parameters of some portion of digital image, based on which values of parameters within digitally detected image are adjusted.
AB    NOVELTY - The method involves acquiring a digital image and identifying a group of pixels corresponding to face image within the digital image. The default values of one or more parameters of some portion of digital image are determined. The values of parameters within digitally detected image are adjusted based on analysis of digital image of face and default values. A manual command is received and executed to remove a false indication of another face within the image based on manual indication that the apparatus has falsely identified another group of pixels as being a face.
   USE - Method of processing digital image in digital camera.
   ADVANTAGE - The decisions for processing the digital image based on face detection, selecting one or more parameters and/or for adjusting values of one or more parameters within the digital image can be performed automatically, semi-automatically or manually. Multiple targets such as multiple faces or facial regions can be tracked and/or zoomed and preferably digitally enhanced in view of aesthetic considerations.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) computer readable storage medium for storing digital image processing program; and
   (2) digital image acquisition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of image correcting operation.
   Picture rendering initiation step (103)
   Picture taking mode initiation step (104)
   Marking step of faces in image (118)
   Face orientation detection step (130)
   Automatic focusing step (180)
PD US2009087042-A1   02 Apr 2009   G06K-009/00   200925   Pages: 38   English
   US7809162-B2   05 Oct 2010   G06K-009/00   201065      English
UT DIIDW:2009G64715
ER

PT P
PN WO2008042879-A1; EP2100253-A1; US2010014720-A1; US8280120-B2; US2012321141-A1; US2012328164-A1; US2013182913-A1; US2013212655-A1; US8818051-B2; US8818052-B2; US9355299-B2; EP2100253-A4
TI Method for authenticating financial transactions using biometrics involves providing authentication decision based on calculated probability of life of person and probability of match between and known biometric information.
AB    NOVELTY - The authenticating method involves acquiring biometric data from a person, calculating probability of life of a person (11) and probability of a match (12) between the person and known biometric information, and providing an authentication decision based on the combination of the calculated probability of life of person and probability of match.
   USE - Method for authenticating financial transactions using biometrics.
   ADVANTAGE - Improves deterrent against attempted fraudulent transactions, and reduces rejection of valid customers. Fraudulent user does not know whether he or she is authorized or declined as a result of a bad or good match, or because the system has captured excellent live-person data that can be used for prosecution or at least embarrassing public disclosure, by combining in one algorithm the live-person result with the match result. Fraudulent user is also not able to determine precisely how well their fraudulent methods are working, which takes away the single most important tool of a fraudster, i.e., feedback on how well their methods are working. Collects a set of live-person data that can be used to compile a database or watch list of people who attempt to perform fraudulent transactions, and this can be used to recognize fraudsters at other transactions such as check-cashing for example by using a camera and another face recognition system. Also ensures that some live-person data is captured, then it provides a means to perform customer redress, for example, if a customer complains then the system can show the customer a picture of them performing a transaction, or a bank agent can manually look at the picture of the user performing the transaction and compare it with a record of the user on file.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for authenticating financial transactions.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flowchart of operation of the authentication system.
   Computing probability of life of person (11)
   Computing probability of biometric match (12)
   Compute authentication decision (14)
   Transaction is authorized (16)
   Transaction is not authorized (17)
PD WO2008042879-A1   10 Apr 2008   G06K-009/00   200828   Pages: 20   English
   EP2100253-A1   16 Sep 2009   G06K-009/00   200962      English
   US2010014720-A1   21 Jan 2010   G06K-009/00   201007      English
   US8280120-B2   02 Oct 2012   G06K-009/00   201266      English
   US2012321141-A1   20 Dec 2012   G06K-009/00   201301      English
   US2013182913-A1   18 Jul 2013   G06K-009/00   201347      English
   US2013212655-A1   15 Aug 2013   G06F-021/32   201354      English
   US8818051-B2   26 Aug 2014   G06K-009/00   201457      English
   US8818052-B2   26 Aug 2014   G06K-009/00   201457      English
   US9355299-B2   31 May 2016   G06K-009/00   201637      English
   EP2100253-A4   12 Jan 2011   G06K-009/00   201749      English
UT DIIDW:2008E00718
ER

PT P
PN US2008080745-A1; US7760917-B2
TI Face similarity determining method for use on social network site, involves identifying person from image, and performing similarity search that includes using text and metadata to determine set of classifications for person.
AB    NOVELTY - The method involves identifying a person from an image. One of text or metadata associated with the image is identified. A similarity search is performed using the person of the image, in order to identify images of other persons that are similar in appearance to the person in the image. Similarity search includes using some of the text and metadata to determine a set of classifications for the person in the image is performed.
   USE - Method for determining face similarity on a social network site and online dating site.
   ADVANTAGE - The method efficiently performs similarity searching to compare images of persons and faces with others in order to find persons who are similar in appearance.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for determining face similarity
   (2) a computer-readable medium for determining face similarity with processor identifying a person from an image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of component for enabling similarity searches of persons.
   Query image input (102)
   Search collection (120)
   Image data (212)
   Metadata extract (220)
   Bio data (222)
PD US2008080745-A1   03 Apr 2008   G06K-009/00   200827   Pages: 12   English
   US7760917-B2   20 Jul 2010   G06K-009/00   201048      English
UT DIIDW:2008D81727
ER

PT P
PN JP2004072655-A; US2004095477-A1; CN1496110-A; CN1256839-C; JP3966461-B2; US2009122164-A1; US8115821-B2
TI Region of interest setting apparatus for electronic camera, selects specific region of interest recognition unit using different ROI recognition methods, based on user instructions and processed image scene kind.
AB    NOVELTY - The setting apparatus selects a specific region of interest (ROI) recognition unit among the group (122) using different methods for recognizing a ROI such as a face in the image data, based on user instructions and processed image scene kind. A controller (120) controls the selected ROI recognition unit to recognize specific image area and performs enlargement or reduction of the recognized image area.
   USE - For electronic image processing apparatus such as electronic camera apparatus (claimed) and digital camera capable of performing image compression.
   ADVANTAGE - Realizes a practical ROI setting function that can set ROI recognition conditions corresponding to seriography of processed still picture or moving image, and user's taste.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) electronic camera apparatus;
   (2) ROI setting method;
   (3) ROI setting program; and
   (4) recorded medium storing ROI setting program.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the ROI setting apparatus. (Drawing includes non-English language text).
   image pick-up (100)
   memory (102)
   display (104)
   compression/expansion unit (106)
   recording medium (110)
   controller (120)
   ROI recognition group (122)
PD JP2004072655-A   04 Mar 2004   H04N-001/41   200424   Pages: 21   Japanese
   US2004095477-A1   20 May 2004   H04N-005/228   200434      English
   CN1496110-A   12 May 2004   H04N-005/225   200452      Chinese
   CN1256839-C   17 May 2006   H04N-005/225   200661      Chinese
   JP3966461-B2   29 Aug 2007   H04N-001/41   200757   Pages: 18   Japanese
   US2009122164-A1   14 May 2009   H04N-005/262   200933      English
   US8115821-B2   14 Feb 2012   H04N-005/235   201212      English
UT DIIDW:2004252092
ER

PT P
PN US2004005086-A1; US7406184-B2
TI Face recognition method for surveillance application, involves generating face template from fused combination of video images obtained in thermal infrared domain and reflective domain.
AB    NOVELTY - A non-uniformity correction (NUC) is applied to the video image (220) produced in thermal infrared domain e.g. long wave infrared (LWIR) spectrum. A template for face is generated from a fused combination of video images (220,210) obtained in thermal infrared domain and reflective domain e.g. visible spectrum, near-infrared spectrum, respectively.
   USE - For application such as access control to restricted areas such as building, room, identification of individuals such as criminals, terrorist, casino card counter, surveillance and monitoring of scenes.
   ADVANTAGE - Enables performing face recognition efficiently and precisely.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for face recognition apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram illustrating thermal infrared video imaging in conduction with reflected video imaging.
   video images (210,220)
   image normalization (216,226)
   face detection (230)
   cameras (250,260)
PD US2004005086-A1   08 Jan 2004   G06K-009/00   200415   Pages: 17   English
   US7406184-B2   29 Jul 2008   G06K-009/00   200852      English
UT DIIDW:2004155158
ER

PT P
PN US2003202704-A1; US6990217-B1
TI Face image classification method involves determining support vectors for identifying hyperplane, from reference images including male and female faces.
AB    NOVELTY - The reference images including male and female faces, are supplied to a vector support machine. The support vectors for identifying a hyperplane, are determined from the images. The gender of an input test image is classified with respect to the hyperplane.
   USE - For classifying images of faces of human beings, according to gender.
   ADVANTAGE - The face image of human beings, is accurately classified according to the gender, irrespective of the resolution of the images.
   DESCRIPTION OF DRAWING(S) - The figure shows the flow diagram explaining the gender classification process.
PD US2003202704-A1   30 Oct 2003   G06K-009/62   200377   Pages: 7   English
   US6990217-B1   24 Jan 2006   G06K-009/00   200608      English
UT DIIDW:2003832510
ER

PT P
PN WO8101764-A; US4277689-A; NO8102706-A; EP41993-A; DK8103581-A; CA1149962-A
TI Manually fed optical cheque reader - has open guiding slot for ensuring proper positioning of cheque relative to scan line.
AB       The cheque reader includes a frame (33) throughout which a slot (22) passes for receiving the cheque. The cheque is moved manually through the slot with one edge in sliding engagement with the base of the frame. Four incandescent lamps (47) mounted on a character scanner (36) light the face of the cheque as it is moved through the slot. An array of silicon photodiodes (50) produces analog electrical signals representing the intensity of light reflected onto them from the face of the cheque. The signals from the array are passed to a quantiser and transition processor and counter. Count information is deposited in a picture buffer of a recognition processor which encodes electronic images into a machine-readable code. The guiding slot ensures proper positioning of the cheque relative to a scan line. The optical character recognition techniques adapt to the speed imparted by manual movement along the slot.
PD WO8101764-A   25 Jun 1981   G06K-009/00   198128      English
   US4277689-A   07 Jul 1981   G06K-009/00   198130      English
   NO8102706-A   12 Oct 1981   G06K-000/00   198145      Portuguese
   EP41993-A   23 Dec 1981   G06K-009/00   198201      English
   CA1149962-A   12 Jul 1983   G06K-009/26   198331      English
UT DIIDW:1981G5836D
ER

PT P
PN US4260880-A; WO8101763-A; EP41992-A; CA1146671-A
TI Optical character scanner for conversion to machine readable code - includes optical viewer intersecting path of alphanumeric characters and recognition processor to encode images.
AB       The optical character reader is for conversion of information to a machine readable code from a strip of alphanumeric characters on the face of a document. It includes an elongated document slot along which the document may be hand guided through a predetermined path. Optical viewing is provided across a scan line intersecting the path of the alpha-numeric characters.
   A character scanning assembly electronically generates images of each character. A recognition processor encodes these images into a machine readable code. All of the physical, optical and electronic components are contained within a compact single enclosure. The character scanning assembly is located on a separable support frame that includes a surface adapted to be slidably engaged by the face of the document being read. This support frame contains the viewing slot through which light is reflected from the document face. It also rigidly mounts an array of light-responsive sensors, and an intermediate reflective mirror surface and lenses for focussing of the reflected light onto the light-responsive sensors.
PD US4260880-A   07 Apr 1981   G06K-007/14   198117      English
   WO8101763-A   25 Jun 1981   G06K-007/14   198128      English
   EP41992-A   23 Dec 1981   G06K-007/14   198201      English
   CA1146671-A   17 May 1983   G06K-007/14   198322      English
UT DIIDW:1981D8685D
ER

PT P
PN US2016371555-A1; WO2016204968-A1; US9665784-B2; TW201716798-A; AU2016278859-A1; IN201717045506-A; CA2989575-A1; SG11201710474-A1; KR2018019654-A; CN107851182-A; EP3311330-A1; PH12017502353-A1; VN58216-A; JP2018524072-W; BR112017027057-A2; HK1251691-A0; MX2017016718-A1; CN107851182-B; AU2016278859-B2; RU2018101202-A; CN110110591-A; ID201809792-A; SG11201710474-B; SG10201906574-A1; KR2061434-B1; KR2019143498-A; RU2715521-C2; CA2989575-C; RU2020106758-A; BR112017027057-B1; HK1251691-A1; KR2131103-B1; RU2725413-C2; JP6735778-B2; HK40011433-A0; CN110110591-B; TW695182-B1; SG10201906574-B
TI Method for performing spoof-detection and liveness analysis using software-based solution on e.g. smartphone, involves determining whether target is spoof based on determination of whether target comprises one of face-like structure.
AB    NOVELTY - The method involves emitting audio signals using an audio output component of a user device. Reflections of the audio signals off a target are received using the audio input component of the user device. A determination is made to check whether the target comprises one of a face-like structure and face-like tissue based on the reflections. A determination is made to check whether the target is a spoof based on the determination of whether the target comprises one of a face-like structure and face-like tissue.
   USE - Method for performing spoof-detection and liveness analysis using a software-based solution on a user device such as smartphone, tablet and laptop (all claimed).
   ADVANTAGE - The method enables detecting spoofs with better accuracy, while continuing to avoid rejection of real users and detecting user-specific sonic-photometric face signatures as a soft biometric by combining improved sonic and photometric face profiling, thus increasing performance of primary biometric modality as an added soft identification modality. The method enables enhancing acoustic interrogations of ocular/facial biometric target by facial photometric responses to structured light imparted by scene interrogating mobile device for better anti-spoofing.
   DETAILED DESCRIPTION - The user device is a mobile device comprising a smartphone, a tablet and a laptop. The audio signals comprise short coded pulse pings, short term chirps and continuous transmission frequency modulation (CTFM) pings. INDEPENDENT CLAIMS are also included for the following:
   (1) a system for performing spoof-detection and liveness analysis using a software-based solution on a user device
   (2) a non-transitory computer-readable medium comprising a set of instructions for performing spoof-detection and liveness analysis using a software-based solution on a user device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for facilitating anti-spoofing and liveness detection.
   Step for detecting three-dimensional face-like structure (202)
   Step for measuring pulse visually (208)
   Step for measuring pulse mechanically (212)
   Step for accepting target (220)
   Step for rejecting target (230)
PD US2016371555-A1   22 Dec 2016   G06K-009/00   201702   Pages: 18   English
   WO2016204968-A1   22 Dec 2016      201702      English
   US9665784-B2   30 May 2017   G06K-009/00   201736      English
   TW201716798-A   16 May 2017   G01S-007/539   201751      Chinese
   AU2016278859-A1   18 Jan 2018   G06K-009/00   201807      English
   IN201717045506-A   19 Jan 2018   G06K-009/00   201808      English
   CA2989575-A1   22 Dec 2016   G01S-015/88   201812      English
   SG11201710474-A1   30 Jan 2018   G06K-009/00   201817      English
   KR2018019654-A   26 Feb 2018   G06K-009/00   201818      
   CN107851182-A   27 Mar 2018   G06K-009/00   201823      Chinese
   EP3311330-A1   25 Apr 2018   G06K-009/00   201829      English
   PH12017502353-A1   25 Jun 2018   G06K-009/00   201849      English
   VN58216-A   25 Jul 2018   G06K-009/00   201857      English
   JP2018524072-W   30 Aug 2018   A61B-005/1171   201858   Pages: 27   Japanese
   BR112017027057-A2   21 Aug 2018   G06K-009/00   201876      English
   HK1251691-A0   01 Feb 2019   G06K-000/00   201913      English
   MX2017016718-A1   09 Nov 2018   G06K-009/00   201924      Spanish
   CN107851182-B   19 Apr 2019   G06K-009/00   201934      Chinese
   RU2018101202-A   16 Jul 2019   G06K-009/00   201960      Russian
   CN110110591-A   09 Aug 2019   G06K-009/00   201963      Chinese
   ID201809792-A   07 Sep 2018   G06K-009/00   201972      
   SG10201906574-A1   27 Sep 2019      201995      English
   KR2061434-B1   31 Dec 2019   G06K-009/00   202003      
   KR2019143498-A   30 Dec 2019   G06K-009/00   202003      
   RU2715521-C2   28 Feb 2020   G06K-009/00   202023      Russian
   CA2989575-C   24 Mar 2020   G01S-015/88   202029      English
   RU2020106758-A   28 Feb 2020   G06K-009/00   202031      Russian
   BR112017027057-B1   12 May 2020   G06K-009/00   202046      English
   HK1251691-A1   21 Feb 2020   G06K-009/00   202049      Chinese
   RU2725413-C2   02 Jul 2020   G06K-009/00   202058      Russian
   JP6735778-B2   05 Aug 2020   A61B-005/1171   202064   Pages: 27   Japanese
   HK40011433-A0   17 Jul 2020   G06K-009/00   202066      Chinese
   CN110110591-B   15 Jan 2021   G06K-009/00   202108      Chinese
   TW695182-B1   01 Jun 2020   G01S-007/539   202149      Chinese
   SG10201906574-B   25 May 2021      202165      English
UT DIIDW:201680888Y
ER

PT P
PN CN105975931-A; CN105975931-B
TI Convolutional neural network multi-scale pool based human face identifying method, involves initializing network parameter, and obtaining largest value element label as class label of test face image to realize face identification process.
AB    NOVELTY - The method involves collecting a standard human face grey-scale map of an individual to obtain a standard human face grey image as a training image. A convolutional neural network parameter is initialized. A scale pool layer feature column vector is obtained to obtain an output expression column vector in a full connection mode. A human face image of the individual is obtained as a test face image based on a one-dimensional classification result label vector. A largest value element label is obtained as a class label of the test face image to realize face identification process.
   USE - Convolutional neural network multi-scale pool based human face identifying method.
   ADVANTAGE - The method enables extracting human face image features with fixed input image size, thus improving network performance by using a multi-scale pool of a convolutional neural network to perform face recognition process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a convolutional neural network multi-scale pool based human face identifying method. '(Drawing includes non-English language text)'
PD CN105975931-A   28 Sep 2016   G06K-009/00   201672   Pages: 15   Chinese
   CN105975931-B   14 Jun 2019   G06K-009/00   201947      Chinese
UT DIIDW:2016634662
ER

PT P
PN CN105933650-A
TI Video monitoring system, has position information obtaining device fixed to video collect device and identification device, where position information obtain device obtains position information of target object based on recognition result.
AB    NOVELTY - The system has a video collecting device i.e. camera for collecting video in different monitoring area. A storing device stores register identity information and posture facial features characteristic of object. An identification device is connected with the video collecting device and the storing device. A position information obtaining device is connected to the video collecting device and the identification device. The position information obtaining device obtains position information of the target object based on recognition result. A display device is fixed to the identification device.
   USE - Video monitoring system.
   ADVANTAGE - The system utilizes the multi-video acquisition device for sharing target object identity information so as to carry out a target object location tracking process in an easy manner and realizing a monitoring process in an effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a video monitoring method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a video monitoring system. '(Drawing includes non-English language text)'
PD CN105933650-A   07 Sep 2016   H04N-007/18   201669   Pages: 19   Chinese
UT DIIDW:2016599158
ER

PT P
PN CN105005774-A; CN105005774-B
TI Method for identifying human face relative relation based on convolution nerve network, involves detecting process key point position by using face image, and identifying human face image and relative relation based on binary classifier.
AB    NOVELTY - The method involves obtaining a human face image and a position of an image. A local image is obtained by a first group. A local map is obtained by a second group. Multiple convolution layers are provided by a convolution nerve network. The local image and the local map are sent to multiple convolution nerve networks. Weight of the convolution nerve network is adjusted based on result of a binary classifier. A process key point position and area position are detected by using the face image. The human face image and relative relation are identified based on result of the binary classifier.
   USE - Method for identifying human face relative relation based on convolution nerve network.
   ADVANTAGE - The method enables realizing relative identification function in an effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a convolution nerve network based human face relative relation identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a method for identifying human face relative relation based on convolution nerve network. '(Drawing includes non-English language text)'
PD CN105005774-A   28 Oct 2015   G06K-009/00   201578   Pages: 9   Chinese
   CN105005774-B   19 Feb 2019   G06K-009/00   201915      Chinese
UT DIIDW:201572177B
ER

PT P
PN CN104796611-A; US2016309124-A1
TI Mobile terminal unmanned plane intelligent remote control flight shooting method, involves receiving input image signal by mobile terminal, and performing unmanned aerial vehicle remote control signal receiving process.
AB    NOVELTY - The method involves receiving (S1) input image signal by a mobile terminal. An aerial shoot image is obtained (S2) by the mobile terminal. Pattern recognition process is performed in the mobile terminal according to processing result. Unmanned aerial vehicle remote control signal receiving process is performed (S3). Target data of the mobile terminal is obtained, where the target data is selected as human face characteristic data. Position coordinate data of a picture center is obtained.
   USE - Mobile terminal unmanned plane intelligent remote control flight shooting method.
   ADVANTAGE - The method enables simplifying unmanned plane intelligent remote control flight shooting process, increasing bearing capacity of human machine, flexibility and endurance ability, and reducing fault rate of human machine.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a mobile terminal unmanned plane intelligent remote control flight shooting system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a mobile terminal unmanned plane intelligent remote control flight shooting method. '(Drawing includes non-English language text)'
   Step for receiving input image signal by mobile terminal (S1)
   Step for obtaining aerial shoot image by mobile terminal (S2)
   Step for performing unmanned aerial vehicle remote control signal receiving process (S3)
PD CN104796611-A   22 Jul 2015   H04N-005/232   201566   Pages: 10   Chinese
   US2016309124-A1   20 Oct 2016   H04N-007/18   201670      English
UT DIIDW:201555517F
ER

PT P
PN CN104504376-A
TI Human face image age classifying method, involves receiving human face image according to characteristic key point, performing alignment process in human face characteristic key point, and obtaining sample image during adjustment process.
AB    NOVELTY - The method involves obtaining preset standard human face picture. Human face image is received according to human face characteristic key point. Alignment and adjustment process is performed in the human face characteristic key point. Sample image is obtained during adjustment process. Human face outline image is obtained using sample image. A classify model is established using an input convolution nerve network system. Detection image is obtained during adjustment process. Sample image is analyzed through the convolution nerve network system (S10).
   USE - Human face image age classifying method.
   ADVANTAGE - The method enables increasing learning precision of the convolution nerve network so as to provide high classification accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face image age classifying system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a human face image age classifying method.'(Drawing includes non-English language text)'
   Step for analyzing sample image through convolution nerve network system (S10)
   Step for obtaining expected result based on exceed time (S20)
   Step for obtaining human face outline picture through classify model (S3)
PD CN104504376-A   08 Apr 2015   G06K-009/00   201540   Pages: 13   Chinese
UT DIIDW:201534361G
ER

PT P
PN CN102289556-A
TI Shopping assistance robot for use in supermarket, has user identification module transmitting user orientation signal to route leading module, where route leading module controls power module to drive robot body.
AB    NOVELTY - The robot has a robot body driven by a power module. A supermarket shopping trolley is pushed by the robot body that is provided with a subscriber identity module and a route leading module. The route leading module is connected with the power module. A user identification module identifies human face or binds wireless signals tracking user. The user identification module transmits the user orientation signal to the route leading module, where the route leading module controls the power module to drive the robot body.
   USE - Shopping assistance robot for use in a supermarket.
   ADVANTAGE - The robot ensures efficient shopping and management and automatic and intelligent operation, increases shopping interest of consumers, improves service level management of supermarket and meets science and technology development for human.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a shopping assistance robot.'(Drawing includes non-English language text)'
PD CN102289556-A   21 Dec 2011   G06F-019/00   201208   Pages: 11   Chinese
UT DIIDW:2012A62352
ER

PT P
PN WO2008133315-A1; JP2008268004-A; US2009241358-A1; EP2141451-A1; CN101663561-A; US7726033-B2; JP5263804-B2; CN101663561-B; EP2141451-B1; EP2141451-A4
TI Multipoint measuring method for surveying apparatus of bedrock, involves scanning measurement plan point and acquiring telephoto image of measurement plan point, to extract and range measuring point from telephoto image.
AB    NOVELTY - The method involves acquiring image containing measurement object by extracting the feature point of a measurement object and making into a measurement plan point. The measurement plan point is scanned and the telephoto image of the measurement plan point is acquired. The point corresponding to the measurement plan point can be extracted from the telephoto image as a measuring point, and the extracted measuring point is ranged.
   USE - Multipoint measuring method for surveying apparatus of bedrock and slope face.
   ADVANTAGE - The multipoint can be measured with high precision efficiently, and the data processing after a measurement can be managed easily.
   DESCRIPTION OF DRAWING(S) - The drawing shows the perspective view of surveying apparatus.
   Surveying apparatus (1)
   Leveling-up unit (2)
   Base unit (3)
   Cradle unit (4)
   Telephoto mirror unit (5)
PD WO2008133315-A1   06 Nov 2008   G01C-015/00   200875   Pages: 29   Japanese
   JP2008268004-A   06 Nov 2008   G01C-015/00   200875   Pages: 16   Japanese
   US2009241358-A1   01 Oct 2009   G01C-003/04   200964      English
   EP2141451-A1   06 Jan 2010   G01C-015/00   201004      English
   CN101663561-A   03 Mar 2010   G01C-015/00   201019      Chinese
   US7726033-B2   01 Jun 2010   G01C-003/04   201036      English
   JP5263804-B2   14 Aug 2013   G01C-015/00   201354   Pages: 15   Japanese
   CN101663561-B   25 Sep 2013   G01C-015/00   201378      Chinese
   EP2141451-B1   30 Sep 2015   G01C-015/00   201564      English
   EP2141451-A4   29 Jun 2011   G01C-015/00   201744      English
UT DIIDW:2008M81423
ER

PT P
PN WO2007071289-A1; US2007150745-A1; EP1969528-A1; US7545962-B2; EP1969528-B1
TI Biometric e.g. iris, authentication system for e.g. allowing physical access to building, has audit module to interact and monitor processing of data by decision unit, for maintaining audit or log of processed biometric data.
AB    NOVELTY - The system (500) has a processing unit (310) including a feature extraction unit to filter uniqueness data out of raw data from a sensor, and to combine the data into a request template. A matcher (315) compares the template with a previously stored reference template and delivers an output score value. A decision unit (320) takes the calculated score value and a threshold score to determine whether a user is authorized or non-authorized. An audit module (505) interacts and monitors processing of data by the decision unit, to maintain an audit or a log of processed biometric data.
   USE - Used for authenticating a user based on a biometric e.g. fingerprint, iris, face image and voice, to provide security at a border crossing, to identify an individual in a citizen identification (ID) scheme, to allow physical access to a building, to provide logical access to networks and a computer application, and to prove identity during a retail transaction, in a distributed network (claimed).
   ADVANTAGE - The audit module interacts and monitors the processing of data by the decision unit, for maintaining the audit or log of the processed biometric data, thus providing a dynamic update of threshold parameters that are used for an authentication process, based on usage data of the system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method of authenticating a user based on a user presented biometric sample.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic of a biometric authentication system.
   Capture device (305)
   Processing unit (310)
   Matcher (315)
   Datastore (316)
   Decision unit (320)
   Threshold evaluator (325)
   Threshold table (330)
   Biometric authentication system (500)
   Audit module (505)
PD WO2007071289-A1   28 Jun 2007   G06K-009/68   200764   Pages: 35   English
   US2007150745-A1   28 Jun 2007   H04K-001/00   200764      English
   EP1969528-A1   17 Sep 2008   G06K-009/68   200862      English
   US7545962-B2   09 Jun 2009   G06K-009/00   200939      English
   EP1969528-B1   25 May 2011   G06K-009/68   201135      English
UT DIIDW:2007689482
ER

PT P
PN US2006088207-A1; WO2006047253-A1; US7848566-B2
TI Object recognition and detection method for two-dimensional (2D) image of object e.g. airplane, cat face, telephone, involves classifying 2D image based on ratio of graphical probability models using visual information.
AB    NOVELTY - Visual information is obtained from the digitized version of a 2D image after receiving digitized version of 2D image containing 2D representation of a three-dimensional (3D) object. 2D image is classified based on the ratio of graphical probability models using visual information.
   USE - For object recognition and detection of two-dimensional (2D) image of object e.g. airplane, cat face, telephone using Bayesian network based classifier.
   ADVANTAGE - Eliminates need for human to select parameters which is highly subject to error by automatically learning the aspects of classifier from labeled training data. Obtains accurate color balancing on human faces and remove red eye from human faces in digital photo development. Performs face detection in real-time or near real-time in robotic toys. Ensures efficient and accurate object detection scheme due to coarse-to-fine object detection strategy coupled with exhaustive object search across different positions and scales of object detector.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a method of providing assistance in detecting the presence of 3D object in a 2D image containing 2D representation of 3D object;
   (B) a system for providing assistance in detecting presence of a 3D object in a 2D representation of 3D object;
   (C) a method of generating a classifier;
   (D) an object recognition and detection system;
   (E) a computer system;
   (F) a system for generating a classifier;
   (G) a computer readable medium;
   (H) a computer readable data storage medium; and
   (I) a system for detecting instances of an object in a 2D image.
   DESCRIPTION OF DRAWING(S) - The figure shows the explanatory drawing of a classification decision process involving a fixed object size, orientation, and alignment.
   Input image (16)
   Fixed-size classification window (32)
   Classifier (34)
   Lighting correction process (36)
PD US2006088207-A1   27 Apr 2006   G06K-009/62   200634   Pages: 58   English
   WO2006047253-A1   04 May 2006   G06K-009/00   200634      English
   US7848566-B2   07 Dec 2010   G06K-009/62   201080      English
UT DIIDW:2006327839
ER

PT P
PN CN1728156-A; CN100385449-C
TI Method and system for automatic recognizing idnetity document of leaving and entering a country as well as fingerprint of biological living body.
AB    NOVELTY - A method for automatically identifying immigration identity document and biological living body fingerprint includes fingerprint identification; picture and words file management of personal information, personal face identification, self inquiry, cipher key management, databank, ring distribution management and self ring road management. .The identification system comprises living body fingerprint identification unit and personal face identification unit.
PD CN1728156-A   01 Feb 2006   G06K-009/00   200643      Chinese
   CN100385449-C   30 Apr 2008   G06K-009/00   200857      Chinese
UT DIIDW:2006415817
ER

PT P
PN US2004008258-A1; JP2004062868-A; US7843495-B2
TI Digital camera requests name of person, when processor could not match face of person in image with any faces currently stored in face database.
AB    NOVELTY - A photosensor (102) captures images, and a face database stores names and face identification information for person. A processor (110) is configured to match the faces to the face identification information. When a face of the person in an image does not match any faces currently stored in face database, the camera requests a name of the person.
   USE - Digital camera.
   ADVANTAGE - Enables creating and maintaining a database of person contained in images, and simplifies identification of person in images.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) method of identifying person in images; and
   (2) known faces database creating method.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of digital imaging system.
   photosensor (102)
   display (104)
   memory (108)
   processor (110)
   digital signal processor (112)
   microphone (114)
   peaker (118)
   wireless link (120)
PD US2004008258-A1   15 Jan 2004   H04N-005/225   200416   Pages: 8   English
   JP2004062868-A   26 Feb 2004   G06T-007/00   200416   Pages: 8   Japanese
   US7843495-B2   30 Nov 2010   H04N-005/76   201079      English
UT DIIDW:2004167626
ER

PT P
PN CN107292813-A; CN107292813-B
TI Multi-pose human face based opponent network data generating method, involves obtaining identity information and target control parameter by trained coder, and obtaining single-posture human face image by generation network.
AB    NOVELTY - The method involves collecting multi-pose human face images. Classification information is obtained according to attitude angle. Cockcroft marking and coding process is performed for detecting an attitude control parameter. A network model is generated. A generation network is connected with an identifying network. Vector and the attitude control parameter are inputted by the generation network. Random sampling operation is performed. Identity information and a target control parameter are obtained by a trained coder. A single-posture human face image is obtained by the generation network.
   USE - Multi-pose human face based opponent network data generating method.
   ADVANTAGE - The method enables generating a different human face image to improve multi-pose human face recognition effect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-pose human face based opponent network data generating method. '(Drawing includes non-English language text)'
PD CN107292813-A   24 Oct 2017   G06T-003/00   201777   Pages: 13   Chinese
   CN107292813-B   22 Oct 2019   G06T-003/00   201983      Chinese
UT DIIDW:2017751759
ER

PT P
PN CN105956518-A; WO2017181769-A1
TI Human face identifying method, involves performing human recognition object identification process, obtaining identification result, performing characteristic fusion function, and selecting plane matching characteristic point.
AB    NOVELTY - The method involves performing human recognition object identification process. A human face image is obtained. A visual angle of a human face image is determined. Human face characteristic information is obtained. Human recognition object identification operation is performed based on human face characteristic information. Depth information is obtained. A planar image is obtained. Identification result is obtained. Human face characteristic vector is determined. Characteristic fusion function is performed. A plane matching characteristic point is selected.
   USE - Human face identifying method.
   ADVANTAGE - The method enables performing human face identifying process in an accurate manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a human face identifying device
   (2) a human face identifying system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face identifying method. '(Drawing includes non-English language text)'
PD CN105956518-A   21 Sep 2016   G06K-009/00   201670   Pages: 23   Chinese
   WO2017181769-A1   26 Oct 2017   G06K-009/00   201771      Chinese
UT DIIDW:2016615097
ER

PT P
PN CN105488478-A; CN105488478-B
TI Human face identification system, has event information sent to terminal by user, key frame connected with human face comparison unit, and information data base formed with single surface, where face comparison unit receives input signal.
AB    NOVELTY - The system has a system body connected with a data input module that is provided with a human face analyzing module and a data output module. The data input module is connected with an image input unit/ information subscription unit. An image output unit is connected with the human face analyzing module. Event information is sent to a terminal by a user. A key frame is connected with a human face comparison unit. A user information data base is formed with a single surface. The human face comparison unit receives an input signal.
   USE - Human face identification system.
   ADVANTAGE - The system is easy to use, and increases accuracy and reduces time consumption.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification method.
PD CN105488478-A   13 Apr 2016   G06K-009/00   201630   Pages: 14   English
   CN105488478-B   07 Apr 2020   G06K-009/00   202032      Chinese
UT DIIDW:201624153N
ER

PT P
PN CN104574817-A
TI Smartphone based driving fatigue detection warning system, has statistical analysis module and processing module that are used to provide assistant by judging current behavior state so as to provide driving behavior analysis result.
AB    NOVELTY - The system has a smartphone comprising a camera to collect face information, speed information, acceleration information and acceleration information of a vehicle by an information collecting module. A fatigue detecting module detects a state of a driver by checking a change in head posture and eye action state by utilizing a behavior analysis module so as to determine driving state information for comprehensive modeling. The statistical analysis module and a processing module are used to provide assistant by judging current behavior state so as to provide a driving behavior analysis result.
   USE - Smartphone based driving fatigue detection warning system.
   ADVANTAGE - The system enables automatically analyzing a driving behavior and forming a database, thus preventing dangerous driving behavior.
   DESCRIPTION OF DRAWING(S) - The drawing shows a smartphone based driving fatigue detection warning system. '(Drawing includes non-English language text)'
PD CN104574817-A   29 Apr 2015   G08B-021/06   201550   Pages: 10   Chinese
UT DIIDW:2015396933
ER

PT P
PN CN104112091-A; EP2960822-A1; US2015379252-A1; WO2015196708-A1; KR2016011612-A; RU2015104161-A; MX2015001563-A1; JP2016534435-W; IN201500068-P2; RU2617393-C2; BR112015002127-A2; US9904774-B2; MX355555-B; JP6360558-B2; EP2960822-B1
TI File locking method, involves receiving biological characteristic identification information of user, where biological characteristic identification information is included with fingerprint information and iris or face information.
AB    NOVELTY - The method involves receiving biological characteristic identification information of a user. The biological characteristic identification information is included with fingerprint information and iris information or face information. Shooting information of the user is received by a screen. Time period of voice information is calculated corresponding to the biological characteristic identification information. A voice signal of the user is received by a microphone. The shooting information is included with a voice message of the user.
   USE - File locking method.
   ADVANTAGE - The method enables locking a file in a convenient manner, improving protecting privacy of a photo or video of the user, and increasing user experience rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a file locking device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a file locking method.'(Drawing includes non-English language text)'
PD CN104112091-A   22 Oct 2014   G06F-021/32   201503   Pages: 32   Chinese
   EP2960822-A1   30 Dec 2015   G06F-021/62   201602      English
   US2015379252-A1   31 Dec 2015   G06F-021/32   201602      English
   WO2015196708-A1   30 Dec 2015   G06F-021/32   201602      Chinese
   KR2016011612-A   01 Feb 2016   G06F-021/32   201612      English
   RU2015104161-A   27 Aug 2016   H04W-012/00   201665      Russian
   MX2015001563-A1   27 Apr 2016   G06F-021/32   201666      Spanish
   JP2016534435-W   04 Nov 2016   G06F-021/32   201673   Pages: 55   Japanese
   IN201500068-P2   26 Aug 2016   G06F-021/32   201704      English
   RU2617393-C2   24 Apr 2017   H04W-012/00   201749      Russian
   BR112015002127-A2   04 Jul 2017   G06F-021/32   201752      English
   US9904774-B2   27 Feb 2018   G06F-021/00   201816      English
   JP6360558-B2   18 Jul 2018   G06F-021/32   201848   Pages: 28   Japanese
   EP2960822-B1   27 Mar 2019   G06F-021/62   201922      English
UT DIIDW:201502317R
ER

PT P
PN CN103778360-A
TI Human face unlocking behavior analyzing method, involves collecting human face image information, matching human face image with prompt user, analyzing face action of user, and matching face action with terminal device.
AB    NOVELTY - The method involves collecting human face image information. A human face image is identified according to the human face image information. The human face image is matched with a prompt user. Prompting information is obtained by a face action. The face action of the user is analyzed. The face action is matched with a terminal device. An initial action time value is calculated. Human face tracking process is performed. The face action is extracted from a pre-set action feature library.
   USE - Human face unlocking behavior analyzing method.
   ADVANTAGE - The method enables avoiding non-authority user through photo or video decoded login and improving security of human face.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a human face unlocking behavior analyzing device
   (2) a human face unlocking behavior analyzing terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face unlocking behavior analyzing method.'(Drawing includes non-English language text)'
PD CN103778360-A   07 May 2014   G06F-021/32   201444   Pages: 21   Chinese
UT DIIDW:2014M62197
ER

PT P
PN US2013279767-A1; GB2501497-A; US8750576-B2
TI Method for managing visiting guests by face recognition, involves extracting face image of visiting guest, monitoring status of visiting guest to examine violation of authorized item, and updating log page and face image data in database.
AB    NOVELTY - The method involves extracting (S10) a face image of a visiting guest through an image capturing device. A face recognition process is executed. An authorized item is set (S50) up. A visiting goal is implemented (S60) using biological mechanism with face recognition. The status of the visiting guest is monitored (S70) to continuously examine if the visiting guest violates the authorized item. A visited department is informed and reminded, and waiting (S80) until the visiting guest leaves. A log page and the face image data of the visiting guest are recorded and updated (S90) in the database.
   USE - Method for managing visiting guests by face recognition.
   ADVANTAGE - The status of the visiting guest is monitored to continuously examine if the visiting guest violates the authorized item, and thus assist security guards to manage the visiting guests by dramatically reducing workload, automatically and solely execute the whole function of guard entrance without careless mistakes, updates visiting guest information in real time, and greatly improves efficiency of managing visiting guests.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method for managing visiting guests by face recognition.
   Extracting a face image of a visiting guest (S10)
   Setting up an authorized item (S50)
   Implementing a visiting goal using biological mechanism with face recognition (S60)
   Monitoring the status of the visiting guest (S70)
   Waiting until the visiting guest leaves (S80)
   Recording and updating log page and the face image data (S90)
PD US2013279767-A1   24 Oct 2013   G06K-009/62   201373   Pages: 4   English
   GB2501497-A   30 Oct 2013   G06K-009/00   201373      English
   US8750576-B2   10 Jun 2014   G06K-009/00   201438      English
UT DIIDW:2013S62155
ER

PT P
PN US2012014567-A1; US8155394-B2
TI Method for identifying person of interest in public square image/audio sample, involves receiving non-empty set of estimated locations by data-processing system, and generating new assessment based on locations photographed by camera.
AB    NOVELTY - One or more of the non-empty set of candidate user identifiers are transmitted to a wireless location system (101). A non-empty set of estimated locations is received by a data-processing system (104) from location system. Each of non-empty set of estimated locations is for a respective wireless terminal associated with a respective one of the candidate user identifiers. A new assessment is generated by the data-processing system based on the received assessment, the non-empty set of estimated locations, and the one or more locations photographed by a camera (105-1, 105-2).
   USE - Method for identifying person of interest in public square using image/audio sample captured by camera/microphone (all claimed) by integrated wireless location and facial/speaker recognition system.
   ADVANTAGE - The integrated system is capable of using information from a wireless location system to improve the performance of the facial recognition and speaker recognition. The system is capable of processing photographs and/or audio samples captured by a camera/microphone at a fixed location e.g., a digital pan-zoom-tilt (PZT) surveillance camera, as well as captured by a mobile camera/microphone e.g., a digital camera and microphone in a smartphone. The system also features a feedback mechanism by which the location-informed results can be used to improve the system's recognition abilities.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of the wireless location and facial/speaker recognition system.
   Wireless location and facial/speaker recognition system (100)
   Wireless location system (101)
   Facial recognition system (102)
   Data-processing system (104)
   Cameras (105-1, 105-2)
PD US2012014567-A1   19 Jan 2012   G06K-009/00   201209   Pages: 13   English
   US8155394-B2   10 Apr 2012   G06K-009/00   201225      English
UT DIIDW:2012A95834
ER

PT P
PN US2011317872-A1; WO2012006097-A2; WO2012006097-A3; US8326001-B2
TI Low confidence facial recognition performing method, involves comparing processed captured image with target profile corresponding to user associated with resource, and selectively recognizing user seeking access to resource.
AB    NOVELTY - The method involves processing a captured image of a face of a potential user (110) seeking to access a resource by conforming a subset of the captured face image to a reference model, where the reference model corresponds to a high information portion e.g. eyes, mouth and tip of nose, of human faces. The processed captured image is compared with a target profile corresponding to a user associated with the resource. The user seeking access is selectively recognized to the resource based on a comparison result.
   USE - Method for performing low confidence facial recognition.
   ADVANTAGE - The method enables reducing impact of lighting conditions and biometric distortions while providing a low-computation solution reasonably low threshold face recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an appliance for performing low confidence facial recognition.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a computerized appliance for performing low confidence facial recognition.
   Panel (100)
   Display (104)
   Image capture device (106)
   Support (108)
   Potential user (110)
PD US2011317872-A1   29 Dec 2011   G06K-009/00   201203   Pages: 22   English
   WO2012006097-A2   12 Jan 2012   G06K-009/00   201205      English
   WO2012006097-A3   05 Apr 2012   G06K-009/00   201224      English
   US8326001-B2   04 Dec 2012   G06K-009/00   201279      English
UT DIIDW:2012A03637
ER

PT P
PN CN101908140-A
TI Living body detection method for face identification, involves performing hidden living body detection by normal prompt information, and performing detection of eye blinking and eyeball rotation by optical flow field.
AB    NOVELTY - The method involves performing hidden living body detection by normal prompt information. A detection of eye blinking and eyeball rotation is performed by optical flow field. Prompting information is displayed when user is faced to a camera. Expected actions characteristics of living body are generated. A movement is generated in eye pit by an eyeball when user is subconscious of correcting posture and eyes gaze at a screen. The living body actions of eyeball movement and eye blinking are detected via the optical flow field.
   USE - Living body detection method for a face identification.
   ADVANTAGE - The eye ball generates larger optical flow in process of eye posture correction, and eye blinking for real face, so as to easily detect movement and obtain better effect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a living body detection method. '(Drawing includes non-English language text)'
PD CN101908140-A   08 Dec 2010   G06K-009/00   201115   Pages: 10   Chinese
UT DIIDW:2011A37873
ER

PT P
PN WO2010012215-A1; CN101639891-A; EP2306367-A1; US2011128362-A1; CN101639891-B; US8754934-B2; BR200916407-A2; EP2306367-A4
TI Face recognition device selects candidate objects having similarity from candidate aggregation results obtained using face image captured by two cameras positioned at different locations.
AB    NOVELTY - The device has cameras (1,2) that are located at different positions, for acquiring face image. A face recognition processing unit (101) receives and recognizes the face image acquired by the cameras, to obtain candidate aggregation results. Candidate objects having similarity are selected according to predetermined rule from the candidate aggregation results as recognition result.
   USE - Face recognition device with dual camera.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for method for face recognition.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of the face recognition device. (Drawing includes non-English language text)
   Cameras (1,2)
   Face recognition processing unit (101)
PD WO2010012215-A1   04 Feb 2010   G06K-009/20   201012      Chinese
   CN101639891-A   03 Feb 2010   G06K-009/00   201012      Chinese
   EP2306367-A1   06 Apr 2011   G06K-009/20   201124      English
   US2011128362-A1   02 Jun 2011   H04N-007/18   201136      English
   CN101639891-B   02 May 2012   G06K-009/00   201249      Chinese
   US8754934-B2   17 Jun 2014   G03B-013/00   201439      English
   BR200916407-A2   16 Feb 2016   G06K-009/20   201633      
   EP2306367-A4   05 Oct 2011   G06K-009/20   201747      English
UT DIIDW:2010B38283
ER

PT P
PN US2007217683-A1; EP1835446-A2; JP2007249280-A; CN101038623-A; JP4093273-B2; CN100472559-C; EP1835446-A3; US7925048-B2; EP1835446-B1
TI Feature point e.g. eye, detecting device for estimating three dimensional position, has estimating module estimating three dimensional position of feature point in input image based on estimated error.
AB    NOVELTY - The device has an image inputting section e.g. charge-coupled device, creating a three dimensional model in which three dimensional positions of a set of nodes each corresponding to a set of feature points are defined. A face detecting section acquires an error estimated amount indicating the displacement of the position of a feature point based on the information on a correlation and a node feature value e.g. luminance value of a pixel, of the node. An estimating module estimates a three dimensional position of the feature point in an input image e.g. X-ray image, based on the error.
   USE - Used for detecting feature points e.g. eye, nose, ear, and mouth, of an object e.g. face of a person, entire body of the person and organs, from an image e.g. X-ray image or computed tomography image, for estimating a three dimensional position of the feature points.
   ADVANTAGE - The feature point detecting device allows detection of the position of the feature point without performing a search of the peripheral region of each node, thus reducing the calculation time.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a feature point detecting method for estimating a three dimensional position of a set of feature points in an image of a predetermined object from an input image
   (2) a feature point detecting program for an information processing device to estimate a three dimensional position of a feature point of a face in an input image using a three dimensional face shape model representing a three dimensional arrangement of a set of feature points in a face of a person.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a flow of a training process.
PD US2007217683-A1   20 Sep 2007   G06K-009/46   200764   Pages: 23   English
   EP1835446-A2   19 Sep 2007   G06K-009/62   200764      English
   JP2007249280-A   27 Sep 2007   G06T-007/60   200765   Pages: 30   Japanese
   CN101038623-A   19 Sep 2007   G06K-009/00   200810      Chinese
   JP4093273-B2   04 Jun 2008   G06T-007/60   200839   Pages: 30   Japanese
   CN100472559-C   25 Mar 2009   G06K-009/00   200966      Chinese
   EP1835446-A3   15 Sep 2010   G06K-009/00   201061      English
   US7925048-B2   12 Apr 2011   G06K-009/00   201126      English
   EP1835446-B1   22 May 2013   G06K-009/62   201336      English
UT DIIDW:2007688665
ER

PT P
PN US7114079-B1
TI Security method for controlling access with card to objects e.g. building involves comparing feature data of card holder and card owner and calculating average error.
AB    NOVELTY - A feature data representing facial features of card owner is fetched from card memory using an identification code retrieved from the card e.g. automatic teller machine (ATM) card. The pictures of the card holder are taken at different positions for capturing facial features and feature data representative of facial features of card holder is generated. The feature data of card holder and owner are compared and the errors are determined. An average error of several errors is calculated to determine security access of card holder.
   USE - For controlling access by human beings to buildings, automobiles, computer, financial assets such as bank accounts using automatic teller machine (ATM) cards, credit cards etc.
   ADVANTAGE - The applicant who is not a card owner can not gain access with an ill-gotten card as the card does not contain the necessary facial features of wrong applicant. The card does not require personal identification number (PIN) which is easily forgettable.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) security access determining system; and
   (2) access card for use with security system.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart for generating facial feature data.
PD US7114079-B1   26 Sep 2006   H04K-001/00   200676   Pages: 26   English
UT DIIDW:2006744018
ER

PT P
PN US2004017930-A1; JP2004054960-A; KR2004008791-A; US7352880-B2; JP4216668-B2; KR474848-B1
TI Face detection and tracking system for digital monitoring system, has extracting unit removing candidate area using skin color probability map that is generated from face skin color model and global probability map.
AB    NOVELTY - The system has an extracting unit removing a candidate area using a skin color probability map that is generated from a face skin color model (80) and a global probability map. A face area determination unit (60) extracts independent component analysis features and determines whether or not the candidate area is a face area. A track unit follows a face area based on a directional kernel that indicates a probability.
   USE - Used for detecting a set of faces in a digital monitoring system.
   ADVANTAGE - The system can be used to track a set of faces in real time and can operate robustly even under a monitoring system having poor illumination.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) a face detection and tracking method
   (b) a computer readable medium having a computer program.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a structure of a system for detecting and tracking a face.
   Background removing unit (10)
   Skin color probability map generation unit (20)
   Probability map generation unit (40)
   Face area determination unit (60)
   Face skin color model (80)
PD US2004017930-A1   29 Jan 2004   G06K-009/00   200415   Pages: 14   English
   JP2004054960-A   19 Feb 2004   G06T-007/00   200415   Pages: 19   Japanese
   KR2004008791-A   31 Jan 2004   G06K-009/00   200435      
   US7352880-B2   01 Apr 2008   G06K-009/00   200825      English
   JP4216668-B2   28 Jan 2009   G06T-007/00   200909   Pages: 21   Japanese
   KR474848-B1   10 Mar 2005   G06K-009/00   200551      
UT DIIDW:2004155755
ER

PT P
PN US2003174773-A1; US7167519-B2
TI Video object generating apparatus for smart cameras, has module that tracks one object in successive image frames and segments from background of another object, and both objects are encoded and then compressed at different rates.
AB    NOVELTY - The apparatus has a module (306) for detection of an object in an image frame of a series of image frames. A module tracks the object in the successive frames and then segments the object from a background of another object. Both the objects are encoded for transmission to a receiver where one object is compressed at a high and other in low compression rate. The receiver merges the objects to get composite image frame.
   USE - Used for smart cameras.
   ADVANTAGE - The apparatus achieves complex video surveillance, traffic control and real-time analysis of various medical image modalities. The apparatus provides detecting, tracking and segmenting one or more objects like face from a background, to be encoded at the same or different compression rates to conserve bandwidth.
   DETAILED DESCRIPTION - A camera (302) acquires the series of image frames. A frame grabber (304) grabs the image frames from the camera and outputs them to the detection and tracking modules. INDEPENDENT CLAIMS are also included for the following:
   (a) a method for video object generation and selective encoding
   (b) a program storage device readable by a machine
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the video object generating apparatus.
   Camera (302)
   Frame grabber (304)
   Detection module (306)
   Encoder (308)
PD US2003174773-A1   18 Sep 2003   H04N-007/12   200382   Pages: 16   English
   US7167519-B2   23 Jan 2007   H04N-007/12   200708      English
UT DIIDW:2003898475
ER

PT P
PN JP2002288670-A; JP4675492-B2
TI Authentication device for personal identification device in motor vehicle, selects face image of user photographed in transverse plane and selected image is superimposed on prestored face image.
AB    NOVELTY - Several cameras photograph the face of an user (10) and a detector detects the direction of user's face from output image of cameras. The face image of user photographed in transverse plane is selected based on face direction and the selected image is superimposed on prestored face image.
   USE - For personal identification device (claimed)in motor vehicle.
   ADVANTAGE - Performs efficient face recognition depending on face direction and improves accuracy of personal identification.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for personal identification device.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of personal identification system. (Drawing includes non-English language text).
   User (10)
PD JP2002288670-A   04 Oct 2002   G06T-007/00   200305   Pages: 17   Japanese
   JP4675492-B2   20 Apr 2011   G06T-007/00   201128   Pages: 19   Japanese
UT DIIDW:2003051957
ER

PT P
PN CN106971317-A
TI Large data analysis based face recognition and advertising effect evaluation and intelligent decision making method, involves recording average value of ratios of advertisement attention degree evaluation values in list of advertisement.
AB    NOVELTY - The method involves evaluating advertisement number of an advertisement set. An advertisement data table of the advertisement set is established. Advertisements in the advertisement set are played by a user electronic billboard. Human face data of a target user is collected by a camera installed on the billboard. Facial feature data of the user is automatically acquired through face recognition. A face table is established according to the feature data of the user. An average value of ratios of advertisement attention degree evaluation values is recorded in a person list of advertisement.
   USE - Large data analysis based face recognition and advertising effect evaluation and intelligent decision making method.
   ADVANTAGE - The method enables scientifically and objectively evaluating advertising effect in a precision manner, and performing selectively intelligent advertising process according to an advertisement audience group.
   DETAILED DESCRIPTION - The facial feature data comprises gender, age, viewing distance, ethnic, smile value, posture, facial feature point position and eye-closing.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a large data analysis based face recognition and advertising effect evaluation and intelligent decision making method. '(Drawing includes non-English language text)'
PD CN106971317-A   21 Jul 2017   G06Q-030/02   201762   Pages: 18   Chinese
UT DIIDW:201752291N
ER

PT P
PN CN105095833-A; CN105095833-B
TI Human face identification gender or age estimation deep layer network construction method, involves obtaining judging result according to error function, and performing iteration process by network construction parameter.
AB    NOVELTY - The method involves dividing a training image into multiple groups. Characteristic vector matrix is obtained. An artificial nerve network is constructed according to the training image. Image group identification process is performed according to gender or age estimation process. Random input initialization process is performed according to a weight value. Error reverse transmitting process is performed. A judging result is obtained according to error function. Iteration process is performed by network construction parameter.
   USE - Human face identification gender or age estimation deep layer network construction method.
   ADVANTAGE - The method enables performing human face identification gender or age estimation deep layer network construction process in an efficient manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an age or gender human face identification method
   (2) an age or gender human face identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an age or gender human face identification system. '(Drawing includes non-English language text)'
PD CN105095833-A   25 Nov 2015   G06K-009/00   201582   Pages: 19   English
   CN105095833-B   15 Mar 2019   G06K-009/00   201923      Chinese
UT DIIDW:201579217V
ER

PT P
PN CN104463172-A; CN104463172-B
TI Human face characteristic point shape driving depth based human face characteristic extraction method, involves finishing human face characteristic point driving shape characteristic extracting process based on human face part condition.
AB    NOVELTY - The method involves establishing human face characteristic point shape driving depth model based on a convolution nerve network. Human face characteristic point shape driving depth model training process is performed corresponding to the convolution nerve network. Human face characteristic point shape driving depth model is modified corresponding to human face characteristic extraction and characteristic fusion operation. The human face characteristic point driving shape characteristic extracting process is finished based on characteristic attribute and human face part condition.
   USE - Human face characteristic point shape driving depth based human face characteristic extraction method.
   ADVANTAGE - The method enables ensuring better human face identification process, avoiding robustness problem according to an illumination condition and equal changing condition and increasing human face identification efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram illustrating a human face characteristic point shape driving depth based human face characteristic extraction method.'(Drawing includes non-English language text)'
PD CN104463172-A   25 Mar 2015   G06K-009/46   201534   Pages: 7   Chinese
   CN104463172-B   22 Dec 2017   G06K-009/46   201803      Chinese
UT DIIDW:201529521J
ER

PT P
PN CN104361276-A; CN104361276-B
TI Multi-mode biological characteristic identity authentication system, has human face identification unit and voice recognition unit that are connected together, and identity authentication device receiving biological characteristic image.
AB    NOVELTY - The system has a system unit arranged with an obtaining module, a voice module, a main processing module and a storing module. The main processing module is arranged with the obtaining module and the voice module. The voice module is electrically connected with a control unit that is electrically connected with a human face recognition unit, a voice recognition unit and a voice detecting unit. The human face identification unit and the voice recognition unit are connected with each other. An identity authentication device receives a biological characteristic image.
   USE - Multi-mode biological characteristic identity authentication system.
   ADVANTAGE - The system reduces recognition error rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multi-mode biological characteristic identity authentication method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating of a multi-mode biological characteristic identity authentication system.'(Drawing includes non-English language text)'
PD CN104361276-A   18 Feb 2015   G06F-021/32   201526   Pages: 8   Chinese
   CN104361276-B   18 Jul 2017   G06F-021/32   201750      Chinese
UT DIIDW:201522811U
ER

PT P
PN US2014112556-A1; US9031293-B2
TI Method for determining emotional state of user, involves extracting emotional state of user from analysis of acoustic features, visual features, linguistic features, and physical features with machine learning algorithms.
AB    NOVELTY - The method involves extracting features including several acoustic features (107), visual features (109), linguistic features (111), physical features (113) from signals obtained by several sensors (102) with a processor. The features are analyzed with several machine learning algorithms (108) implemented on the processor. An emotional state of the user is extracted from analysis of the features including analysis of the acoustic features, visual features, linguistic features, and physical features with the machine learning algorithms.
   USE - Method for determining emotional state of user in games and man-machine interface.
   ADVANTAGE - The emotion recognition accuracy is improved by effectively analyzing the acoustic features such as prosodic feature and speaking rate, visual features such as facial expression, head poses and body gestures, linguistic features such as semantics, syntax and lexical features, and physical features such as vital signs and biometric data of the users.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) an apparatus for determining emotional state of user; and
   (2) a non-transitory computer-readable storage medium storing program for determining emotional state of user.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the process for determining emotional state of user.
   Sensor (102)
   Acoustic features (107)
   Machine learning algorithm (108)
   Visual features (109)
   Linguistic features (111)
   Physical features (113)
PD US2014112556-A1   24 Apr 2014   G06K-009/66   201431   Pages: 16   English
   US9031293-B2   12 May 2015   G06K-009/00   201532      English
UT DIIDW:2014G93359
ER

PT P
PN CN103208212-A
TI Online remote anti-cheating examination method, involves comparing online remote examination system report with examination human face head portrait image for judging whether examinee is required in testing state.
AB    NOVELTY - The method involves identifying an examination human face head portrait image captured by a camera through an examinees-identity database. A sign of an electronic examination paper is monitored by a remote online examination system during testing process. An online remote examination system report is compared with the examination human face head portrait image for judging whether an examinee is required in a testing state. A group of bar codes is generated in the electronic examination paper. The generated group of bar codes is transmitted to an examinee information management system.
   USE - Online remote anti-cheating examination method.
   ADVANTAGE - The method enables periodically or aperiodically transmitting a human face identification comparison request to an examination area and transmitting a vibration or voice reminding signal when human face of the examinee is far away from an identifying region at certain time period.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an online remote anti-cheating examination system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an online remote anti-cheating examination system.'(Drawing includes non-English language text)'
PD CN103208212-A   17 Jul 2013   G09B-007/02   201372   Pages: 11   Chinese
UT DIIDW:2013T50442
ER

PT P
PN CN102932212-A
TI Multi passage interaction intelligent household appliance control system, has controller provided with front end sensor, camera and audio sensor, and signal pre-processing module provided with video and audio pre-processing modules.
AB    NOVELTY - The system has a controller provided with a front end sensor, a front end camera and an audio sensor. A signal pre-processing module is provided with a video pre-processing module and an audio pre-processing module. A core processing chip is provided with a lip language identification module, a human face identification module, a gesture identification module and a voice identification module. The human face identification module is utilized for determining location of a user. A control network unit is provided with a controller communication module.
   USE - Multi passage interaction intelligent household appliance control system.
   ADVANTAGE - The system is comfortable to operate, and ensures stable operation.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an operation of a multi passage interaction intelligent household appliance control system. '(Drawing includes non-English language text)'
PD CN102932212-A   13 Feb 2013   H04L-012/28   201343   Pages: 8   Chinese
UT DIIDW:2013H96870
ER

PT P
PN CN101526997-A
TI Embedded infrared human face identification method for e.g. office, involves returning human face features to rejection identification information when human face features do not match with eigenvector.
AB    NOVELTY - The method involves reserving human face features in a human face training feature information base as standard features. The human face features are compared with the standard features to compute Euclidean space distance of the human face features. The human face features are returned to corresponding individual identity information when the human face features match with an eigenvector in the feature information base. The human face features are returned to rejection identification information when the human face features do not match with the eigenvector.
   USE - Method for embedded infrared identification of a human face, in office, home and fields such as criminal investigation field, medical field and transportation field.
   ADVANTAGE - The method can be implemented by utilizing the human face characteristics information to realize the automatic collection and identification of the human face images and to realize the verification of individual identification, thus satisfying the office/home needs such as entrance guard, work attendance check and identity verification. The method can be implemented together with other biologic features identification method and safety protection system so as to form complete individual identity verification and safety protection system. The method can support breaking of cases through the human face identification in criminal investigation field, human analysis and diagnosis in medical field, anti-fatigue driving in the transportation field.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an embedded infrared human face identification method.'(Drawing includes non-English language text)'
PD CN101526997-A   09 Sep 2009      200963   Pages: 17   Chinese
UT DIIDW:2009N83609
ER

PT P
PN US2009087030-A1; US7634109-B2
TI Digital image processing method for digital acquisition device, involves adjusting values of parameters within digital image, and rendering, transmitting, transferring, storing, uploading and displaying modified image.
AB    NOVELTY - The method involves acquiring a main digital image, identifying a group of pixels that correspond to an image of a face within the main digital image, and obtaining a collection of low resolution images including the image of the face. Default values of parameters e.g. tone, of the face within the main image is determined based on analysis of the main image. The values of the parameters within the digital image are adjusted based on an analysis of the low resolution images and the default values. The modified image is rendered, transmitted, transferred, stored, uploaded and displayed.
   USE - Method for processing a digital image using face detection in a digital acquisition device (claimed). Can also be used for an external device e.g. personal computer, a digital rendering device e.g. printer such as a laser printer, ink jet printer and a thermal printer, and soft copy devices such as monitor, TV, LCD, LED, and OLED.
   ADVANTAGE - The method enables automatic or manual detection of facial images in the digital image, thus improving the appearance of faces in images automatically or semi-automatically. The method enables assigning, marking, and mapping important regions e.g. face, of the image and automatically performing and suggesting based on the information relating to important regions of the images. The method enables generation of meta-data in the camera, thus enhancing the image based on the face information.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a processor readable storage device having a set of processor readable codes to perform a method for processing a digital image using face detection within the image to achieve desired image processing parameter.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for correcting images based on finding faces in the images.
PD US2009087030-A1   02 Apr 2009   G06K-009/00   200944   Pages: 38   English
   US7634109-B2   15 Dec 2009   G06K-009/00   200982      English
UT DIIDW:2009G66482
ER

PT P
PN US2008298672-A1; WO2008153721-A1; TW200907826-A; US8126260-B2
TI Three dimensional object registering method for machine vision system, involves computing optimal pose estimate which maps training features onto corresponding three-dimensional rays of runtime features.
AB    NOVELTY - The method involves determining a three dimensional (3D) pose transformation between a pose of the object at training time and a pose of the object to be registered at run time by defining features in each of the runtime images as 3D rays through an origin of cameras (110, 112, 114), respectively. The 3D rays are matched to corresponding runtime features from the training images. An optimal pose estimate which maps the training features onto the corresponding 3D rays of runtime features is computed using iterative, reweighted least squares analysis.
   USE - Method for registering an object in three dimension using a machine vision of a machine vision system through charge couple device (CCD) camera and complementary metal oxide semiconductor-based imaging element camera.
   ADVANTAGE - The method determines position of a viewed object in three dimensions by employing two dimensional (2D) machine vision processes on each of a set of planar faces of the object, thus refining the location of the object. The method minimizes the total error between the edgelets of each camera's view trained model image and the associated cameras acquired runtime edgelets.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for registering an object in three dimensions using machine vision
   (2) a medium comprising a set of instructions for performing a method for registering an object in three dimensions using machine vision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective view of a machine vision arrangement with one or more imaging devices interfaced to a processor, and showing a calibration procedure.
   Machine vision system (100)
   Cameras (110, 112, 114)
   Computer (120)
   Memory (124)
   Calibration data (170)
PD US2008298672-A1   04 Dec 2008   G06K-009/00   200881   Pages: 19   English
   WO2008153721-A1   18 Dec 2008   G06T-007/00   200902      English
   TW200907826-A   16 Feb 2009   G06K-009/00   200968      Chinese
   US8126260-B2   28 Feb 2012   G06K-009/00   201216      English
UT DIIDW:2008N99805
ER

PT P
PN US2008019565-A1; US7466866-B2
TI Still image in-camera processing method for e.g. face recognition, involves automatically in-camera processing one portion with enhanced quality processing to generate processed image, and outputting in-camera processed image.
AB    NOVELTY - The method involves identifying pixels that correspond to a face within a digitally-acquired still image. A processing portion of the image including the pixels is determined based on a collection of low resolution images. Another processing portion of the image other than the pixels is determined. The former portion is automatically in-camera processed with enhanced quality processing compared with the latter portion to generate a processed image including the face. An in-camera processed image including the face is output to a digital rendering device e.g. laser printer, for viewing.
   USE - Method for in-camera processing of a still image for face recognition application, face detection application and face tracking application and in the field of digital photography.
   ADVANTAGE - The method provides an automated image processing for determining and/or selecting regions of importance that is maintained with low compression or high resolution compared with regions determined and/or selected for less importance in the image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating image orientation based on orientation of faces.
   Ellipses (210, 220)
   Faces (212, 222)
   Axes (214, 224)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The image includes information that is encoded in a frequency-based or temporal-based method such as JPEG or Wavelet encoding.
PD US2008019565-A1   24 Jan 2008   G06K-009/00   200811   Pages: 37   English
   US7466866-B2   16 Dec 2008   G06K-009/36   200903      English
UT DIIDW:2008B60144
ER

PT P
PN US2007183653-A1; WO2008013575-A2; WO2008013575-A9; WO2008013575-A3; EP1982292-A2; IN200806726-P1; KR2008108430-A; CN101395613-A; JP2009525543-W; US7856125-B2; EP1982292-A4
TI Face reconstruction method, involves using sparse three-dimensional face features to analyze images, to find dense three-dimensional features using data driven approach, without using prior knowledge of face.
AB    NOVELTY - The method involves analyzing images of a face, to find sparse three-dimensional (3D) face features using prior knowledge of a face. The sparse 3D face features are utilized to analyze the images, to find dense 3D features using a data driven approach, without using the prior knowledge. Image pairs are tested to provide an angular baseline greater than a specified amount, and correspondence between features in the pair that is greater than another specified amount. The images are analyzed to find image clusters that have feature point matches.
   USE - Used for reconstructing a face (claimed). Can also be used for a face based application such as facial animation, recognition, or rendering.
   ADVANTAGE - The method prevents unrealistic data, eliminates the need for additional hardware infrastructure, and provides constraints on a camera pose refinement process, thus minimizing cost and improving the stability of a face reconstructing process. The method captures subtle face details by disregarding prior face knowledge or generic face constraints.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a face reconstruction system comprising a camera
   (2) a method comprising analyzing images of a face to find sparse information about the face
   (3) a face processing method comprising analyzing images of a face
   (4) a method for automatically reconstructing a three dimensional (3D) face from two dimensional (2D) images of a human face.
   DESCRIPTION OF DRAWING(S) - The drawing shows an overall flowchart of an image processing operation.
PD US2007183653-A1   09 Aug 2007   G06K-009/00   200770   Pages: 10   English
   WO2008013575-A2   31 Jan 2008   G06K-009/00   200812      English
   WO2008013575-A9   20 Mar 2008   G06K-009/00   200825      English
   WO2008013575-A3   19 Jun 2008   G06K-009/00   200843      English
   EP1982292-A2   22 Oct 2008   G06K-009/00   200874      English
   IN200806726-P1   24 Oct 2008   G06K-009/00   200919      English
   KR2008108430-A   15 Dec 2008   G06T-017/00   200919      
   CN101395613-A   25 Mar 2009   G06K-009/00   200936      Chinese
   JP2009525543-W   09 Jul 2009   G06T-001/00   200945   Pages: 17   Japanese
   US7856125-B2   21 Dec 2010   G06K-009/00   201101      English
   EP1982292-A4   29 May 2013   G06K-009/00   201771      English
UT DIIDW:2007750084
ER

PT P
PN EP1336372-A2; US2003156742-A1; US6873714-B2; EP1336372-A3
TI Eye tracking method e.g. for warning driver of motor vehicle of inattention, in person identification is used to retrieve ocular profiles.
AB    NOVELTY - High resolution methods are used to examine the entire face of a driver of a vehicle. An ocular profile is retrieved based upon the identification of the driver, or may be prepared and stored using a high-resolution image of the eye if the driver is not recognized.
   USE - Monitoring the state of alertness of a vehicle driver to warn the driver of inattention e.g. due to use of telephone, drowsiness, conversing with another vehicle occupant etc. Person identification, determining psychological response and how a person scans, medical research etc.
   ADVANTAGE - Eye tracking with large field of view beyond the area around the eye, and maintains high resolution. Automatically calibrated for each driver.
   DETAILED DESCRIPTION - The eye tracking method involves viewing an entire face of a person to obtain predetermined facial features of the person to identify or not identify the person. If the person is identified, a previously stored ocular profile of the person is retrieved based upon the predetermined facial features. The ocular profile is used to track the movement of an eye of the person. An INDEPENDENT CLAIM is included for an eye tracking apparatus.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic top plan view of an eye tracking system of the invention.
   Eye tracking system (10)
   IR sources (20,22)
   Eye (24)
   Face (26)
   Sensor module (30)
   Processing platform (40)
PD EP1336372-A2   20 Aug 2003   A61B-003/113   200364   Pages: 10   English
   US2003156742-A1   21 Aug 2003   G06K-009/00   200364      English
   US6873714-B2   29 Mar 2005   G06K-009/00   200522      English
   EP1336372-A3   16 Jun 2004   A61B-003/113   201732      English
UT DIIDW:2003672870
ER

PT P
PN EP1280095-A1; JP2003108984-A; KR2003011582-A; US2003185423-A1; EP1280095-B1; DE60201867-E; DE60201867-T2; US7239725-B2; TW265459-B1; JP2008257750-A; JP2009002150-A; JP2009009588-A; JP4945526-B2; KR494007-B1
TI Face image recognition apparatus to recognize face images in security management for control of passage of persons after input of identification information.
AB    NOVELTY - A camera (101) captures an image of a person (100) illuminated by illuminating sections (102,103) and using a key section (104) to input identification information. The face image is supplied to an input section (106) in a processing section (105) while the input identification information is processed in a number processing section (107). The obtained information is then processed in a recognition section (108) to output a decision.
   USE - Recognizing faces of persons in security management.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for a passage control apparatus and for a face image recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows the apparatus
   Camera (101)
   Illuminating sections (102,103)
   Number processing section (107)
   Recognition section (108)
PD EP1280095-A1   29 Jan 2003   G06K-009/00   200324   Pages: 39   English
   JP2003108984-A   11 Apr 2003   G06T-001/00   200334   Pages: 19   Japanese
   KR2003011582-A   11 Feb 2003   G06K-009/46   200339      
   US2003185423-A1   02 Oct 2003   G06K-009/00   200365      English
   EP1280095-B1   10 Nov 2004   G06K-009/00   200473      English
   DE60201867-E   16 Dec 2004   G06K-009/00   200482      German
   DE60201867-T2   27 Oct 2005   G06K-009/00   200571      German
   US7239725-B2   03 Jul 2007   G06K-009/00   200746      English
   TW265459-B1   01 Nov 2006   G06K-000/00   200752      Chinese
   JP2008257750-A   23 Oct 2008   G06T-001/00   200876   Pages: 25   Japanese
   JP2009002150-A   08 Jan 2009   E05B-049/00   200906   Pages: 21   Japanese
   JP2009009588-A   15 Jan 2009   G06T-007/00   200911   Pages: 22   Japanese
   JP4945526-B2   06 Jun 2012   E05B-049/00   201238   Pages: 20   Japanese
   KR494007-B1   10 Jun 2005   G06K-009/46   200659      
UT DIIDW:2003241247
ER

PT P
PN US2003007669-A1; US7027620-B2
TI Face recognition method involves dividing subspace into specific divisions that are modeled using Gaussian distribution, to calculate global identity of test face.
AB    NOVELTY - A subspace representing localization errors is calculated by using eigenspace representation of the test face, based on which the morphed faces are projected onto the eigenspace. The subspace is divided into n-different local units that are modeled using Gaussian distribution to calculate global identity of the test face, based on which the face is recognized.
   USE - For recognizing face, image, etc.
   ADVANTAGE - The global identity of any test image is calculated easily and efficiently by adding all local probabilities defined by Gaussian distribution.
   DESCRIPTION OF DRAWING(S) - The figure explains the process of calculating subspace.
PD US2003007669-A1   09 Jan 2003   G06K-009/00   200333   Pages: 12   English
   US7027620-B2   11 Apr 2006   G06K-009/00   200626      English
UT DIIDW:2003353068
ER

PT P
PN JP11066320-A; US5926251-A; KR99023061-A; KR296215-B1
TI Eye image tracking apparatus - digitizes input image of face from which image of eye and its periphery are tracked for testing.
AB       NOVELTY - The face image obtained from a face input unit (4) is digitized by a digitization unit (5) and image search unit (6) does searching of eye image and eye peripheral image within the digitized image. Image tracking unit (7) tracks the searched eye image and eye peripheral image.
   USE -   For testing eyes.
   ADVANTAGE -   It not only traces eye image but also eye peripheral image and hence accurate testing of eyes during loss is managed. DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of eye image tracking apparatus. (4) Face input unit; (5) Digitization unit; (6) Image search unit; (7) Image tracking unit.
PD JP11066320-A   09 Mar 1999   G06T-007/60   199920   Pages: 18   Japanese
   US5926251-A   20 Jul 1999   A61B-003/14   199935      English
   KR99023061-A   25 Mar 1999   B60K-028/06   200024      
   KR296215-B1   22 Nov 2001   G06T-007/60   200243      
UT DIIDW:1999237771
ER

PT P
PN US5867588-A
TI 3D geometric type facial image construction method used in 3D geometric overlay system.
AB    NOVELTY - The method involves providing a pentagon complex coupling regular pentagons with equal dimensions. The regular pentagons in pentagon complex, are superimposed in an inverted relationship with a common center. Radials are created by lines joining vertices of opposite regular pentagons and another pentagon complex is formed. The two pentagon complex are related by phi (phi) proportion, and shares a common radial or intersect of radials. The radials and intersects of radials are selected to create a geometric form comprising major anthropometric points of face in frontal and lateral views. The frontal and lateral views are combined to create 3D face.
   USE - In 3D geometric overlay system used as pre surgical aid in orthodontics, oral surgery and plastic surgery.
   ADVANTAGE - Analysis variation between recorded face image and face of overlay system. Enables 3D aesthetic quality of face. Provided unifying model that describes individual features of face.
   DETAILED DESCRIPTION - The appearance of human face is altered by application of cosmetics to more closely coincide with 3D geometric form. An INDEPENDENT CLAIM is also included for 3D geometric overlay system.
   DESCRIPTION OF DRAWING(S) - The figure shows flow diagram of 3D face construction method.
PD US5867588-A   02 Feb 1999   G06K-009/00   199912   Pages: 84   English
UT DIIDW:1999142313
ER

PT P
PN JP8131252-A; US6453052-B1
TI Hair style simulation method - by displaying face taken by camera and adjoining feature point on outline of hair area to obtain simulated pattern.
AB       The method entails obtaining the image of a face using a camera (2) to associate varying hair styles stored in a memory (8) of a computer system based on the outline of a hair area. An input-output unit (22) enables a user to appoint a hair style to fit the image taken by the camera. If the hair style information inclines in the way of the hair area then its position is adjusted.
   The method also involves showing the adjacent relation of a feature point formed at the outline of the hair area with numerous face outline vector data. The feature point adjoins the outline of the hair area thereby forming the simulated pattern.
   ADVANTAGE -   Performs hair style simulation in short time; allows many users access.
PD JP8131252-A   28 May 1996   A45D-044/00   199631   Pages: 11   Japanese
   US6453052-B1   17 Sep 2002   G06K-009/00   200264      English
UT DIIDW:1996303920
ER

PT P
PN CN104715236-A
TI Beauty photographing method, involves capturing iris image of current user according to identity recognition information of current user, performing binding information searching process, and storing picture of current user in photo library.
AB    NOVELTY - The method involves obtaining identity recognition information of a current user. A beautifying parameter is determined according to the obtained identity recognition information. Pre-stored data is obtained. Fingerprint identification process and human face recognition process are performed. An iris image of the current user is captured according to the identity recognition information of the current user. Binding information searching process is performed. A picture of the current user is stored in a photo library.
   USE - Beauty photographing method.
   ADVANTAGE - The method enables increasing working efficiency of a shooting unit.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a beauty photographing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a beauty photographing method. '(Drawing includes non-English language text)'
PD CN104715236-A   17 Jun 2015   G06K-009/00   201562   Pages: 13   Chinese
UT DIIDW:201548947Y
ER

PT P
PN US2012256820-A1; EP2515526-A2; US8913005-B2; EP2515526-A3
TI Ergonomic sensor module for use with e.g. display device of computer system, has processing element analyzing image data to determine whether image data indicates that user of display utilizes display within ergonomic use range.
AB    NOVELTY - The module has a processing element obtaining image data from an image sensing device and analyzing the image data to determine whether the image data indicates that a user of a display utilizes the display within ergonomic use range based on predefined data defining the ergonomic range of a display device. An input/output interface is connected to the display device and the processing element to output a feedback message utilizing the display device in response to determining that the image data indicates that the user of the display does not uses the display within the ergonomic range.
   USE - Ergonomic sensor module for use with a display device of a computer system. Uses include but are not limited to an LCD display device, plasma display device, CRT display device and a TV display device of a desktop, laptop, server, game system and a mobile device.
   ADVANTAGE - The processing element carries out an ergonomic analysis routine to determine ergonomic use independently of changes in ambient lighting conditions by analyzing intensity values across an entire image on ongoing basis, so that measured intensity across the image is maintained in threshold to determine low light conditions, thus allowing algorithm to pick an area of the image proximate to the user's face and above to remove effect of the user's dark clothing lowering the average intensity value. The module utilizes an imaging device interfaced to process hardware to obtain and analyze image data depicting the user of the display device to indicate ergonomic uses of the display device, so that the image of the user can be analyzed to provide real-time feedback i.e. warnings, when the user's behavior falls outside the ergonomic use range for the display device, thus ensuring ergonomically-proper utilization of the display device for the user.
   DETAILED DESCRIPTION - The processing element is selected from a group consisting of a microprocessor, digital signal processor (DSP) and an application-specific integrated circuit (ASIC). The image sensing device is an image sensor. INDEPENDENT CLAIMS are also included for the following:
   (1) a computerized method for facilitating ergonomic feedback using image analysis
   (2) a computer program product comprising a set of instructions to perform a computerized method for ergonomic feedback using image analysis.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating an user's yaw angle relative to a display.
PD US2012256820-A1   11 Oct 2012   G09G-005/00   201269   Pages: 11   English
   EP2515526-A2   24 Oct 2012   H04N-005/232   201270      English
   US8913005-B2   16 Dec 2014   G09G-005/00   201501      English
   EP2515526-A3   24 Dec 2014   H04N-005/232   201502      English
UT DIIDW:2012N02559
ER

PT P
PN CN101964056-A; CN101964056-B
TI Bimodal face authentication method for use in bimodal face authentication system, involves calculating weighted sum of visible light distance and near-infrared distance through image authentication module.
AB    NOVELTY - The method involves collecting visible light training images and near-infrared training images of head parts to be authenticated through an image collecting module. A visible light distance is computed from face visible light image to visible light training image in the visible light images through a distance calculation module. A minimal visible light distance is compared with a set visible light distance threshold through a living-body detection module. A weighted sum of the visible light distance and the near-infrared distance is calculated through an image authentication module.
   USE - Method for authenticating a bimodal face for use in a bimodal face authentication system (claimed).
   ADVANTAGE - The method enables utilizing the bimodal combined recognition of the visible light training images and near-infrared training images of the faces, thus improving recognition and authentication accuracy, and avoiding recognition failure effectively. The method enables effectively avoiding a personator from cheating through photos or models with a living-body detection module.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method for authenticating a bimodal face. '(Drawing includes non-English language text)'
PD CN101964056-A   02 Feb 2011   G06K-009/00   201128   Pages: 14   Chinese
   CN101964056-B   27 Jun 2012   G06K-009/00   201273      Chinese
UT DIIDW:2011D20229
ER

PT P
PN CN101734527-A
TI Lift predetermining system for recognition of face in e.g. office, has lift control modules connected with recognition result outputting modules for running lift by adjusting lift stopping according to fed back information by users.
AB    NOVELTY - The system has a lift box provided with a lift operation plate, and a lift door provided with a lift call device. Face recognition modules are provided for recording and telling users faces information by a camera device, and for recognizing and storing the recorded result. User stopping floor modeling modules are connected with the face recognition modules for gaining the recognized result. Lift control modules are connected with recognition result outputting modules for running the lift by adjusting the lift stopping according to fed back information by users.
   USE - Lift predetermining system for recognition of a face in an office or residence environment.
   ADVANTAGE - The system can control the permission and improve the security of office or residence environment, and can save users time by automatically recognizing and setting the user stopping floor.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a realizing method for a lift predetermining system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a realizing method for a lift predetermining system.'(Drawing includes non-English language text)'
PD CN101734527-A   16 Jun 2010   B66B-001/00   201048   Pages: 11   Chinese
UT DIIDW:2010J00886
ER

PT P
PN CN101593352-A
TI Driving safety monitoring system, has driver safe driving judging module that judges mouth area at unsafe driving state and sends alarm command according to ratio interval of preset face left and right areas.
AB    NOVELTY - The system has a vision sensor that acquires head image of a driver. An intelligent processor comprises an acquisition module, and a fringe extraction module extracts fringe to obtain head image contour. An eye detection module performs Hough conversion for marginalized face, and a face orientation analysis module determines mouth area. A driver safe driving judging module judges the mouth area at unsafe driving state and sends an alarm command according to ratio interval of preset face left and right areas when current ratio of left end right areas is out of preset area.
   USE - Driving safety monitoring system.
   ADVANTAGE - The system has strong anti-interference performance and high accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a process involved in a driving safety monitoring system.'(Drawing includes non-English language text)'
PD CN101593352-A   02 Dec 2009   G06T-007/00   201001   Pages: 22   Chinese
UT DIIDW:2009S27411
ER

PT P
PN EP1818855-A2; US2007189583-A1; EP1818855-A3; CN101021898-A; KR2007081773-A; JP2008181468-A
TI Infrared face authentication apparatus used in security apparatus, has authenticator that performs authentication of face using infrared image from camera which detects reflected light of infrared light irradiated onto face.
AB    NOVELTY - The apparatus (100) includes an infrared LED (60) for irradiating infrared light having a wavelength of 760 nanometers or more onto a face. A camera (110) detects the reflected of the infrared light to output an infrared image. An authenticator (130) performs authentication of the face using the infrared image. The authenticator specifies the contour of the entire face, positions of eyes, positions of nostrils, and position of a mouth from the infrared image to perform authentication of the face.
   USE - Used in security apparatus (claimed), e.g. mobile telephone, automatic teller machine (ATM), safe deposit box, copier, personal computer, for face authentication using infrared image obtained from reflected light if infrared light irradiated onto a face. Also applicable for installation at the entrance of a building or door of an automobile or room.
   ADVANTAGE - Provides an infrared face authentication apparatus using an infrared image to improve a face authentication rate. Ensures that face authentication can be realized using a highly precise, small apparatus, in which biometric authentication using an iris does not need to be combined with biometric authentication using face recognition. Enables an authenticator to specify a contour of the entire face and positions of eyes from the infrared image to perform authentication of the face. Ensures that a probability of recognizing a face wrong is lower since the infrared face authentication apparatus permeates into the eyes using infrared light to specify the positions of the eyes. Ensures that authentication is hardly affected by lenses of the glasses if a face of a person wearing glasses is authenticated using infrared image. Enables an authenticator to perform face authentication of a person accessing a movable or fixed terminal. Enables a security apparatus to authenticate a face of a person using infrared light, guaranteeing security. Prevents unauthorized withdrawal from ATM using a face photograph, and prevents unauthorized access in e.g. copier.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) Portable terminal including the infrared face authentication apparatus; and
   (2) Face authentication method.
   DESCRIPTION OF DRAWING(S) - The figure is a block diagram of the infrared face authentication apparatus.
   Monitor (30)
   Infrared LED (60)
   Infrared face authentication apparatus (100)
   Camera (110)
   Authenticator (130)
PD EP1818855-A2   15 Aug 2007   G06K-009/00   200760   Pages: 24   English
   US2007189583-A1   16 Aug 2007   G06K-009/00   200760      English
   EP1818855-A3   02 Jan 2008   G06K-009/00   200805      English
   CN101021898-A   22 Aug 2007   G06K-009/00   200809      Chinese
   KR2007081773-A   17 Aug 2007   G06T-007/00   200821      
   JP2008181468-A   07 Aug 2008   G06T-007/00   200854   Pages: 29   Japanese
UT DIIDW:2007627248
ER

PT P
PN EP1701308-A2; EP1701308-A3; JP2006287917-A; US2006204135-A1; US7773782-B2; EP1701308-B1
TI Image output apparatus for use as display device, has image layout section laying out image within output region based on object orientation such that the object orientation faces to center of region.
AB    NOVELTY - The apparatus has an object orientation identification section for identifying the orientation of the object in an image. An image layout section lays out the image within an output region based on the object orientation identified by the object orientation identification section such that the object orientation identified by the identification section faces to the center of the output region. An image output section outputs the image laid out within the output region by the image layout section.
   USE - Used as a display device for displaying an image data over a network e.g. Internet and LAN.
   ADVANTAGE - The image layout section automatically lays out the image within the output region based on the object orientation such that the object orientation faces to the center of the output region, so that the viewer can output the image having the layout which does not cause the viewer to feel uncomfortable about the composition of the image without spending time and effort, that is, without determining the image layout by the viewer.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image output method
   (2) an image output program for an image output apparatus for outputting an image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block representation depicting a functional configuration of an image output apparatus.
PD EP1701308-A2   13 Sep 2006   G06T-011/60   200843   Pages: 37   English
   EP1701308-A3   03 Oct 2007   G06T-011/60   200843      English
   JP2006287917-A   19 Oct 2006   H04N-001/387   200843   Pages: 30   Japanese
   US2006204135-A1   14 Sep 2006   G06K-009/00   200843      English
   US7773782-B2   10 Aug 2010   G06K-009/00   201053      English
   EP1701308-B1   28 Nov 2012   G06T-011/60   201279      English
UT DIIDW:2008G76359
ER

PT P
PN WO2006078343-A2; US2006204049-A1; US7287013-B2; GB2438103-A; WO2006078343-A3; US2009171623-A1; GB2438103-B; US8190540-B2
TI Data set determination method for use in e.g. fingerprint identification, involves determining false acceptance rate using portability partition array and index set.
AB    NOVELTY - The probability partition arrays comprising values corresponding to probability of authentic match and false match, respectively for information in a data set, are provided. The index sets are identified and false acceptance rate (FAR) is determined using one of the portability partition arrays and one of the index sets. The determined FAR is compared with desired FAR. The data set is rejected if the determined FAR is greater than desired FAR.
   USE - For determining acceptance of data set for biometric identification such as fingerprint identification, iris recognition, voice recognition, facial recognition, hand geometry, signature recognition, signature gait recognition, vascular patterns, lip shape recognition, ear shape recognition and palm print recognition.
   ADVANTAGE - The acceptance of data set is determined efficiently.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for computer readable memory device storing program for determining acceptance of data set.
   DESCRIPTION OF DRAWING(S) - The figure shows a graph illustrating portability density functions for three biometric data sets.
PD WO2006078343-A2   27 Jul 2006   G06K-009/00   200658   Pages: 60   English
   US2006204049-A1   14 Sep 2006   G05B-019/00   200661      English
   US7287013-B2   23 Oct 2007   G06K-009/00   200771      English
   GB2438103-A   14 Nov 2007   G06K-009/00   200778      English
   WO2006078343-A3   23 Apr 2009   G06K-009/00   200929      English
   US2009171623-A1   02 Jul 2009   G06F-017/18   200946      English
   GB2438103-B   18 Aug 2010   G06K-009/00   201054      English
   US8190540-B2   29 May 2012   G06K-009/03   201236      English
UT DIIDW:2006568852
ER

PT P
PN US2006102843-A1; US7602942-B2
TI Infrared and visible fusion face recognition system, for detection and recognition of people under various lighting condition, e.g. in parking lot, has secondary integrator which inserts frame that surrounds face in third image.
AB    NOVELTY - A thresholder nulls the elements of a fourth image with differences of intensities that do not meet a certain threshold intensity value. A feature selector finds facial features in fourth image and detects a face with the features. A framer places a frame around the face in fourth image. A primary integrator inserts frame that surrounds the face in a first image. A secondary integrator inserts frame that surrounds the face in a third image.
   USE - For detection and recognition of people under various lighting condition, e.g. in parking lot, crowd, store, airport, military area, jungle, vehicle, security point, alert area.
   ADVANTAGE - The novel system enables adequate detection of humans in vehicles at night and in poor weather, since infrared cameras provide clear imaging signals, even in foul weather conditions e.g. hazy condition.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for:
   (A) A detection system;
   (B) A method of detecting a face in a scene; and
   (C) A device for detecting faces.
   DESCRIPTION OF DRAWING(S) - The figure shows the above infrared and visible fusion face recognition system.
   Camera (50)
   Computer (53)
   Scene (55)
   Illuminator (58)
   Power supply (62)
PD US2006102843-A1   18 May 2006   G01J-005/02   200637   Pages: 47   English
   US7602942-B2   13 Oct 2009   G06K-009/00   200967      English
UT DIIDW:2006360940
ER

PT P
PN US2005147280-A1; US7095879-B2
TI Face recognition method in personal computer, involves synthesizing face pose images of input image using three-dimensional model created by deforming generic three-dimensional model to conform shape of input face image.
AB    NOVELTY - The image of a face having a particular pose, to be recognized is input. The input generic three-dimensional (3-D) face model is deformed to conform to the shape of the face depicted in the input image, for creating specific 3-D face model. The various face pose images are synthesized using the specific 3-D model which is used for training images to train a recognizer.
   USE - For face recognition using synthesized training images in e.g. personal computer (PC).
   ADVANTAGE - Requires only small amount of actual training data to train recognition classifier. Minimizes cost and effort required to obtain training data and makes recognition systems practical for even low cost consumer applications.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for process for generating synthesized face images.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart of the face recognition method.
PD US2005147280-A1   07 Jul 2005   G06K-009/00   200553   Pages: 16   English
   US7095879-B2   22 Aug 2006   G06K-009/00   200656      English
UT DIIDW:2005520258
ER

PT P
PN US2005129290-A1; WO2005059824-A2; US6993166-B2; EP1695284-A2; KR2006105780-A; IN200601355-P2; JP2007521577-W; WO2005059824-A3; CN101233529-A
TI Biometric image e.g. fingerprint, enrolling method for biometric identification system, involves determining whether scores are equal to preset score threshold, and removing each image having relating score equal to threshold.
AB    NOVELTY - The method involves capturing images for a user into a capture folder, and selecting one image. The selected image is compared to each remaining images to generate a score for each remaining image. A determination is made to find whether the scores are equal to a predetermined score threshold. The image having the score equal to the threshold is removed from the capture folder.
   USE - Used for enrolling a biometric image e.g. fingerprint, facial image and palm print image, in a biometric identification system that is utilized in criminal application and civil application e.g. credit card or personal identity fraud and print identification.
   ADVANTAGE - The method removes the image having the score equal to the threshold from the capture folder, thus enhancing matching accuracy during a verification stage in the identification system, and hence achieving optimal accuracy and speed for the identification system, while keeping storage requirements to a minimum.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for biometric image enrollment.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a method for biometric image enrollment.
PD US2005129290-A1   16 Jun 2005   G06K-009/00   200546   Pages: 16   English
   WO2005059824-A2   30 Jun 2005   G06T-000/00   200546      English
   US6993166-B2   31 Jan 2006   G06K-009/00   200610      English
   EP1695284-A2   30 Aug 2006   G06K-009/00   200657      English
   KR2006105780-A   11 Oct 2006   G06K-009/00   200731      
   IN200601355-P2   04 May 2007   G06T-000/00   200746      English
   JP2007521577-W   02 Aug 2007   G06T-007/00   200753   Pages: 14   Japanese
   WO2005059824-A3   21 Dec 2007   G06K-009/00   200802      English
   CN101233529-A   30 Jul 2008   G06K-009/00   200858      Chinese
UT DIIDW:2005456925
ER

PT P
PN US2005105806-A1; JP2005174308-A; US7822233-B2; JP5388399-B2
TI Computer-based method for organizing digital photos, involves applying face recognition algorithm to determine similarity of faces isolated from digital photos, with reference model.
AB    NOVELTY - The faces are extracted from several digital photos, and the digital photos are cropped to generate images of isolated faces. A face recognition algorithm is applied to determine similarity of isolated faces with reference model. The faces arranged as a function of determined similarity, are displayed and associated with particular classification based on user input.
   USE - For organizing digital photos of family, classmates and work colleagues obtained using digital camera, in personal computer, laptop computer and network accessible server for face recognition application.
   ADVANTAGE - Enables to organize digital photos conveniently and quickly.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for apparatus for organizing digital photos.
   DESCRIPTION OF DRAWING(S) - The figure shows a flow chart explaining the organization process of the digital photos.
PD US2005105806-A1   19 May 2005   G06K-009/46   200543   Pages: 12   English
   JP2005174308-A   30 Jun 2005   G06F-017/30   200543   Pages: 12   Japanese
   US7822233-B2   26 Oct 2010   G06K-009/00   201071      English
   JP5388399-B2   15 Jan 2014   G06F-017/30   201406   Pages: 12   Japanese
UT DIIDW:2005423789
ER

PT P
PN JP1198174-A; US5136665-A
TI Duplex sheet reading device for facsimile - carrying device, reading devices, and correction device to facilitate recognition of format sheet frame NoAbstract Dwg 3/4.
AB       The two-sided original reading apparatus includes carying device for carrying an original in a predetermined carrying direction. First reading circuitry reads the image on the first face of the original at a first reading position and generates first analog image signals. Second reading circuitry reads the image from the second face of the original at a second reading position different from the first reading position with respect to the carrying direction, and generates second analog image signals.
   Converting circuitry converts the two analog signals into two digital signals, respectively. Delay circuitry is provided for delaying one of the two digital image signals with respect to the other by a delay time which corresponds to a deviation between the two reading positions.
   USE/ADVANTAGE -   For image information filing appts. Simultaneous reading improves speed. (First major country equivalent to JP1198174).
PD JP1198174-A   09 Aug 1989      198938      Japanese
   US5136665-A   04 Aug 1992      199234   Pages: 4   English
UT DIIDW:1989273794
ER

PT P
PN CN107105340-A
TI Method for displaying character information on video based on artificial intelligence, involves obtaining human face recognition result according to relevant information of image, and displaying related information of person by TV screen.
AB    NOVELTY - The method involves capturing current TV picture according to received character recognition instruction. To-be identified human face image is obtained according to the current TV picture. The to-be identified human face image is transmitted to a server. Human face recognition is performed by the server. Human face recognition result is obtained according to relevant information of the -be identified human face image. Related information of person is displayed by a TV screen. User input is received by a control terminal through voice information.
   USE - Method for displaying character information on video based on artificial intelligence.
   ADVANTAGE - The method enables viewing related information of character on the TV screen by the user with out switching other device so as to reduce complexity of user operation, so that information acquisition efficiency of the user can be improved and improving user experience.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for displaying character information on video based on artificial intelligence,
   (2) a system for displaying character information on video based on artificial intelligence.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for displaying character information on video based on artificial intelligence. '(Drawing includes non-English language text)'
PD CN107105340-A   29 Aug 2017   H04N-021/44   201768   Pages: 24   Chinese
UT DIIDW:201761042X
ER

PT P
PN CN105631439-A; CN105631439-B
TI Human face image processing method, involves receiving detecting human face image, determining shielding state of human face multi-key part, and judging whether shielding state of detecting human face image is determined.
AB    NOVELTY - The method involves receiving a detecting human face image. Quality of the detecting human face image is evaluated by a depth convolutional network. Detecting human face image check operation is performed by the depth convolutional network. The quality of the detecting human face image is determined. Fuzzy degree of the human face image is detected. Shielding state of a human face multi-key part is determined. A judgment is made to check whether the shielding state of the detecting human face image is determined.
   USE - Human face image processing method.
   ADVANTAGE - The method enables improving evaluating accuracy of detecting human face image quality and accuracy of human face identification.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face image processing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face image processing method. '(Drawing includes non-English language text)'
PD CN105631439-A   01 Jun 2016   G06K-009/00   201645   Pages: 30   Chinese
   CN105631439-B   08 Nov 2019   G06K-009/00   201989      Chinese
UT DIIDW:201640600Y
ER

PT P
PN CN104834849-A; CN104834849-B
TI Voiceprint recognition and face recognition double factor identity authentication method, involves determining whether voiceprint similarity is larger than threshold value to obtain identity authentication.
AB    NOVELTY - The method involves utilizing a password generating unit for generating dynamic password text. A voice characteristic vector extraction unit is utilized for extracting a voice print characteristic vector. A user vocal print characteristic vector is utilized for judging vocal print similarity is greater than a set threshold value. Determination is made to check whether voiceprint similarity is larger than a threshold value to obtain identity authentication when a sound groove is less than a similarity threshold value as authentication failure.
   USE - Voiceprint recognition and face recognition double factor identity authentication method.
   ADVANTAGE - The method enables increasing safety and reliability of authentication.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a voiceprint recognition and face recognition double factor identity authentication system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a voiceprint recognition and face recognition double factor identity authentication method. '(Drawing includes non-English language text)'
PD CN104834849-A   12 Aug 2015   G06F-021/32   201569   Pages: 12   Chinese
   CN104834849-B   18 Sep 2018   G06F-021/32   201867      Chinese
UT DIIDW:201559388Y
ER

PT P
PN CN104268539-A; CN104268539-B
TI Human face identification method, involves detecting multi-angle human face according to human face characteristic point location algorithm, and identifying human face by using average LBP characteristic vector library.
AB    NOVELTY - The method involves detecting multi-angle human face according to a human face characteristic point location algorithm. A two-dimensional human face image is extracted according to gradient histogram HOG characteristic final. LBP characteristic is obtained according to the human face image. Multiple illumination environments are obtained by using a SVM tree structure. A three-dimensional human face model is selected from a sample library. Human face illumination is obtained by using a gesture classifier. The human face is identified by using an average LBP characteristic vector library.
   USE - Human face identification method.
   ADVANTAGE - The method enables achieving human face recognition property without illumination condition, reducing human face posture change factor affect and calculation complexity and increasing human face recognition rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face identification method.'(Drawing includes non-English language text)'
PD CN104268539-A   07 Jan 2015   G06K-009/00   201519   Pages: 16   Chinese
   CN104268539-B   31 Oct 2017   G06K-009/00   201774      Chinese
UT DIIDW:201516191X
ER

PT P
PN CN102413282-A; CN102413282-B
TI Self-timer guiding method, involves performing shooting process by human face identification frame, and requesting user to adjust shooting mode until human face location frame is matched with human face identification frame.
AB    NOVELTY - The method involves locating a setting human face location frame at a predetermined position on a device screen. Shooting process is performed by a human face identification frame. The human face identification frame is located in the human face location frame. Judgment is made whether the human face identification frame is located at the human face location frame or not. A user is requested to adjust a shooting mode until the human face location frame is matched with the human face identification frame.
   USE - Self-timer guiding method.
   ADVANTAGE - The method enables realizing self-shooting effect without utilizing an auxiliary camera device, reducing cost and improving success rate and efficiency of the device.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face shooting device comprising an image pick-up unit.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a self self-timer guide method.'(Drawing includes non-English language text)'
PD CN102413282-A   11 Apr 2012   H04N-005/232   201229   Pages: 9   Chinese
   CN102413282-B   18 Feb 2015   H04N-005/232   201523      Chinese
UT DIIDW:2012E69621
ER

PT P
PN CN102053629-A; CN102053629-B
TI Method for automatically adjusting position of display of computer, involves providing capture tracking module in top frame centre part of display, where camera is provided with human face capturing and distance measurement functions.
AB    NOVELTY - The method involves providing a capture tracking module in a top frame center part of a display. A camera is provided with functions of human face capturing, distance measurement, angle measurement and tracking, and transmitting information acquired by the camera to a personal computer (PC) through an inter-integrated circuit bus (I2C) bus. The PC is provided with an acquired information processing module and a correction information processing module for performing human face analysis and pupil positioning on information acquired by the camera and the acquired information processing module.
   USE - Method for automatically adjusting position of a display of a device i.e. computer.
   ADVANTAGE - The method enables guaranteeing a user to use a computer and a display device in a right manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device i.e. computer comprising a display device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a method for automatically adjusting position of a display of a device.'(Drawing includes non-English language text)'
PD CN102053629-A   11 May 2011   G05D-003/12   201142   Pages: 17   Chinese
   CN102053629-B   16 Jan 2013   G05D-003/12   201327      Chinese
UT DIIDW:2011G65822
ER

PT P
PN US2009303342-A1; US7916897-B2
TI Digital image acquiring method for face detection and recognition, involves analyzing characteristics of region of preview image, and adjusting acquisition parameters of main acquired image, based on analysis.
AB    NOVELTY - The method involves determining an initial location or a size of a face in a preview image of a preview image stream, and determining a subsequent location or a size for the face in another preview image. A region of a third preview image is predicted based on the initial and subsequent locations or sizes, where the face is expected to occur again in the region. Characteristics e.g. sharpness, of the region are analyzed. Acquisition parameters e.g. color balance, of a main acquired image are adjusted based on the analysis.
   USE - Method for acquiring a digital image using a digital image acquisition device (claimed) e.g. portable digital camera, hand-held computer or camera phone, for face detection and recognition.
   ADVANTAGE - The method avoids calculations of a complete highest resolution integral image for every acquired image in the image stream, thus reducing processing overhead for face detection and tracking or allowing longer classifier chains to be employed during a frame-to-frame processing interval to provide higher quality results, and providing enhanced real-time face tracking in the digital image acquisition device, and hence improving the performance and/or accuracy of real-time face detection and tracking.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a digital image acquisition device comprising a lens
   (2) a computer-readable storage devices comprising a set of instructions to perform a method of acquiring an image based on tracking a face in a preview image stream with a digital image acquisition device
   (3) a computer-readable storage devices comprising a set of instructions to perform a method of tracking faces in a preview image stream acquired with a digital image acquisition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an image processing apparatus.
   Face-tracking module (111)
   Image subsampler (112)
   Integral image generator (115)
   Camera memory (140)
   Image store (150)
PD US2009303342-A1   10 Dec 2009   H04N-005/228   201001   Pages: 24   English
   US7916897-B2   29 Mar 2011   G06K-009/00   201124      English
UT DIIDW:2009S25552
ER

PT P
PN CN101398886-A; CN101398886-B
TI Binocular passive three-dimensional vision based fast three-dimensional face recognition method, involves carrying out expression normalizations for different faces, and carrying out recognitions for normalized three-dimensional faces.
AB    NOVELTY - The method involves adopting two high definition digital cameras to create a non-contact short-axis parallel binocular three-dimensional visual system. A dual-wavelet phase related algorithm is adopted to quickly carry out pyramid searches for corresponding sub-pixel matches in small regions in three-dimensional pairs. Expression normalizations for different faces are carried out according to assumption that geodesic distances of surfaces of faces are constant. Recognitions for normalized three-dimensional faces are carried out by the algorithm.
   USE - Binocular passive three-dimensional vision based fast three-dimensional face recognition method.
   ADVANTAGE - The method based on the passive three-dimensional vision quickly and automatically obtains dense and accurate face three-dimensional point cloud information with different poses and expressions. The method makes the three-dimensional face recognition processes quicker, safer, concealed and reliable.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a binocular passive three-dimensional vision based fast three-dimensional face recognition method.'(Drawing includes non-English language text)'
PD CN101398886-A   01 Apr 2009   G06K-009/00   200930      Chinese
   CN101398886-B   10 Nov 2010   G06K-009/00   201121      Chinese
UT DIIDW:2009H66056
ER

PT P
PN FR2885071-A1; WO2006117692-A1; EP1877118-A1; IN200704206-P2; CN101247844-A; KR2008011408-A; JP2008539136-W; MX2007013315-A1; US2009159654-A1; FR2885071-B1; BR200611340-A2; CN101247844-B; JP4938000-B2; US8196807-B2; MX302849-B; KR1392574-B1; EP1877118-B1; EP3045191-A1; BR200611340-B1; IN314799-B; EP3045191-B1; EP3045191-B9; ES2861433-T3
TI Identifying medical/diagnostic container to e.g. receive drug, comprises sparing an outline in/on a substrate to constitute container, marking zone and identification code, and annealing the zone to reduce substrate internal tension.
AB    NOVELTY - Identifying medical/diagnostic container, comprises sparing an outline in/on a substrate to constitute container, marking zone and identification code, and annealing the zone to reduce substrate internal tension. The outline (2) is obtained by cutting, hot forming and deforming of a part. The zone (4) is illuminated by an electromagnetic radiation beam (7) to remove/modify the substrate constitutive of the wall, is spared on the internal face (3a) of the container wall, and is reheated with a flame.
   USE - To receive drug, vaccine and physiological fluid products, and freezing powder/liquid.
   ADVANTAGE - The container has good mechanical properties and impact resistance, and provides effective identification.
   DETAILED DESCRIPTION - Identifying medical/diagnostic container, comprises sparing an outline in/on a substrate to constitute container, marking zone and identification code, and annealing the zone to reduce substrate internal tension. The outline (2) is obtained by cutting, hot forming and deforming of a part. The zone (4) is illuminated by an electromagnetic radiation beam (7) to remove/modify the substrate constitutive of the wall, is spared on the internal face (3a) of the container wall, and is reheated with a flame. The code (5) is made up of discrete elements that are annealed individually/by a group and by a distinct heat source. The wall (3) forms the container of a syringe, bottle, cartridge, tube and a plate.
   INDEPENDENT CLAIMS are also included for:
   (1) a container; and
   (2) an injection device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a identification of a medical/diagnostic container:
   Outline; (2)
   Wall; (3)
   Internal face; (3a)
   Marking zone; (4)
   Identification code; (5)
   Electromagnetic radiation beam. (7)
PD FR2885071-A1   03 Nov 2006   B41M-005/24   200676   Pages: 25   French
   WO2006117692-A1   09 Nov 2006   A61M-005/31   200676      English
   EP1877118-A1   16 Jan 2008   A61M-005/31   200807      English
   IN200704206-P2   06 Jun 2008   A61M-005/31   200849      English
   CN101247844-A   20 Aug 2008   A61M-005/31   200861      Chinese
   KR2008011408-A   04 Feb 2008   A61J-001/00   200862      
   JP2008539136-W   13 Nov 2008   B65B-057/10   200877   Pages: 18   Japanese
   MX2007013315-A1   01 Feb 2008   A61M-005/31   200914      Spanish
   US2009159654-A1   25 Jun 2009   G06F-019/00   200942      English
   FR2885071-B1   12 Feb 2010   B41M-005/24   201030      French
   BR200611340-A2   31 Aug 2010   A61M-005/31   201061      
   CN101247844-B   15 Jun 2011   A61M-005/31   201171      Chinese
   JP4938000-B2   23 May 2012   B65B-057/10   201235   Pages: 15   Japanese
   US8196807-B2   12 Jun 2012   G06F-017/00   201238      English
   KR1392574-B1   09 May 2014   A61J-001/00   201434      
   EP1877118-B1   09 Mar 2016   A61M-005/31   201618      English
   EP3045191-A1   20 Jul 2016   A61M-005/31   201648      English
   BR200611340-B1   09 Apr 2019   A61M-005/31   201928      English
   IN314799-B   05 Jul 2019   A61M-005/31   201953      English
   EP3045191-B1   06 Jan 2021   A61M-005/31   202105      English
   EP3045191-B9   06 Oct 2021   A61M-005/31   202182      English
   ES2861433-T3   06 Oct 2021   A61M-005/31   202195      Spanish
UT DIIDW:2006732725
ER

PT P
PN EP1703442-A1; JP2006259921-A; US2006210258-A1; KR2006100249-A; CN1834988-A; EP1703442-B1; DE602006005528-E; CN100524340-C; US7705737-B2; JP4696608-B2; KR1060834-B1
TI Object identifying device for mobile phone, has controller for comparing input shooting start operation and previously registered normal operation based on which relevant object is identified.
AB    NOVELTY - A controller compares an input operation for starting the shooting operation received by an input unit with a previously registered normal operation to decide truth or falseness. An object is matched based on the shot image and a previously registered data and the relevant object is identified when both the truth or a falseness decision result and a matched result are true.
   USE - In face identifying device (claimed) for mobile phone (claimed).
   ADVANTAGE - The wrong recognition of object can be easily prevented thereby preventing illegal use of the mobile phone.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) object identifying method; and
   (2) computer-readable medium storing instructions for operating object identifying device.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of non-limiting object identifying device.
PD EP1703442-A1   20 Sep 2006   G06K-009/00   200675   Pages: 27   English
   JP2006259921-A   28 Sep 2006   G06T-007/00   200675   Pages: 19   Japanese
   US2006210258-A1   21 Sep 2006   G03B-013/00   200675      English
   KR2006100249-A   20 Sep 2006   G06K-009/00   200705      
   CN1834988-A   20 Sep 2006   G06K-009/00   200707      Chinese
   EP1703442-B1   11 Mar 2009   G06K-009/00   200919      English
   DE602006005528-E   23 Apr 2009   G06K-009/00   200929      German
   CN100524340-C   05 Aug 2009   G06K-009/00   201001      Chinese
   US7705737-B2   27 Apr 2010   G08B-023/00   201029      English
   JP4696608-B2   08 Jun 2011   G06T-007/00   201137   Pages: 20   Japanese
   KR1060834-B1   30 Aug 2011   G06K-009/00   201160      
UT DIIDW:2006719494
ER

PT P
PN WO2006093074-A1; US2008117323-A1; CN101133438-A; JP2007505911-X; CN101133438-B; JP4896002-B2; US8184132-B2
TI Electronic display medium e.g. electronic book, controls display state of screen which is displaying content, according to estimated physiology state of user.
AB    NOVELTY - An image acquisition unit photographs a user's face image. An analysis unit acquires user's physiology information from face image, analyzes the physiology information and estimates user's physiology state. A display control unit controls display state of screen which is displaying content, according to estimated physiology state.
   USE - For electronic book.
   ADVANTAGE - The readability of the medium and visibility can be improved according to user's physiology state.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the electronic book. (Drawing includes non-English language text)
PD WO2006093074-A1   08 Sep 2006   G09G-005/00   200670   Pages: 39   Japanese
   US2008117323-A1   22 May 2008   H04N-005/222   200835      English
   CN101133438-A   27 Feb 2008   G09G-005/00   200839      Chinese
   JP2007505911-X   07 Aug 2008   G09G-005/00   200854   Pages: 36   Japanese
   CN101133438-B   19 May 2010   G09G-005/00   201043      Chinese
   JP4896002-B2   14 Mar 2012   G09G-005/00   201219   Pages: 16   Japanese
   US8184132-B2   22 May 2012   H04N-007/16   201235      English
UT DIIDW:2006680324
ER

PT P
PN EP1615160-A2; JP2006024219-A; US2006008150-A1; KR2006003666-A; KR601957-B1; EP1615160-A3; US7715659-B2; JP4606955-B2
TI Image recognition system use for security purposes, has comparator that determines which reference image has greatest correlation to input image, based on comparisons between reference image sub-regions and input image sub-regions.
AB    NOVELTY - A comparator receives an image divided into image sub-regions. The comparator collates the image sub-regions with corresponding reference image sub-regions of the reference images. The comparator determines which of the reference images has a greatest correlation to the image, based on the comparisons between the reference image sub-regions and the image sub-regions.
   USE - Use for security purposes.
   ADVANTAGE - Lowers error ratio in face recognition under different expressions, illuminations and occlusions.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a method of determining a correspondence between an obtained image and a set of reference images;
   (B) a method of removing an influence of illumination or occlusions of an obtained image; and
   (C) a computer program.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart of the method for performing feature extraction.
PD EP1615160-A2   11 Jan 2006   G06K-009/46   200610   Pages: 39   English
   JP2006024219-A   26 Jan 2006   G06T-007/00   200610   Pages: 34   Japanese
   US2006008150-A1   12 Jan 2006   G06K-009/46   200610      English
   KR2006003666-A   11 Jan 2006   G06K-009/00   200659      
   KR601957-B1   14 Jul 2006   G06K-009/00   200728      
   EP1615160-A3   07 Oct 2009   G06K-009/46   200966      English
   US7715659-B2   11 May 2010   G06K-009/54   201031      English
   JP4606955-B2   05 Jan 2011   G06T-007/00   201105   Pages: 33   Japanese
UT DIIDW:2006092516
ER

PT P
PN WO2005008567-A1; KR2005009959-A; KR587422-B1
TI Omnidirectional iris recognition method for entrance and exit, involves controlling iris image camera based on eye position determined from face image captured by wide field of view camera.
AB    NOVELTY - Face image of a person is acquired using a wide field of view camera (112), when the movement of a person is detected using the wide field of view camera. Zoom and focus of an iris image camera (122) in a narrow field of view device (120), are controlled based on the eye position determined from the face image for acquiring iris images.
   USE - For entrance and exit in public institutions such as airports, harbors, city halls and police stations.
   ADVANTAGE - Eliminates need for user to be positioned in specific range in front of iris input device, to acquire iris images by controlling iris image camera to obtain iris image according to eye position detected from face image acquired by wide field of view camera. Monitors movements of various users in real-time in omnidirectional view using wide field of view camera and acquires iris images of person whose irises are to be recognized using iris image camera having narrow field of view.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for iris recognition apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the omnidirectional iris recognition apparatus.
PD WO2005008567-A1   27 Jan 2005   G06K-009/00   200513   Pages: 38   English
   KR2005009959-A   26 Jan 2005   G06K-009/00   200535      
   KR587422-B1   09 Jun 2006   G06K-009/00   200708      
UT DIIDW:2005122950
ER

PT P
PN EP1411459-A1; JP2004139596-A; KR2004034342-A; US2004170305-A1; CN1504961-A; JP4061260-B2; CN100342399-C; US7391889-B2; EP1411459-B1; DE60327589-E; KR486738-B1
TI Feature vector extraction method for face recognition, involves generating Fourier and intensity feature vectors to obtain composite feature vector.
AB    NOVELTY - An entire Fourier feature vector for entire face area and central Fourier feature vector for central face area of normalized face image are generated. Entire intensity feature vector for entire face area and local intensity feature vector for specific number of face component areas, are generated. The generated Fourier and intensity feature vectors are combined to obtain composite feature vector.
   USE - For extracting feature vectors for face recognition and retrieval.
   ADVANTAGE - Image recognition and retrieval precision are increased, by separately generating Fourier and intensity feature vectors and then combining them into single composite feature vector.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for feature vector extraction apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the feature vector extracting apparatus.
PD EP1411459-A1   21 Apr 2004   G06K-009/00   200430   Pages: 27   English
   JP2004139596-A   13 May 2004   G06T-007/00   200432   Pages: 25   Japanese
   KR2004034342-A   28 Apr 2004   G06K-009/00   200455      
   US2004170305-A1   02 Sep 2004   G06K-009/00   200458      English
   CN1504961-A   16 Jun 2004   G06T-005/00   200465      Chinese
   JP4061260-B2   12 Mar 2008   G06T-007/00   200821   Pages: 26   Japanese
   CN100342399-C   10 Oct 2007   G06T-007/00   200835      Chinese
   US7391889-B2   24 Jun 2008   G06K-009/00   200844      English
   EP1411459-B1   13 May 2009   G06K-009/00   200932      English
   DE60327589-E   25 Jun 2009   G06K-009/00   200942      German
   KR486738-B1   03 May 2005   G06K-009/00   200657      
UT DIIDW:2004319191
ER

PT P
PN CN107748858-A
TI Concatenated convolution neural network based multi-posture eye locating method, involves performing pyramid conversion process for model input image, and multi-posture eye location prediction process by non-maximum suppression algorithm.
AB    NOVELTY - The method involves establishing a multi-task concatenated convolution neural network model. Collected human face images are pre-processed. A data set is formed corresponding to marking data and training data. A multi-task convolution neural network is constructed. The data set is input to the multi-task convolution neural network. Fast data set training process is performed to obtain a network model. Pyramid conversion process is performed for a model input image. Multi-posture eye location prediction process is performed by using a non-maximum suppression algorithm.
   USE - Concatenated convolution neural network based multi-posture eye locating method.
   ADVANTAGE - The method enables performing human face detection and human face key point detection by using the multi-task convolution neural network, so that multi-posture eye locating effect is improved.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a concatenated convolution neural network based multi-posture eye locating method. '(Drawing includes non-English language text)'
PD CN107748858-A   02 Mar 2018   G06K-009/00   201819   Pages: 10   Chinese
UT DIIDW:201819168C
ER

PT P
PN CN107644204-A; CN107644204-B
TI Method for identifying and tracking human body in security system, involves comparing ID of human body target detected in each frame with position of human body target having same ID in different frames.
AB    NOVELTY - The method involves performing (S104) feature extraction on the detected face area to obtain a face feature vector. The face feature data is matched (S105) with the face features in the database. An ID is assigned to the detected human target corresponding to the human face. A frame containing a human body target is detected and tracked (S106), when no face detection or face detection is performed on the frame or frames. The ID of the human body target detected in each frame is compared (S107) with the position of the human body target having the same ID in different frames.
   USE - Method for identifying and tracking human body in security system.
   ADVANTAGE - The method realizes the real-time face detection and recognition, and is not easy to lose the goal or track the failure.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the process of identifying and tracking human body in security system. (Drawing includes non-English language text)
   Step for performing face detection in a frame containing a human body (S103)
   Step for performing feature extraction on detected face area (S104)
   Step for matching face feature data with the face features in the database (S105)
   Step for detecting and tracking frame containing a human body target, when no face detection or face detection is performed on the frame or frames (S106)
   Step for comparing ID of human body target with position of human body target having same ID (S107)
PD CN107644204-A   30 Jan 2018   G06K-009/00   201812   Pages: 10   Chinese
   CN107644204-B   10 Nov 2020   G06K-009/00   202094      Chinese
UT DIIDW:201811300C
ER

PT P
PN CN107609383-A; WO2019080580-A1; CN107609383-B
TI Three dimensional face identity authentication method, involves obtaining depth image, and comparing feature information in target face two-dimensional image with feature information in reference face two-dimensional image.
AB    NOVELTY - The method involves obtaining a depth image. The depth image and a reference face three dimensional (3D) texture image registrations is performed for obtaining posture information of the target face. A target face two-dimensional image is obtained by dividing the two-dimensional image aligned by posture information. Feature information of the target face in the two- dimensional image is extracted. Feature information in the target face two-dimensional image is compared with feature information in a reference face two-dimensional image.
   USE - 3D face identity authentication method.
   ADVANTAGE - The method enables improving an identification precision, updating data to improve a user examination and reducing a false accept rate and a coping face change.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a 3D face identity authentication device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a 3D face identity authentication method. '(Drawing includes non-English language text)'
PD CN107609383-A   19 Jan 2018   G06F-021/32   201811   Pages: 19   Chinese
   WO2019080580-A1   02 May 2019   G06F-021/32   201932      Chinese
   CN107609383-B   26 Jan 2021   G06F-021/32   202112      Chinese
UT DIIDW:201808447F
ER

PT P
PN CN107247949-A; CN107247949-B
TI Method for identifying human face in electronic device based on deep learning, involves removing last full connection layer by network model after training, and utilizing forward propagation as face characteristic data for face recognition.
AB    NOVELTY - The method involves adopting an excitation function layer to select a ReLU function and an NReLU function as an excitation function. Adjacent convolution groups are connected by a short circuit layer of a residual network. A convolutional neural network model is trained. A training data is inputted to the convolutional neural network model. The convolutional neural network model is trained by utilizing a stochastic gradient descent process. A last full connection layer is removed after training. A forward propagation is utilized as a face characteristic data for face recognition.
   USE - Method for identifying human face in an electronic device based on deep learning (claimed).
   ADVANTAGE - The method enables utilizing the ReLU + NReLU as the excitation function, reducing operation quantity and model size, ensuring precision and improving operation speed.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for identifying human face in an electronic device based on deep learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for identifying human face in an electronic device based on deep learning. '(Drawing includes non-English language text)'
PD CN107247949-A   13 Oct 2017   G06K-009/00   201776   Pages: 12   Chinese
   CN107247949-B   19 Jun 2020   G06K-009/00   202054      Chinese
UT DIIDW:201771852F
ER

PT P
PN CN106790054-A; WO2018113526-A1
TI Face recognition and fingerprint identification based interactive authentication system, has terminal connected to server through network, where server receives voice audio data of user and terminal receives verification success information.
AB    NOVELTY - The system has a terminal connected to a server through a network. The terminal is utilized for obtaining a detected face video and collecting voice audio data of a user. The server receives the voice audio data of the user. A display displays prompt information through the server. The server realizes user facial feature parameters and user vocal print characteristic vector matching process and obtains voiceprint recognition result and face identification result. The terminal is utilized for receiving verification success information.
   USE - Face recognition and fingerprint identification based interactive authentication system.
   ADVANTAGE - The system ensures safety.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face recognition and fingerprint identification based interactive authentication method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a face recognition and fingerprint identification based interactive authentication system. '(Drawing includes non-English language text)'
PD CN106790054-A   31 May 2017   H04L-029/06   201748   Pages: 18   Chinese
   WO2018113526-A1   28 Jun 2018   H04L-029/06   201843      Chinese
UT DIIDW:201738348B
ER

PT P
PN CN105760836-A
TI Method for aligning multi-angle human face by using shoot terminal based on depth study, involves determining preset human face rotating angle corresponding to human face gesture type, and obtaining coordinate position of face key point.
AB    NOVELTY - The method involves collecting a human face sample image according to human face rotation angle. Human face key point is obtained form the human face sample image. A human face rotating angle input convolution nerve network training process is performed corresponding to the human face sample image. Preset human face rotating angle is determined corresponding to a human face gesture type. Human face key point is obtained from a detecting image by using human face angle mode. Coordinate position of the human face key point is obtained.
   USE - Method for aligning a multi-angle human face by using a shoot terminal (claimed) based on a depth study.
   ADVANTAGE - The method enables reducing model occupied small space and user complexity and improving alignment precision.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for aligning a multi-angle human face by using a shoot terminal based on a depth study.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for aligning a multi-angle human face by using a shoot terminal based on a depth study. '(Drawing includes non-English language text)'
PD CN105760836-A   13 Jul 2016   G06K-009/00   201651   Pages: 11   Chinese
UT DIIDW:201645049X
ER

PT P
PN CN104866810-A; CN104866810-B
TI Convolution nerve network human face identification method, involves performing transmission process by network, arranging residual layer in gradient direction, and calculating vector value by test sample.
AB    NOVELTY - The method involves connecting a random input unit with a hidden unit for obtaining a bias number. An input training image is obtained. Forward conducting process is performed according to a mark value. Layer deviation operation is performed in a gradient direction. Transmission process is performed out by a network. A residual layer is arranged in the gradient direction. A vector value is calculated by a test sample. Distance between multiple sample classes is determined according to a mean value. A sample average value is calculated.
   USE - Convolution nerve network human face identification method.
   ADVANTAGE - The method enables reducing system complexity.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a convolution nerve network human face identification system. '(Drawing includes non-English language text)'
PD CN104866810-A   26 Aug 2015   G06K-009/00   201571   Pages: 13   Chinese
   CN104866810-B   13 Jul 2018   G06K-009/00   201848      Chinese
UT DIIDW:201561895E
ER

PT P
PN US9104908-B1
TI Computer implemented method for generating non-transient record embodying measures of adaptive set of feature locations characterizing face of person, involves storing locations of features to non-transient record.
AB    NOVELTY - The method involves receiving a video sequence that constitutes a physical record of a face of a specified individual person. A feature locator update model is applied to the video sequence. A regularized linear regression is characterized by regularization that includes a spatial smoothness term. Multiple locations are extracted corresponding to the adaptive set of feature locations. The locations of features of the face of the specified individual person are characterized, based on the feature locator update model. The locations of features are stored to a non-transient record.
   USE - Computer implemented method for generating non-transient record embodying measures of adaptive set of feature locations characterizing face of person for use in applications such as performance driven facial animation, behavior analysis, photographic and video manipulation, facial recognition, gaze analysis, product design analytics and attention measurement.
   ADVANTAGE - The use of the calibration information results in increased robustness and improved accuracy of the feature location and expression measurements in a subsequent unconstrained facial performance. The improvement in accuracy is achieved without introducing extra computational cost when analyzing frames after the calibration process.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a non-transitory computer readable medium storing program for generating a non-transient record embodying measures of an adaptive set of feature locations characterizing a face of a person.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic diagram of the system where on-stage pre-visualization of performance driven character animation is created and displayed.
   Subject (102)
   Analysis system (205)
   Animate avatar module (1500)
   Device (1800)
PD US9104908-B1   11 Aug 2015   G06K-009/00   201554   Pages: 34   English
UT DIIDW:201545834G
ER

PT P
PN CN104239851-A; CN104239851-B
TI Behavior analysis based intelligent cell inspection system, has server connected with mobile inspection terminal that is arranged with intelligent mobile phone, and where server is connected with video monitor to perform video image process.
AB    NOVELTY - The system has a server connected by a mobile inspection terminal. An intelligent access control system is provided with a camera. The mobile inspection terminal obtains a suspicious character of human face image information. A camera head obtains a video message or image information. The intelligent entrance guard system receives double certification of the human face image information. The server is connected with a video monitor to perform video image process. The mobile inspection terminal is arranged with an intelligent mobile phone.
   USE - Behavior analysis based intelligent cell inspection system.
   ADVANTAGE - The system increases safe precaution ability through an entrance guard system of double authentication and burst processing efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a behavior analysis based intelligent cell inspecting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a behavior analysis based intelligent cell inspection system.'(Drawing includes non-English language text)'
PD CN104239851-A   24 Dec 2014   G06K-009/00   201513   Pages: 9   Chinese
   CN104239851-B   01 May 2018   G06K-009/00   201831      Chinese
UT DIIDW:2015117607
ER

PT P
PN CN104200804-A; CN104200804-B
TI Multi-class information deep learning based human-computer interaction emotion recognition method, involves extracting voice data characteristics, and obtaining emotion recognition class message by emotion recognition model.
AB    NOVELTY - The method involves providing a camera device with a microphone. Human face expression video data and voice data are received. Voice identification process is performed by a voice identification tool corresponding to the voice data. Text content characteristic extracting process is performed. Text information characteristic is analyzed. Emotion dictionary word found result is obtained. Voice data characteristics are extracted. A voice data process terminal is selected. An emotion recognition model is obtained. Emotion recognition class message is obtained by the emotion recognition model.
   USE - Multi-class information deep learning based human-computer interaction emotion recognition method.
   ADVANTAGE - The method enables realizing human-computer interaction emotion recognition operation and increasing emotional state human-computer interaction accuracy level.
PD CN104200804-A   10 Dec 2014   G10L-015/02   201507   Pages: 15   Chinese
   CN104200804-B   17 May 2017   G10L-015/02   201735      Chinese
UT DIIDW:2015087835
ER

PT P
PN WO2014180093-A1; US2014341422-A1; CN104143079-A; CN104143079-B; US9679195-B2; US2017249502-A1; US10438052-B2
TI Method for facial property identification, involves acquiring image sample, and classifying textural features of effective area image by race, gender and age to obtain race property, gender property and age property of face.
AB    NOVELTY - The method (100) involves acquiring (101) an image sample. An effective area image of a face in the image sample is acquired (102). The textural features of the effective area image are extracted (103). The textural features of the effective area image are classified (104) by race, gender and age using a race classifier, a gender classifier and an age classifier successively to obtain a race property, a gender property and an age property of the face. The race classifier, the gender classifiers and the age classifier are successively established.
   USE - Method for facial property identification.
   ADVANTAGE - The textural features of the effective area image are classified by race, gender and age using a race classifier, a gender classifier and an age classifier successively to obtain a race property, a gender property and an age property of the face which ensures that the efficiency and accuracy of facial property identification is improved effectively and the cost of facial property identification is reduced.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a system for facial property identification; and
   (2) a non-transitory computer readable storage medium comprising programming instructions for facial property identification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method for facial property identification.
   Facial property identification method (100)
   Acquiring an image sample (101)
   Acquiring effective area image of a face in the image sample (102)
   Extracting textural features of the effective area image (103)
   Classifying textural features of the effective area image by race, gender and age using a race classifier, a gender classifier and an age classifier successively to obtain a race property, a gender property and an age property of the face (104)
PD WO2014180093-A1   13 Nov 2014   G06K-009/66   201475   Pages: 35   English
   US2014341422-A1   20 Nov 2014   G06K-009/00   201476      English
   CN104143079-A   12 Nov 2014   G06K-009/00   201505      Chinese
   CN104143079-B   17 Aug 2016   G06K-009/00   201660      Chinese
   US9679195-B2   13 Jun 2017   G06K-009/00   201740      English
   US2017249502-A1   31 Aug 2017   G06K-009/00   201758      English
   US10438052-B2   08 Oct 2019   G06K-009/00   201977      English
UT DIIDW:2014U41186
ER

PT P
PN CN102760077-A
TI Self-adaptive face recognition method, involves comparing shot user face image to human face identification to estimate user age, and self-adaptively estimating user age matched with scene mode based on contextual model configuration file.
AB    NOVELTY - The method involves shooting a user's face image, and comparing the shot user face image to a human face identification to estimate user age. The user age matched with a scene mode is self-adaptively estimated based on a contextual model configuration file, where the contextual model configuration file comprises parameters such as interface layout, link number, link relation, brightness, contrast, subject, font, word size, resolution, type, size and touch tone.
   USE - Method for self-adaptive face recognition based on application profiles.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a Portable electronic device for self-adaptive face recognition based on application profiles.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a self-adaptive face recognition method.'(Drawing includes non-English language text)'
PD CN102760077-A   31 Oct 2012   G06F-009/48   201314   Pages: 11   Chinese
UT DIIDW:2013B36226
ER

PT P
PN CN102663413-A; CN102663413-B
TI Method for authenticating human face image and attitude based on age, involves utilizing linear combination component to analyze sub-block according to weighting summation output human face image authentication result.
AB    NOVELTY - The method involves detecting an original face image from two given images of a human face image. The detected face image is utilized to initialize a key point of the human face image. The key point is utilized to position the detected image according to a local gray scale of the key point. A process of a linear combination component is utilized to generate a new human face shape vector to replace an original human face shape vector. The linear combination component is utilized to analyze a sub-block according to a weighting summation output human face image authentication result.
   USE - Method for authenticating human face image and attitude based on age.
   ADVANTAGE - The method enables providing robustness to a human face and improving accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating method for authenticating human face image and attitude based on age.'(Drawing includes non-English language text)'
PD CN102663413-A   12 Sep 2012   G06K-009/62   201310   Pages: 18   Chinese
   CN102663413-B   27 Nov 2013   G06K-009/62   201406      Chinese
UT DIIDW:2012P32415
ER

PT P
PN CN102306290-A; CN102306290-B
TI Method for tracing and identifying human face, involves counting histogram of picture for obtaining proper value.
AB    NOVELTY - The method involves detecting frame and face position information from a decoded video. The human face is traced using the face tracking algorithm. A histogram of the picture is counted to get a proper value, and the statistics attribute proper value is received as input into the human face recognizer to match the human face.
   USE - Method for tracing and identifying human face.
   ADVANTAGE - The systematic exactness of the sequence classification and the number of times the feature extraction contrasts with the human face are reduced.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for finger print method for tracking human face.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram explaining the method for tracing and identifying human face. (Drawing includes non-English language text)
PD CN102306290-A   04 Jan 2012   G06K-009/00   201212   Pages: 17   Chinese
   CN102306290-B   30 Oct 2013   G06K-009/00   201403      Chinese
UT DIIDW:2012B01919
ER

PT P
PN US7916976-B1
TI Images e.g. digital photographs, organizing method, involves providing display interface of images of identified individual within album including image and excluding another image.
AB    NOVELTY - The method involves receiving a first image (1101) of an individual (1107), and searching for a second image and a third image of the identified individual with a portrait matching module of a computer system. The first image, a link to a webpage for an album (1105) containing the second image of the individual, and another link to another webpage for all images of the individual are displayed in a storage system, where the former webpage provides a display interface of the images of the identified individual within the album including the second image and excluding the third image.
   USE - Method for organizing images such as digital photographs using facial recognition in a computer system. Uses include but are not limited to personal computer, workstation, console device, laptop, handheld device and cellular phone.
   ADVANTAGE - The method enables detection of individuals in each image uploaded into the system using facial recognition or similar methods. The method allows users and viewers of the images to view dynamic albums based on the interrelationships of individuals in images. The method allows the users and viewers to browse all images with the individual or view albums of images with two selected individuals or similar combinations based on the relationships between users.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an apparatus for organizing images comprising a receiving unit
   (2) a non-transitory computer readable medium having a set of instructions that cause a computer to perform a set of operations for organizing images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a screen shot of an album viewing interface.
   Images (1101)
   Rating meter (1103)
   Album (1105)
   Individual (1107)
   Character (1109)
PD US7916976-B1   29 Mar 2011   G06K-009/00   201126   Pages: 24   English
UT DIIDW:2011D31365
ER

PT P
PN WO2010119854-A1; US2012006904-A1; JP2011509291-X; US8418928-B2; US2013200163-A1; US8690070-B2; US2014117099-A1; JP5510450-B2; JP2014147097-A; US8876010-B2; JP5725225-B2; JP2015149753-A; JP5896065-B2; JP2016105647-A; JP6137362-B2
TI Square C-shaped joint electrode for wireless integrated circuit (IC) device in radio frequency identification system, has edges capacitively coupled to each other and bonding portions that are bonded together through wireless IC chip.
AB    NOVELTY - The joint electrode (125) stuck to thin metal plate (115) on top surface of base material (110), has a pair of edges (126c,126d) that are capacitively coupled to each other facing the metal plate. A pair of bonding portions (126a,126b) are bonded together through wireless IC chip (5).
   USE - Square C-shaped joint electrode for wireless integrated circuit (IC) device used in radio frequency identification (RFID) system.
   ADVANTAGE - The electrical length of the joint electrode is increased and favorable impedance matching between wireless IC chip and metal plate is ensured by capacitive coupling of edges of joint electrode. Hence the joint electrode can be favorably combined with the thin metal plate on the base material.
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective view of the wireless IC device with joint electrode.
   Wireless IC chip (5)
   Base material (110)
   Thin metal plate (115)
   Joint electrode (125)
   Coupling portions (126a,126b)
   Bonding portions (126c,126d)
PD WO2010119854-A1   21 Oct 2010   H01Q-001/38   201071   Pages: 41   Japanese
   US2012006904-A1   12 Jan 2012   G06K-019/077   201205      English
   JP2011509291-X   22 Oct 2012   H01Q-007/00   201270   Pages: 26   Japanese
   US8418928-B2   16 Apr 2013   G06K-009/00   201326      English
   US2013200163-A1   08 Aug 2013   G06K-019/077   201352      English
   US8690070-B2   08 Apr 2014   G06K-019/06   201425      English
   US2014117099-A1   01 May 2014   G06K-019/077   201430      English
   JP5510450-B2   04 Jun 2014   H01Q-007/00   201436   Pages: 17   Japanese
   JP2014147097-A   14 Aug 2014   H01Q-001/38   201453   Pages: 16   Japanese
   US8876010-B2   04 Nov 2014   G06K-019/06   201472      English
   JP5725225-B2   27 May 2015   H01Q-001/38   201535   Pages: 17   Japanese
   JP2015149753-A   20 Aug 2015   H01Q-007/00   201555   Pages: 18   Japanese
   JP5896065-B2   30 Mar 2016   H01Q-009/26   201624   Pages: 17   English
   JP2016105647-A   09 Jun 2016   H01Q-009/28   201639   Pages: 17   Japanese
   JP6137362-B2   31 May 2017   H01Q-001/24   201738   Pages: 17   Japanese
UT DIIDW:2010N20107
ER

PT P
PN CN101777116-A; CN101777116-B
TI Face expression analyzing method, involves using expression parameters of multiple human face features obtained by tracking as expression analysis features, and analyzing expressions by improved fuzzy clustering algorithm.
AB    NOVELTY - The method involves pre-processing an input video image, and detecting a human face and locating human face key points so as to determine human face position. A human face and expression motions are modeled by a three-dimensional parameterization human face grid model. Expression parameters of multiple human face features obtained by tracking as expression analysis features are used. The expressions are analyzed by an improved fuzzy clustering algorithm based on Gaussian distance measurement so as to figure out an expression fuzzy description.
   USE - Method analyzing face expression.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a method analyzing face expression.'(Drawing includes non-English language text)'
PD CN101777116-A   14 Jul 2010   G06K-009/00   201062   Pages: 15   Chinese
   CN101777116-B   25 Jul 2012   G06K-009/00   201275      Chinese
UT DIIDW:2010M34785
ER

PT P
PN CN101256700-A
TI Automatic teller machine, has client end system for accepting user code and transmitting information to service end program for identification, and management end system collecting voice data and face data of user.
AB    NOVELTY - The machine has a client end system for accepting a user code and transmitting information to a service end program for identification, and a data base system for storing various information relevant to the user. An identification level setting module is used for code identification, code and voice identification, code and face identification and code, voice and face identification according to the amount withdrawn by the user. A management end system collects voice data and face data of the user, when the user applies for opening a bank card.
   USE - Automatic teller machine.
   ADVANTAGE - The machine has mixed user identity authentication, thus ensures better safety and reliability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an automatic teller machine.'(Drawing includes non-English language text)'
PD CN101256700-A   03 Sep 2008   G07F-019/00   200868   Pages: 15   Chinese
UT DIIDW:2008L55556
ER

PT P
PN US2006285755-A1; WO2006138525-A2; WO2006138525-A3; EP1897033-A2; JP2008547094-W; US7929775-B2; JP4691158-B2; EP1897033-A4
TI Class instances recognition method involves matching class instances in two dimensional image to three dimensional class models, by identifying image features in two dimensional image.
AB    NOVELTY - The method involves constructing the database of three dimensional (3D) class models. The class instances in the two dimensional (2D) image are matched to the 3D class models, by identifying image features in the 2D image and by comparing the identified image features with classes belonging to the 3D class models.
   USE - For recognizing class instances in two dimensional (2D) image using 3D class models, and also for specialized programmable processors such as specialized hardware including digital signal processors (DSPs), graphic processors (GPUs), cell processors, media processors and streaming processors, and for electronic hardware, integrated circuit (IC) technologies including field programmable gate arrays (FPGAs), gate arrays, standard cell and full custom, and for face recognition, inspection, assembly, and logistics.
   ADVANTAGE - The changes in the position of the object parallel to the imaging plane of a camera causes changes only in the position of a feature in the image and does not affect the appearance of the image. The problems involved in the distance, slant, and tilt of the image are solved by using the registered range and intensity data to create 3D class models. The element geometry is standardized by scaling operation or other geometric normalization to produce canonical geometry.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) class instances recognition system;
   (2) three dimensional (3D) class models database; and
   (3) computer readable medium storing class instances recognition program.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart explaining the operations involved in the class instances recognition process.
PD US2006285755-A1   21 Dec 2006   G06K-009/62   200717   Pages: 33   English
   WO2006138525-A2   28 Dec 2006   G06K-009/62   200717      English
   WO2006138525-A3   21 Dec 2007   G06K-009/62   200802      English
   EP1897033-A2   12 Mar 2008   G06K-009/62   200820      English
   JP2008547094-W   25 Dec 2008   G06T-007/00   200903   Pages: 55   Japanese
   US7929775-B2   19 Apr 2011   G06K-009/62   201128      English
   JP4691158-B2   01 Jun 2011   G06T-007/00   201136   Pages: 58   Japanese
   EP1897033-A4   24 Jun 2015   G06K-009/62   201767      English
UT DIIDW:2007172980
ER

PT P
PN CN1794266-A; CN100356388-C
TI Biocharacteristics fusioned identity distinguishing and identification method.
AB    NOVELTY - This invention relates to a sorter integration and mode identification field characterizing in obtaining characters of man-face, iris, on-line signature and off-line handwriting of a user by various collecting devices then sending them to corresponding certification sub-modules to pick up the characters and match with the molding boards and outputting the matched marks to be normalized and sent to an identification combined module to get the final identification result by confidence integration or sent to a certification module to be imaged to a multi-dimension space to be sorted by a sorter to get a final result or to be certified and combined to get the final identification result.
PD CN1794266-A   28 Jun 2006   G06K-009/00   200717      Chinese
   CN100356388-C   19 Dec 2007   G06K-009/00   200832      Chinese
UT DIIDW:2007159993
ER

PT P
PN US2002106114-A1; US6975750-B2
TI Computerized face recognition method for security application, involves synthesizing various face angels of recognized person using specific 3-D model, to obtain training images.
AB    NOVELTY - A particular face pose and a generic 3-D face model are input and a corresponding specific 3-D model is obtained by deforming the generic face model. Various face angles are synthesized using specific 3-D model and used as training images to train the recognizing person.
   USE - For security applications using personal computer, server computer, laptop computer, multiprocessor system, microprocessor based system, set-top box, programmable consumer electronic, network PC, mini computer, mainframe computer, etc.
   ADVANTAGE - Allows face recognition even in the absence of significant amount of data and even in the absence of actual training images. Small amount of actual training data minimizes the cost and effort required to obtain the recognition data.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) Synthesized face image generation system; and
   (2) Computer readable medium storing face recognition program.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the face recognition system.
PD US2002106114-A1   08 Aug 2002   G06K-009/00   200302   Pages: 16   English
   US6975750-B2   13 Dec 2005   G06K-009/00   200581      English
UT DIIDW:2003028488
ER

PT P
PN CN106981503-A; IN201724037087-A; US2018129852-A1; DE102017125292-A1; US10185861-B2; CN106981503-B; IN392623-B
TI Display panel of electronic device e.g. mobile phone, has red organic light emitting structure and/or green organic light emitting structure which emits light as light source of fingerprint recognition unit.
AB    NOVELTY - The panel has a red organic light emitting structure (31) and/or a green organic light emitting structure (32) emits light as a light source of a fingerprint recognition unit. The red organic light emitting structure and/or the green organic light emitting structure provided as a light source of the fingerprint recognition unit are smaller than the light emitting area of the green organic light emitting structure toward the display side facing away from the display panel and the light transmission area on the display side.
   USE - Display panel of electronic device such as mobile phone, tablet computer, wearable device.
   ADVANTAGE - The accuracy of the fingerprint identification is realized.
   DESCRIPTION OF DRAWING(S) - The drawing shows a cross sectional view of the display panel of electronic device.
   Organic light-emitting structure (30)
   Red organic light emitting structure (31)
   Green organic light emitting structure (32)
   Blue organic light-emitting structure (33)
   Pixel driving circuit (34)
PD CN106981503-A   25 Jul 2017   H01L-027/32   201762   Pages: 57   Chinese
   IN201724037087-A   10 Nov 2017   H01J-017/49   201777      English
   US2018129852-A1   10 May 2018   G06K-009/00   201831      English
   DE102017125292-A1   31 Oct 2018   G06K-009/28   201874      German
   US10185861-B2   22 Jan 2019   G06F-003/041   201907      English
   CN106981503-B   15 Nov 2019   H01L-027/32   201990      Chinese
   IN392623-B   25 Mar 2022   H01J-017/49   202231      English
UT DIIDW:201752793M
ER

PT P
PN CN106570496-A; CN106570496-B
TI Face emotion recognition method, involves obtaining voice emotion recognition result according to voice message, obtaining facial emotion identification result, and judging whether emotional state of user is same mood classification.
AB    NOVELTY - The method involves obtaining a voice emotion recognition result according to a voice message of a user, where the voice emotion recognition result comprises pre-set emotion classifications. A facial emotion identification result is obtained based on a user face image, where the facial emotion identification result comprises preset emotion classifications. An emotion type of the obtained voice emotion recognition result is same as the emotion type the obtained facial emotion identification result. Judgment is made to check whether an emotional state of the user is a same mood classification.
   USE - Face emotion recognition method.
   ADVANTAGE - The method enables differentiating a response service according to the emotional state of the user.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an intelligent interactive method
   (2) a emotion recognition device
   (3) a intelligent interactive device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a face emotion recognition method. '(Drawing includes non-English language text)'
PD CN106570496-A   19 Apr 2017   G06K-009/00   201729   Pages: 19   Chinese
   CN106570496-B   01 Oct 2019   G06K-009/00   201979      Chinese
UT DIIDW:2017268807
ER

PT P
PN EP3065085-A1; US2016259994-A1; CA2921672-A1; AU2016201292-A1; CN105938559-A; IN201644004851-A; US9524450-B2; AU2016201292-B2; CA2921672-C; CN105938559-B; EP3065085-B1
TI Method for processing e.g. digital still image to classify particular features or objects in digital image, involves classifying extent of damage for object in each image in validation set by aggregating predictions from ensemble of network.
AB    NOVELTY - The method (500) involves determining whether an intermediate convolutional neural network (CNN) meets a validation threshold (535). Iterative process is repeated until a predetermined number of intermediate CNNs meet the validation threshold, where each intermediate CNN includes different values for selected candidate parameters. An ensemble of intermediate CNNs is created (550) from the predetermined number of intermediate CNNs. Extent of damage for an object in each image in a validation set is classified (555) by aggregating predictions from the ensemble of intermediate CNNs.
   USE - Method for processing a digital image such as digital still image and digital video, to ascertain, detect, and/or classify particular features or objects in the digital image in various applications e.g. facial recognition, detection of land features from aerial photographs and vehicle number plate determination applications. Uses include but are not limited to a house, furniture, clothing, vehicle equipment, land, toy and a computing device e.g. smartphone, computing tablet, laptop computer and desktop computer.
   ADVANTAGE - The method enables providing an advanced deep learning architecture to exhibit superior classification accuracy to assess property damage and an iterative image processing system to determine that advanced deep learning architecture. The method enables iteratively selecting candidate architectures and candidate parameters so as to optimize CNN's ability to accurately classify the extent of damage for the object in the image after the training set is created.
   DETAILED DESCRIPTION - The candidate parameters include learning parameters selected from one of learning rate between 0.05 and 0.1, batch size between 2 and 128 images, and maximum number of training epochs between 100 and 200, convolution and sub-sampling parameters selected from one of convolutional filter size, number of feature maps, and sub-sampling pool size, classifier parameters selected from one of image input size, number of hidden layers, number of units in each hidden layer, classifier algorithm i.e. multilayer perceptron (MLP) algorithm, and number of output classes. The predetermined number of intermediate CNNs is 25. INDEPENDENT CLAIMS are also included for the following:
   (1) an image processing server
   (2) a non-transitory computer readable medium comprising a set of instructions for processing digital images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for classifying objects in a digital image using CNNs.
   Method for processing image of object in image capture device (500)
   Step for creating training set from images of damaged objects (505)
   Step for determining whether intermediate CNN meets validation threshold (535)
   Step for creating ensemble of intermediate CNNs from predetermined number of intermediate CNNs (550)
   Step for classifying extent of damage for object in image in validation set (555)
PD EP3065085-A1   07 Sep 2016   G06K-009/46   201660   Pages: 19   English
   US2016259994-A1   08 Sep 2016   G06K-009/62   201660      English
   CA2921672-A1   04 Sep 2016   G06N-003/02   201661      English
   AU2016201292-A1   22 Sep 2016   G06N-003/08   201664      English
   CN105938559-A   14 Sep 2016   G06K-009/62   201665      Chinese
   IN201644004851-A   07 Oct 2016   H03M-013/00   201669      English
   US9524450-B2   20 Dec 2016   G06K-009/00   201701      English
   CN105938559-B   06 Sep 2019   G06K-009/62   201970      Chinese
   EP3065085-B1   01 Apr 2020   G06K-009/46   202029      English
UT DIIDW:2016550920
ER

PT P
PN CN105913037-A
TI Radio frequency identification based human face monitoring and tracking system, has personnel characteristic message data base and mobile personnel information data base that are connected with monitoring server.
AB    NOVELTY - The system has a RFID reader connected with a video image collecting device for obtaining geographic location data. A mobile personnel information data base is connected with a mobile information terminal. A target monitor is connected with the video image collecting device and the RFID reader. A personnel characteristic message data base and the mobile personnel information data base are connected with a monitoring server to monitor output track search result. The personnel mobile information terminal is provided with a search condition query module and a query processing module.
   USE - Radio frequency identification based human face monitoring and tracking system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a radio frequency identification based human face monitoring and tracking system. '(Drawing includes non-English language text)'
PD CN105913037-A   31 Aug 2016   G06K-009/00   201665   Pages: 11   Chinese
UT DIIDW:2016555200
ER

PT P
PN CN105790955-A; CN105790955-B
TI MAC address based human face identification information associating method, involves matching MAC address with human face by matching process, and obtaining group relevant data by completing MAC address and human face matching process.
AB    NOVELTY - The method involves collecting low gain relevant scene by a video frequency flow camera. Position of a human face is extracted by a human face detecting and tracking process. Characteristic of the human face is extracted by a human face characteristic extraction process. Multi-scene is obtained based on the characteristic of the human face. Large probability of human face sequence is searched and grouped. MAC address is matched with the human face by a matching process. Group relevant data is obtained by completing MAC address and human face matching process.
   USE - MAC address based human face identification information associating method.
   ADVANTAGE - The method enables ensuring better security, and searching suspected personnel by criminal investigation personnel in a rapid manner by providing strong and powerful support.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a MAC address based human face identification information associating system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a MAC address based human face identification information associating method. '(Drawing includes non-English language text)'
PD CN105790955-A   20 Jul 2016   H04L-009/32   201652   Pages: 9   Chinese
   CN105790955-B   05 Feb 2019   H04L-009/32   201914      Chinese
UT DIIDW:201646463J
ER

PT P
PN CN105447473-A; CN105447473-B
TI PCANet-based CNN random attitude human face expression identification method, involves obtaining unified pixels of gray level image, and identifying attitude of human face expression image by identification module.
AB    NOVELTY - The method involves obtaining unified pixels of a gray level image by performing image pre-processing operation (S1). Characteristics of a positive face image and a side face image are obtained (S2). Characteristic of the positive face image is mapped (S3) with characteristic of the side face image according to supervised learning process. Posture of the positive face image is obtained (S4) by a support vector machine. Attitude of a unified identification module is obtained (S5). Attitude of the human face expression image is identified (S6) by the identification module.
   USE - PCANet-based CNN random attitude human face expression identification method.
   ADVANTAGE - The method enables increasing multi-posture human face expression image identifying accuracy rate.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a PCANet-based CNN random attitude human face expression identification method. '(Drawing includes non-English language text)'
   Step for obtaining unified pixels of a gray level image (S1)
   Step for obtaining characteristic of a positive face image and a side face image (S2)
   Step for mapping positive face image characteristic with side face image characteristic (S3)
   Step for obtaining posture of the positive face image (S4)
   Step for obtaining attitude of a unified identification module (S5)
   Step for identifying attitude of the human face expression image (S6)
PD CN105447473-A   30 Mar 2016   G06K-009/00   201626   Pages: 13   English
   CN105447473-B   08 Jan 2019   G06K-009/00   201906      Chinese
UT DIIDW:201619933D
ER

PT P
PN DE102010061922-A1; US2012089321-A1; JP2012084108-A; KR2012037253-A; CN102442248-A; KR1231510-B1; US8862380-B2; JP5863233-B2; CN102442248-B
TI Front crash risk alerting system for car, has obstruction detecting unit detecting obstruction along direction, opposite to viewing direction of driver, where output of obstruction detecting unit is analyzed by engine control unit.
AB    NOVELTY - The system has a recognition unit for recognizing viewing direction (10) of a driver. An obstruction detecting unit detects existing obstructions along directions into which the driver does not view. An engine control unit receives and analyzes direction data of driver face from the recognition unit if the vehicle brings in intersections. An obstruction detecting unit detects obstruction in direction (12), opposite to the viewing direction of the driver. The output of the obstruction detecting unit is analyzed by an engine control unit.
   USE - System for alerting a front crash risk to a vehicle i.e. car.
   ADVANTAGE - The system prevents the driver from existence of obstructions if a vehicle brings in intersection such as traffic accident due to a carelessness of a user or the carelessness from pedestrians or joggers.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a vehicle comprising a system for alerting with a front crash risk.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a vehicle motion during intersection.'(Drawing includes non-English language text)'
   Vehicle (2)
   Intersection (4)
   Directions (10, 12)
PD DE102010061922-A1   12 Apr 2012   G08G-001/16   201227   Pages: 18   German
   US2012089321-A1   12 Apr 2012   G06F-019/00   201227      English
   JP2012084108-A   26 Apr 2012   G08G-001/16   201229   Pages: 10   Japanese
   KR2012037253-A   19 Apr 2012   B60W-050/08   201229      
   CN102442248-A   09 May 2012   B60Q-009/00   201234      Chinese
   KR1231510-B1   07 Feb 2013   B60W-050/08   201313      
   US8862380-B2   14 Oct 2014   G06F-019/00   201470      English
   JP5863233-B2   16 Feb 2016   G08G-001/16   201613   Pages: 12   English
   CN102442248-B   16 Mar 2016   B60Q-009/00   201621      English
UT DIIDW:2012E14047
ER

PT P
PN US2011007949-A1; US8260008-B2
TI Method for performing biometric recognition, involves associating face and iris images together if multiple images are determined to form expected sequence of images, where multiple images of face and iris of individual are acquired.
AB    NOVELTY - The method involves acquiring multiple images of a face and an iris of an individual. Multiple images are determined to form an expected sequence of images. The face and iris images are associated together if multiple images are determined to form the expected sequence. The iris images and the face images are compared to iris and face images stored in a database if the face and iris images are associated together, where the iris and face images comparison are performed automatically by a computer and manually by a human, respectively.
   USE - Method for performing biometric recognition.
   ADVANTAGE - The method enables providing biometric recognition by acquiring the iris and face images using a single camera by changing one of the zoom, position, or dynamic range of the camera, where the face and iris images are associated together if multiple images are determined to form an expected sequence. The dynamic range is adjusted by adjusting the gain settings of the camera, adjusting the exposure time, and/or adjusting the illuminator brightness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for associating face and iris imagery in a sequence.
PD US2011007949-A1   13 Jan 2011   G06T-007/20   201111   Pages: 18   English
   US8260008-B2   04 Sep 2012   G06K-009/00   201259      English
UT DIIDW:2011A66019
ER

PT P
PN US2010189342-A1; US7907774-B2
TI Method for generating three dimensional representation of portion of human e.g. human face used in entertainment, involves modifying parameters representing portion of human based on comparison of statistical model to two dimensional image.
AB    NOVELTY - The method involves collecting training data and building statistical model having a set of parameters representing portion of human using the training data. The statistical model is compared to a two dimensional image of the portion of human. The parameter is modified based on the comparison of the statistical model to the two dimensional image. The modified set of parameters representing the portion of the human is passed through the statistical model.
   USE - Method for generating three dimensional representation of portion of human such as human face or human head used in entertainment, advertising, and security and multi-modal applications such as internet, personal computer and mobile phones. Can also be used in animation in video games, animations for customer relationship management (CRM), CRM for automatic teller machine (ATM) and airport help desks and for creation of doll heads and bobble head dolls.
   ADVANTAGE - By modifying the parameters representing the portion of human based on comparison of the statistical model to two dimensional image, the accuracy of facial recognition is improved and the quality is improved. Hence the cost of production is reduced and the production time is reduced.
   DESCRIPTION OF DRAWING(S) - The drawings show the photographic views of the reconstructed head with texture.
PD US2010189342-A1   29 Jul 2010   G06K-009/00   201053   Pages: 78   English
   US7907774-B2   15 Mar 2011   G06K-009/00   201120      English
UT DIIDW:2010K15568
ER

PT P
PN US2009310828-A1; US8090160-B2
TI Three-dimension-aided two-dimension face recognition performing method, involves lifting texture of target face from two-dimension probe image, comparing two textures, and verifying and identifying target face based on value of metric.
AB    NOVELTY - The method involves obtaining raw 2D data of a target face from a 2D imagining/scanning apparatus. A texture of the target face is lifted from a 2D probe image. A specific fitted annotated face model is selected from the gallery for an authentication experiment or the gallery models. The gallery texture is relighted from the selected specific fitted annotated face model to change an illumination of the gallery texture to match the texture lifted from the probe image. The two matched textures are compared, and a target face is verified/identified based on a value of a metric.
   USE - Method for performing 3D-aided 2D face recognition under large pose and illumination variations implemented on a computer over a distributed computer network.
   ADVANTAGE - The method automatically performs human face modeling and relighting with application to face recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system comprising an enrollment station.
   DESCRIPTION OF DRAWING(S) - The drawing shows an open-set identification of subjects using a watchlist database in a surveillance scenario.
PD US2009310828-A1   17 Dec 2009   G06K-009/00   201002   Pages: 61   English
   US8090160-B2   03 Jan 2012   G06K-009/00   201204      English
UT DIIDW:2009S41014
ER

PT P
PN CN101596895-A
TI Vehicle anti-theft method involves comparing captured facial image of driver with image registered in computer memory.
AB    NOVELTY - The method involves collecting facial images of authenticated drivers through infrared camera (11). The facial images are registered in the computer memory. The facial image of the driver is captured before the start of the vehicle. The captured facial image is compared with the registered image. An alarm signal is generated through an anti-theft sensor (16) when the captured image is not matched with the registered image.
   USE - Vehicle anti-theft method.
   ADVANTAGE - The captured facial image is compared with the registered image. Hence the drive of the vehicle by unauthorized driver can be prevented. The theft of the vehicle can be prevented.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for vehicle anti-theft device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the vehicle anti-theft device. (Drawing includes non-English language text)
   Infrared camera (11)
   Video collecting module (12)
   Face identification module (14)
   Vehicle starting module (15)
   Anti-theft sensor (16)
PD CN101596895-A   09 Dec 2009   B60R-025/00   201005   Pages: 11   Chinese
UT DIIDW:2009S52823
ER

PT P
PN CN101436247-A; CN101436247-B
TI Biological identity e.g. fingerprint, identifying method for computer e.g. desktop computer, involves comparing biological characteristic identifier with pre-stored biological characteristic identifier.
AB    NOVELTY - The method involves extracting a characteristic value, acquiring a biological characteristic identifier, and comparing the biological characteristic identifier with a pre-stored biological characteristic identifier. A judgment is made whether biological identity identification data information input by a user is matched with pre-stored biological identity identification data information. The user authentication is successful if the biological identity identification data information input by the user is matched with the pre-stored biological identity identification data information.
   USE - Method for identifying a biological identity e.g. fingerprint, hand form, facial form and retina, using a computer device such as a desktop computer, notebook computer, server, hand-held device, touch screen computer and an intelligent telephone (all claimed), based on an uniform extensible firmware interface (UEFI).
   ADVANTAGE - The method enables integrating a biological characteristic information characteristic value extracting algorithm in a uniform extensible firmware interface (UEFI) chip, and supporting a graphic interface operation, thus improving the security of a computer system, and providing convenience for the users to operate biological identity authentication with higher reliability.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer device comprising a biological identity identifying system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a biological identity identifying method.'(Drawing includes non-English language text)'
PD CN101436247-A   20 May 2009   G06K-009/00   200943   Pages: 17   Chinese
   CN101436247-B   11 Apr 2012   G06K-009/00   201234      Chinese
UT DIIDW:2009J95186
ER

PT P
PN JP2008310775-A; WO2008156184-A1; EP2168097-A1; CN101689303-A; US2010189358-A1; JP4999570-B2; US8254691-B2; EP2168097-A4
TI Facial recognition apparatus for imaging device e.g. electronic still camera, determines facial expression of face detected by detector based on characteristic information obtained with respect to detected face.
AB    NOVELTY - The acquisition unit acquires reference characteristic information based on the image information of face detected by face detector. The facial expression determination unit determines the facial expression of face detected by detector based on the extracted characteristic information and reference characteristic information.
   USE - Facial recognition apparatus for imaging device (claimed) e.g. electronic still camera.
   ADVANTAGE - The facial expression recognition can be determined efficiently and reliably without registering image of special facial expression.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for facial expression recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart explaining the operation of imaging device. (Drawing includes non-English language text)
PD JP2008310775-A   25 Dec 2008   G06T-007/20   200903   Pages: 28   Japanese
   WO2008156184-A1   24 Dec 2008   G06T-007/20   200903      English
   EP2168097-A1   31 Mar 2010   G06T-007/20   201023      English
   CN101689303-A   31 Mar 2010   G06T-007/20   201028      Chinese
   US2010189358-A1   29 Jul 2010   G06K-009/46   201050      English
   JP4999570-B2   15 Aug 2012   G06T-007/20   201253   Pages: 28   Japanese
   US8254691-B2   28 Aug 2012   G06K-009/46   201256      English
   EP2168097-A4   05 Mar 2014   G06K-009/00   201770      English
UT DIIDW:2009A63779
ER

PT P
PN US2007014485-A1; DE102006032484-A1; US7209577-B2; CN1901665-A; DE102006032484-B4
TI Video image modification method involves producing modified video feed by applying modification to video feed and providing modified video fed to application program.
AB    NOVELTY - A video feed including an image of a person is received and several facial features are detected from the video feed. An initial facial feature and a modification for initial facial feature are selected. The modification is applied to video feed and a modified video feed is produced. The modified video feed is provided to an application program.
   USE - For varying facial features of person on live video.
   ADVANTAGE - The delay caused by the use of feature recognition and morphing software is less than one frame of video. The three dimensional positioning of head and features are localized and the deformation is adjusted from frontal position to an actual position. The video is modified to only less than 20% or less than 10% and hence the amount of processing is limited and the speed of morphing is increased.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for video image modification apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of system incorporating morphing of video image modification.
PD US2007014485-A1   18 Jan 2007   G06K-009/36   200726   Pages: 11   English
   DE102006032484-A1   15 Feb 2007   G06T-013/00   200726      German
   US7209577-B2   24 Apr 2007   G06K-009/00   200729      English
   CN1901665-A   24 Jan 2007   H04N-007/14   200735      Chinese
   DE102006032484-B4   24 Sep 2009   G06T-013/00   200963      German
UT DIIDW:2007268476
ER

PT P
PN US2006067573-A1; US7657083-B2
TI Three dimensional representation generation method of e.g. person face, involves comparing statistical model with two dimensional image of portion of organism, and modifying parameter of statistic model based on comparison result.
AB    NOVELTY - The training data is collected and statistical model having set of parameters, is created using the training data. The statistical model is compared with two dimensional image of portion of organism. The parameter is modified based on comparison result. The modified set of parameters representing portion of organism, is passed through the statistical model.
   USE - For generating three dimensional representation of organism or object e.g. human face, head of person and doll e.g. bobble head doll and personalized doll in applications such as animation in cell phone and personal computer, video gaming, animation for customer relationship management of automated teller machine, entertainment industry, medical application and facial recognition system used in security system.
   ADVANTAGE - Enables generation of high accurate three dimensional representation of object, quickly.
   DESCRIPTION OF DRAWING(S) - The figure shows a flow diagram illustrating two dimensional to three dimensional conversion process.
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The compression of image conforms to JPEG standard.
PD US2006067573-A1   30 Mar 2006   G06K-009/00   200627   Pages: 80   English
   US7657083-B2   02 Feb 2010   G06K-009/00   201010      English
UT DIIDW:2006261233
ER

PT P
PN WO2006001525-A1; JP2006011978-A; EP1774470-A1; CN1977286-A; US2007242856-A1; JP4217664-B2; US7912253-B2; CN1977286-B; EP1774470-B1; EP1774470-A4
TI Image processing method for use in personal identification system, involves defining localized regions containing feature groups, and determining objects from feature groups.
AB    NOVELTY - The features in an object of interest included in an image, are detected. The localized regions containing the feature groups to obtain a shape and positional relationship of the object, are set. The existence of different objects is determined, based on the feature groups in the localized regions.
   USE - For use in personal identification system and in other object recognitions.
   ADVANTAGE - Provides a compact system and reduces processing costs, since the feature vector generation unit is integrated with the face detector.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) image processing apparatus;
   (2) computer program for performing image processing; and
   (3) computer readable storage medium storing image processing program.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the image processing apparatus.
PD WO2006001525-A1   05 Jan 2006   G06T-007/00   200609   Pages: 56   English
   JP2006011978-A   12 Jan 2006   G06T-007/00   200609   Pages: 18   Japanese
   EP1774470-A1   18 Apr 2007   G06T-007/00   200729      English
   CN1977286-A   06 Jun 2007   G06T-007/00   200770      Chinese
   US2007242856-A1   18 Oct 2007   G06K-009/00   200770      English
   JP4217664-B2   04 Feb 2009   G06T-007/00   200910   Pages: 18   Japanese
   US7912253-B2   22 Mar 2011   G06K-009/00   201122      English
   CN1977286-B   14 Sep 2011   G06T-007/00   201178      Chinese
   EP1774470-B1   24 Dec 2014   G06K-009/00   201502      English
   EP1774470-A4   29 Jul 2009   G06T-007/00   201746      English
UT DIIDW:2006090097
ER

PT P
PN FR2870948-A1; WO2006000670-A1; EP1749237-A1; US2007159548-A1; IN200604320-P4; JP2008500571-W; US7978883-B2; JP4829221-B2; IN240201-B; EP1749237-B1; ES2392555-T3
TI User`s face positioning device for biometric recognition, has super-positioning unit superposing positional reference of user on displayed images, where positional reference and displayed images are arranged in same plane.
AB    NOVELTY - The device has a camera (7) for providing horizontally reversed images, and a display screen (5) for displaying the horizontally reversed images. A super-positioning unit is intended to superpose a positional reference (6) of a user on the displayed images, where the positional reference and the displayed images are arranged in a same plane. A control unit permits to control magnification of the displayed images on the screen.
   USE - Used for positioning face of a user with respect to a device for capturing an image of eyes of an individual for biometric recognition.
   ADVANTAGE - The display screen plays the role of the mirror, so that the user can visualize and correct his position while aligning him in the positional reference. The positional reference and the displayed images are arranged in same plane, thus avoiding the problems of parallax errors and need to clearly and simultaneously visualize user`s image and the positional reference. The device thus allows to position the face of the user with respect to the image capturing device in a rapid and precise manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a process for positioning user with respect to an image capturing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an image capturing device.
   Image capturing device (1)
   User face positioning device (4)
   Display screen (5)
   Positional reference (6)
   Camera (7)
PD FR2870948-A1   02 Dec 2005   G03B-005/00   200602   Pages: 19   French
   WO2006000670-A1   05 Jan 2006   G03B-013/10   200603      French
   EP1749237-A1   07 Feb 2007   G03B-013/10   200713      French
   US2007159548-A1   12 Jul 2007   H04N-005/222   200748      English
   IN200604320-P4   29 Jun 2007   G09G-003/36   200768      English
   JP2008500571-W   10 Jan 2008   G03B-015/00   200806   Pages: 17   Japanese
   US7978883-B2   12 Jul 2011   G06K-009/00   201145      English
   JP4829221-B2   07 Dec 2011   G03B-015/00   201181   Pages: 9   Japanese
   IN240201-B   07 May 2010      201227      English
   EP1749237-B1   03 Oct 2012   G03B-013/10   201265      French
   ES2392555-T3   11 Dec 2012   G03B-013/10   201326      Spanish
UT DIIDW:2006012804
ER

PT P
PN EP1600883-A2; JP2005339388-A; US2005265605-A1; CN1702683-A; CN100343866-C; US7403641-B2; EP1600883-A3; JP4449576-B2; EP1600883-B1
TI Object recognition system for human face recognition has mask locator having detection module that designates at least one of the candidates computed by vote module as center of mask area.
AB    NOVELTY - A mask locator (40) has a vote module (60) that obtains a density gradient directional value for each pixel of a directional subject image. The vote module also reads parameter from a table (52) having the same density gradient directional value to calculate candidates for the center of a mask area. A detection module (70) designates at least one of the candidates as the center of the mask area.
   USE - For human face recognition.
   ADVANTAGE - Determines mask area from within subject image accurately and rapidly for reliable and easy recognition.
   DETAILED DESCRIPTION - An image converter (42) transforms an image size template from a template memory (20) and a subject image taken by an image reader (30) respectively into a density gradient directional template image and a density gradient directional subject image each having an array of pixels each storing a density gradient directional value. A feature analyzer (50) selects reference points around a predetermined center point within the directional template image to obtain a distance and an angle for each reference point. The table stores the distance and angle for each reference point in association with the corresponding density gradient directional value.
   DESCRIPTION OF DRAWING(S) - The figure is a block diagram of the object recognition system.
   Template memory (20)
   Image reader (30)
   Mask locator (40)
   Image converter (42)
   Feature analyzer (50)
   Table (52)
   Vote module (60)
   Detection module (70)
PD EP1600883-A2   30 Nov 2005   G06K-009/64   200601   Pages: 38   English
   JP2005339388-A   08 Dec 2005   G06T-007/00   200601   Pages: 23   Japanese
   US2005265605-A1   01 Dec 2005   G06K-009/62   200601      English
   CN1702683-A   30 Nov 2005   G06K-009/00   200629      Chinese
   CN100343866-C   17 Oct 2007   G06K-009/00   200829      Chinese
   US7403641-B2   22 Jul 2008   G06K-009/00   200850      English
   EP1600883-A3   01 Apr 2009   G06K-009/64   200924      English
   JP4449576-B2   14 Apr 2010   G06T-007/00   201026   Pages: 28   Japanese
   EP1600883-B1   13 Jun 2012   G06K-009/64   201239      English
UT DIIDW:2006001463
ER

PT P
PN WO2005008566-A1; EP1644865-A1; US2006279726-A1; CN1849614-A; JP2007527270-W; CN100419779-C
TI Facial liveness assessment system for detecting impostor, analyses difference between optical radiations reflected from subject's face under different illumination conditions.
AB    NOVELTY - The optical radiation reflected by the subject's face is detected for different illumination conditions. The difference between the reflected radiations are analyzed by the processor (12), for assessing whether or not the subject's face is real.
   USE - For assessing facial liveness for detecting impostor and masked-face of person to control access to various systems.
   ADVANTAGE - The different stages of illumination conditions at the time of obtaining particular image of the subject's face is very sensitive. Hence malpractice can be detected easily.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) facial verification system; and
   (2) facial liveness assessment method.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the facial liveness assessment system.
   ubject (1)
   camera (10)
   monitor (11)
   processor (12)
   controller (13)
   light source (15)
   computer (20)
PD WO2005008566-A1   27 Jan 2005   G06K-009/00   200513   Pages: 20   English
   EP1644865-A1   12 Apr 2006   G06K-009/00   200626      English
   US2006279726-A1   14 Dec 2006   G06K-009/74   200701      English
   CN1849614-A   18 Oct 2006   G06K-009/00   200715      Chinese
   JP2007527270-W   27 Sep 2007   A61B-005/117   200765   Pages: 15   Japanese
   CN100419779-C   17 Sep 2008   G06K-009/00   200903      Chinese
UT DIIDW:2005122949
ER

PT P
PN EP593275-A1; JP6133300-A; EP593275-B1; US5790695-A; DE69320292-E; JP3133517-B2
TI Image area detector for specified image area, esp. for moving image coding of image signal at each frame - has motion inter-frame compensation prediction unit for predictive coding of each frame of image signal, before orthogonally transforming, quantiser and image area detector.
AB       The device has circuitry to divide an input image signal into a number of blocks and for storing the image signal. Using the stored data, the amount of motion of each frame at each block is determined, together with the average luminance and chrominance in each block.
   A differential value of luminance in each block is produced and used to derive an average luminance for each overall frame. Based on the derived values of the various quantities, the specified image area is determined. The selected area is typically a face area.
   ADVANTAGE -   Recognition of important, selected image area allows image quality to be maintained in area, if loss of information in communication line is reduced.
PD EP593275-A1   20 Apr 1994   H04N-007/13   199416   Pages: 18   English
   JP6133300-A   13 May 1994   H04N-007/137   199424   Pages: 12   Japanese
   EP593275-B1   12 Aug 1998   H04N-007/24   199836      English
   US5790695-A   04 Aug 1998   G06K-009/00   199838      English
   DE69320292-E   17 Sep 1998   H04N-007/24   199843      German
   JP3133517-B2   13 Feb 2001   H04N-007/32   200111   Pages: 11   Japanese
UT DIIDW:1994128079
ER

PT P
PN US9786084-B1; WO2017223530-A1; EP3475920-A1; EP3475920-A4
TI System for generating three dimensional (3D) head model used in computer animation from captured image, has processors which optimizes customized static 3D model using gradient-based optimization framework.
AB    NOVELTY - The system has memory with instructions stored which directs processors (205) to receive captured image in which face is visible. A customized static 3D model of head is generated from captured image by determining geometry, texture and lighting components of face visible in captured image and determining camera properties of captured image. The geometry, texture, lighting components and camera properties are applied to generative model to generate customized static 3D model of head based on image. The customized static 3D model using gradient-based optimization framework is optimized.
   USE - System for generating 3D head model used in computer animation from captured image. Can also be used in face detection and face recognition.
   ADVANTAGE - The geometry component, texture components, lighting components, and camera properties are applied to generative model to generate a customized static 3D model of a head based on the image and the customized static 3D model is optimized using a gradient-based optimization framework by the process. The pixels landing on a visible portion of the geometry are facial skin and hair and the remaining pixels is categorized as background pixels and eliminated during the projection. A mnemonic descent method process learns the best way to refine the landmarks for a given image and is more robust to head pose and image content than other landmark identification methods by training a convolutional and a Recurrent neural networks to minimize the error between the predicted landmarks and the ground truth landmarks in a dataset.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of component of a processing system in a device that executes processes to provide an animation-ready 3D model of a head.
   Processing system (200)
   Processors (205)
   Non-volatile memory (210)
   Volatile memory (215)
PD US9786084-B1   10 Oct 2017   G06T-013/40   201769   Pages: 29   English
   WO2017223530-A1   28 Dec 2017   G06T-013/40   201802      English
   EP3475920-A1   01 May 2019   G06T-013/40   201932      English
   EP3475920-A4   15 Jan 2020   G06T-013/40   202006      English
UT DIIDW:201768846X
ER

PT P
PN CN107203953-A; CN107203953-B
TI Internet, face recognition and speech recognition based teaching system, has user input module connected for collecting operation information of user, standard template arranged in database, and terminal connected with cloud platform.
AB    NOVELTY - The system has a first terminal provided with a content presentation module. Three-dimensional head portrait of a teaching course is arranged in a downloading content. An audio playing module is connected in a main body. A video information collecting module is connected for collecting video data information of a user. A user input module is connected for collecting operation information of the user. An analyzing processor obtains sound characteristic. A standard template is arranged in database. A second terminal is connected with a cloud platform.
   USE - Internet, face recognition and speech recognition based teaching system.
   ADVANTAGE - The system realizes teaching software mobility, ensures student to study homework, and improves auxiliary real online teaching mode and traditional Chinese teaching mode.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an internet, face recognition and speech recognition based teaching system realizing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an internet, face recognition and speech recognition based teaching system. '(Drawing includes non-English language text)'
PD CN107203953-A   26 Sep 2017   G06Q-050/20   201774   Pages: 9   Chinese
   CN107203953-B   28 May 2021   G06Q-050/20   202146      Chinese
UT DIIDW:201768150Q
ER

PT P
PN CN107065863-A
TI Human face recognition technology based tour guide explaining robot, has chassis installed on work control machine that is connected with laser radar, where UWB positioning tag and chassis control system mounted on work control machine.
AB    NOVELTY - The robot has an upper computer communicated with a cloud service platform by a wireless module. A robot machine body is provided with a mobile chassis. A bottom of the mobile chassis is arranged with four universal wheels and two driving wheels. The two driving wheels are rotated by a motor. The motor is connected with a motor driver that is connected with a chassis control system. The chassis is installed on a work control machine that is connected with a laser radar. An UWB positioning tag and the chassis control system are mounted on an upper part of the work control machine.
   USE - Human face recognition technology based tour guide explaining robot.
   ADVANTAGE - The robot is convenient, and has simple structure and complete function, and can realize an environment map and self-real-time positioning by an intelligent locating navigation module of the robot chassis and an autonomous movement by utilizing three ultrasonic sensors, avoid obstacles, achieve 3- 5 meter long-distance range voice interaction by a voice module, capture and identify effective human face information by an identification terminal of the robot and send to the cloud server through a network, store the human face information in an information library of a server by utilizing a recognition algorithm, identify an identity of a target person, provide service of individuation and improve user experience and satisfaction of visitors.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face recognition technology based tour guide explaining robot positioning method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective view of a human face recognition technology based tour guide explaining robot.
PD CN107065863-A   18 Aug 2017   G05D-001/02   201767   Pages: 9   Chinese
UT DIIDW:201758049A
ER

PT P
PN CN106878677-A; CN106878677-B
TI Student classroom based multi-sensor level evaluation system, has source module comprising video collecting module and audio frequency collecting module, and suggestion module for analyzing fusion result and providing suggestions.
AB    NOVELTY - The system has an information source module provided with a video collecting module, an audio frequency collecting module and a result recording module. The video collecting module is connected with a face expression classifier module for outputting a facial expression classification result. The audio frequency collecting module is connected with a voice classifier module for outputting a speech classification result. The result recording module is connected with a score classifier module. A suggestion module analyzes a fusion result and provides suggestions.
   USE - Student classroom based multi-sensor level evaluation system.
   ADVANTAGE - The system can accurately evaluate classroom student to master status, judge a teaching evaluation result and give corresponding advice.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a student classroom based multi-sensor level evaluation method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a student classroom based multi-sensor level evaluation system. '(Drawing includes non-English language text)'
PD CN106878677-A   20 Jun 2017   H04N-007/18   201756   Pages: 24   Chinese
   CN106878677-B   07 Jan 2020   H04N-007/18   202005      Chinese
UT DIIDW:201744918E
ER

PT P
PN CN106778607-A
TI Face recognition based method for identity authentication of person and identity card, involves transmitting characteristic feature vector and identity information to background for authentication.
AB    NOVELTY - The method involves collecting face image. A position of the person face is detected and a characteristic vector is extracted according to the face image. The determination is made whether a user is a living object. The face image is re-collected if the user is not a living object. The identity information is collected if the user is a living object. The characteristic feature vector and the identity information are transmitted to a background for authentication. The background returns the authentication result and prompts the user.
   USE - Face recognition based method for identity authentication of person and identity card.
   ADVANTAGE - The authentication efficiency of the same person is improved, which is convenient for large-scale application on various occasions. The problems of inaccurate authentication made by slight change in face over time, and difficult certificate authentication of non carrying personnel can be solved. The risk of using the identity card of the resident is avoided and the problem that the gap between the document and the actual face is large due to the large span of the customer ID card is effectively alleviated. The temporary lack of identity card of customer service problems is solved, and the accuracy and efficiency of verfication check are improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a face recognition based device for identity authentication of person and identity card.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating the face recognition based method for identity authentication of person and identity card. (Drawing includes non-English language text)
PD CN106778607-A   31 May 2017   G06K-009/00   201747   Pages: 9   Chinese
UT DIIDW:201738610F
ER

PT P
PN CN106228293-A
TI Teaching evaluation method, involves determining face key point position of user in video for extracting face position of user, and performing teaching quality evaluation of face key point position of user.
AB    NOVELTY - The method involves determining a human face position of a user in a video. A face key point position of the human face position the user in the video is determined for extracting face position of the user. Teaching quality evaluation of the face key point position is performed. The video of the user face is recorded by using a camera or video recorder. SURF characteristic i.e. global characteristic of the face key point position of the human face position is determined by using a random forest algorithm for obtaining translation quantity of the face key point position.
   USE - Teaching evaluation method.
   ADVANTAGE - The method enables improving teaching efficiency and teaching state of evaluation system for improving teaching effect line or offline.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a teaching evaluation device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a teaching evaluation method. '(Drawing includes non-English language text)'
PD CN106228293-A   14 Dec 2016   G06Q-010/06   201705   Pages: 13   Chinese
UT DIIDW:201680112Y
ER

PT P
PN EP3023911-A1; US2016148080-A1; CN105631398-A; KR2016061856-A; US9928410-B2; US2018181799-A1; CN105631398-B
TI Method for recognizing object, involves recognizing set of elements associated with input image using recognizer pre-trained to recognize set of elements simultaneously by providing input image as input to recognizer.
AB    NOVELTY - The method involves receiving an input image (1710) e.g. skin probability channel image and local binary pattern channel image, and recognizing a set of elements associated with the input image using a recognizer pre-trained to recognize a set of elements simultaneously by providing the input image as input to the recognizer and obtaining the set of elements as simultaneous output from the recognizer, where the recognizer comprises a layer contributing to recognition of the elements of the set of elements to be recognized.
   USE - Method for recognizing an object.
   ADVANTAGE - The method enables reducing information by projecting image data in a low-dimensional eigenvector space while minimizing loss of intrinsic information of an image. The method enables filtering a set of feature images with the filtering module, outputting feature values corresponding to the set of elements based on an output of the filtering module and recognizing the set of elements based on the feature values so as to recognize a set of elements simultaneously, thus increasing recognition accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a non-transitory computer-readable medium including a set of instructions to perform a method for training a recognizer
   (2) a recognition apparatus
   (3) an apparatus for training a recognizer.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a face recognition apparatus.
   Input image (1710)
   Face extractor (1720)
   Face part segmenter (1730)
   Multi-channel generator (1740)
   Multi-task trainer (1750)
PD EP3023911-A1   25 May 2016   G06K-009/46   201636   Pages: 55   English
   US2016148080-A1   26 May 2016   G06K-009/66   201636      English
   CN105631398-A   01 Jun 2016   G06K-009/00   201639      Chinese
   KR2016061856-A   01 Jun 2016   G06K-009/00   201639      
   US9928410-B2   27 Mar 2018   G06K-009/62   201822      English
   US2018181799-A1   28 Jun 2018   G06K-009/00   201843      English
   CN105631398-B   13 Nov 2020   G06K-009/00   202095      Chinese
UT DIIDW:2016307104
ER

PT P
PN CN105574506-A; CN105574506-B
TI Human face identification based intelligent learning pursuit evasion system, has video frame connected with for performing human face identification process, and distribution server for obtaining output result.
AB    NOVELTY - The system has a video input unit connected with a main multi-way network camera that collects video frequency flow decoding data. A video frame is connected with a distribution server that is connected with the video input unit. The video frame is connected with a human face recognition server and performs human face identification process. The distribution server obtains an output result. The video input unit is provided with an image collecting unit and a decoding unit. The image collecting unit receives a video frequency flow signal.
   USE - Human face identification based intelligent learning pursuit evasion system.
   ADVANTAGE - The system has high image quality, recognition rate, safety performance and working reliability, and reduces robustness.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification based intelligent pursuit evasion method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face identification based intelligent learning pursuit evasion system. '(Drawing includes non-English language text)'
PD CN105574506-A   11 May 2016   G06K-009/00   201635   Pages: 17   English
   CN105574506-B   17 Mar 2020   G06K-009/00   202025      Chinese
UT DIIDW:2016302549
ER

PT P
PN CN105513221-A; CN105513221-B
TI Three-dimensional human face recognition based ATM machine anti-cheating device, has human face detecting unit for collecting user visible light human face image, and background server terminal for receiving information comparison result.
AB    NOVELTY - The device has a human face detecting unit for collecting a user visible light human face image, a three-dimensional human face image and an infrared human face image. A processing unit is fixed with the human face detecting unit for extracting characteristic information of the user visible light human face image, the three-dimensional human face image and the infrared human face image. Judgment is made to check whether extracted characteristic information is matched with pre-stored characteristic information. A background server terminal receives information comparison result.
   USE - Three-dimensional human face recognition based ATM machine anti-cheating device.
   ADVANTAGE - The device has better ATM machine operation safety performance.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a three-dimensional human face recognition based ATM machine anti-cheating system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a three-dimensional human face recognition based ATM machine anti-cheating device. '(Drawing includes non-English language text)'
PD CN105513221-A   20 Apr 2016   G07F-019/00   201632   Pages: 10   English
   CN105513221-B   14 Aug 2018   G07F-019/00   201856      Chinese
UT DIIDW:2016274381
ER

PT P
PN US9317785-B1
TI Method for determining ethnicity class, involves applying voting scheme to classification instances and producing final ethnicity vectors for input facial images of person.
AB    NOVELTY - The method involves setting up multi-classifier architecture. The auxiliary class machines are trained. The image features of faces detected from an input image frame (330) are extracted. The first outputs and second outputs are aggregated from the auxiliary class machines for the faces. The ethnicity scores of ethnicity classes are computed by weighting ethnicity outputs from the auxiliary class machines. A voting scheme (262) is applied to classification instances and final ethnicity vectors are produced for input facial images of a person.
   USE - Method for determining ethnicity class.
   ADVANTAGE - The skin tone detection and motion detection can be performed in an input image frame to limit the search space for the face detection, and falsely detected faces are reduced.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an apparatus for determining ethnicity class.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the method for determining ethnicity class.
   Voting scheme (262)
   Input image frame (330)
   Face detection (360)
   Face tracking (361)
   Dimensional facial pose estimation correction (363)
PD US9317785-B1   19 Apr 2016   G06K-009/00   201630   Pages: 25   English
UT DIIDW:201622746L
ER

PT P
PN CN104504365-A
TI Video sequence face smile identification system, has preprocessing module provided with detection unit, and characteristic extraction module for obtaining characteristic of smiling face by adopting random forest algorithm.
AB    NOVELTY - The system has a preprocessing module provided with a human face detection unit. The human face detection unit is connected with a video collection unit. Pyramid Histogram of Oriented Gradients (PHOG) characteristic unit is connected with the human face detection unit that is provided with a characteristic extracting module. A classify recognition module is connected with a characteristic extraction module. A characteristic of a smiling face obtains the characteristic extraction module by adopting random forest (RF) algorithm and PHOG algorithm.
   USE - Video sequence face smile identification system.
   ADVANTAGE - The system improves smiling face identification accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a video sequence face smile identification method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram a video sequence face smile identification system.'(Drawing includes non-English language text)'
PD CN104504365-A   08 Apr 2015   G06K-009/00   201541   Pages: 32   Chinese
UT DIIDW:201534361Q
ER

PT P
PN US8873836-B1
TI Apparatus for performing cluster-based classification of e.g. MRI image, has process rules and settings which are adapted based on results in processing classification request and additional classification requests.
AB    NOVELTY - The apparatus has a cluster based data classification system (202) with a learner module (210). A classification director (211) is configured to determine particular process rules and settings to be applied to classification request. A complex classifier (214) is instantiated to process classification request in accordance with process rules and settings determined by classification director. Process rules and settings are adapted under control of learner module based on results obtained in processing classification request and additional classification requests.
   USE - Apparatus for performing cluster-based classification of high-resolution data such as image such as high-resolution computer tomography (CT) scan and MRI images in medical applications such as digital pathology to detect and diagnose numerous diseases and illnesses. Can also be used in security applications involving image or face recognition, oil and gas exploration applications involving analysis of geological images, and astrophysics applications involving analysis of large-scale imagery of galaxies and molecular clouds.
   ADVANTAGE - The storage requirements and retrieval times of the images is improved, since lower resolution images are kept in the highest tier of storage with higher resolution versions of the image is kept in different storage tiers. Storing copies of these multi-resolution images, and having the ability to use the appropriate level of resolution, is used for diagnosing certain types of illnesses. Thus an improved approach to act on medical imagery so as to enable more accurate and faster diagnoses is provided.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a method for performing cluster-based classification; and
   (2) a computer program product performing cluster-based classification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view classification apparatus.
   Information processing system (200)
   Cluster based data classification system (202)
   Learner module (210)
   Classification director (211)
   Classification director (214)
PD US8873836-B1   28 Oct 2014   G06K-009/00   201473   Pages: 12   English
UT DIIDW:2014T79238
ER

PT P
PN CN103824050-A; CN103824050-B
TI Cascade regression based human face key point e.g. eyes location method, involves collecting human face picture data based on initial key point position, and carrying out fine cascade regression process on human face picture data.
AB    NOVELTY - The method involves collecting human face picture data based on an initial key point position. A fine cascade regression process is carried out on the human face picture data for providing an input to a coarse regression device for training and learning processes. An initial shape of a human face is return to a real human face shape by the coarse regression process. A precise coordinate human face key point is obtained by the fine cascade regression process. The coarse regression process is comprised with two linear regression process levels.
   USE - Cascade regression based human face key point such as eyes, nose, mouth and face outline location method.
   ADVANTAGE - The method enables providing large number of learning samples by a coarse-to-fine cascade regression process and realizing multi-feature characteristics fusion function so as to increase speed and robustness of algorithm. The method enables avoiding light difference so as to achieve better posture and lower face critical point positioning results, which effectively improves accuracy and speed of face critical point positioning function.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a cascade regression based human face key point location method.'(Drawing includes non-English language text)'
PD CN103824050-A   28 May 2014   G06K-009/00   201449   Pages: 8   Chinese
   CN103824050-B   15 Mar 2017   G06K-009/00   201726      Chinese
UT DIIDW:2014N87557
ER

PT P
PN US8417000-B1
TI Method for determining location at which photograph submitted by user is captured using camera in computing device, involves determining photograph capturing time when photograph is captured based on image depicted in photograph.
AB    NOVELTY - The method (300) involves determining (320) photograph capturing time when the photograph is captured based on the image depicted in the photograph. The facial recognition function of the photograph is applied to identify the person. The location data indicative of respective locations of the person is received at multiple instances of time. The location is determined (330) when the photograph is captured using the determined time. The location data is associated (340) with a location of the person at the time the photograph is captured corresponds with the location of the camera.
   USE - Method for determining a location at which a photograph submitted by a user was captured, i.e. as a photo geolocation system in which a computing device is used to analyze images stored in a database and then to associate a geographic location with a photograph, e.g. as a tag.
   ADVANTAGE - Automatically determines a location at which a photograph was captured and does not depend on GPS information being available at the time of picture taking.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a tangible computer-readable unit for storing instructions, which are executed by a processor.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a photograph associating method.
   Photograph associating method (300)
   Determining a time at which the photograph is captured based on the image depicted in the photograph (320)
   Determining a location at which the photograph is captured using the determined time (330)
   Associating location data with a location of the person at the time the photograph is captured corresponds with the location of the camera (340)
PD US8417000-B1   09 Apr 2013   G06K-009/00   201330   Pages: 17   English
UT DIIDW:2013F39910
ER

PT P
PN US2011244919-A1; US8660355-B2
TI Image processing method for selecting image processing operation appropriate to desired set of image data captured by digital camera of mobile phone, involves applying recognition process based on deciding act.
AB    NOVELTY - The method involves analyzing image data to determine colorfulness metric i.e. saturation, or contrast metric i.e. Weber contrast. The metric is utilized to decide a set of different image recognition processes to be applied to the image data captured by a camera i.e. digital camera, of a mobile phone in order to present information derived from different types of imagery to a user from the phone. Each process is applied based on a deciding act, where the process is selected from one of barcode reading, facial recognition, object recognition and optical character recognition (OCR) processes.
   USE - Image processing method for selecting an image processing operation appropriate to a desired set of image data captured by a camera i.e. digital camera, of a mobile phone (claimed). Can also be used for a smart phone e.g. iPhone (RTM: Internet-enabled multimedia mobile phone), Android (RTM: operating system) such as G1 phone, Droid (RTM: Internet and multimedia enabled smartphone) and Nexus (RTM: not defined) phone, personal digital assistant (PDA), an organizer, a portable music player, a desktop computer, a laptop computer, a tablet computer, a netbook (RTM: small, lightweight, legacy-free and inexpensive laptop computer), ultraportable and a server.
   ADVANTAGE - The method enables using color saturation of the image data as the metric to discriminate whether a set of image recognition processes is more likely to be relevant than another set of image recognition processes, thus improving performance and usefulness of selection procedure.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a mobile phone comprising a memory
   (2) a computer readable storage medium comprising a set of non-transitory software instructions for selecting an image processing operation appropriate to a desire set of image data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a local device in a cloud process.
PD US2011244919-A1   06 Oct 2011   G06K-009/00   201168   Pages: 78   English
   US8660355-B2   25 Feb 2014   G06K-009/00   201415      English
UT DIIDW:2011M63274
ER

PT P
PN CN101980305-A
TI Household intelligent access control system, has processor for analyzing and processing video image to abnormally detect face to capture and identify face which is sent to mobile phone of manager via third generation.
AB    NOVELTY - The system has a digital signal central processor for analyzing and processing a video image to abnormally detect a face to capture and identify the face, where the system timely communicates via a third generation (3G). A manager sends an instruction to the system via the third generation. A magcard is detected by the system via a radio frequency identification to finish an auxiliary face identification function and an intelligent door opening function. A detected face or a captured abnormal image is sent to a mobile phone of the manager via the third generation.
   USE - Household intelligent access control system.
   ADVANTAGE - The system realizes to collect the data, to process the data and to transmit to control the data based on the Linux program.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a household intelligent access control system. '(Drawing includes non-English language text)'
PD CN101980305-A   23 Feb 2011   G07C-009/00   201129   Pages: 7   Chinese
UT DIIDW:2011E33111
ER

PT P
PN US2011007174-A1; US8488023-B2
TI Method for recognizing facial expression and/or identity of face of computer game player, involves identifying probable facial expression based on similarities between features of face and library of reference feature sets.
AB    NOVELTY - The method involves detecting and identifying a face within a digital image, and separately extracting one or more features of the face within the digital image. An active appearance model (AAM) including multiple shape parameters is applied to two independent eyes or subsets of features of the eyes, or to lips or partial lips or one or more mouth features and one or both eyes, or both. One or more similarities are determined between the features of the face and a library of reference feature sets, and a probable facial expression is identified based on the determination of the similarities.
   USE - Method for recognizing a facial expression and/or an identity of a face of a computer game player.
   ADVANTAGE - The method enables improved determination of facial expressions captured from a low-resolution video stream, thus enabling to determine the emotional state of a player of a computer game, and provide suggestion to integrate information about emotional state of the player into the workflow of the game.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a digital image acquisition device to recognize a facial expression or an identity of a face, or both, comprising a lens
   (2) a computer-readable medium having code for recognizing a facial expression and/or an identity of a face.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic illustration of a facial analysis and classification system.
   Still and/or video image acquisition component (2)
   Raw data block (4)
   Face detection module (6)
   Face tracking and/or pose estimation module (8)
   Feature extraction module (10)
PD US2011007174-A1   13 Jan 2011   H04N-005/228   201109   Pages: 29   English
   US8488023-B2   16 Jul 2013   A61B-001/04   201347      English
UT DIIDW:2011A57444
ER

PT P
PN CN101499128-A; CN101499128-B
TI Three-dimensional face action detection and tracking method, involves re-detecting and re-positioning face and key points in video image of frame, and re-initializing face vein model according to evaluation result.
AB    NOVELTY - The method involves detecting and positioning a face and key points in an input video image by utilizing automatic face detection and positioning algorithm. A three-dimensional changeable face grid model and a face vein model are utilized for tracking the face position. Results of real-time tracking are evaluated by utilizing the face vein model and a principal component analysis face sub space. The face and the key points are re-detected and re-positioned in the video image of a frame. The face vein model is re-initialized according to the evaluation result.
   USE - Three-dimensional face action detection and tracking method.
   ADVANTAGE - The method increases the head gesture tracking range, thus enhancing the accurate facial expression detail and certain robustness for radiation and shielding with higher practical value, hence enhancing the human-computer interaction, facial expression analysis and entertainment.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a three-dimensional face action detection and tracking method.'(Drawing includes non-English language text)'
PD CN101499128-A   05 Aug 2009   G06K-009/00   200956   Pages: 29   Chinese
   CN101499128-B   29 Jun 2011   G06K-009/00   201172      Chinese
UT DIIDW:2009M55364
ER

PT P
PN CN101430759-A; CN101430759-B
TI Optimal face recognition pre-processing method, involves rebuilding low-frequency components after histogram equalization process is performed for high-frequency components, and performing filter process to obtain pre-processed images.
AB    NOVELTY - The method involves collecting multiple face images by a camera, and converting the face images into grayscale images. Scales of the grayscale images are normalized for same size, and the face images are divided into low-frequency components and high-frequency components based on a wavelet transformation. The low-frequency components are equalized in a histogram, and the low-frequency components are rebuilt after a histogram equalization process is performed for the high-frequency components. A filter process is performed to obtain multiple pre-processed images.
   USE - Optimal pre-processing method for face recognition.
   ADVANTAGE - The method adjusts the grayscale area of the face image, enhances the contrast and brightness, and improves the face recognition efficiency in the complex light environment and different poses.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of an optimal face recognition pre-processing method.'(Drawing includes non-English language text)'
PD CN101430759-A   13 May 2009   G06K-009/00   200935   Pages: 15   Chinese
   CN101430759-B   17 Nov 2010   G06K-009/00   201110      Chinese
UT DIIDW:2009J44961
ER

PT P
PN EP2037320-A1; US2009073304-A1; JP2009069551-A; CN101387732-A; BR200803678-A2; JP4544282-B2; RU2399937-C2; RU2008136813-A; CN101387732-B; US8068164-B2; EP2037320-B1
TI Data processing device, has detecting unit moving focus lens within limits set by setting unit, and control unit determining focal point as focus position and determining attributes of subject based on features of face region.
AB    NOVELTY - The device has a face-detecting unit (200) detecting a face region from an input image input from an imaging unit. A setting unit calculates subject distance based on the face region detected by the face-detecting unit. A detecting unit moves a focus lens within limits set by a setting unit. A control unit (120) e.g. CPU, determines a focal point as a focus position, determines attributes of a subject e.g. person, based on features of the face region and performs estimation of a face size based on the attributes.
   USE - Data processing device.
   ADVANTAGE - The detecting unit moves the focus lens within limits set by the setting unit, thus suppressing occurrence of background focus and performing correct tracing of a principal subject, and realizing high-speed and stable focus control.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a data processing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a hardware configuration of an imaging apparatus.
   Monitor (106)
   Timing generator (109)
   Motor driver (110)
   Control unit (120)
   Face-detecting unit (200)
PD EP2037320-A1   18 Mar 2009   G03B-013/36   200921   Pages: 43   English
   US2009073304-A1   19 Mar 2009   H04N-005/232   200921      English
   JP2009069551-A   02 Apr 2009   G02B-007/28   200924   Pages: 42   Japanese
   CN101387732-A   18 Mar 2009   G02B-007/28   200926      Chinese
   BR200803678-A2   29 Sep 2009      200964      
   JP4544282-B2   15 Sep 2010   G02B-007/28   201061   Pages: 34   Japanese
   RU2399937-C2   20 Sep 2010      201062      Russian
   RU2008136813-A   20 Mar 2010      201104      Russian
   CN101387732-B   01 Dec 2010   G02B-007/28   201113      Chinese
   US8068164-B2   29 Nov 2011   G03B-013/00   201179      English
   EP2037320-B1   29 Feb 2012   G03B-013/36   201216      English
UT DIIDW:2009F89447
ER

PT P
PN WO2008109644-A2; US2008232711-A1; WO2008109644-A3; KR2009122275-A; JP2010520727-W; CN201788344-U; US7970182-B2; KR1049188-B1; US2011211095-A1; US2011262034-A1; US2011262035-A1; US2011262038-A1; US8126217-B2; US8131021-B2; US8175342-B2; US8180115-B2
TI Red-eye defect detection method for digital image, involves determining second set of confirmed redeye regions by reanalyzing rejected candidate red eye regions, and correcting to generate second red eye corrected image.
AB    NOVELTY - The detecting method involves acquiring an image (110) with red eye defect. A first set of confirmed red-eye regions (108) is determined through initial segmentation (103), and corrected to produce a first red eye corrected image (112). A second set of confirmed redeye regions is determined by reanalyzing a subset of rejected candidate red eye regions, and corrected to generate a second red eye corrected image, which is electronically stored, transmitted, processed, edited, and displayed.
   USE - Method for detecting red-eye defect, which is an unnatural reddish coloration in the pupils of a person in a digital image.
   ADVANTAGE - Ensures accurate image analysis by providing two-stage redeye filter in first redeye filter process. Allows optimized redeye detection and correction to occur in playback mode without loss of image quality. Allows lossless saving and restoration of image within the camera, while facilitating incremental process of an image which is not limited to redeye, but may be applied to other in-camera methods such as face detection or recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an embedded image acquisition and processing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flow diagram of the red-eye defect detection method.
   Red-eye correction (102)
   Initial segmentation (103)
   Candidate red-eye region (104)
   Falsing and verification filter (106)
   Confirmed red-eye regions (108)
   Image (110)
   First red eye corrected image (112)
PD WO2008109644-A2   12 Sep 2008   G06K-009/40   200862   Pages: 38   English
   US2008232711-A1   25 Sep 2008   G06K-009/40   200866      English
   WO2008109644-A3   23 Oct 2008   G03B-015/03   200872      English
   KR2009122275-A   26 Nov 2009   G06T-007/00   200979      
   JP2010520727-W   10 Jun 2010   H04N-009/07   201038   Pages: 25   Japanese
   CN201788344-U   06 Apr 2011   G03B-015/03   201132      Chinese
   US7970182-B2   28 Jun 2011   G06K-009/00   201143      English
   KR1049188-B1   14 Jul 2011   G06T-007/00   201156      
   US2011211095-A1   01 Sep 2011   H04N-005/217   201159      English
   US2011262034-A1   27 Oct 2011   G06K-009/00   201170      English
   US2011262035-A1   27 Oct 2011   G06K-009/34   201170      English
   US2011262038-A1   27 Oct 2011   G06T-005/00   201170      English
   US8126217-B2   28 Feb 2012   G06K-009/00   201216      English
   US8131021-B2   06 Mar 2012   G06K-009/00   201218      English
   US8180115-B2   15 May 2012   G06K-009/00   201234      English
UT DIIDW:2008K43116
ER

PT P
PN US2008152213-A1; US8126261-B2
TI Face reconstruction method used for e.g. facial recognition, involves analyzing several images based on sparse three-dimensional face features so as to determine dense three-dimensional face features without using prior face knowledge.
AB    NOVELTY - The method involves analyzing several images of face for determining (100) sparse three-dimensional face features using the prior face knowledge or a generic face. Several images are analyzed using the sparse three-dimensional face features so as to determine the dense three-dimensional face features without using the prior face knowledge.
   USE - Face reconstruction method used for facial recognition, animation and face rendering.
   ADVANTAGE - Since the sufficient degree of freedom is provided, the information and the subsequent reconstruction can capture all obvious details of the original face and the quality of the realistic image can be improved.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flowchart explaining the face reconstruction process.
   Step for determining sparse three-dimensional face features (100)
   Step for obtaining pose selection (120)
   Step for performing global optimization (130)
   Step for rectifying pair of images (135)
   Step for removing outliers (145)
PD US2008152213-A1   26 Jun 2008   G06K-009/00   200934   Pages: 8   English
   US8126261-B2   28 Feb 2012   G06K-009/00   201216      English
UT DIIDW:2009B55978
ER

PT P
PN US2008019589-A1; KR776801-B1; US8014567-B2
TI Gesture recognition method for image processing system for controlling TV includes setting gesture search areas, selecting detection area with respect to gesture in hand area, and transferring image of detection area as TV control command.
AB    NOVELTY - A method includes detecting a face area in an image captured and input from a camera (210). The image is divided to predetermined areas with reference line to a predetermined location of the face area. The predetermined areas are set to gesture search areas. A hand area is detected in the gesture search areas. A gesture is determined in the detected hand area. A detection area is selected with respect to the gesture. An image of the selected detection area is transferred as a control command to control the TV (220).
   USE - Gesture recognition method for an image processing system that controls the TV. Can also be used for an image processing system that controls a home robot or a game.
   ADVANTAGE - Both real-time processing and long distance processing are achieved. The user can control the TV by making simple gestures without using a remote controller. The method is effective for people with poor sight or the disabled, as well as ordinary people. A gesture is recognized separately from other movement information since an accurate hand area is detected and gesture search areas are set. This reduces undesired operation caused by incorrect gesture recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an apparatus for recognizing a gesture in an image processing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a construction of an image processing system for gesture recognition.
   Camera (210)
   TV (220)
PD US2008019589-A1   24 Jan 2008   G06K-009/00   200815   Pages: 13   English
   KR776801-B1   19 Nov 2007   G06K-009/80   200859      
   US8014567-B2   06 Sep 2011   G06K-009/00   201159      English
UT DIIDW:2008C04072
ER

PT P
PN US2007237421-A1; WO2007126753-A1; US7787664-B2
TI Image recomposition method for creating photomontage using e.g. a digital camera includes using features from the target and source faces to perform facial geometry and appearance corrections to the source face.
AB    NOVELTY - A method includes automatically detecting faces in the input target frame. A target face is identified from the detected faces that need to be replaced. Faces are automatically detected in the input source frame. A source face is identified from the source frame detected faces for replacing the target face. Features from the target and source faces are used to perform facial geometry and appearance corrections to the source face. The target face is replaced with the corrected source face by blending the corrected source face into the target frame.
   USE - For replacing a face in the target frame with a face from source frame for creating photomontage using a digital camera or software in a personal computer.
   ADVANTAGE - Each slider on the pop-up panel allows a user to fine tune the quality of the composite image using the parameters or settings automatically determined as the starting points to reduce the knowledge and operations required on the user's part. An interface is provided to allow a user to click on the two eyes of a subject in case automatic face detection fails to find the face. The facial feature finding module is capable of using the clicked eye positions to locate the face contour and other facial features.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for
   (1) method for producing a composite group image from a series of images of the group captured by an image capture device;
   (2) method for producing a composite image from a series of frames captured by a video capture device
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating an overview of the image recomposition method.
PD US2007237421-A1   11 Oct 2007   G06K-009/36   200774   Pages: 12   English
   WO2007126753-A1   08 Nov 2007   H04N-001/387   200774      English
   US7787664-B2   31 Aug 2010   G06K-009/00   201057      English
UT DIIDW:2007796416
ER

PT P
PN WO2006052681-A1; US2006171586-A1; US7876934-B2
TI Segmenting an anatomical structure of interest within an image involves identifying database images similar to structure, using identified images to detect structure in image and to determine shape of structure.
AB    NOVELTY - The anatomical structure of interest is compared to a database of image of like anatomical structures. The database images similar to the anatomical structure of interest are identified, then using those identified images to detect the anatomical structure of interest in the image. The identified images are then used to determine the shape of the anatomical structure of interest, after which the structure is segmented.
   USE - Segmenting an anatomical structure of interest within an image. Applicable e.g. for detecting regional wall motion abnormalities in the heart, recognizing features such as facial features or other body features, two-dimensional, three-dimensional, four-dimensional data analysis e.g. medical analysis of anatomical structures such as the heart, lungs or tumors, which can be evolving over time.
   ADVANTAGE - Directly exploits expert annotation of the interest structure in large databases by formulating the segmentation as a learning problem.
   DESCRIPTION OF DRAWING(S) - The figure shows illustrates a method for database guided shape inference of a structure.
PD WO2006052681-A1   18 May 2006   G06K-009/62   200637   Pages: 40   English
   US2006171586-A1   03 Aug 2006   G06K-009/34   200651      English
   US7876934-B2   25 Jan 2011   G06K-009/00   201108      English
UT DIIDW:2006363869
ER

PT P
PN US2005226472-A1; JP2005301742-A; JP4059224-B2; US7466847-B2
TI Driver's appearance recognition system for vehicle, has image processing circuit which performs individual authentication and closed-eye detection with different illumination intensities of near infrared ray LED by control of current supply.
AB    NOVELTY - An image processing circuit performs an individual authentication process wherein the illumination intensity of a near infrared ray LED during the image capture by a camera is increased by increasing the current supply to the LED, and a closed-eye detection process wherein the illumination intensity is maintained to an appropriate amount with the appropriate current supply to the LED.
   USE - For vehicle. Applicable for car antitheft system and vehicular accident prevention system.
   ADVANTAGE - Enables preventing a driver from falling asleep during driving since closed-eye state of driver can be detected. Prevents theft of vehicle since face of driver can be recognized and authenticated during driving. Enables prolonging service life of near infrared ray LED due to current supply to LED based on processing operation.
   DETAILED DESCRIPTION - The driver's appearance recognition system includes the camera which captures a driver's face image. The near infrared ray LED radiates a near infrared ray to a driver's face. A LED current control circuit regulates the amount of current supplied to the near infrared ray LED.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart for driver's appearance recognition process.
PD US2005226472-A1   13 Oct 2005   G06K-009/00   200574   Pages: 14   English
   JP2005301742-A   27 Oct 2005   G06T-001/00   200574   Pages: 19   Japanese
   JP4059224-B2   12 Mar 2008   G06T-001/00   200821   Pages: 20   Japanese
   US7466847-B2   16 Dec 2008   G06K-009/00   200902      English
UT DIIDW:2005723776
ER

PT P
PN WO2005038743-A1; DE10348109-A1; EP1673751-A1; US2006257024-A1; EP1673751-B1; CN1867954-A; DE502004002814-G; JP2007515853-W; ES2280049-T3; US7593573-B2; CN1867954-B; JP4512595-B2
TI Surveillance method for visualizing a motor vehicle's surroundings detects a traffic situation faced by a driver with digital optical and infrared cameras.
AB    NOVELTY - In a camera system (CS) (2) there is an optical camera (3) to capture video images in a visible spectral range and an infrared camera (4) to capture video images from an infrared spectral range. As digital cameras, these cameras have sensors sensitive to the corresponding spectral ranges. An object-recognition system (5) is connected in series downstream to the CS or a fusion device (9).
   USE - For visualizing a motor vehicle's surroundings in traffic situations.
   ADVANTAGE - A night-vision information system can be used. Video images are analyzed with weighting factors and analysis of objects.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for visualizing a motor vehicle's surroundings.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block connection diagram for a device for visualizing a motor vehicle's surroundings.
   Camera system (2)
   Optical camera (3)
   Infrared camera (4)
   Object-recognition system (5)
   Fusion device (9)
PD WO2005038743-A1   28 Apr 2005   G08G-001/16   200541   Pages: 23   German
   DE10348109-A1   19 May 2005   G08G-001/16   200541      German
   EP1673751-A1   28 Jun 2006   G08G-001/16   200643      German
   US2006257024-A1   16 Nov 2006   G06K-009/00   200677      English
   EP1673751-B1   24 Jan 2007   G08G-001/16   200710      German
   CN1867954-A   22 Nov 2006   G08G-001/16   200720      Chinese
   DE502004002814-G   15 Mar 2007   G08G-001/16   200726      German
   JP2007515853-W   14 Jun 2007   H04N-007/18   200741   Pages: 16   Japanese
   ES2280049-T3   01 Sep 2007   G08G-001/16   200761      Spanish
   US7593573-B2   22 Sep 2009   G06K-009/00   200962      English
   CN1867954-B   28 Apr 2010   G08G-001/16   201041      Chinese
   JP4512595-B2   28 Jul 2010   H04N-007/18   201049   Pages: 9   Japanese
UT DIIDW:2005404673
ER

PT P
PN US2005089206-A1; WO2005043453-A1; EP1678660-A1; JP2007511256-W; US7388971-B2
TI Selected emotions sensing method for human subject, involves processing image of face of human subject to identify movements in selected critical areas of face, where movements are compared with database.
AB    NOVELTY - The method involves generating an image of substantially all face of a human subject. The image is processed to identify movements in selected critical areas of the face. The identified movements are compared with a database that associates movements in the selected critical areas with specific emotional and physical conditions. A report of the emotional and physical condition of the subject is generated.
   USE - Used for sensing selected emotions or stress in a human subject.
   ADVANTAGE - The method enables a reliable assessment of the affect and credibility of the subject based on the established database. The method can simultaneously detect and analysis multiple facial muscle groups, during a brief period of involuntary activity following application of an external stimulus.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an apparatus for sensing selected emotions in a human subject.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an apparatus for sensing selected emotions in a human subject.
   Face (10)
   Speckle detection and tracking sensor (12)
   Image processing module (14)
   Display (16)
   Report (20)
PD US2005089206-A1   28 Apr 2005   G06K-009/00   200538   Pages: 8   English
   WO2005043453-A1   12 May 2005   G06K-009/62   200538      English
   EP1678660-A1   12 Jul 2006   G06K-009/62   200648      English
   JP2007511256-W   10 May 2007   A61B-005/16   200731   Pages: 13   Japanese
   US7388971-B2   17 Jun 2008   G06K-009/00   200842      English
UT DIIDW:2005370871
ER

PT P
PN US2005031195-A1; US7218774-B2
TI Object e.g. animal, three dimensional model generating process for face recognition system, involves aligning features of generic model of identified object with features of object in image to get tailored model.
AB    NOVELTY - The process involves obtaining an image of an object e.g. animal. The object to be modeled in the image is identified. A set of features of the object in the image is determined. Features of a generic model of the identified object are aligned with the features of the object in the image for obtaining a tailored generic model. A texture of the object in the image is applied to the tailored generic model.
   USE - Used for generating a three dimensional model of an object e.g. animal, face and human body, for face recognition system, and constructing personalized models for games, animation and on-line chat.
   ADVANTAGE - The method automatically models the three dimensional object from a single image, without requiring user interaction. The method is fast, robust, and computationally efficient to model people of any skin types in various lighting conditions.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a system for creating a face model from an input of single image of a face
   (B) a computer-readable medium having computer-executable instructions for creating a three-dimensional model of an object in a single image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a simplified flow diagram of an overall modeling process employed by an object modeling system.
PD US2005031195-A1   10 Feb 2005   G06K-009/00   200518   Pages: 14   English
   US7218774-B2   15 May 2007   G06K-009/00   200732      English
UT DIIDW:2005171611
ER

PT P
PN EP1494210-A1; JP2005022065-A; US2005043956-A1; KR2005004107-A; CN1591569-A; EP1494210-B1; DE602004004324-E; DE602004004324-T2; CN1312576-C; CN101030370-A; JP4048492-B2; KR1057705-B1; US8209179-B2; US2012232891-A1; CN101030370-B; US8321221-B2; US2013060566-A1; US8538750-B2
TI Speech communication system in entertainment robot, includes tracking control unit to track existence of user partner using both face and voice recognition results.
AB    NOVELTY - A tracking control unit tracks the existence of the user based on the result from both face image recognition unit and speech recognition unit. A conversation control unit continues or finishes the conversation with the user based on the tracking result.
   USE - Used in robot such as entertainment robot, amusement robot and caring robot.
   ADVANTAGE - Recognizes the existence of user accurately under any environment.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) speech communication method; and
   (2) robot with speech communication system.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of robot.
PD EP1494210-A1   05 Jan 2005   G10L-015/26   200508   Pages: 27   English
   JP2005022065-A   27 Jan 2005   B25J-013/00   200510   Pages: 27   Japanese
   US2005043956-A1   24 Feb 2005   G10L-011/00   200515      English
   KR2005004107-A   12 Jan 2005   G10L-015/22   200535      
   CN1591569-A   09 Mar 2005   G10L-015/00   200542      Chinese
   EP1494210-B1   17 Jan 2007   G10L-015/26   200706      English
   DE602004004324-E   08 Mar 2007   G10L-015/26   200726      German
   CN1312576-C   25 Apr 2007   G06F-009/00   200757      Chinese
   CN101030370-A   05 Sep 2007   G10L-015/22   200810      Chinese
   JP4048492-B2   20 Feb 2008   B25J-013/00   200816   Pages: 26   Japanese
   KR1057705-B1   18 Aug 2011   G10L-015/22   201159      
   US8209179-B2   26 Jun 2012   G10L-015/00   201243      English
   US2012232891-A1   13 Sep 2012   G10L-015/00   201260      English
   CN101030370-B   04 Jul 2012   G10L-015/22   201272      Chinese
   US8321221-B2   27 Nov 2012   G10L-015/00   201279      English
   US2013060566-A1   07 Mar 2013   G10L-015/00   201318      English
   US8538750-B2   17 Sep 2013   G10L-015/00   201361      English
UT DIIDW:2005067698
ER

PT P
PN WO2004081875-A2; US2004208341-A1; DE112004000393-T5; CN1781123-A; JP2006524534-W; US7558402-B2; JP2009226226-A; CN1781123-B; JP4861165-B2; WO2004081875-A3; DE112004000393-B4
TI Method for tracking global shape of object e.g. ventricle in motion, involves tracking control points along global shape as the object is in motion, and exploiting uncertainty of location of control points to constrain global shape.
AB    NOVELTY - One or more control points are defined along the global shape of an object in motion, and each control point is tracked by bayesian kernel matching approach, as the object is in motion. The uncertainty of control point locations is represented and exploited to constrain the global shape using a prior shape model.
   USE - Used for recognizing movement of human features such as head, face, hand and ventricle, Also for 2,3,4 dimensional medical analyses of anatomical structures such as heart, lungs or tumors.
   ADVANTAGE - Abnormal regions can be identified directly and easily by human eyes.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for tracking a global shape of an object in motion;
   (2) a method for visually tracking movement of a shape of an object; and
   (3) a system for visually tracking movement of a shaped of an object.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of a moving object global shape tracking system.
PD WO2004081875-A2   23 Sep 2004   G06T-007/20   200467   Pages: 43   English
   US2004208341-A1   21 Oct 2004   G06K-009/00   200470      English
   DE112004000393-T5   02 Mar 2006      200617      German
   CN1781123-A   31 May 2006   G06T-007/20   200663      Chinese
   JP2006524534-W   02 Nov 2006   A61B-008/08   200672   Pages: 31   Japanese
   US7558402-B2   07 Jul 2009   G06K-009/00   200945      English
   JP2009226226-A   08 Oct 2009   A61B-008/08   200966   Pages: 19   Japanese
   CN1781123-B   12 May 2010   G06T-007/20   201045      Chinese
   JP4861165-B2   25 Jan 2012   A61B-008/08   201209   Pages: 26   Japanese
   WO2004081875-A3   06 Oct 2005   G06T-007/20   201218      English
   DE112004000393-B4   18 Jul 2013   G06T-007/20   201347      German
UT DIIDW:2004690765
ER

PT P
PN EP1460588-A2; JP2004289254-A; US2004201666-A1; CN1532775-A; US7202886-B2; CN100359941-C; JP2009112027-A; JP4896118-B2; EP1460588-A3
TI Videophone terminal for video communication, has virtual character generator which modifies features of user face image in response to predetermined user operation.
AB    NOVELTY - A virtual character generator generates a virtual character based on feature points extracted from captured user face image, modifies features and transmits to other terminal in response to predetermined user operation.
   USE - For video communication.
   ADVANTAGE - The emotions and impressions of a user are transmitted in a manner that is easily understood by co-communicant.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the videophone terminal.
PD EP1460588-A2   22 Sep 2004   G06T-015/70   200467   Pages: 9   English
   JP2004289254-A   14 Oct 2004   H04N-007/14   200467   Pages: 9   Japanese
   US2004201666-A1   14 Oct 2004   H04N-007/14   200468      English
   CN1532775-A   29 Sep 2004   G06T-015/00   200504      Chinese
   US7202886-B2   10 Apr 2007   H04N-007/14   200726      English
   CN100359941-C   02 Jan 2008   H04N-007/14   200833      Chinese
   JP2009112027-A   21 May 2009   H04N-007/14   200934   Pages: 9   Japanese
   JP4896118-B2   14 Mar 2012   H04N-007/14   201219   Pages: 9   Japanese
   EP1460588-A3   02 Feb 2005   G06T-015/70   201730      English
UT DIIDW:2004679745
ER

PT P
PN CN1495658-A; CN1275185-C
TI Driver's face image identification and alarm device and method.
AB    NOVELTY - Firstly, the identity of the driver can be identified, if identity is wrong, driver can be regarded as robber, the second alarm can be given to the car owner and traffic control department; if the face image camera in the car is broken by robber, the identification alarm device can give out first alarm. If the driver identity is lawful, device can be used for monitoring and identifying the action and spirit state of driver, if the driven posture is irregular and driver is deficient in energy, device can give out third alarm; and if the driver is tired and sleepy, device can give out fourth alarm to use high pitch voice to wave driver up.
   USE - The present invention provides a device and method for providing face image identification of driver and giving out four intelligent alarms.
PD CN1495658-A   12 May 2004   G06K-009/00   200451      Chinese
   CN1275185-C   13 Sep 2006   G06K-009/00   200706      Chinese
UT DIIDW:2004526382
ER

PT P
PN US2003217294-A1; WO2003098343-A2; US6725383-B2; AU2003229052-A1; EP1512063-A2; AU2003229052-A8; WO2003098343-A3; WO2003098343-A8; EP1512063-A4
TI Face image identity verification system processes digital face image and formatted data by applying face recognition technique, and relays results of recognition to user through output device.
AB    NOVELTY - A communication control device (20) converts the face image sent by a camera (22) and reference data sent by an input device (13) into digital image file and formats the file into network protocol standard. A CPU (24) processes the formatted data and digital image by applying face recognition technique. The results of recognition are relayed to user through visual output device (13).
   USE - Face image identity verification system.
   ADVANTAGE - Enables bi-directional communication of digitized photographic images over computer network.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) communication control device;
   (2) method for image capture and verification; and
   (3) face recognition based method for verifying the identity of an individual.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic view of the identity verification system.
   data input device (13)
   visual output device (15)
   communication device (20)
   camera (22)
   CPU (24)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The communication control device communicates in accordance with RS 232 or RS 485 standards and HTTP, UDP or ARP protocol . The photographic images are converted in accordance with JPEG or MPEG standards.
PD US2003217294-A1   20 Nov 2003   H04L-009/32   200408   Pages: 12   English
   WO2003098343-A2   27 Nov 2003   G03B-000/00   200408      English
   US6725383-B2   20 Apr 2004   G06F-001/24   200427      English
   AU2003229052-A1   02 Dec 2003   H04L-009/32   200442      English
   EP1512063-A2   09 Mar 2005   G06F-001/24   200518      English
   AU2003229052-A8   27 Oct 2005   G06F-001/24   200624      English
   WO2003098343-A3   10 Jun 2004   G06F-001/24   201213      English
   WO2003098343-A8   21 Oct 2004   G06K-009/00   201732      English
   EP1512063-A4   28 Nov 2007   G06K-009/00   201752      English
UT DIIDW:2004080699
ER

PT P
PN JP2003150932-A; JP3984029-B2
TI Image processor for personal identification, extracts face image from input image read from recording medium and searches corresponding person's reference image and displays searched images.
AB    NOVELTY - The registration units (11a,11b) register a face image as a search reference image. An extraction unit (15a) extracts a face image from an input image read from a recording medium (6). A search unit (13a) searches the extracted image and the corresponding person's reference image, from the registration units. A display section (14) displays the searched images.
   USE - Face image processor e.g. personal computer (PC) for personal identification and photography collation.
   ADVANTAGE - The face image is searched at high speed, and the person is specified by simple operation, since the face image registered as the reference image, is extracted and displayed along with the input image.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for program storing instruction for processing face image.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the face image processor. (Drawing includes non- English language text).
   recording medium (6)
   registration units (11a,11b)
   earch unit (13a)
   display section (14)
   extraction unit (15a)
PD JP2003150932-A   23 May 2003   G06T-001/00   200342   Pages: 12   Japanese
   JP3984029-B2   26 Sep 2007   G06T-001/00   200765   Pages: 16   Japanese
UT DIIDW:2003445075
ER

PT P
PN EP1255225-A2; JP2002342756-A; US2003021448-A1; US7092554-B2; EP1255225-A3
TI Digital image processing method for face image recognition, involves detecting eye position using pixel cluster of digital face image to generate signature curve using which eye to mouth co-ordination is identified.
AB    NOVELTY - The iris colored pixels in the digital face image are detected and are grouped into iris pixel cluster. The eye position is detected using the pixel cluster, and the salient pixels relating to facial feature, are identified. A signature curve is generated using salient pixels, based on which the eye to mouth co-ordination is identified.
   USE - For identifying eye to mouth co-ordination of human during face image recognition using personal computer.
   ADVANTAGE - The eye and mouth position in the face image are identified automatically, reducing the computation required to locate eye and mouth and the incidence, thereby the human face image is recognized efficiently.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart explaining the eye-mouth co-ordination determination process.
PD EP1255225-A2   06 Nov 2002   G06K-009/00   200304   Pages: 17   English
   JP2002342756-A   29 Nov 2002   G06T-007/00   200309   Pages: 11   Japanese
   US2003021448-A1   30 Jan 2003   G06K-009/00   200311      English
   US7092554-B2   15 Aug 2006   G06K-009/00   200654      English
   EP1255225-A3   31 Mar 2004   G06K-009/00   201732      English
UT DIIDW:2003042108
ER

PT P
PN JP2000083141-A; US6798905-B1; JP3629969-B2
TI Image recognition apparatus for copier performs top and bottom recognition for every page, based on image information of every facing page.
AB    NOVELTY - Image information for every facing page is read for every page. Based on the obtained image, top and bottom recognition is performed for each page. The recognized result of top and bottom of another page is adopted, when the evaluation of top and bottom of the page is not made.
   USE - For copier.
   ADVANTAGE - Direction of original document for every page is recognized exactly.
PD JP2000083141-A   21 Mar 2000   H04N-001/04   200025   Pages: 10   Japanese
   US6798905-B1   28 Sep 2004   G06K-009/00   200464      English
   JP3629969-B2   16 Mar 2005   H04N-001/387   200520   Pages: 13   Japanese
UT DIIDW:2000288982
ER

PT P
PN CN107423690-A; CN107423690-B
TI Human face identification method, involves extracting face image of Haar feature, pre-constructing legal face database, and determining whether face image is same as legal user corresponding to output value of multi-task learning model.
AB    NOVELTY - The method involves extracting a face image of a Haar feature. Human face region image multi-scale characteristic is extracted using a convolution neural network model. A characteristic vector of a human face region image is obtained. A legal face database is pre-constructed. A determination is made to check whether the face image is same as a legal user corresponding to an output value of a multi-task learning model. Balance factor of a multi-task learning model is determined. Each single task is obtained according to trade-off factor.
   USE - Human face identification method.
   ADVANTAGE - The method enables extracting features with better robustness and better generalization ability. The method enables improving face recognition rate and accuracy of face recognition so as to improve security of identity authentication.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face identification method. '(Drawing includes non-English language text)'
PD CN107423690-A   01 Dec 2017   G06K-009/00   201802   Pages: 17   Chinese
   CN107423690-B   13 Nov 2020   G06K-009/00   202095      Chinese
UT DIIDW:2017837749
ER

PT P
PN CN107316261-A
TI Face analysis based teaching quality evaluation system, has student classroom camera for collecting face image, and teaching quality feedback mechanism establishes relationship between facial expression change and teaching process.
AB    NOVELTY - The system has a student classroom camera for collecting a face image according to face recognition and attendance and monitoring a classroom. A face posture analysis module is utilized for analyzing state of multiple students. A facial expression analysis module is utilized for performing students expression state analysis process. A teaching quality feedback mechanism obtains classroom student teaching content by using big data analysis technology. The teaching quality feedback mechanism establishes relationship between facial expression change and teaching process.
   USE - Face analysis based teaching quality evaluation system.
   ADVANTAGE - The system improves quantization teaching quality and teaching quality evaluation performance of the students in the classroom.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a face analysis based teaching quality evaluation system. '(Drawing includes non-English language text)'
PD CN107316261-A   03 Nov 2017   G06Q-050/20   201778   Pages: 7   Chinese
UT DIIDW:201776324E
ER

PT P
PN CN106599830-A; CN106599830-B
TI Human face key point locating method, involves scaling tested image of human face, and scaling tested image of human face determining specified resolution, and detecting different key points corresponding to key point detection model.
AB    NOVELTY - The method involves scaling a tested image of a human face for determining first specified resolution. Tested image zooming process is performed. The face image is inputted to a first-stage multitask key point location model for obtaining a key point location coordinate. A head pose estimation value of the human face is calculated for determining second specified resolution. A local region image is inputted to a second-stage key point detection model to obtain a final key point location coordinate. Different key points are detected corresponding to a third-stage key point detection model.
   USE - Human face key point locating method.
   ADVANTAGE - The method enables improving locating effect.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face key point locating device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face key point locating method.
PD CN106599830-A   26 Apr 2017   G06K-009/00   201732   Pages: 14   Chinese
   CN106599830-B   17 Mar 2020   G06K-009/00   202025      Chinese
UT DIIDW:201728110B
ER

PT P
PN CN105590102-A
TI Front vehicle face recognition method involves vehicle image corresponding to each vehicle type is obtained and recognized, using trained Softmax classifier for classification and recognition.
AB    NOVELTY - The method involves reading bayonet transportation vehicles collected image, obtaining front vehicle face image, and pre-processing the front vehicle face image. The convolutional neural network is constructed, and the front vehicle face image is inputted to the convolutional neural network for training. The front vehicle face image of the vehicle to be identified is obtained. The front vehicle face image is modularized. The vehicle image corresponding to each vehicle type is obtained and recognized, using the trained Softmax classifier for classification and recognition.
   USE - Front vehicle face recognition method.
   ADVANTAGE - The accuracy rate of front vehicle face recognition is improved, and the detection of vehicle track is performed efficiently.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the front vehicle face recognition method. (Drawing includes non-English language text)
PD CN105590102-A   18 May 2016   G06K-009/00   201639   Pages: 11   English
UT DIIDW:2016316877
ER

PT P
PN CN105023005-A; CN105023005-B
TI Human face recognition device, has human face recognition processor module connected with human face sample database, and host or terminal connected with storage device by performing human face recognition.
AB    NOVELTY - The device has a direct connecting device connected with a mainframe terminal. A human face image is captured by a dual camera. A storage device is connected with a human face recognition processor module, which is connected with a communication module. A visible light imaging head is connected with a near infrared camera. A human face recognition processor module is connected with a human face sample database unit. A host or terminal is connected with the storage device by performing human face recognition.
   USE - Human face recognition device.
   ADVANTAGE - The device is simple in structure, convenient, practical and easy to use, and has better safety performance, high reliability and wide range of applications, and increases working efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face recognition device. '(Drawing includes non-English language text)'
PD CN105023005-A   04 Nov 2015   G06K-009/00   201578   Pages: 11   Chinese
   CN105023005-B   07 Dec 2018   G06K-009/00   201901      Chinese
UT DIIDW:2015731109
ER

PT P
PN CN104408435-A
TI Convolution nerve network of quick pool human face identification method, involves establishing standard human face grey-scale map to obtain human face image, and calculating maximum element value to indentify human face image.
AB    NOVELTY - The method involves establishing a standard human face grey-scale map to obtain a human face image. Mean value and gauss distribute variance value of multi-pixel in the image is calculated. A convolution characteristic diagram is drawn to establish a characteristic matrix. A cascaded dimensional characteristic vector is calculated based on the matrix. A characteristic vector verification process is performed by a dimension softmax classifier to establish the classifier map. Maximum element value is calculated based on the classifier map to indentify human face image.
   USE - Convolution nerve network of quick pool human face identification method.
   ADVANTAGE - The method enables adopting a convolution nerve network to perform a quick pool human face identification process under high speed condition, and increasing identification accuracy rate and wide application range of pool.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a convolution nerve network of quick pool human face identification method.'(Drawing includes non-English language text)'
PD CN104408435-A   11 Mar 2015   G06K-009/00   201530   Pages: 11   Chinese
UT DIIDW:201526661T
ER

PT P
PN CN102968612-A
TI Bank identification system, has bank service server connected with human face recognition device and gesture identifying device for receiving face identifying result data and gesture recognition result data.
AB    NOVELTY - The system has a camera provided with a user terminal for transmitting face data and gesture data. A human face identifying device is connected with a network device to receive the human face data and output the human face recognition result data. A gesture recognition device is connected with the network device to receive the gesture data and output the gesture identification result data. A bank service server is connected with the human face recognition device and the gesture identifying device for receiving the face identifying result data and gesture recognition result data.
   USE - Bank identification system.
   ADVANTAGE - The system can identify the client identity in an efficient manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a bank identification method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a bank identification system.
PD CN102968612-A   13 Mar 2013   G06K-009/00   201345   Pages: 17   Chinese
UT DIIDW:2013J68706
ER

PT P
PN CN102800017-A
TI Human face identification checking system for use in bank, has background server database storing final checking result, and third party server reading needed information from client file database through communication interface.
AB    NOVELTY - The system has a second-generation identity card reader reading identity card number, picture and other related information and downloading the corresponding picture extracting human face biologic identification characteristic. A human face identifying module intercepts a human face image, extracts a picture of human face biological feature from the human face image and feeds back a check result to a client. A background server database stores a final checking result. A third party server reads the needed information from a client file database through a communication interface.
   USE - Human face identification checking system for use in bank.
   ADVANTAGE - The system improves the accuracy of client identity rate, greatly reduces the human risk of bank service, lays a solid foundation for sustainable development of the bank and provides self-collected information and automatic identification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an operating process involved in a human face identification checking system. '(Drawing includes non-English language text)'
PD CN102800017-A   28 Nov 2012   G06Q-040/02   201320   Pages: 9   Chinese
UT DIIDW:2013D25443
ER

PT P
PN WO2012094143-A1; US2012270654-A1; US8929609-B2
TI Gesture recognition apparatus for scaling gesture recognition to physical dimensions of user, has processing system that obtains at least one physical dimension of user and determines gesture of user based on obtained physical dimension.
AB    NOVELTY - The gesture recognition apparatus has a processing system configured to obtain at least one physical dimension of a user, and determine a gesture of the user based on the physical dimension independent of a location of the user relative to the gesture recognition apparatus. The processing system is further configured to identify at least one movement of the user, and determine the gesture of the user based also on the identified movement.
   USE - Gesture recognition apparatus used for scaling gesture recognition to physical dimensions of user.
   ADVANTAGE - Performs simultaneous scaling of user gestures for multiple users to improve gesture input experience for multiple users. Provides a gesture recognition apparatus that can use any other node that can be remotely controlled by gestures, such as a computer with gesture recognition capability to reduce or eliminate the need for traditional keyboard and mouse setup, a robotic system capable of gesture recognition, personal computing device, e.g. laptop computer, personal computer (PC), an entertainment device, e.g. game console, digital medial player, television, sign language recognition systems, facial gesture recognition systems, or any other suitable node responsive to input methods other than traditional touch pointing device, and speech.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a gesture recognition method;
   (2) a computer program product used in executing gesture recognition method; and
   (3) a game console for gesture recognition.
   DESCRIPTION OF DRAWING(S) - The drawing is a flow diagram showing the process for scaling gesture recognition.
   Remote system communication process (210)
   User for input prompting process (214)
   Physical dimensional obtaining process (218)
   User movement identification process (222)
   Scale calibration process (226)
   User gesture determination process (230)
   User gesture information storing process (234)
PD WO2012094143-A1   12 Jul 2012   G06F-003/01   201248   Pages: 36   English
   US2012270654-A1   25 Oct 2012   G06F-003/01   201270      English
   US8929609-B2   06 Jan 2015   G06K-009/00   201509      English
UT DIIDW:2012H96052
ER

PT P
PN US2012158700-A1; CN102609434-A; US8341145-B2; CN102609434-B
TI Computer-readable storage media storing program for indexing and identifying faces based on visual and social data, includes instructions for storing main face in database in association with main text strings and social data.
AB    NOVELTY - The image of a main face is analyzed to produce a vector that quantifies features of main face. The main text strings are created that represent vector. The social data (118) is obtained that concerns main photo in which main face appears. The main face is associated with main text strings and social data. The main face is stored in a database in association with main text strings and social data.
   USE - Computer-readable storage media storing program for indexing and identifying faces based on visual and social data.
   ADVANTAGE - The face recognition technology can allow the people appear in photos to be identified automatically in efficient manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) method for indexing and identifying faces based on visual and social data; and
   (2) system for identifying main face based on visual and social data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the process for indexing and identifying faces based on visual and social data.
   Social data (118)
   Step for categorizing face (204)
   Step for creating vector (206)
   Step for creating text string (208)
   Locality-sensitive hash (218)
PD US2012158700-A1   21 Jun 2012   G06F-017/30   201242   Pages: 12   English
   CN102609434-A   25 Jul 2012   G06F-017/30   201263      Chinese
   US8341145-B2   25 Dec 2012   G06F-007/00   201302      English
   CN102609434-B   25 Nov 2015   G06F-017/30   201603      English
UT DIIDW:2012H05644
ER

PT P
PN US8036431-B1
TI Method for identification and verification of identity of potential crime suspect to law enforcement professional, involves receiving data relating to processed fingerprint images, and displaying data received on display of handheld device.
AB    NOVELTY - The method involves capturing an image of a fingerprint of a person at a portable handheld device, and enhancing the fingerprint image. The fingerprint image that satisfy a predetermined fingerprint quality level to a central processor for processing. The transmitted fingerprint images are processed to determine if there is matching fingerprint information in a central data storage. Data relating to the processed fingerprint images is received from the processor at the handheld device. The data received on a display (13) of the handheld device is displayed.
   USE - Method for identification and verification of the identity of a person i.e. potential crime suspect, to law enforcement professional.
   ADVANTAGE - The data relating to the processed fingerprint images is received from the processor at the handheld device, and the data received on the display of the handheld device is displayed, thus allows law enforcement professionals to perform immediate identity and background checks on the individual while in the field. The method allows the portable handheld device to be operated singled handedly, while eliminating the problems caused by direct exposure of a finger-receiving surface to bright ambient lighting. The method provides both finger alignment and core position location, thus reducing the workload of the investigator while capturing the fingerprint, and hence allowing closer monitoring of the person being fingerprinted. The method allows capture of latent fingerprints at crime scenes, thus reducing investigation time. The method allows displaying and recording facial and incident scene images in conditions ranging from well lit to total darkness, recording and playing back incident scene audio information for incident description, and contacting voice recordings for identification.
   DETAILED DESCRIPTION - The processor is a Pentium (RTM: Series of x86-compatible microprocessors) processor. INDEPENDENT CLAIMS are also included for the following:
   (1) a portable apparatus for identification and verification of a fingerprint
   (2) a portable system for identification and verification of the fingerprint.
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective view of a portable handheld device with an ergonomic design of a single hand operation handle.
   Keypad (4)
   Display (13)
   Ergonomic grip (15)
   Primary switches (24)
   Switch (25)
PD US8036431-B1   11 Oct 2011   G06K-009/00   201169   Pages: 15   English
UT DIIDW:2011M69976
ER

PT P
PN US2011221936-A1; US8135184-B2
TI Portable image acquisition and processing device for e.g. portable digital camera, for automatically detecting and correcting face region defects within e.g. preview video, has processor correcting image defects based on optimal combination.
AB    NOVELTY - The device has a processor acquiring (105) a digital image and applying an image analysis prefilter (130) to preview or reference images of approximately same scene as the image. The processor interprets results obtained by applying the prefilter to determine an optimal correction strategy for the digital image, and determines an optimal combination of corrective image processing and filter adaptation based on the strategy. The processor corrects image defects based on the optimal combination by detecting and correcting defects within a specific region of interest within the digital image.
   USE - Portable image acquisition and processing device for use with a digital appliance e.g. portable digital camera, for automatically detecting and correcting face region defects within a digital image such as preview video, pre-exposed image and post-exposed image. Can also be used for a hand-held personal digital assistant (PDA), a mobile phone and a printer.
   ADVANTAGE - The device eliminates noise, color shifts, incorrect exposure, blur and over sharpening before performing the detection process, thus improving performance and success rate of the device, while ensuring that criteria for detection can be tightened and narrowed down, and hence providing higher accuracy both in the positive detection and reduction in the false detection without the need to modify the image, while ensuring accurate face detection and location results with better image quality.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a processor-readable media having instructions for detecting and correcting multiple image defects within a digital image
   (2) a method for detecting and correcting multiple image defects within a digital image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a redeye detection system.
   Digital camera (100)
   Step for acquiring digital image by the processor (105)
   Image subsampler (112)
   Image analysis prefilter (130)
   Image store (170)
PD US2011221936-A1   15 Sep 2011   H04N-005/217   201163   Pages: 29   English
   US8135184-B2   13 Mar 2012   G06K-009/00   201219      English
UT DIIDW:2011L84138
ER

PT P
PN CN102034288-A; CN102034288-B
TI Entrance guard system, has data processor comparing classification melt processed signal with predetermined sample database, and entrance guard controller that opens electric latch when processed signal is coupled with signal in database.
AB    NOVELTY - The system has an entrance guard controller transmitting a received sound signal and a face image signal to a data processor. The data processor extracts the characteristics of the received sound signal and the face image signal. The extracted corresponding signal is unified and made to perform a classification melting process. A data processor compares the classification melt processed signal with a predetermined sample database. The entrance guard controller opens the electric latch when the classification melt processed signal is coupled with the signal in a sample database.
   USE - Entrance guard system.
   ADVANTAGE - The system ensures identification reliability, and has high accuracy, and is convenient, safe and reliable to use.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an entrance guard system.'(Drawing includes non-English language text)'
PD CN102034288-A   27 Apr 2011   G07C-009/00   201164   Pages: 10   Chinese
   CN102034288-B   20 Jun 2012   G07C-009/00   201272      Chinese
UT DIIDW:2011M35061
ER

PT P
PN US2010080483-A1; EP2169614-A1; JP2010079909-A; KR2010035613-A; JP4987053-B2; EP2169614-B1; US8260002-B2; ES2391418-T3; KR1375555-B1
TI Video analytics e.g. motion detection, processing method, involves scattering pixels of interest into image after processing according to predetermined binary mask, where gathering and scattering of pixels are performed independently.
AB    NOVELTY - The method involves gathering multiple pixels of interest from an image according to a predetermined binary mask, where the mask defines a location of the pixels of interest on the image. The pixels of interest are arranged in a pixel matrix. Video analytics processing is performed on the pixel matrix using single instructions multiple data (SIMD) instructions on a data processor. The pixels of interest are scattered into the image after processing according to the mask, where gathering and scattering of the pixels of interest are performed independently.
   USE - Method for processing video analytics e.g. motion detection. Can also be used for facial recognition, object recognition, background/foreground separation and pattern analysis.
   ADVANTAGE - The video analytics processing is performed on the pixel matrix, and the pixels are scattered using the predetermined binary mask, thus allowing multiple pixels to be processed simultaneously, and hence increasing overall performance. The method enables performing low-level video analytics processing and divides the processing into three phases in order to efficiently use single instructions multiple data (SIMD) instructions of modem data processors. The filter kernel is applied to the pixel matrix after the pixels of interest are gathered in the gathering phase, thus improving filtering efficiency. The network camera performing video analytics reduces the work load of a centralized image processing system, and conserves valuable network bandwidth. The method enables significantly reducing processing demands placed on the CPU processing the image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer-readable medium storing a set of instructions to perform a video analytics processing method
   (2) a network camera comprising a lens.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating morphological dilation.
PD US2010080483-A1   01 Apr 2010   G06K-009/40   201024      English
   EP2169614-A1   31 Mar 2010   G06T-001/20   201024      English
   JP2010079909-A   08 Apr 2010   G06T-001/20   201025   Pages: 15   Japanese
   KR2010035613-A   05 Apr 2010   G06T-007/00   201028      
   JP4987053-B2   25 Jul 2012   G06T-001/20   201249   Pages: 16   Japanese
   EP2169614-B1   01 Aug 2012   G06T-001/20   201250      English
   US8260002-B2   04 Sep 2012   G06K-009/00   201259      English
   ES2391418-T3   26 Nov 2012   G06T-001/20   201326      Spanish
   KR1375555-B1   27 Mar 2014   G06T-007/00   201424      
UT DIIDW:2010D75327
ER

PT P
PN CN101639894-A; CN101639894-B
TI Train driver's behavior and fatigue state detection method, involves calculating proportion i.e. PERCLOS value, of time of closed eyes and total testing time in real-time to represent fatigue state.
AB    NOVELTY - The method involves detecting the face in an image collected by a camera to obtain a face image and position coordinate to determine whether a driver is in better position and condition to drive. The eyes of the driver are determined to be open when the open eye image is searched. The eyes of the driver are determined to be closed when the open eye image is not obtained. Fatigue state of the driver is determined, and a proportion i.e. PERCLOS value, of time of the closed eyes is calculated, and total testing time is determined in real-time to represent the fatigue state.
   USE - Method for detecting behavior and fatigue state of a train driver through online.
   ADVANTAGE - The method allows effective monitoring and detection of behavior and fatigue state of a train driver through online.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a detection system for implementing behavior and fatigue state detection method, comprising a camera.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating train driver's behavior and fatigue state detection method.'(Drawing includes non-English language text)'
PD CN101639894-A   03 Feb 2010   G06K-009/00   201015   Pages: 15   Chinese
   CN101639894-B   20 Mar 2013   G06K-009/00   201338      Chinese
UT DIIDW:2010B59809
ER

PT P
PN US2009060288-A1; US7587070-B2
TI Digital facial image matching system for use in e.g. wireless mobile telephone, has wireless network performs transmission of image between communication device and classification server, and output module partitioned into device outputs.
AB    NOVELTY - The system has a mobile communication device generating a digital facial image of an individual and for wirelessly transmitting a digital facial image. An image classification server receives the digital facial image from the mobile communication device. A wireless network performs transmission of the image between the mobile communication device and the image classification server. An output module is partitioned into wireless device outputs, sender`s web page output and a voting web page (250). A database feature vector matches with a primary feature vector to create matched images.
   USE - System for matching a digital facial image of an individual with an image of a celebrity by using a digital camera of a wireless communication device e.g. wireless mobile telephone and personal digital assistant (PDA), which is utilized by a government agency such as department of homeland security (DHS) and department of motor vehicles (DMV) for detecting terrorists, suspected cases of identity fraud, automating border and passport control and correcting mistakes in a facial image database.
   ADVANTAGE - The system reduces complexity of the digital image data into a set of variables that represent features of the image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for weighting feature vectors for images
   (2) a method for matching images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a screen shot of a web page with a match published for voting to obtain human perception data.
   Voting web page (250)
   Published match (255)
PD US2009060288-A1   05 Mar 2009   G06K-009/00   200920   Pages: 23   English
   US7587070-B2   08 Sep 2009   G06K-009/00   200959      English
UT DIIDW:2009F60578
ER

PT P
PN US2009025022-A1; US8031272-B2
TI Display view angle adjusting method for e.g. flat screen TV, involves determining location of multiple viewers, calculating optimal viewing location of multiple viewers and adjusting display based on optimal viewing location.
AB    NOVELTY - The method involves determining a location of multiple viewers, e.g. by facial recognition, thermal detection, or some other means, and calculating an optimal viewing location for multiple viewers. A display (30) e.g. TV monitor, is adjusted based on the optimal viewing location of multiple viewers, where adjustment e.g. vertical adjustment, of the display is performed by determining the location of multiple viewers with respect to a default position. An average angle or distance of multiple viewers is calculated from the display, where the display is adjusted to the default position.
   USE - Method for adjusting a viewing angle of a display. Uses include but are not limited to a TV monitor, a computer monitor, a static image display i.e. advertisement display, a digital picture frame display and a cash register display.
   ADVANTAGE - The method allows the viewing angle of the display to be automatically adjusted based on the average viewing location of the viewers.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for deploying an application for adjusting a viewing angle of a display
   (2) a computer program product comprising a computer usable medium for adjusting a viewing angle of a display.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a computing environment.
   Environment (10)
   Computer infrastructure (12)
   Computing device (14)
   Processor (20)
   Memory (22a)
   Sensor (25)
   Input/output device (28)
   Display (30)
PD US2009025022-A1   22 Jan 2009   H04H-009/00   200912   Pages: 17   English
   US8031272-B2   04 Oct 2011   H04N-005/64   201165      English
UT DIIDW:2009E16685
ER

PT P
PN US2009021602-A1; CN101350883-A; JP2009027259-A; JP4702635-B2; JP2011139481-A; US8045014-B2; JP5214749-B2; CN101350883-B
TI Auto white balance (AWB) correction value calculation method for image pickup device, involves calculating total AWB correction value in accordance with face AWB and normal AWB correction value, based on comparison result of feature data.
AB    NOVELTY - The method involves inputting image data and calculating a normal auto white balance (AWB) correction value based on the inputted image data. A face area is identified from the image data. The face AWB correction value is calculated based on the face area of the image data. The feature data from the image data and feature data from image data in the face area are extracted. The total AWB correction value in accordance with the face AWB correction value and the normal AWB correction value is calculated based on a comparison result of the feature data.
   USE - Auto white balance correction value calculation method for image pickup device (claimed).
   ADVANTAGE - The erroneous white balance adjustment can be prevented efficiently when correcting the white balance of face area even when the face without normal skin color is identified.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) Auto white balance correction value calculation device; and
   (2) image pickup device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the image signal processing circuit.
   Face area identification unit (52a)
   Face AWB correction value calculation unit (52b)
   Face AWB feature data extraction unit (52c)
   Feature data comparison unit (52f)
   Total AWB correction value calculation unit (52g)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The image data are compressed according to JPEG format.
PD US2009021602-A1   22 Jan 2009   H04N-009/73   200909   Pages: 67   English
   CN101350883-A   21 Jan 2009   H04N-001/60   200912      Chinese
   JP2009027259-A   05 Feb 2009   H04N-009/73   200915   Pages: 39   Japanese
   JP4702635-B2   15 Jun 2011   H04N-009/73   201140   Pages: 37   Japanese
   JP2011139481-A   14 Jul 2011   H04N-009/73   201147   Pages: 38   Japanese
   US8045014-B2   25 Oct 2011   H04N-009/73   201170      English
   JP5214749-B2   19 Jun 2013   H04N-009/73   201344   Pages: 38   Japanese
   CN101350883-B   04 Sep 2013   H04N-001/60   201378      Chinese
UT DIIDW:2009E17713
ER

PT P
PN CN101320484-A; CN101320484-B
TI Face virtual images generating method for computer vision, involves locating two-dimensional face images according to two-dimensional face shape models and local texture models, and reconstructing two-dimensional face images.
AB    NOVELTY - The method involves obtaining two-dimensional face shape models by utilizing multiple subspaces of two-dimensional face image in a preset database. Two-dimensional face local texture models are obtained from local textures of the two-dimensional face images. The two-dimensional face images are accurately located according to the two-dimensional face shape models and the local texture models. The two-dimensional face images are reconstructed three-dimensionally. Virtual images with variable postures are obtained by processing illumination model treatment for three-dimensional face images.
   USE - Method for generating face virtual images for computer vision and pattern recognition.
   ADVANTAGE - The method increases postures of the images and sample spaces of illumination variation, improves speed for three-dimensional reconstruction, thus recognizing face images with high efficiency and recognition rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for three-dimensional face recognition based on a full automatic face location.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a face virtual images generating method.'(Drawing includes non-English language text)'
PD CN101320484-A   10 Dec 2008   G06T-017/00   200903   Pages: 26   Chinese
   CN101320484-B   04 Jan 2012   G06T-017/00   201208      Chinese
UT DIIDW:2009A48109
ER

PT P
PN EP1973064-A2; JP2008234653-A; CN101271515-A; US2008253664-A1; EP1973064-A3; JP5256806-B2; US8660317-B2; CN101271515-B
TI Object e.g. face, image detection device, has image rotation unit for rotating object image classification unit based on detected orientation of input image, and detection unit detecting object images.
AB    NOVELTY - The device (10) has an object image classification unit for determining whether the object images are included in an image having predetermined orientation. An image orientation detection unit detects orientation of an input image (101), and an image rotation unit rotates the object image classification unit based on the detected orientation of the input image. A detection unit detects the object images from the input image by using the rotated object image classification unit.
   USE - Device for detecting a set of object images such as face and car, from an input image.
   ADVANTAGE - The device rapidly detects the inclined object image from the input image without great deal of computation, and receives face image signals having different orientation from the image having normal orientation.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for detecting a set of object images from an input image
   (2) a program product comprising instructions to perform a method for detecting a set of object images from an input image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an object image detection device.
   Object image detection device (10)
   Input image (101)
   Image orientation detector (110)
   Reference image classification unit (120)
   Object image detector (140)
   Image output unit (150)
PD EP1973064-A2   24 Sep 2008   G06K-009/32   200865   Pages: 19   English
   JP2008234653-A   02 Oct 2008   G06T-001/00   200866   Pages: 14   Japanese
   CN101271515-A   24 Sep 2008   G06K-009/00   200868      Chinese
   US2008253664-A1   16 Oct 2008   G06K-009/70   200869      English
   EP1973064-A3   12 Jan 2011   G06K-009/32   201105      English
   JP5256806-B2   07 Aug 2013   G06T-001/00   201352   Pages: 14   Japanese
   US8660317-B2   25 Feb 2014   G06K-009/00   201415      English
   CN101271515-B   19 Mar 2014   G06K-009/00   201432      Chinese
UT DIIDW:2008L03743
ER

PT P
PN CN1932840-A; CN100403331-C
TI Multi-modal biological characteristic identification system based on iris and human face.
AB    NOVELTY - One system to identify the multi-mode biologic characteristic status bases on iris and person face. The biology character collection unit gathers pictures of iris and person face by two absolute gather channels and its collection sustaining flat roof makes users with different stature to input iris and person face pictures freely and cozily. The biology character identify unit uses picture disposing and Wavelet transform technology to get character template and to calculate the matching percent of iris characteristic template and mesh degree of person face character template, and get the finalidentify result by data fusion method. The biology character data bank storage the final inborn character template and congruence iris character template on person face character template to keep theprivacy of biology character data and to increase the security of system. It combines biology character, mode identification, data amalgamation and computer technology to identify people figure withlow error rate for iris identify and person face identify.
PD CN1932840-A   21 Mar 2007   G06K-009/00   200749      Chinese
   CN100403331-C   16 Jul 2008   G06K-009/00   200864      Chinese
UT DIIDW:2007498447
ER

PT P
PN US2007047775-A1; EP1760635-A1; JP2007065766-A; CN1924897-A; KR2007026080-A; EP1760635-B1; DE602006011990-E; JP4595750-B2; CN1924897-B; US8249310-B2; KR1217349-B1
TI Face image processing apparatus for personal recognition at home, has local-feature-value calculator calculating feature values at feature positions and mapping unit mapping feature values using predetermined mapping functions.
AB    NOVELTY - The apparatus has an image input unit for detecting a region of face image, and an arbitrary-direction face detector detecting a direction that a face in the face image. A face feature-position detector detects feature positions corresponding to features of the face. A local-feature-value calculator calculates feature values at the feature positions. A mapping unit maps the feature values using predetermined mapping functions. A face recognizer recognizes whether the face is a registered face, using the feature values and feature values registered in advance.
   USE - Used for processing a face image (claimed) of a person for personal recognition in a certain place such as home
   ADVANTAGE - The feature values are mapped to obtain data extracted from face images captured from the same direction, so that the amount of computation and the computation time are reduced compared with an existing method in which a face image is mapped to a three-dimensional model and the three-dimensional model is rotated in a predetermined direction. The apparatus therefore avoids the use the three-dimensional model, so that the problem of degradation in the accuracy of recognition caused by mismatch between the person relevant to recognition and the three-dimensional model can be avoided.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image processing method
   (2) a program for allowing a computer to execute processing for recognizing whether a detected face is a registered face.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a leaming process executed by an image processing apparatus.
PD US2007047775-A1   01 Mar 2007   G06K-009/00   200736   Pages: 22   English
   EP1760635-A1   07 Mar 2007   G06K-009/00   200736      English
   JP2007065766-A   15 Mar 2007   G06T-007/00   200736   Pages: 31   Japanese
   CN1924897-A   07 Mar 2007   G06K-009/00   200746      Chinese
   KR2007026080-A   08 Mar 2007   G06K-009/80   200755      
   EP1760635-B1   27 Jan 2010   G06K-009/00   201008      English
   DE602006011990-E   18 Mar 2010   G06K-009/00   201020      German
   JP4595750-B2   08 Dec 2010   G06T-007/00   201080   Pages: 30   Japanese
   CN1924897-B   13 Jul 2011   G06K-009/00   201172      Chinese
   US8249310-B2   21 Aug 2012   G06K-009/00   201256      English
   KR1217349-B1   31 Dec 2012   G06K-009/80   201305      
UT DIIDW:2007387320
ER

PT P
PN JP2005339425-A
TI Personal identification apparatus mounted in mobile phone, has camera that captures visible light image of user's face and infrared image of user's iris, for user authentication.
AB    NOVELTY - A camera (110) captures visible light image of user's face and infrared image of user's iris. An iris authentication unit (120) authenticates the user based on the captured visible light image and a face authentication unit (130) authenticates the user based on the captured infrared image.
   USE - Personal identification apparatus including camera with visible light image and infrared image capturing function, mounted in mobile phone, personal digital assistant (PDA).
   ADVANTAGE - Miniaturization of the personal identification apparatus is possible, as single camera is used for imaging visible light image and infrared image.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the personal identification apparatus. (Drawing includes non-English language text).
   personal identification apparatus (100)
   camera (110)
   iris authentication unit (120)
   face authentication unit (130)
   control unit (150)
   interface (160)
PD JP2005339425-A   08 Dec 2005   G06T-001/00   200605   Pages: 10   Japanese
UT DIIDW:2006041425
ER

PT P
PN US2005157908-A1; JP2005202653-A; JP4481663-B2; US8094881-B2
TI Action recognition apparatus for person authentication in building, learns state/action of identified person by associating actions with specific meaning respective to identified person.
AB    NOVELTY - A recognition unit (3) identifies a person based on the input image data. A state detector (4) detects the state/action of the identified person from the image data. A learning unit (5) learns the detected state/action of the person, by associating actions with specific meaning respective to the person.
   USE - For recognizing action related to body parts such as face, eye, arm, hand and foot of person during authentication, in ceiling wall, door, building.
   ADVANTAGE - Enables to identify the person with high accuracy, even when the person is positioned far away from the lighting conditions.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) moving object recognition apparatus;
   (2) device control apparatus;
   (3) action recognition method;
   (4) moving object recognition method;
   (5) device control method;
   (6) action recognition program;
   (7) moving object recognition program; and
   (8) device control program.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the action recognition apparatus.
   image input unit (1)
   recognition unit (3)
   tate detector (4)
   learning unit (5)
   processing control unit (6)
PD US2005157908-A1   21 Jul 2005   G03B-003/10   200555   Pages: 25   English
   JP2005202653-A   28 Jul 2005   G06T-007/20   200555   Pages: 23   Japanese
   JP4481663-B2   16 Jun 2010   G06T-007/20   201039   Pages: 24   Japanese
   US8094881-B2   10 Jan 2012   G06K-009/00   201205      English
UT DIIDW:2005540748
ER

PT P
PN FR2860629-A1; WO2005034018-A1; EP1668564-A1; KR2006069874-A; CN1856795-A; US2007052959-A1; JP2007508610-W; IN200601102-P4; CN1332238-C; RU2321887-C2; KR760669-B1; US7414737-B2; EP1668564-B1; DE602004017757-E; ES2317058-T3; JP4335920-B2; IN230696-B
TI User face positioning device for use with identification device, has horizontal and vertical polarizers permitting to define two optical paths between each eye of user and corresponding positioning location.
AB    NOVELTY - The device has a frame (1) with two positioning locations (5a, 5b) each arranged to be visible by an eye of a user when he is correctly positioned. Horizontal and vertical polarizers permit to define two optical paths between each eye of the user and the corresponding positioning location. The optical paths are optically separated from each other with respect to the eyes of the user.
   USE - Used for positioning face of a user before an identification device that performs recognition of the face or eyes of the user.
   ADVANTAGE - The device is capable of permitting an accurate positioning of the face of the user.
   DESCRIPTION OF DRAWING(S) - The drawing shows a top schematic view of a positioning device.
   Frame (1)
   Positioning locations (5a, 5b)
   Reflecting prism (7)
   Reflecting prism face (9)
   Cameras (100)
PD FR2860629-A1   08 Apr 2005   G06K-009/32   200528   Pages: 17   French
   WO2005034018-A1   14 Apr 2005   G06K-009/00   200528      French
   EP1668564-A1   14 Jun 2006   G06K-009/00   200641      French
   KR2006069874-A   22 Jun 2006   G06K-009/00   200675      
   CN1856795-A   01 Nov 2006   G06K-009/00   200720      Chinese
   US2007052959-A1   08 Mar 2007   G01J-004/00   200720      English
   JP2007508610-W   05 Apr 2007   G06T-001/00   200726   Pages: 12   Japanese
   IN200601102-P4   17 Aug 2007   G06K-009/00   200780      English
   CN1332238-C   15 Aug 2007   G02B-027/32   200812      Chinese
   RU2321887-C2   10 Apr 2008   G06K-009/00   200828      Russian
   KR760669-B1   20 Sep 2007   G06K-009/00   200839      
   US7414737-B2   19 Aug 2008   G01B-011/14   200857      English
   EP1668564-B1   12 Nov 2008   G06K-009/00   200877      French
   DE602004017757-E   24 Dec 2008   G06K-009/00   200907      German
   ES2317058-T3   16 Apr 2009   G06K-009/00   200927      Spanish
   JP4335920-B2   30 Sep 2009   G06T-001/00   200965   Pages: 8   Japanese
   IN230696-B   27 Mar 2009   G06K-009/00   200982      English
UT DIIDW:2005265734
ER

PT P
PN WO2004006570-A1; AU2003242940-A1; BR200305349-A; EP1522187-A1; KR2005016973-A; JP2005531908-W; CN1666505-A; US2006062424-A1; IN200402966-P4; CN1331349-C; EP1522187-B1; DE60331916-E
TI Movie ambient light controlling method, involves receiving video signal, presenting video signal by presentation device, analyzing by expression recognition and setting property of ambient light in proximity of presentation device.
AB    NOVELTY - The method involves receiving a video signal by a receiver. The video signal is presented by a presentation device (110). The video signal is analyzed by face and expression recognition. A property of the ambient light is set based upon the analyzed video signal. The property of the ambient light is set in proximity of the presentation device.
   USE - Used for controlling an ambient lighting in movie.
   ADVANTAGE - The property of an ambient light is set in proximity of the presentation device, thereby enhancing experience of main data by ambient light. The video signal is analyzed by expression recognition, thereby improving the setting of the ambient light.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a lighting unit.
   DESCRIPTION OF DRAWING(S) - The drawing shows an ambient light environment.
   Light units (102, 104, 106, 108, 112)
   Presentation device (110)
   Couch (114)
   Chair (120)
   Table (122)
PD WO2004006570-A1   15 Jan 2004   H04N-005/57   200411   Pages: 17   English
   AU2003242940-A1   23 Jan 2004   H04N-005/57   200459      English
   BR200305349-A   05 Oct 2004   H04N-005/57   200475      
   EP1522187-A1   13 Apr 2005   H04N-005/57   200525      English
   KR2005016973-A   21 Feb 2005   H04N-005/57   200544      
   JP2005531908-W   20 Oct 2005   H05B-037/02   200569   Pages: 12   Japanese
   CN1666505-A   07 Sep 2005   H04N-005/57   200608      Chinese
   US2006062424-A1   23 Mar 2006   G06K-009/00   200622      English
   IN200402966-P4   28 Sep 2007   H04N-005/57   200805      English
   CN1331349-C   08 Aug 2007   H04N-005/57   200812      Chinese
   EP1522187-B1   31 Mar 2010   H04N-005/57   201023      English
   DE60331916-E   12 May 2010   H04N-005/57   201032      German
UT DIIDW:2004109125
ER

PT P
PN EP1136947-A2; JP2001268346-A; US2001040980-A1; CN1324058-A; KR2001089182-A; US6885755-B2; CN1172268-C; EP1136947-B1; DE60126698-E; DE60126698-T2; JP4495824-B2; EP1136947-A3; KR378911-B1
TI Information processing method, involves superimposing modulated pattern image information on the main image information to form synthetic image information.
AB    NOVELTY - Main image information recognizable by the naked eye is prepared, after which pattern image information are prepared. Color difference modulation is executed for the pattern image information based on a preset color difference modulation value. The modulated pattern image information is then superimposed on the main image information to form synthetic image information.
   USE - For sensing the sub image information from the recorded synthetic image information.
   ADVANTAGE - Enables issuance of an ID card bearing a face image which is for identification use and which provides a high degree of security. Maintains the high quality of main image since the information used for authenticity determination is embedded in a state that cannot be recognized to the naked eye.
   DETAILED DESCRIPTION - The resulting synthetic image information is recorded as a visual image into a recording medium.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart illustrating the overall flow of the information processing method.
PD EP1136947-A2   26 Sep 2001   G06T-001/00   200167   Pages: 26   English
   JP2001268346-A   28 Sep 2001   H04N-001/387   200172   Pages: 14   Japanese
   US2001040980-A1   15 Nov 2001   G06K-009/00   200172      English
   CN1324058-A   28 Nov 2001   G06T-005/00   200219      Chinese
   KR2001089182-A   29 Sep 2001   H04N-005/913   200220      
   US6885755-B2   26 Apr 2005   G06K-009/00   200528      English
   CN1172268-C   20 Oct 2004   G06T-005/00   200615      Chinese
   EP1136947-B1   21 Feb 2007   G06T-001/00   200717      English
   DE60126698-E   05 Apr 2007   G06T-001/00   200726      German
   DE60126698-T2   25 Oct 2007   G06T-001/00   200772      German
   JP4495824-B2   07 Jul 2010   H04N-001/387   201045   Pages: 19   Japanese
   EP1136947-A3   26 Nov 2003   G06T-001/00   201731      English
   KR378911-B1   07 Apr 2003   H04N-005/913   200353      
UT DIIDW:2001591579
ER

PT P
PN TW445434-A; US6600830-B1
TI System and method for automatically extracting facial features - simplify the extracting process and reduce the cost.
AB    NOVELTY - A system for automatically extracting facial features is provided to analyze a face image and find the featured portion of the face. In the pre-process, a second-chance region growing method is used to determine the facial area data. In the feature extraction, the left eye, right eye and mouth are first extracted, so as to locate the other featured portion. In the process for searching featured points, a simple featured sample can be used to calculate the cost function for each point, and the genetic algorithm can be used to speed up the task of searching the featured points.
PD TW445434-A   11 Jul 2001   G06T-011/80   200233      Chinese
   US6600830-B1   29 Jul 2003   G06K-009/00   200354      English
UT DIIDW:2002290667
ER

PT P
PN JP2000132688-A
TI Facial organs position detection procedure for use in security system, involves comparing probability distribution of position of facial organs in sample image with that of standard image data.
AB    NOVELTY - The pixel position corresponding to each facial organ in an input image is calculated using a template matching technique. The normal probability distribution of the position of the facial organs in the input image is compared with that of the standard image data. Weighted mean of facial organs position is applied to the calculated positional data to obtain the accurate position of the facial organs.
   USE - For use in security system for determining position of facial organs such as eyes, nasal opening in image of person for identification.
   ADVANTAGE - For facial organs such as eye, nasal opening, mouth opening. Extracts position of eye and mouth opening accurately.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for the facial organs position detection apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of facial organ detector.
PD JP2000132688-A   12 May 2000   G06T-007/00   200034   Pages: 7   Japanese
UT DIIDW:2000392094
ER

PT P
PN US6002782-A
TI Automatic face recognition method for enabling entrance to secured areas.
AB    NOVELTY - A 2D image of unidentified person (20) is aligned with stored 3D model of person until any three preset points such as outer corners of eyes and center of mouth have same coordinates in 2D and 3D images. After alignment, 2D image model is generated from stored 3D model and correlation between models is identified. Unidentified person is recognized by correlation value exceeding preset threshold.
   USE - For enabling entrance in secured areas and also for identification of parts in industries.
   ADVANTAGE - The two-dimensional model generated from three-dimensional model has very high correlation to access image due to transformation in the viewing direction and thus gain access to a restricted area is raised by reducing occurrence of recognition errors.
   DESCRIPTION OF DRAWING(S) - The figure shows electronic system which recognizes face of a person by generating two-dimensional image of the face from transformed three-dimensional model.
   Unidentified person (20)
PD US6002782-A   14 Dec 1999   G06K-009/00   200008   Pages: 12   English
UT DIIDW:2000096192
ER

PT P
PN US5960099-A
TI Creation method of digital likeness of children using computer to produce personalized figures like dolls.
AB    NOVELTY - Front and side view photographs of the child are taken and image data is transmitted to a computer. Coordinate grids are superimposed on the images, and coordinate points in grid for various facial features are identified using geometric figures. Personalized image is generated by determining facial features of the images.
   USE - To manufacture 3D sculpture such as action figure, doll or for printing characters in story book given as gift to children.
   ADVANTAGE - Easy creation and manipulation of facial features like smile and frown are enabled. The created image can be reproduced on flat surface like paper, plastic or used to form a sculpture automatically using computer aided manufacture system. Realistic appearance can be generated.
   DETAILED DESCRIPTION - The facial features are determined by applying different mathematical formulas and equations. The grid superimposition process involves aligning and moving the coordinate grid for aligning origin of first grid with nose tip of the side view image and aligning another grid such that its origin coincides with nose tip of the front view image. 3D grid is produced from the two coordinate grids for manufacturing a 3D figure using a rectangular mold, by computer aided manufacture system.
   DESCRIPTION OF DRAWING(S) - The figure shows the elevation view showing determination of coordinates of facial features.
PD US5960099-A   28 Sep 1999   G06K-009/00   199952   Pages: 16   English
UT DIIDW:1999609399
ER

PT P
PN EP849699-A2; JP10232938-A; US6185337-B1; EP849699-B1; DE69730811-E; JP3894522-B2; EP849699-A3
TI Image recognition system for recognising object from global image - by extracting global characteristic of input global image and evaluating its consistency, then analytically processing several local images.
AB       The system includes a global image processing device which performs analytical processing on a global image. It has a device for extracting a global characteristic of an input global image and a device for evaluating consistency of the extracted global characteristic. A local image processor performs analytical processing on several local images. The local image processor is made up from several local modules corresponding respectively to each local image.
   Each local module includes a device for extracting a characteristic of an input local image and a device for evaluating consistency of the extracted characteristic with the image to be recognised. The global image processor receives both an input image and inputs from the local modules and deactivates functions of the local modules which are inconsistent with the global characteristic while activating functions of the local modules which are consistent with the global characteristic.
   USE -   Approximates biological vision system. Operates on image from camera etc. For recognising e.g. human faces.
   ADVANTAGE -   Reduces computational burden on computer. Increases speed of image recognition.
PD EP849699-A2   24 Jun 1998   G06K-009/00   199829   Pages: 20   English
   JP10232938-A   02 Sep 1998   G06T-007/00   199845   Pages: 15   Japanese
   US6185337-B1   06 Feb 2001   G06K-009/70   200109      English
   EP849699-B1   22 Sep 2004   G06K-009/00   200462      English
   DE69730811-E   28 Oct 2004   G06K-009/00   200471      German
   JP3894522-B2   22 Mar 2007   G06T-007/00   200723   Pages: 22   Japanese
   EP849699-A3   12 Jan 2000   G06K-009/00   201735      English
UT DIIDW:1998324943
ER

PT P
PN CN107545245-A
TI Method for estimating age, involves inputting to-be-tested face image into training-completed depth convolution neural network to obtain age estimation result of to-be-tested face image.
AB    NOVELTY - The method involves obtaining (S1) a well-balanced age distribution database including the face image and its corresponding age. A deep convolutional neural network is constructed (S2) with residual structure for age estimation. The deep convolution neural network is trained (S3) with a residual structure by using the face age database with balanced distribution of age and a migration learning method. A to-be-tested face image is inputted (S4) into the training-completed depth convolution neural network to obtain an age estimation result of the to-be-tested face image.
   USE - Method for estimating age.
   ADVANTAGE - The method obtains a high degree of age estimation result.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) device for estimating age; and
   (2) non-transitory computer readable storage storing program for estimating age.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the age estimation method. (Drawing includes non-English language text)
   Step for obtaining the well-balanced age distribution database (S1)
   Step for constructing deep convolutional neural network with residual structure for age estimation (S2)
   Step for training deep convolution neural network with residual structure (S3)
   Step for inputting to-be-tested face image into the training-completed depth convolution neural network (S4)
PD CN107545245-A   05 Jan 2018   G06K-009/00   201806   Pages: 19   Chinese
UT DIIDW:201803704E
ER

PT P
PN US2017286773-A1; WO2017172782-A1; WO2017172790-A1; CA3018381-A1; AU2017245132-A1; CN109154993-A; EP3437031-A1; KR2019031431-A; JP2019513274-W; EP3437031-A4; US10565548-B2
TI Planogram assisted inventory method used in warehouse product inventory system, involves detecting and reading shelf labels to localize and associate possible products with both initial planogram and possible products in bounding box.
AB    NOVELTY - The method involves providing an initial planogram with identified products and creating a panoramic image including products on a shelf with possible products being surrounded by bounding boxes. The shelf labels are detected (610) and read to localize and associate possible products with both the initial planogram and the possible products in the bounding boxes. The possible products are identified with a product classifier. The features descriptors are extracted for comparison with possible products in the bounding boxes using vision based product recognition.
   USE - Planogram assisted inventory method used in retail or warehouse product inventory system (claimed).
   ADVANTAGE - The reduction in glare from products having highly reflective surfaces is permitted, since multiple cameras pointed in slightly different directions result in image with little or no glare. The data transfer requirements are reduced and permits operation even when local or cloud servers are not available. The rasterized delay of the rolling shutter reduces artifacts that occurs while the robot is traveling in its path. The overall accuracy of inventory monitoring is improved by use of prepared planograms and manual categorization or identification of products. The efficient and accurate product segmentation is facilitated by accounting for products with more than one facing.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a manually assisted robot inventory monitoring method for detecting and reading shelf labels using an autonomous robot; and
   (2) a inventory system inventory systems that use a planogram.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating how a product space for a realogram is created without requiring an initial planogram.
   Step for detecting shelf label (610)
   Step for comparing shelf label to depth map (612)
   Step for defining a bounding box (614)
   Step for associating image within bounding box with shelf label (616)
   Step for building training data set (618)
PD US2017286773-A1   05 Oct 2017   G06K-009/00   201768   Pages: 18   English
   WO2017172782-A1   05 Oct 2017   G06Q-010/00   201769      English
   WO2017172790-A1   05 Oct 2017   G06Q-010/00   201769      English
   CA3018381-A1   05 Oct 2017   G06Q-010/00   201866      English
   AU2017245132-A1   11 Oct 2018   G06Q-010/00   201867      English
   CN109154993-A   04 Jan 2019   G06K-017/00   201906      Chinese
   EP3437031-A1   06 Feb 2019   G06Q-010/00   201911      English
   KR2019031431-A   26 Mar 2019   G06Q-010/06   201924      
   JP2019513274-W   23 May 2019   G06Q-010/08   201938   Pages: 23   Japanese
   EP3437031-A4   27 Nov 2019   G06K-009/00   201992      English
   US10565548-B2   18 Feb 2020   G06Q-010/08   202015      English
UT DIIDW:201768417Q
ER

PT P
PN CN107194346-A
TI Automobile fatigue driving prediction method, involves obtaining eye depth visual characteristic model, obtaining closed state of testing video frame image eye, and obtaining alarm state when driver fatigue is arranged in fatigue state.
AB    NOVELTY - The method involves constructed an image pyramid. A primary candidate face window is obtained by a first-stage convolutional neural network (CNN1). An one-stage input is transmitted to a two-stage convolutional neural network (CNN2). A secondary candidate face window is obtained. A human face feature point predicting image is obtained by face feature points information. A human eye depth visual characteristic model is obtained. A closed state of a testing video frame image eye is obtained. An alarm state is obtained when a driver fatigue is arranged in a fatigue state.
   USE - Automobile fatigue driving prediction method.
   ADVANTAGE - The method enables detecting fatigue state of driver during various illumination and expression, and realizing detection result with high robustness so as to reduce illumination pose and expression factors with driver fatigue detection.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an automobile fatigue driving prediction method. '(Drawing includes non-English language text)'
PD CN107194346-A   22 Sep 2017   G06K-009/00   201773   Pages: 15   Chinese
UT DIIDW:201767079N
ER

PT P
PN CN105550671-A
TI Human face identification method involves extracting and comparing individual characteristic information with individual library template characteristic information so as to start relative application program.
AB    NOVELTY - The method involves detecting (S101) whether video frequency flow comprising human face image so as to determine effective identification of human face. The video flow image of single frame image is extracted to generate individual characteristic of head portrait, when effective identification satisfies living body identification condition. The individual characteristic information is extracted and compared with individual library template characteristic information. A relative application program is started, when similarity is larger than preset similarity threshold value.
   USE - Human face identification method.
   ADVANTAGE - The influence of external environment during the human face identification operation is effectively removed, so that the user identity accuracy for judging human face is increased. Hence the human face identification rate is increased.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a human face identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the human face identification process. (Drawing includes non-English language text)
   Step for detecting whether video frequency flow comprising human face image (S101)
PD CN105550671-A   04 May 2016   G06K-009/00   201638   Pages: 19   English
UT DIIDW:201630897V
ER

PT P
PN CN105404877-A
TI Multi-task learning human face property prediction method based on depth of learning, involves deploying training process to obtain nerve network module, and using nerve network model for prediction of human face attribute of image.
AB    NOVELTY - The human face image is collected and is marked (S1) corresponding to multi-attribute of classes, and the human face detection is carried out (S2) according to the human face key point. The attribute for coding is determined through the whole multi-key point in the aligned human face. The training process is deployed to obtain (S4) the nerve network module, and the nerve network model is used (S5) for the prediction (S6) of human face attribute of image.
   USE - Multi-task learning human face property prediction method based on depth of learning.
   ADVANTAGE - Multiple property prediction is carried out efficiently, and the prediction effect is increased.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a multi-task learning human face property prediction device based on depth of learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of the multi-task learning human face property prediction method. (Drawing includes non-English language text)
   Collecting and marking human face image (S1)
   Carrying out human face detection (S2)
   Obtaining nerve network module (S4)
   Using nerve network model (S5)
   Predicting human face attribute of image (S6)
PD CN105404877-A   16 Mar 2016   G06K-009/00   201622   Pages: 11   English
UT DIIDW:201617774K
ER

PT P
PN WO2014181323-A1; IL226147-A1
TI Method for analyzing retail image of shelving module, involves obtaining retail image representative of flank side of shelving module including a shelf, and rating retail image based on the dimension of line segment.
AB    NOVELTY - The method involves obtaining (S111) a retail image representative of a flank side of a shelving module including a shelf. Multiple line segment corresponding to a flank edge of the shelf are detected (S112) in the retail image by performing edge detection on the retail image and performing shape recognition on the retail image. A dimension of the line segment is determined (S113) in the retail image. The retail image is rated (S114) based on the dimension of the line segment. An orientation of the line segment is determined with respect to a reference orientation.
   USE - Method for analyzing a retail image of a shelving module.
   ADVANTAGE - The retail image is rated based on the dimension of the line segment which ensures that the distortions due to door motion are reduced effectively by configuring the imaging sensors to face the access opening at a predetermined angle which improves the imaging of a shelving module.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a method of inspecting a shelving module;
   (2) a processor module for analyzing a retail image;
   (3) a retail unit comprises a retail inspection system;
   (4) a computer program for performing a method; and
   (5) a computer readable storage medium comprises a program.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method for analyzing a retail image.
   Obtaining a retail image representative of a flank side of a shelving module including a shelf (S111)
   Detecting multiple line segment corresponding to a flank edge of the shelf (S112)
   Determining dimension of the line segment in the retail image (S113)
   Rating retail image based on the dimension of the line segment (S114)
PD WO2014181323-A1   13 Nov 2014   G06T-007/00   201476   Pages: 30   English
   IL226147-A1   30 Nov 2014   A47F-010/02   201625      English
UT DIIDW:2014U40823
ER

PT P
PN CN103927802-A; CN103927802-B
TI Home electric door lock controlling method, involves fixing lock control circuit plate with electric door lock, and performing dynamic encryption operation on identification instruction by control circuit plate to open electric door lock.
AB    NOVELTY - The method involves obtaining a human face image data by a mobile terminal. Human face feature value is determined according to the human face image data and sent to a server. A determination is made by the server to check whether the human face feature value is matched with pre-set human face feature value. An identification instruction is sent to the mobile terminal. A door lock control circuit plate is fixed with an electric door lock. Dynamic encryption operation is performed on the identification instruction by the door lock control circuit plate to open the electric door lock.
   USE - Home electric door lock controlling method.
   ADVANTAGE - The method enables realizing intelligent unlocking function so as to increase anti-theft safety level of a home. The method enables fixing the electric door lock with a magnetic card brush card lock so as to copy identification information of a user.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a home electric door lock controlling system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a home electric door lock controlling method.'(Drawing includes non-English language text)'
PD CN103927802-A   16 Jul 2014   G07C-009/00   201467   Pages: 9   Chinese
   CN103927802-B   25 Jan 2017   G07C-009/00   201711      Chinese
UT DIIDW:2014S23291
ER

PT P
PN WO2013165198-A1; JP2013235582-A; KR2013123316-A; US2013293456-A1; AU2013205535-A1; CN103383595-A; EP2685352-A2; EP2685352-A3; US9239617-B2; US2016085496-A1; US9459826-B2; IN201409959-P1; US2017010668-A1; CN103383595-B; AU2013205535-B2; CN107831904-A; JP6362831-B2; US10114458-B2; EP2685352-B1; IN367443-B; CN107831904-B
TI Apparatus for controlling mobile terminal based on analysis of user's face, has controller determining user state information based on face/eye recognition results and performing predetermined function of terminal according to information.
AB    NOVELTY - The apparatus has an image processor (105) performing face detection or eye detection on an input image upon reception of the input image through an image input unit (100) i.e. camera. A controller (120) determines user state information based on face recognition results or eye recognition results from the image processor and performs a predetermined function of a mobile terminal according to the user state information, where the user state information includes presence or absence of user, distance from the mobile terminal and position of the user's face.
   USE - Apparatus for controlling a mobile terminal based on analysis of a user's face.
   ADVANTAGE - The apparatus can correctly detect eye positions even in the situation at which the face detection has failed, so that desired operation of the mobile terminal or hardware interfaces such as keypads or touch-screens, can be easily performed and controlled through eye detection without direct inputs from the user, thus guaranteeing better quality of user experience.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for controlling a function based on analysis of a user's face in a mobile terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a mobile terminal.
   Image input unit (100)
   Image processor (105)
   Controller (120)
   Memory (130)
   Display unit (140)
   Communication unit (150)
   Speaker (170)
PD WO2013165198-A1   07 Nov 2013   H04B-001/40   201376   Pages: 39   English
   JP2013235582-A   21 Nov 2013   G06T-007/20   201376   Pages: 24   Japanese
   KR2013123316-A   12 Nov 2013   H04B-001/40   201376      
   US2013293456-A1   07 Nov 2013   G06F-003/01   201376      English
   AU2013205535-A1   21 Nov 2013   G06K-009/00   201378      English
   CN103383595-A   06 Nov 2013   G06F-003/01   201404      Chinese
   EP2685352-A2   15 Jan 2014   G06F-003/01   201406      English
   EP2685352-A3   30 Jul 2014   G06F-003/01   201450      English
   US9239617-B2   19 Jan 2016   G06F-003/01   201607      English
   US2016085496-A1   24 Mar 2016   G06F-003/14   201622      English
   US9459826-B2   04 Oct 2016   G06F-003/16   201666      English
   IN201409959-P1   10 Jun 2016   H04B-001/40   201668      English
   US2017010668-A1   12 Jan 2017   G06F-003/01   201706      English
   CN103383595-B   02 Jan 2018   G06F-003/01   201804      Chinese
   CN107831904-A   23 Mar 2018   G06F-003/01   201822      Chinese
   JP6362831-B2   25 Jul 2018   G06T-007/20   201850   Pages: 24   Japanese
   US10114458-B2   30 Oct 2018   G09G-005/00   201872      English
   EP2685352-B1   25 Nov 2020   G06F-003/01   202096      English
   IN367443-B   28 May 2021   H04B-001/40   202148      English
   CN107831904-B   01 Jun 2021   G06F-003/01   202148      Chinese
UT DIIDW:2013U07527
ER

PT P
PN CN103353935-A; CN103353935-B
TI Three-dimensional dynamic gesture recognition method of intelligent household system, involves finding background average depth in depth image, and storing central point coordinate and average depth value of palm area.
AB    NOVELTY - The method involves collecting depth image and RGB image by camera. The depth image is pre-processed, and pure white or pure black in depth image is removed. A background average depth in depth image is found. Human face detection in RGB image is carried out. A search process of finding human face region is performed, when human face region is not detected. A human body hand region image is separated to obtain average depth of human face area. A central point coordinate and average depth value of palm area are stored.
   USE - Three-dimensional dynamic gesture recognition method of intelligent household system.
   ADVANTAGE - The intelligent household system can be operated easily, by using three-dimensional dynamic gesture recognition process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating the three-dimensional dynamic gesture recognition process. (Drawing includes non-English language text)
PD CN103353935-A   16 Oct 2013   G06K-009/00   201402   Pages: 7   Chinese
   CN103353935-B   08 Jun 2016   G06K-009/00   201640      Chinese
UT DIIDW:2014A14034
ER

PT P
PN CN103136533-A; CN103136533-B
TI Human face identifying method, involves determining highest similarity of human face image with template face images, and determining scene type and image threshold value by scene feature module.
AB    NOVELTY - The method involves determining highest similarity of a human face image with template face images. Face image information is extracted by a scene feature module. A scene type and an image threshold value are determined by the scene feature module. A judgment is made whether a scene image is matched with the registered human face image or not. A cluster is provided with a scene classification training set and a scene category model.
   USE - Human face identifying method.
   ADVANTAGE - The method enables reducing scene threshold determination problem and improving human face identification passing rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identifying device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face identifying method.'(Drawing includes non-English language text)'
PD CN103136533-A   05 Jun 2013   G06K-009/62   201368   Pages: 12   Chinese
   CN103136533-B   25 Nov 2015   G06K-009/62   201602      English
UT DIIDW:2013R27486
ER

PT P
PN CN102973253-A; CN102973253-B
TI Visual information based automatic human physiological index monitoring method, involves collecting visual information of to-be-tested object through collecting device, processing visual information, and analyzing processing result.
AB    NOVELTY - The method involves collecting visual information of a to-be-tested object through a collecting device for generating a three-dimensional video detecting human face image, and storing the collected data, where visual information is selected from predetermined visual information. The visual information is processed. A processing result is analyzed to extract a hidden human body blood volume pulse wave signal and human body physiological index. The extracted physiological index is displayed.
   USE - Visual information based automatic human physiological index monitoring method.
   ADVANTAGE - The method enables performing continuous monitoring and synchronous detection of multiple physiological indexes with high measuring accuracy in a simple and easy manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a visual information based automatic human physiological index monitoring system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a visual information based automatic human physiological index monitoring method.'(Drawing includes non-English language text)'
PD CN102973253-A   20 Mar 2013   A61B-005/02   201346   Pages: 13   Chinese
   CN102973253-B   29 Apr 2015   A61B-005/02   201543      Chinese
UT DIIDW:2013K29820
ER

PT P
PN US2011052004-A1; CN102006402-A; US8374405-B2; CN102006402-B
TI Camera device for conducting facial recognition for security purpose, has microprocessor comparing three-dimensional information of face to be tested with three-dimensional information of determined face to output recognition signal.
AB    NOVELTY - The device has a light detection and ranging system (30) i.e. optical sensing system, transmitting laser light according to record coordinates of a face to be tested in an image and scanning the face to be tested in a determined field for obtaining three-dimensional information of the face to be tested. A storage module (50) stores three-dimensional information of the determined face. A microprocessor (40) compares three-dimensional information of the face to be tested with three-dimensional information of the determined face to output a recognition signal based on comparison result.
   USE - Camera device for conducting facial recognition for security purpose.
   ADVANTAGE - The light detection and ranging system transmits the laser light according to record coordinates of the face to be tested in the image and scans the face to be tested in the determined field for obtaining three-dimensional information of the face to be tested, thus providing accurate facial recognition for the security purpose for a system to allow an entry of a person according to the face to be tested, and hence avoiding the entry of an unauthorized person.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an identity recognition method for a camera device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a camera device.
   Image capture module (10)
   Face detection module (20)
   Light detection and ranging system (30)
   Microprocessor (40)
   Storage module (50)
PD US2011052004-A1   03 Mar 2011   G06K-009/00   201120   Pages: 7   English
   CN102006402-A   06 Apr 2011   H04N-005/225   201132      Chinese
   US8374405-B2   12 Feb 2013   G06K-009/00   201312      English
   CN102006402-B   19 Feb 2014   H04N-005/225   201426      Chinese
UT DIIDW:2011C15666
ER

PT P
PN US2010158315-A1; WO2010075430-A1; US7876352-B2; US2011251972-A1; US8442922-B2
TI Computer-implemented digital media clearinghouse system for athletic event e.g. bicycle race, has image processing module to identify images tagged with registrant name, download identified image files and associate files with registrant.
AB    NOVELTY - The system has an image processing module coupled to a machine-recognition component for recognizing a bib number worn by a registrant appearing in a media item. The module identifies a human face that appears in the item when the number is not recognizable, where the item is associated with the registrant. A distribution service generates and distributes digital output content, and aggregates the media item from recognized sources. The module accesses a remote web site to identify images tagged with registrant name, download identified image files and associate the files with the registrant.
   USE - Computer-implemented digital media clearinghouse system for an athletic event e.g. marathon and bicycle race. Can also be used for boat race, motorcycle race and foot-race.
   ADVANTAGE - The system ensures that information regarding events of interest is aggregated by a server system from a set of external sources, thus ensuring that users of the website can conveniently search for events of interest or participants of interest, and hence improving recognition accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a process for on-line collection, aggregation analysis and distribution of an athletic event.
PD US2010158315-A1   24 Jun 2010   G06K-009/00   201045   Pages: 14   English
   WO2010075430-A1   01 Jul 2010   G06K-009/00   201045      English
   US7876352-B2   25 Jan 2011   H04N-007/18   201108      English
   US2011251972-A1   13 Oct 2011   G06Q-099/00   201167      English
   US8442922-B2   14 May 2013   G06Q-099/00   201655      English
UT DIIDW:2010H13374
ER

PT P
PN US2010054592-A1; WO2010025908-A1; US8254674-B2
TI Red-eye detection performing method for acquired digital image, involves acquiring image, identifying multiple corrective processes, and applying red eye correction to image according to characteristics.
AB    NOVELTY - The method involves acquiring a digital image (170-1), and analyzing multiple partial face regions within the image by determining multiple characteristics of the image. Multiple corrective processes are identified, and a red eye correction is beneficially applied to the image according to the characteristics. The corrective processes are applied to the image. Red-eye defects are detected in another acquired image based on the former image. A third acquired image is corrected based on analysis of the latter acquired image. Multiple types of faces are recognized within the former image.
   USE - Method for performing a red-eye detection in an acquired digital image during flash photography application.
   ADVANTAGE - The variables that effect the success of the red-eye detection algorithm such as noise, color shifts, incorrect exposure, blur and over sharpening, can be pre-eliminated before performing the detection process, thus improving the success rate of the red-eye detection. The variables can be pre-accounted by changing the parameters for the detection process, thus improving the performance and the success rate. The parameters for the detection and correction algorithm can be modified, thus providing higher accuracy both in the positive detection and reduction in the false detection without the need to modify the image. The misclassification of pixels and regions belonging to defect areas is reduced, thus enabling a reduction of undetected correct positives. The color misclassifications of pixels and regions belonging to non-defect areas are reduced, thus enabling a reduction of false positives. The method enables performing fast and accurate red-eye detection to allow individual images in a batch to be analyzed and corrected in real-time prior to printing.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a processor-readable medium having a set of instructions executed on a processor to perform a method for performing a red-eye detection in an acquired digital image
   (2) a portable processor-based device comprising an image acquisition component.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a red-eye detection system.
   In-camera redeye filter (90)
   Pixel locator filter (92)
   Camera (100)
   Image analysis prefilter (130)
   Image compensation prefilter (135)
   Digital image (170-1)
PD US2010054592-A1   04 Mar 2010   G06K-009/00   201018   Pages: 54   English
   WO2010025908-A1   11 Mar 2010   H04N-005/225   201019      English
   US8254674-B2   28 Aug 2012   G06K-009/00   201257      English
UT DIIDW:2010C36701
ER

PT P
PN CN101630363-A; CN101630363-B
TI Intricate background image face detection method, involves detecting non-face by false-alarm suppression process based on space constraints and geometric constraint to complete face detection process.
AB    NOVELTY - The method involves constructing a face complexion mixed model according to multiple collected complexion sample data, and dividing complexion of an image. A face candidate area is located according to integrated complexion pixel rate to reduce face searching space. The face for a window is detected by an unsymmetrical adaboost algorithm. Non-face is detected by a false-alarm suppression process based on space constraints and geometric constraints to complete face detection process. A distribution area is drawn according to collected complexion sample data.
   USE - Method for fast detecting of image face of intricate background.
   ADVANTAGE - The method performs quick location of multiple front faces in the image with high detection efficiency and low false-alarm rate. The method allows formation of a data base and multiple videos and colorful image test set with high resolving capability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the steps involved in intricate background image face detection method.'(Drawing includes non-English language text)'
PD CN101630363-A   20 Jan 2010   G06K-009/00   201012   Pages: 24   Chinese
   CN101630363-B   23 Nov 2011   G06K-009/00   201182      Chinese
UT DIIDW:2010B25282
ER

PT P
PN EP2127599-A1; US2009293589-A1; CA2667102-A1; WO2009155048-A1; AU2009202108-A1; EP2127599-B1; US8359901-B2; NZ589525-A; NZ616615-A; AU2016203033-A1; CA2667102-C; AU2017276228-A1; AU2017276228-B2
TI Chemical impairment detection system for vehicular applications, home confinement and business applications has chemical impairment detection sampling device which receives chemical impairment detection test sample from tester.
AB    NOVELTY - The chemical impairment detection system (10) has video surveillance system (18) which records facial image (20) of tester (16). A control and relay module (12,30) with proper view and position determiner is adapted to determine if facial image of tester is properly positioned within the field of view (24) during chemical impairment detection test. A LED alerts tester when to begin chemical impairment detection test and a chemical impairment detection sampling device (12) is responsive with video surveillance system and receives chemical impairment detection test sample from tester.
   USE - Chemical impairment detection system for vehicular applications. Can also be used for home confinement, alcohol abstinence monitoring programs and business applications.
   ADVANTAGE - Provides low cost chemical impairment detection system which allows accurate testing by aborting the test if the tester fails to provide enough volume of breath to assure deep lung sample.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a conducting method for chemical impairment detection test; and
   (2) a circumvention reducing method for chemical impairment detection test.
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective view of a chemical impairment detection system.
   Chemical impairment detection system (10)
   Chemical impairment detection sampling device (12)
   Control and relay module (12,30)
   Tester (16)
   Video surveillance system (18)
   Facial image (20)
   Field of view (24)
PD EP2127599-A1   02 Dec 2009   A61B-005/08   200981   Pages: 22   English
   US2009293589-A1   03 Dec 2009   G01N-033/497   200981      English
   CA2667102-A1   28 Nov 2009   A61B-005/117   201001      English
   WO2009155048-A1   23 Dec 2009   H04N-007/18   201001      English
   AU2009202108-A1   17 Dec 2009   G06T-007/00   201009      English
   EP2127599-B1   21 Mar 2012   A61B-005/08   201221      English
   US8359901-B2   29 Jan 2013   G01N-033/497   201309      English
   NZ589525-A   29 Nov 2013   H04N-007/18   201402      English
   NZ616615-A   27 Feb 2015   H04N-007/18   201518      English
   AU2016203033-A1   02 Jun 2016   G06T-007/00   201646      English
   CA2667102-C   18 Apr 2017   A61B-005/1171   201729      English
   AU2017276228-A1   18 Jan 2018   G06T-007/00   201929      English
   AU2017276228-B2   18 Apr 2019   G06T-007/00   201930      English
UT DIIDW:2009R81879
ER

PT P
PN US2009092290-A1; US7831072-B2
TI Biometric measurement performing method for analysis of e.g. face, involves deriving multispectral image of purported skin site from received light, and performing biometric function with derived multi-spectral image.
AB    NOVELTY - The method involves illuminating a purported skin site of an individual with a set of distinct optical conditions during an illumination session, when the purported skin site is not in contact with a biometric measurement device. Light scattered from the purported skin site is received for each of the set of distinct optical conditions, when the purported skin site is not in contact with the biometric measurement device. A multispectral image of the purported skin site is derived from the received light. A biometric function is performed with the derived multi-spectral image.
   USE - Method for performing a biometric measurement using a biometric measurement device for analysis of characteristics of biometric identification such as face, iris, hand geometry, vein structure, and fingerprint pattern, of living bodies.
   ADVANTAGE - The method enhances sensor usability by reducing the constraints on an individual for precise contact and positioning on the individual's skin.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a biometric measurement device comprising a red illumination source
   (2) a method for performing a biometric measurement on an individual using a biometric measurement device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a biometric determinations performing method for identity and sample authenticity using measurements at a set of distinct optical conditions.
PD US2009092290-A1   09 Apr 2009   G06K-009/00   200929   Pages: 35   English
   US7831072-B2   09 Nov 2010   G06K-009/00   201074      English
UT DIIDW:2009G90641
ER

PT P
PN CN101393598-A
TI Method for unlocking camera cell phone, involves comparing related coefficient of user image with standard human face identification template and cell phone is unlocked automatically when user image is matched with standard template.
AB    NOVELTY - The method involves establishing standard human face identification template. The face of the user is shot by a camera in the cell phone. The related coefficient of user image and standard human face identification template are calculated and compared. The cell phone is unlocked automatically when the user image is matched with the established standard human face identification template. The cell phone is remained unlocked when the user image is not matched with established standard human face identification template.
   USE - Method for unlocking camera cell phone.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart explaining process for unlocking camera cell phone. (Drawing includes non-English language text)
PD CN101393598-A   25 Mar 2009   G06K-009/00   200925   Pages: 7   Chinese
UT DIIDW:2009G98682
ER

PT P
PN CN101383001-A; CN101383001-B
TI Front face identifying method for front posture identification of human, involves determining whether face includes leftward and rightward rotation problems by calculating amount of proportion of skin color points of strips.
AB    NOVELTY - The method involves detecting face and eyes by utilizing adaboost classifiers. An orbital cavity region and a central position of an orbital cavity are determined by utilizing a skin color test and mass centre calculation process. A non-front face is filtered out according to the central position of the cavity. A naked face is cut according to the central positions of the cavities. Determination is made whether the face includes leftward and rightward rotation problems by calculating an amount of proportion of skin color points of strips on sides of the naked face.
   USE - Method for identifying front face for front posture identification of a human.
   ADVANTAGE - The method provides filtering out the non-front face according to the central position of the orbital cavity and determining whether the face includes leftward and rightward rotation problems by calculating the amount of proportion of the skin color points of the strips, thus acquiring standard front face image, ensuring posture identification and increasing posture identification rate.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for identifying a front face.'(Drawing includes non-English language text)'
PD CN101383001-A   11 Mar 2009   G06K-009/00   200924   Pages: 16   Chinese
   CN101383001-B   02 Jun 2010   G06K-009/00   201056      Chinese
UT DIIDW:2009G39351
ER

PT P
PN CN101178773-A; CN101178773-B
TI Image e.g. fingerprint image, recognition system, has feature extraction module, which is used for extracting features of corresponding sorts from received recognizing images and outputting features to classifier arranged in branches.
AB    NOVELTY - The system has a feature extraction module, which is used for extracting features of corresponding sorts from received recognizing images and outputting the features to a classifier arranged in branches. A classifier is used for matching and recognizing samples in the received features and a database, and recognizing result is output to a decision module. The decision module is used for deciding according to the recognizing result of each branch, and for outputting to obtain decision result. A feature extraction module is respectively combined to the classifier with different sorts.
   USE - System for recognizing image such as human face image, fingerprint image and iris image.
   ADVANTAGE - The system can adapt to different environments, and the feature extraction modules and several classifies are effectively organized to recognize images, thus improving the reliability of the system. The system has no limitation to the number of sorts for the feature and classifier, which can randomly increase feature extraction module and/or classifiers with different sorts with stronger expansibility. The system can be applied in multiple image recognitions based on feature extraction and classifier such as human face recognition, fingerprint recognition and iris recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an image recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an image fingerprint recognition system.'(Drawing includes non-English language text)'
PD CN101178773-A   14 May 2008   G06K-009/62   200841   Pages: 18   Chinese
   CN101178773-B   11 Aug 2010   G06K-009/62   201074      Chinese
UT DIIDW:2008G38524
ER

PT P
PN CN101035280-A; CN100563335-C
TI Terminal system for checking classified content is comprise of modules and interfaces, modules are, namely, program management module used to preserve source address and related information of program.
AB    NOVELTY - This invention claims a terminal system for checking the classified contents, it is used to examine and verify the video and audio contents. This system is comprised of the following modules and interfaces, the modules are, namely, program management module used to preserve the source address and the related information of the program, voice identification module used to identify the voice for input audio flow, content analysis module used to extract the related video and subtitle information, subtitle identification module used to decollate and identify the input subtitle information, face identification module used to extract and then identify the input characteristic information of faces, fusion score module to give a score according to the identification result of voice, subtitle and faces and to determine the validation of the content according to the result of the scoring and the content is force-out or the doubtful files for further examination and verification, strategy caching module used to download strategy of updated content checking from the strategy server and decoder module used to decode the video and audio files in a real-time manner.
PD CN101035280-A   12 Sep 2007   H04N-007/24   200825      Chinese
   CN100563335-C   25 Nov 2009   H04N-007/24   201004      Chinese
UT DIIDW:2008D28490
ER

PT P
PN US2007043527-A1; WO2007022413-A2; WO2007022413-A3; EP1915737-A2; KR2008045175-A; CN101288103-A; JP2009505107-W; IN200800421-P3; KR955136-B1; US8154612-B2; JP2012168181-A; CN101288103-B; JP5496509-B2; IN258584-B
TI Imaging sensor e.g. charge-coupled device sensor, characterizing method for e.g. human skin tone detection, involves obtaining predicted response of sensor to each of human skin surfaces based on locations and parameters.
AB    NOVELTY - The method involves obtaining a set of color values in a color space, where each color value is based on a response by a sensor e.g. charge-coupled device (CCD) sensor, to corresponding targets, and each target includes different reflectance spectrum. Parameters e.g. coefficient vector, are obtained based on a decomposition of a reflectance spectrum of a surface into a combination of reflectance spectra of the targets. A predicted response of the sensor to each of human skin surfaces e.g. hand, is obtained, based on the location and parameters.
   USE - Used for characterizing an imaging sensor e.g. charge-coupled device (CCD) sensor and complementary metal-oxide-semiconductor (CMOS) sensor, for color detection and classification of e.g. human skin tone e.g. face and hand, detection that is utilized in an application e.g. people tracking, blocking mature-content web image, facilitating human-computer interaction, and serves as an enabling technology for face detection, localization, recognition and tracking, video surveillance and image database management, that is adapted via a portable communication device e.g. cellular telephone which is equipped with a camera.
   ADVANTAGE - The predicted response of the imaging sensor to each of the human skin surfaces are obtained, based on the locations and parameters, thus permitting to analyze or classify images captured with the same sensor from the predicted sensor responses, and hence improving skin tone by enhancing the color and also enhancing autofocus, auto-white balance and auto-exposure process.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method of image processing
   (2) a data storage medium having a set of instructions to perform a method of characterizing a sensor
   (3) an image processing apparatus comprising an image sensor.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of an imaging sensor characterizing method.
PD US2007043527-A1   22 Feb 2007   G01C-025/00   200736   Pages: 30   English
   WO2007022413-A2   22 Feb 2007   G06T-007/40   200736      English
   WO2007022413-A3   06 Mar 2008   G06T-007/40   200819      English
   EP1915737-A2   30 Apr 2008   G06T-007/40   200831      English
   KR2008045175-A   22 May 2008      200869      
   CN101288103-A   15 Oct 2008   G06T-007/40   200875      Chinese
   JP2009505107-W   05 Feb 2009   G01J-003/51   200910   Pages: 36   Japanese
   IN200800421-P3   26 Jun 2009   G06T-007/40   200953      English
   KR955136-B1   28 Apr 2010   G06T-007/00   201035      
   US8154612-B2   10 Apr 2012   H04N-005/228   201225      English
   JP2012168181-A   06 Sep 2012   G01J-003/51   201259   Pages: 34   Japanese
   CN101288103-B   05 Sep 2012   G06T-007/40   201277      Chinese
   JP5496509-B2   21 May 2014   G01J-003/51   201434   Pages: 34   Japanese
   IN258584-B   24 Jan 2014   G06T-007/40   201536      English
UT DIIDW:2007385850
ER

PT P
PN WO2007011709-A2; EP1907980-A2; US2012008837-A1; WO2007011709-A3; US8306284-B2; EP1907980-B1; ES2399030-T3; EP1907980-A4
TI Face image index method for use in photograph, involves displaying sets of image to operator and manually merging sets representing same person with no false-positive or false-negative errors.
AB    NOVELTY - A face image is automatically indexed from the collection of images for creating a multiple sets of face images where each set determining image of same person with false positive error or false negative error. The set is presented to operator for determining whether it represents the same person and manually splitting the set representing more than one person into multiple of sets. One face image from several sets is displayed to the operator and manually merging the sets having same face image and no false-positive or false-negative errors.
   USE - For face recognition in photographs.
   ADVANTAGE - Reduces the amount of time required to manually-assist automated indexing of images using facial recognition and reduces the number of manual comparisons. The accuracy of manually-assisted automated indexing of images is improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) false-positive association removal method; and
   (2) face image index system.
   DESCRIPTION OF DRAWING(S) - The figure shows a flow diagram of manually-assisted automated indexing of images.
PD WO2007011709-A2   25 Jan 2007   G06T-007/20   200727   Pages: 23   English
   EP1907980-A2   09 Apr 2008   G06K-009/00   200827      English
   US2012008837-A1   12 Jan 2012   G06K-009/62   201205      English
   WO2007011709-A3   21 Jun 2007   G06K-009/00   201226      English
   US8306284-B2   06 Nov 2012   G06K-009/00   201274      English
   EP1907980-B1   02 Jan 2013   G06K-009/00   201304      English
   ES2399030-T3   25 Mar 2013   G06K-009/00   201338      Spanish
   EP1907980-A4   26 Aug 2009   G06K-009/00   201755      English
UT DIIDW:2007283057
ER

PT P
PN US2006098872-A1; US7212330-B2
TI Three-dimensional imaging system used for pattern recognition, has variable focal length micromirror lenses for capturing two dimensional images, and extracts in-focus pixels from captured images for performing pattern recognition.
AB    NOVELTY - An image sensor (16) receives two dimensional images captured using variable focal length micromirror lenses (13,14). An image processor (17) extracts the in-focus pixels from the images to generate all-in-focus image and position information of the photographed object (11). A pattern recognition system (19) performs pattern recognition processing using the all-in-focus image.
   USE - Three-dimensional imaging system using camera with micromirror lenses, for imaging object such as human face and for pattern recognition.
   ADVANTAGE - The micromirror lens has large focal length variation with fine resolution, high optical focusing efficiency and very fast response time. The micromirror lens is compact and inexpensive, and the focusing system using the micromirror lens consumes less power. The imaging system detects real-time three-dimensional motions, and needs only a single imaging sensor and provides images for achieving high precision pattern recognition. The wide or narrow field of view image can be obtained within a short time.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic view of the real-time three-dimensional imaging system.
   object (11)
   objective lens (12)
   micromirror lenses (13,14)
   image sensor (16)
   image processor (17)
   pattern recognition system (19)
PD US2006098872-A1   11 May 2006   G06K-009/00   200636   Pages: 11   English
   US7212330-B2   01 May 2007   G02B-026/08   200730      English
UT DIIDW:2006350811
ER

PT P
PN WO2005119581-A1; EP1751695-A1; IN200607043-P1; EP1751695-B1; DE602005007735-E; US2008187183-A1; ES2306153-T3; US7650020-B2
TI Image characteristics identification method e.g. for iris image, involves generating variation code indicating variation between digitized sample strings, to identify characteristics of image.
AB    NOVELTY - The method involves obtaining digitized sample strings by averaging respective individual linear image samples. A variation code (70) indicating variation between sample strings, is generated from the sample strings. The characteristics of image is identified based on the variation code.
   USE - For identifying characteristic of image of iris, face, hand, ear or fingerprint, for biometric identification of human being or animal.
   ADVANTAGE - The image characteristic is identified reliably while improving false rejection rate (FRR).
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) apparatus for identifying characteristic of image;
   (2) computer program for identifying characteristic of image; and
   (3) computer readable medium storing program for identifying characteristic of image.
   DESCRIPTION OF DRAWING(S) - The figures shows an explanatory view of the human or animal iris and a block diagram of the image characteristic identification apparatus.
   eye (10)
   iris (12)
   pupil (18)
   eyelid (20)
   one-dimensional data strings (32`,36`)
   binary outputs (64,66)
   code (70)
PD WO2005119581-A1   15 Dec 2005   G06K-009/50   200605   Pages: 18   English
   EP1751695-A1   14 Feb 2007   G06K-009/50   200715      English
   IN200607043-P1   31 Aug 2007   G06K-009/00   200781      English
   EP1751695-B1   25 Jun 2008   G06K-009/50   200844      English
   DE602005007735-E   07 Aug 2008   G06K-009/50   200854      German
   US2008187183-A1   07 Aug 2008   G06K-009/00   200854      English
   ES2306153-T3   01 Nov 2008   G06K-009/50   200914      Spanish
   US7650020-B2   19 Jan 2010   G06K-009/00   201007      English
UT DIIDW:2006047815
ER

PT P
PN US2005213810-A1; EP1587024-A2; JP2005284487-A; EP1744265-A1; EP1587024-B1; DE602005006341-E; DE602005006341-T2; EP1744265-B1; EP1967984-A1; DE602005009169-E; EP1967984-B1; DE602005014573-E; US7630525-B2; JP5025893-B2; EP1587024-A3
TI Information processing apparatus e.g. robot, has two lower nodes each with set of weak classifiers that learn learning samples with two different labels of two different ranges, based on results of classification by upper nodes.
AB    NOVELTY - The apparatus has upper nodes each with a set of weak classifiers that learn learning samples with a label of a range among the samples classified by a classification unit (5). Two lower nodes each with a set of weak classifiers learn learning samples with two different labels of two different ranges, based on results of classification by the upper nodes. One range among the two ranges is being a part of the former range.
   USE - Used for detecting an object e.g. face image, from a complex video scene. Can also be used with image forming apparatus for receiving and displaying moving picture as well as still picture.
   ADVANTAGE - The weak classifiers increases the speed of learning and detection by reducing the amount of computation when an object of interest is detected based on ensemble learning.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) an information processing method
   (B) a recording medium having recorded computer-readable program to execute information processing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a functional block diagram showing processing functions of an object detecting apparatus.
   Image output unit (2)
   Scaling unit (3)
   Scanning unit (4)
   Classification unit (5)
   Ensemble learning machine (6)
PD US2005213810-A1   29 Sep 2005   G06K-009/00   200570   Pages: 62   English
   EP1587024-A2   19 Oct 2005   G06K-009/00   200570      English
   JP2005284487-A   13 Oct 2005   G06N-003/00   200570   Pages: 53   Japanese
   EP1744265-A1   17 Jan 2007   G06K-009/00   200706      English
   EP1587024-B1   30 Apr 2008   G06K-009/00   200831      English
   DE602005006341-E   12 Jun 2008   G06K-009/00   200841      German
   EP1744265-B1   20 Aug 2008   G06K-009/00   200857      English
   EP1967984-A1   10 Sep 2008   G06K-009/00   200860      English
   DE602005009169-E   02 Oct 2008   G06K-009/00   200866      German
   EP1967984-B1   20 May 2009   G06K-009/00   200934      English
   DE602005014573-E   02 Jul 2009   G06K-009/00   200943      German
   US7630525-B2   08 Dec 2009   G06K-009/00   200981      English
   JP5025893-B2   12 Sep 2012   G06N-003/00   201260   Pages: 53   Japanese
   EP1587024-A3   05 Apr 2006   G06K-009/00   201741      English
UT DIIDW:2005683085
ER

PT P
PN US6920231-B1
TI Vector space construction method for object recognition, involves constructing sample space from raw matching score and generating rotation matrix by mapping matching scores into sample space scores.
AB    NOVELTY - A raw matching score is determined for the basis sample elements and database samples. A sample space is constructed from the raw matching scores, by generating covariance matrix. A rotation matrix is generated by mapping matching scores into sample space scores. The sample space is truncated by eliminating the subset of sample modes based on distribution of sample scores for each sample space modes.
   USE - For constructing vector space for recognizing object such as face and fingerprint.
   ADVANTAGE - The distance between object data samples encoded in the sample space or recognition space is determined, rapidly.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for space construction and encoding system.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart showing the method of operation recognition space rotation module.
PD US6920231-B1   19 Jul 2005   G06K-009/00   200554   Pages: 16   English
UT DIIDW:2005529907
ER

PT P
PN GB2395781-A; WO2004051553-A2; EP1567971-A2; JP2006508463-W; US2006072811-A1; US7515739-B2; WO2004051553-A3
TI Face detection apparatus has controller which modifies likelihood values depending upon face size or range of face sizes appropriate to the classification of the image and metadata associated with the image.
AB    NOVELTY - Likelihood values indicating test region containing face of different face group sizes is detected by comparing test region with face data. A controller modifies the detected values depending upon the face size or range of face sizes appropriate to the classification of the image and metadata associated with the image defining one of a set of predetermined classifications of the image.
   USE - For face detection for camcorder, video conferencing apparatus (claimed), surveillance apparatus (claimed) and also used as non-linear editing system.
   ADVANTAGE - Improves accuracy and/or data processing efficiency of the face detection process.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) video conferencing system;
   (2) surveillance apparatus;
   (3) method of face detection;
   (4) computer software for face detection; and
   (5) providing medium storing face detection software.
PD GB2395781-A   02 Jun 2004   G06T-007/60   200445   Pages: 56   English
   WO2004051553-A2   17 Jun 2004   G06K-009/00   200445      English
   EP1567971-A2   31 Aug 2005   G06K-009/00   200561      English
   JP2006508463-W   09 Mar 2006   G06T-007/00   200620   Pages: 50   Japanese
   US2006072811-A1   06 Apr 2006   G06K-009/62   200625      English
   US7515739-B2   07 Apr 2009   G06K-009/00   200925      English
   WO2004051553-A3   15 Jul 2004   G06K-009/00   201213      English
UT DIIDW:2004470496
ER

PT P
PN US2003165269-A1; US7271809-B2
TI Affective information determining method for imaging systems e.g. digital cameras, involves sequentially displaying digital images and monitoring viewing time for each digital image to determine affective information.
AB    NOVELTY - The method involves sequentially displaying digital images for a user (2) in a display monitor (14). A processor monitors the viewing time for each digital image. The affective information for the digitals image is determined using the viewing time. A degree of preference for each digital image is determined based on the users facial expression.
   USE - Used for determining affective information in imaging systems e.g. digital cameras.
   ADVANTAGE - The method provides unique personal classification of digital images for future usage e.g. retrieval, communication, sharing, advertising and marketing. The method provides the information automatically by measuring the viewing time of the user. The degree of interest of the user can be determined using the viewing time
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for providing affective information for images in an imaging system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram representing a system for providing affective information.
   User (2)
   Video camera (4)
   Home computer (10)
   Display monitor (14)
PD US2003165269-A1   04 Sep 2003   G06K-009/00   200379   Pages: 16   English
   US7271809-B2   18 Sep 2007   G09G-005/00   200763      English
UT DIIDW:2003851959
ER

PT P
PN WO200182216-A1; AU200157299-A; US2002031255-A1; CN1383522-A; EP1301894-A1; JP2004505233-W; US6947586-B2; AU2001257299-A8; CN1214340-C; EP1301894-B1; DE60139071-E; ES2328450-T3; CA2377602-C; JP4945045-B2; WO200182216-A8; EP1301894-A4
TI Biological sample image classification method using multineural network imaging system, involves selecting and modifying sample classification class based on multiple predetermined classification classes.
AB    NOVELTY - Multiple features from a sample image are extracted and selected to determine physical characteristic of the sample, based on which a sample classification class is determined. Another group of extracted features are selected to determine a group of classes from which a particular classification class is determined and modified based on the predetermined classes.
   USE - For detecting and classifying biological samples such as human urine etc., using multineural network imaging system. For identifying and classifying shape/size and location of eyes, nose, mouth or other facial features for facial recognition.
   ADVANTAGE - The required sample classification class is determined with less number of classification, thereby reducing total number of decisions taken for each classification by neural network increasing system accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for imaging apparatus for element classification.
   DESCRIPTION OF DRAWING(S) - The figure shows the schematic diagram of the imaging apparatus.
PD WO200182216-A1   01 Nov 2001   G06K-009/00   200176   Pages: 62   English
   AU200157299-A   07 Nov 2001   G06K-009/00   200219      English
   US2002031255-A1   14 Mar 2002   G06K-009/62   200222      English
   CN1383522-A   04 Dec 2002   G06K-009/00   200322      Chinese
   EP1301894-A1   16 Apr 2003   G06K-009/00   200328      English
   JP2004505233-W   19 Feb 2004   G01N-015/00   200414   Pages: 89   Japanese
   US6947586-B2   20 Sep 2005   G06K-009/00   200562      English
   AU2001257299-A8   20 Oct 2005   G06K-009/00   200615      English
   EP1301894-B1   24 Jun 2009   G06K-009/00   200942      English
   DE60139071-E   06 Aug 2009   G06K-009/00   200952      German
   ES2328450-T3   13 Nov 2009   G06K-009/00   201003      Spanish
   CA2377602-C   10 Apr 2012   G06K-009/00   201228      English
   JP4945045-B2   06 Jun 2012   G01N-015/00   201238   Pages: 25   Japanese
   WO200182216-A8   31 Dec 2003   G01N-033/493   201732      English
   EP1301894-A4   25 Jun 2003   G06K-009/00   201774      English
UT DIIDW:2001663297
ER

PT P
PN CN108052925-A; CN108052925-B
TI Cell personnel file intelligent management method, involves establishing cell file management mechanism, utilizing pre-warning system to realize alarming, and combining cell access record to realize early warning of abnormal event.
AB    NOVELTY - The method involves obtaining a monitoring picture by a front end camera. Face detection and face recognition of the monitoring picture is performed by a human face detecting module and a face identification module. Processing result is transmitted to perform data cleaning operation for updating a training model. A cell file management mechanism is established to effectively manage a cell. A pre- warning system is utilized to realize alarming for a personnel. Cell access record is combined to realize early warning of an abnormal event. The cell file management mechanism is divided.
   USE - Cell personnel file intelligent management method.
   ADVANTAGE - The method enables realizing standardized management of a cell, and timely warning of the people and abnormal event. The method enables reducing working load of a security staff, effectively improving safety coefficient of the cell so as to ensure intelligent cell management.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a cell personnel file intelligent management method. '(Drawing includes non-English language text)'
PD CN108052925-A   18 May 2018   G06K-009/00   201837   Pages: 13   Chinese
   CN108052925-B   03 Aug 2021   G06K-009/00   202166      Chinese
UT DIIDW:2018408073
ER

PT P
PN CN107103281-A
TI Aggregated loss depth metric learning based human face recognizing method, involves determining face feature expression of pre-processed human face image for identifying different human face to realize face identification process.
AB    NOVELTY - The method involves pre- processing a training image for obtaining specification of a human face image. A deep convolution neural network model is established based on a key point of a pre-processed human face image. An initial center of the pre-processed human face image is determined. The deep convolution neural network is adjusted by using an aggregated loss depth metric learning function. Face feature expression of the pre-processed human face image is determined for identifying different human face to realize a face identification process.
   USE - Aggregated loss depth metric learning based human face recognizing method.
   ADVANTAGE - The method enables utilizing a small scale data for training and realizing high human face recognition accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an aggregated loss depth metric learning based human face recognizing method. '(Drawing includes non-English language text)'
PD CN107103281-A   29 Aug 2017   G06K-009/00   201768   Pages: 16   Chinese
UT DIIDW:201760729F
ER

PT P
PN CN106650621-A
TI Deep learning based emotion recognition system, has expression early warning module for performing control analyzing process and determining different level of warning according to abnormal expression levels and times.
AB    NOVELTY - The system has a human face image collecting module for obtaining a front human face image and storing the front human face image in an image database according to a mode of identity (ID) and beating card date of employee. A deep learning expression recognition module realizes deep learning network parameter in training phase of fine adjustment. An expression early warning module performs control analyzing process according to historical data on human facial expression characteristic of staff and determines different level of warning according to abnormal expression levels and times.
   USE - Deep learning based emotion recognition system.
   ADVANTAGE - The system ensures analyzing employee emotion by using advanced learning algorithm so as to improve humanistic care effect of deep level.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based emotion recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based emotion recognition system. '(Drawing includes non-English language text)'
PD CN106650621-A   10 May 2017   G06K-009/00   201737   Pages: 6   Chinese
UT DIIDW:201732310Y
ER

PT P
PN CN106407914-A; CN106407914-B
TI Human face detecting method, involves identifying texture information of human face according to speckle pattern, and combining depth information with texture information for determining whether detected face belongs to living body.
AB    NOVELTY - The method involves capturing a bi-identification image that is collected by two cameras. Depth information of a human face is identified according to the bi-identification image. The human face is analyzed under irradiation of an infrared light spot to form a speckle pattern. Texture information of the human face is identified according to the speckle pattern. The depth information is combined with the texture information for determining whether detected face belongs to a living body. Identity information of the human face is obtained.
   USE - Human face detecting method.
   ADVANTAGE - The method enables providing rapid speed, high security and low remote teller system fit requirements.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a human face detecting device
   (2) a remote teller system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face detecting method. '(Drawing includes non-English language text)'
PD CN106407914-A   15 Feb 2017   G06K-009/00   201718   Pages: 21   Chinese
   CN106407914-B   10 Dec 2019   G06K-009/00   201997      Chinese
UT DIIDW:2017133409
ER

PT P
PN CN105989357-A
TI Human face video processing-based heart rate detection method involves performing frequency domain analysis of iterative blood volume pulse (BVP) signal to calculate frequency of BVP signal for estimating heart rate.
AB    NOVELTY - The method involves controlling a camera to collect the face video. The feature point of collected face video is detected and tracked for performing the tilting correction of collected face video. The stable face video is obtained. The color space conversion of face video is performed. A clean BVP signal is extracted from the human face video image by using the video color enlargement process. The frequency domain analysis of iterative BVP signal is performed to calculate the frequency of BVP signal for estimating the heart rate.
   USE - Human face video processing-based heart rate detection method.
   ADVANTAGE - The occurrence of disturbance in background and rigid motion of the human head is prevented effectively. The blood volume and heart rate of human body are estimated efficiently. The heart rate detection accuracy of human body is improved.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the human face video processing-based heart rate detection process. (Drawing includes non-English language text)
PD CN105989357-A   05 Oct 2016   G06K-009/00   201702   Pages: 15   Chinese
UT DIIDW:201664599Y
ER

PT P
PN US9430697-B1
TI Face recognition method for use in security systems, involves classifying input face image, and presenting recognition face class of input face image.
AB    NOVELTY - The method (500) involves obtaining several training face images. Several training dictionaries are obtained. An input face image is obtained. The input face image is partitioned into several blocks. Corresponding deep feature vectors of blocks of the input face image are extracted. A collaborative representation model is applied (S510). Residual errors are computed for the face classes. The input face image is classified by selecting a face class that yields a minimum residual error as a recognition face class. The recognition face class of the input face image is presented (512).
   USE - Face recognition method for use in security systems, interactive video applications, image editing and archiving applications, and computer vision applications.
   ADVANTAGE - The method implements collaborative representation of one image using multiple similar images to improve face recognition performance.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a face recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flowchart of a face recognition method for use in security systems.
   Face recognition method (500)
   Step for detecting faces (506)
   Step for generating face tracks (508)
   Step for applying a collaborative representation model (510)
   Step for presenting the recognition face class (512)
PD US9430697-B1   30 Aug 2016   G06K-009/00   201659   Pages: 17   English
UT DIIDW:201652452P
ER

PT P
PN CN105868769-A
TI Human face image key point locating method, involves detecting human face image, determining shape index characteristic according to image pixel value, and obtaining original image location information.
AB    NOVELTY - The method involves detecting a human face image. Human face area range is determined. The human face area range is confirmed from a personal face key point by a prediction module. The human face area range is confirmed from an image pixel value by the prediction module. Non-linear mapping relationship is established between the human face area range and a human face key point position value. An initial position of the personal face key point is determined. Shape index characteristic is determined according to the image pixel value. Original image location information is obtained.
   USE - Human face image key point locating method.
   ADVANTAGE - The method enables improving location efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face image key point locating device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face image key point locating method. '(Drawing includes non-English language text)'
PD CN105868769-A   17 Aug 2016   G06K-009/62   201659   Pages: 15   Chinese
UT DIIDW:2016529692
ER

PT P
PN CN105117692-A
TI Real-time depth studying based human face identification method, involves obtaining human face image corresponding to characteristic point of human face, and comparing human face base with human face image.
AB    NOVELTY - The method involves obtaining a human face image corresponding to characteristic point of a human face. Alignment process is performed to obtain the characteristic point of the human face image. Characteristic information of the human face is obtained corresponding to the image. A human face base is compared with the human face image. Histogram process is performed to cut the human face image. The human face image is located by using a depth study module. The depth study module is provided with a convolution nerve network.
   USE - Real-time depth studying based human face identification method.
   ADVANTAGE - The method enables increasing human face identification process accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a real-time depth studying based human face identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a real-time depth studying based human face identification method. '(Drawing includes non-English language text)'
PD CN105117692-A   02 Dec 2015   G06K-009/00   201601   Pages: 9   English
UT DIIDW:2015805244
ER

PT P
PN CN102324025-A; CN102324025-B
TI Human face detecting and tracking method based on gaussian complexion model, involves obtaining human face target detection result by extracting image retained in skin color area based on improved camshift algorithm.
AB    NOVELTY - The method involves selecting the adaptive threshold value to divide the image. The skin color area is obtained. The image containing the skin color area is closed to remove the shot noise. The result of the skin area after closing is detected using the human face geometrical characteristic and the face structure characteristic. The human face target detection result is obtained by extracting image retained in the skin color area based on the improved camshift algorithm.
   USE - Human face detecting and tracking method based on gaussian complexion model.
   ADVANTAGE - The human face can be identified accurately. The identification precision and processing speed of the system can be improved.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of the human face detecting and tracking method. (Drawing includes non-English language text)
PD CN102324025-A   18 Jan 2012   G06K-009/00   201215   Pages: 19   Chinese
   CN102324025-B   20 Mar 2013   G06K-009/00   201337      Chinese
UT DIIDW:2012B83860
ER

PT P
PN US2010295944-A1; JP2010273125-A; CN101895727-A; TW201126426-A; JP5397014-B2; CN101895727-B; TW435279-B1; US8982208-B2
TI Monitoring system for performing moving object detection, has analysis apparatus including extended analysis section performing analysis processing, and metadata output section outputting metadata to image capturing apparatus.
AB    NOVELTY - The system has an image capturing apparatus i.e. Internet protocol camera (100), comprising a basic analysis section i.e. moving object detection processing section, that performs analysis processing. A metadata output section outputs a metadata to a monitoring apparatus i.e. center server (10). An analysis apparatus i.e. analysis server (130), comprises an extended analysis section performing analysis processing different from that of the basic analysis section and generating another metadata. Another metadata output section outputs the latter metadata to the image capturing apparatus.
   USE - Monitoring system for performing analysis processing e.g. simple processing such as moving object detection and complex processing such as face recognition, vehicle recognition and matching processing.
   ADVANTAGE - The system enables efficiently performing analysis processing.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a monitoring method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating configuration and a connection of a monitoring system.
   Center server (10)
   Proxy server (20)
   Client terminal (30)
   Internet protocol camera (100)
   Analysis server (130)
PD US2010295944-A1   25 Nov 2010   H04N-007/18   201080   Pages: 21   English
   JP2010273125-A   02 Dec 2010   H04N-007/18   201080   Pages: 19   Japanese
   CN101895727-A   24 Nov 2010   H04N-007/18   201102      Chinese
   TW201126426-A   01 Aug 2011   G06K-009/36   201261      Chinese
   JP5397014-B2   22 Jan 2014   H04N-007/18   201407   Pages: 19   Japanese
   CN101895727-B   30 Apr 2014   H04N-007/18   201440      Chinese
   TW435279-B1   21 Apr 2014   G06K-009/36   201443      Chinese
   US8982208-B2   17 Mar 2015   H04N-007/18   201520      English
UT DIIDW:2010P65782
ER

PT P
PN US2010052852-A1; US8058972-B2
TI Secure identification document e.g. passport, preparing method for e.g. secured place, involves printing visible watermarked facial image and biographical information on secure identification document, and attaching storage chip to document.
AB    NOVELTY - The method involves collecting biographic information from an individual, generating four random numeric keys and storing the four random numeric keys in a central database. A facial image is obtained from the individual. A pair of encryption keys, biometric watermark facial image and final facial image is stored in the central database. The visible watermarked facial image and the biographical information are printed on a secure identification document i.e. passport. A storage chip i.e. RFBD chip, is attached to the secure identification document.
   USE - Method for preparing a secure identification document e.g. passport, driver's license, credit card, bank card, debit card and automatic teller machine (ATM) card (claimed), for an individual e.g. employee and student, by utilizing a biometric information capturing device in a secured place. Uses include but are not limited to a corporate world, national laboratory, nuclear power plant, power station, bank, and university.
   ADVANTAGE - The method utilizes a combination of visible watermarking, invisible-fragile watermarking decoding, invisible-robust extraction, and decryption watermarking and encryption to provide multiple layers of protection with four biometric based keys and avoid the information to be tampered, thus protecting the biometric information to maintain privacy in an effective manner. The method allows verifying the biometric information in a host media in a unique, secure and reliable manner, so that the biometric information can not be stolen by an unauthorized person.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for verifying identification of an individual possessing of a secure identification document
   (2) a device for capturing, encrypting, and watermarking biometric information and preparing a secure identification document for an individual
   (3) a device for decrypting and verifying biometric information in a secure identification document for an individual.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a method for protecting biometric information stored in a central database and used in an identification document.
PD US2010052852-A1   04 Mar 2010   G06F-007/04   201018   Pages: 11   English
   US8058972-B2   15 Nov 2011   G05B-019/00   201175      English
UT DIIDW:2010C37812
ER

PT P
PN CN101448085-A; CN101448085-B
TI Photograph processing method, involves executing face detection to frame of image, obtaining image feature information of face region, when face exists in frame of image, and adjusting image control parameters.
AB    NOVELTY - The method involves executing face detection to a frame of an image by utilizing a face detecting module. Image feature information of a face region is obtained by utilizing an image processing module, when the face exists in the frame of the image. The image feature information of the face region is utilized as evidence, and image control parameters are adjusted. A selection unit is utilized for selecting a pixel point of red green blue (RGB) values in the face region located in the RGB color space corresponding to a predetermined skin color.
   USE - Method for processing a photograph.
   ADVANTAGE - The method can support the photograph processing of face detection, and obtain an optimal photograph effect.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a photograph processing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a photograph processing method.'(Drawing includes non-English language text)'
PD CN101448085-A   03 Jun 2009   H04N-005/232   200941   Pages: 19   Chinese
   CN101448085-B   21 Aug 2013   H04N-005/232   201375      Chinese
UT DIIDW:2009K37628
ER

PT P
PN CN101281598-A; CN100557624-C
TI Face recognition method, involves calculating euclidean distance between mixed projection characteristic value of face to be recognized and value of known face as similarity between part image of face to be recognized.
AB    NOVELTY - The method involves processing a Gabor filter to all face images in a training gather to obtain a Gabor characteristic image of five parts. A projection characteristic value of a gray level image of five face parts is extracted. A mixed projection characteristic value of the five face parts is obtained. A euclidean distance between the mixed value of a face to be recognized and the value of the known face as the similarity between the part image of the face to be recognized is calculated. An integrated similarity of the face to be recognized and the known face is obtained.
   USE - Method for recognizing face based on a multi-part multi-characteristic integration.
   ADVANTAGE - The method recognizes the face with higher face recognition rate.
PD CN101281598-A   08 Oct 2008   G06K-009/00   200874   Pages: 8   Chinese
   CN100557624-C   04 Nov 2009   G06K-009/00   201001      Chinese
UT DIIDW:2008M52259
ER

PT P
PN US2008037839-A1; US7460694-B2
TI Real-time face detecting method for e.g. digital image acquisition device, involves applying fast face detector to portion of acquired image to provide set of candidate face regions according to determined orientation.
AB    NOVELTY - The method involves determining an orientation of a digital image acquisition device for an image of an image stream. The image is acquired from the stream. Fast face detector (280) is applied to a portion of the acquired image to provide a set of candidate face regions (141-146) according to the determined orientation. Face recognition algorithm (160) with a database (161) is applied to a candidate face region. An identifier is provided for a recognized face in the candidate region. The identifier for the recognized face is stored in association with the image of the stream.
   USE - Method for detecting real-time face in an image stream utilized in a digital image acquisition device, hand held image acquisition device e.g. digital camera, hand-held computer and cellular phone equipped with a camera, and image processing apparatus (claimed).
   ADVANTAGE - The method applies the fast face detector to the portion of the acquired image to provide the set of candidate face regions according to the determined orientation, thus detecting, tracking or recognizing faces within the acquired digital images of the image stream. The method avoids the calculations of a complete highest resolution integral image for the acquired image in the image stream, thus reducing integral image calculations in a face tracking system, and reducing processing overhead for face detection and tracking, and hence significantly improving the performance and accuracy of real-time face detection and tracking.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of components of an image processing apparatus.
   Integral image generator (115)
   Candidate face regions (141-146)
   Face recognition algorithm (160)
   Database (161)
   Motion sensor (180)
   Fast face detector (280)
PD US2008037839-A1   14 Feb 2008   G06K-009/62   200815   Pages: 12   English
   US7460694-B2   02 Dec 2008   G06K-009/00   200903      English
UT DIIDW:2008C05691
ER

PT P
PN WO2007108225-A1; JP2007257221-A; EP2026233-A1; CN101405744-A; KR2009008256-A; TW200741562-A; US2009110248-A1; JP4862447-B2; US8340366-B2; EP2026233-A4
TI Face recognition system for personal identification apparatus used in building, extracts feature value from selected region of face and compares with persons face, to recognize the user.
AB    NOVELTY - A database stores the face image along with feature value of persons face acquired from the input unit, and a face position detector detects position of face region from stored image. A feature value extraction unit extracts feature value from selected region of face with respect to persons face stored in database. A recognition unit compares the extracted feature value with the persons face and recognizes the user.
   USE - For personal identification apparatus used in preventing approach of unqualified person into building.
   ADVANTAGE - The feature value selection unit selects the region of face and extracts a feature value so that even if the case when person sticks the mask and eye patch or changes the hair style, the face can be recognized appropriately with reduced user burden.
   DESCRIPTION OF DRAWING(S) - The figure shows a structural block diagram of face recognition system.(Drawing includes non-English language text)
PD WO2007108225-A1   27 Sep 2007   G06F-021/20   200776   Pages: 20   Japanese
   JP2007257221-A   04 Oct 2007   G06F-021/20   200776   Pages: 11   Japanese
   EP2026233-A1   18 Feb 2009   G06F-021/20   200914      English
   CN101405744-A   08 Apr 2009   G06F-021/20   200929      Chinese
   KR2009008256-A   21 Jan 2009   G06F-021/20   200929      
   TW200741562-A   01 Nov 2007   G06K-009/62   200933      Chinese
   US2009110248-A1   30 Apr 2009   G06K-009/00   200938      English
   JP4862447-B2   25 Jan 2012   G06F-021/20   201207   Pages: 10   Japanese
   US8340366-B2   25 Dec 2012   G06K-009/00   201306      English
   EP2026233-A4   02 Dec 2009   G06F-021/20   201747      English
UT DIIDW:2007816195
ER

PT P
PN WO2007016936-A1; EP1910977-A1; KR2008033486-A; US2009074259-A1; BR200520469-A2; US8275175-B2; KR1185525-B1; EP1910977-B1
TI Automatic biometric identification method for e.g. security application, involves training support vector machine based on feature vectors, where positive examples of user`s class are used for training support vector machine.
AB    NOVELTY - The method involves acquiring a number of user`s face images. A Fourier-based transform is computed in a frequency domain of each image to generate a respective spectrum. A feature vector for each image is formed based on coefficients of frequencies of the respective spectrum. A support vector machine (SVM) that is a one-class SVM, is trained based on the feature vectors, to generate a user`s reference template that is support vectors that characterize a user`s class. Positive examples of the user`s class are used for training the SVM. The user is identified based on the template.
   USE - Used for a security application and human-machine interface, to automatically identify a biometric sample from a user. Can also be used for an embedded system e.g. handheld device, cellular phone and smart phone.
   ADVANTAGE - The positive examples of the user`s class are used for training the support vector machine, thus performing a very fast and significantly less resource consuming face recognition procedure, and maintaining a high level of recognition.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an automatic biometric identification system based on face recognition and support vector machines
   (2) a computer program product that is loaded and run in a processing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block representation of an enrollment sub-system.
   Enrollment sub-system (1)
   Biometric sample image acquisition module (2)
   Image processing module (3)
   Face detection and extraction module (4)
   Image selection module (5)
   Image scaling and normalization module (6)
   Feature extraction module (7)
   One-class support vector machine training module (8)
PD WO2007016936-A1   15 Feb 2007   G06K-009/62   200734   Pages: 44   English
   EP1910977-A1   16 Apr 2008   G06K-009/62   200829      English
   KR2008033486-A   16 Apr 2008   G06K-009/62   200870      
   US2009074259-A1   19 Mar 2009   G06K-009/00   200924      English
   BR200520469-A2   13 Oct 2009   G06K-009/62   200968      
   US8275175-B2   25 Sep 2012   G06K-009/00   201263      English
   KR1185525-B1   24 Sep 2012   G06K-009/62   201268      
   EP1910977-B1   30 Nov 2016   G06K-009/62   201679      English
UT DIIDW:2007360919
ER

PT P
PN CN1908960-A; CN100426314-C
TI Feature classification based multiple classifiers combined people face recognition method.
AB    NOVELTY - The disclosed multi-classifier combination face recognition method based on feature sorting comprises: extracting face area from initial image for pre-process and feature extraction; feature sorting to obtain different face feature groups; designing component classifier for every group to recognize face and combine results for optimal effect. This invention overcomes dimension disaster, reduces algorithm complexity, and improves recognition performance.
PD CN1908960-A   07 Feb 2007   G06K-009/00   200741      Chinese
   CN100426314-C   15 Oct 2008   G06K-009/00   200907      Chinese
UT DIIDW:2007422519
ER

PT P
PN US2006050932-A1; US7212655-B2
TI Biometric verification system for security application in computer system, compares user and prestored facial images for rank ordering database, before comparing user fingerprint with fingerprints associated with rank-ordered database.
AB    NOVELTY - A processor compares acquired digital representation of user facial image with facial images prestored in database, using heuristic ordering software, for generating a match confidence and rank-ordering the database from highest to lowest match confidence. A digital representation of user fingerprint is compared with stored fingerprints associated with rank-ordered database, to verify user identity and output recognition result.
   USE - For remote keyless entry system and for myriad security applications for providing control access to facility, computer system for financial transaction.
   ADVANTAGE - Enables verification of individual fingerprint of user in a large user database, rapidly using facial image based heuristic search algorithm.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) heuristic search method for controlling access to secure areas; and
   (2) one-to-many biometric verification system.
   DESCRIPTION OF DRAWING(S) - The figure shows a functional block diagram explaining the principal component analysis (PCA) of a biometric verification system.
   facial images of person (171,481,482,488)
   PCA unit (400)
PD US2006050932-A1   09 Mar 2006   G06K-009/00   200623   Pages: 17   English
   US7212655-B2   01 May 2007   G06K-009/00   200730      English
UT DIIDW:2006220177
ER

PT P
PN US2005226471-A1; WO2005096216-A1; US7542592-B2
TI Image processing method for e.g. surveillance application, involves fusing outputs of recognizers that recognize image object, and recognizing face from image database, using fused output of recognizers.
AB    NOVELTY - The method involves detecting an image object using a set of pose-specific object detectors. Outputs from the detectors are fused. The image object is recognized using set of pose-specific object recognizers (301-303) that utilized the outputs and the fused outputs from the detectors. The outputs of the recognizers are then fused. The face is recognized from an image database, using the fused output of recognizers.
   USE - Used for processing an image for face detection and recognition in surveillance and access control applications.
   ADVANTAGE - The image object is recognized using the set of pose-specific object recognizers, thus reducing the range of feasible operating conditions, and capture images robust to variations in the external illumination conditions. The method also provides good quality pictures with high signal-to-noise ratio in the outside lighting conditions with active infrared lighting.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a method for detecting and recognizing a face in an image
   (B) a program storage device readable a program of instructions executable by a machine to recognize a face in an image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a multi-pose detection and recognition system.
   Pose-specific object detectors (201-203)
   Pose-specific object recognizers (301-303)
   Object database (331-333)
   Recognizer fusion module (34)
PD US2005226471-A1   13 Oct 2005   G06K-009/00   200574   Pages: 15   English
   WO2005096216-A1   13 Oct 2005   G06K-009/00   200574      English
   US7542592-B2   02 Jun 2009   G06K-009/00   200937      English
UT DIIDW:2005723775
ER

PT P
PN WO2005057472-A1; CN1627317-A; CN1898678-A; JP2007516525-W; CN100361135-C; US2008212849-A1
TI Human face recognition method involves irradiating token area of human face using initiating lamp-house, so that influence of beam intensity of initiative lamp house is more than beam intensity of environment lamp house.
AB    NOVELTY - The token area of a human face is irradiated using an initiating lamp-house in different irradiation condition, so that influence of beam intensity of initiative lamp house is more than beam intensity of environment lamp house, to acquire face image using an electron image collection unit.
   USE - For recognizing of human face, for taking photos.
   ADVANTAGE - Reduces the change of light influence on human face images in different irradiation conditions, efficiently.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for human face image acquisition system.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic view of the face image recognition device. (Drawing includes non-English language text).
PD WO2005057472-A1   23 Jun 2005   G06K-009/00   200549   Pages: 37   Chinese
   CN1627317-A   15 Jun 2005   G06K-009/78   200566      Chinese
   CN1898678-A   17 Jan 2007   G06K-009/00   200740      Chinese
   JP2007516525-W   21 Jun 2007   G06T-001/00   200742   Pages: 39   Japanese
   CN100361135-C   09 Jan 2008   G06K-009/00   200833      Chinese
   US2008212849-A1   04 Sep 2008   G06K-009/00   200859      English
UT DIIDW:2005488335
ER

PT P
PN WO2005050583-A1; CN1597399-A; US2006261931-A1; CN1910633-A
TI Vehicle security defense alarm system has recording unit that records statuses on inside and outside of vehicle during, before and after emergent event picked up by camera of face identification and discrimination unit.
AB    NOVELTY - The system has a redundancy module, anti-interference module, anti-destructive detection module and a recording unit that records the statuses on inside and outside of vehicle during, before and after an emergent event picked up by a camera of face identification and discrimination unit. A multi-functional burglar alarm transmits a radio image to a remote equipment for reporting an incident.
   USE - Vehicle security defense alarm system. Also applicable for supervision systems.
   DESCRIPTION OF DRAWING(S) - The figure shows the schematic view of vehicle security defense alarm system.
PD WO2005050583-A1   02 Jun 2005   G08B-013/194   200545   Pages: 47   Chinese
   CN1597399-A   23 Mar 2005   B60R-025/00   200545      Chinese
   US2006261931-A1   23 Nov 2006   B60R-025/10   200678      English
   CN1910633-A   07 Feb 2007   G08B-013/194   200743      Chinese
UT DIIDW:2005444697
ER

PT P
PN US2003142853-A1; US7136513-B2
TI Automated entry checkpoint identification and screening system for airport passenger terminals, has facial recognition program for comparing video images taken of face of subject with pre-stored images in database for potential match.
AB    NOVELTY - The automated entry checkpoint identification and screening system includes a facial recognition program for comparing the video images taken of the face of a subject with pre-stored images in a database for potential match. A transmitter provides instructions to the subject to make movements until the subject's face is in position for taking video images.
   USE - Used in screening and identifying individuals passing through the secure entry or checkpoints in e.g. airport passenger terminal, government offices, office buildings, military facilities, laboratories.
   ADVANTAGE - Improves image quality in facial recognition systems deployed at security checkpoints. Enables detecting, stopping and instructing a subject at a security checkpoint to move into position so that facial identification and recognition may be performed on the subject. Overcomes lighting problems encountered in facial identification and recognition systems deployed at security checkpoints having poor ambient lighting. Obtains optimal view of the subject's facial characteristics by performing facial identification and recognition on the subject.
   DETAILED DESCRIPTION - A facial scanning program determines whether the face of the subject is in position for taking video images using a camera (25). A sensor (19) detects the presence of a subject at the checkpoint. INDEPENDENT CLAIMS are also included for the following:
   (a) an automated method for improving the quality of video images obtained by a facial recognition system at an entry checkpoint; and
   (b) an apparatus for improving the quality of the video images obtained by a facial recognition system at an entry checkpoint.
   DESCRIPTION OF DRAWING(S) - The figure shows the explanatory view of the automated entry checkpoint identification and screening system.
   Post (11)
   Sensor (19)
   Window (20)
   Sound holes (22)
   Camera (25)
PD US2003142853-A1   31 Jul 2003   G06K-009/00   200401   Pages: 15   English
   US7136513-B2   14 Nov 2006   G06K-009/00   200675      English
UT DIIDW:2004009337
ER

PT P
PN US2003108223-A1; US6751340-B2
TI Image alignment method for unknown person identification, involves identifying coincident minutiae in two different spectral images, and stretching, warping and shrinking one image with respect to image of standard anatomy.
AB    NOVELTY - The spectrum dependent minutiae of each spectral image is identified, and the coincident minutiae of two different spectral images are identified. The image is stretched, warped and shrunk with respect to image of standard anatomy.
   USE - For unknown person identification in infrared surveillance imagery, and medical sensor images, and for face animation during photography of criminal in driving license and passport photographs.
   ADVANTAGE - Image alignment is performed reliably, based on the anatomical structure. Bandwidth is reduced significantly and automatic resynchronization of voice and image is performed reliably. Ensures the authentication of the image reliably.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) unknown person identification method;
   (2) facial expression encoding method;
   (3) head video compression method; and
   (4) head video compression apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the image alignment apparatus.
   minutiae processor (56)
   comparator (58)
   evaluator (60)
PD US2003108223-A1   12 Jun 2003   G06K-009/00   200362   Pages: 25   English
   US6751340-B2   15 Jun 2004   G06K-009/00   200439      English
UT DIIDW:2003659039
ER

PT P
PN US2003072474-A1; WO2003034729-A1; AU2002349896-A1; KR2004045826-A; US6970580-B2; IN200400763-P4; BR200213296-A; IL161313-A; KR921842-B1; IN237701-B; SG103994-B; BR200213296-B1
TI Video image maintenance system for wireless communication system, has input device receiving video image which is analyzed by processor configured to recognize and compare face characteristics of image with preset value.
AB    NOVELTY - The system has an image-recognition processor (126) analyzing a video image received by a video-input device (118) .The processor is configured to recognize and compare the face characteristic of the image with predetermined face characteristic, and generate feedback signal. A signaling device (168) generates user signal based on the feedback signal and a transmitter (170) transmits the image to a remote location.
   USE - Used for maintaining a video image in wireless communication devices e.g. cellular phones.
   ADVANTAGE - The system signals to the user for need of action and maintains the video image in proper orientation in image frame.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method to provide feedback on a face in a video image received by wireless communication device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating the wireless communication device with video image feedback system.
   Voice-input device (118)
   Image -recognition processor (126)
   Signaling device (168)
   Transmitter (170)
PD US2003072474-A1   17 Apr 2003   G06K-009/00   200356   Pages: 10   English
   WO2003034729-A1   24 Apr 2003   H04N-007/14   200356      English
   AU2002349896-A1   28 Apr 2003   H04N-007/14   200461      English
   KR2004045826-A   02 Jun 2004   H04N-007/15   200465      
   US6970580-B2   29 Nov 2005   G06K-009/00   200578      English
   IN200400763-P4   13 Jan 2006   H04N-007/14   200615      English
   BR200213296-A   23 May 2006   H04N-007/14   200637      
   IL161313-A   18 Nov 2009   G06K-009/00   200977      English
   KR921842-B1   13 Oct 2009   H04N-007/15   201004      
   IN237701-B   08 Jan 2010   H04N-007/14   201016      English
   SG103994-B   31 May 2006   G06K-009/00   201407      English
   BR200213296-B1   11 Jul 2017   H04N-007/14   201752      English
UT DIIDW:2003596753
ER

PT P
PN EP1239405-A2; US2002126880-A1; JP2002334335-A; KR2002072198-A; US7050608-B2; EP1239405-A3; KR481116-B1
TI Face image recognition apparatus, for use in security management, has a feature amount adding section if the recognition rated is below a given value, e.g. if a beard, or if spectacles are worn by the person.
AB    NOVELTY - The apparatus includes a registration information holding section (108) in which a reference feature amount of the face of at least one to-be-recognized person is previously registered. A recognition section (107) determines the recognition rate between the extracted feature amount and the reference feature amount registered in the registration information holding section.
   USE - In security management to control passage of a person base on a face image.
   ADVANTAGE - Can alleviate a lowering in the person recognition rate due to a variation in the face image caused by a variation in the standing position of a person and a variation in the face itself, e.g. if a beard is grown, or spectacles are worn, and recognize the face image with high precision.
   DETAILED DESCRIPTION - A feature amount adding section (109) additionally registers the feature amount extracted by the feature amount extracting section (106) as a new reference feature amount into the registration information holding section when it is determined that the determined recognition rate is lower than a preset value. The recognition section calculates similarity between the feature amount extracted by the feature amount extracting section and the reference feature amount registered in the memory (108) and recognizes the face image input by the image input section (105) based on the calculated similarity. The feature amount adding section determines that the recognition rate of the recognition section is lower than a preset value when the similarity calculated by the recognition section is smaller than a preset determining reference value.
   A camera (101) is used to photograph a face image of a person and an illumination device (102, 103) used to apply light toward a face of a to-be-photographed person to be photographed by the camera and in which the image input section inputs the face image photographed by the camera.
   DESCRIPTION OF DRAWING(S) - The figure shows the face image recognition apparatus.
   person being scrutinized (100)
   camera (101)
   illumination device (102,103)
   image input section (105)
   feature amount extracting section (106)
   recognition section (107)
   registration information holding section (108)
   feature amount adding section (109)
PD EP1239405-A2   11 Sep 2002   G06K-009/66   200360   Pages: 15   English
   US2002126880-A1   12 Sep 2002   G06K-009/00   200360      English
   JP2002334335-A   22 Nov 2002   G06T-007/00   200362   Pages: 10   Japanese
   KR2002072198-A   14 Sep 2002   G06K-009/00   200362      
   US7050608-B2   23 May 2006   G06K-009/00   200635      English
   EP1239405-A3   19 May 2004   G06K-009/66   201731      English
   KR481116-B1   07 Apr 2005   G06K-009/00   200568      
UT DIIDW:2003629038
ER

PT P
PN US2002054714-A1; WO200237421-A1; AU200183863-A; EP1332473-A1; JP2004512884-W; US6959119-B2
TI Cosmetic product effectiveness demonstration method for consumer, involves allowing consumer to compare transformed image with displayed image.
AB    NOVELTY - The image of portion of body over which selected cosmetic product is applied, is captured and displayed. The selected body feature on the displayed image is digitally transformed in conformance with the predicted effect of the cosmetic product and is compared with the displayed image until the consumer has chosen an optimal transformation.
   USE - For demonstrating effectiveness of cosmetic product on a portion of consumer's body e.g. facial skin, texture, sags, lines, wrinkles and radiance/color.
   ADVANTAGE - Consumers are allowed to regulate the degree to which they adjust each component of a composite cosmetic attribute. Capable of displaying a transformed image which contains an attribute not present in the captured image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) Cosmetic needs identification method;
   (2) Kit for displaying consumer's skin image; and
   (3) Cosmetic product treatment affect identification method.
PD US2002054714-A1   09 May 2002   G06K-009/36   200255   Pages: 5   English
   WO200237421-A1   10 May 2002   G06T-011/00   200255      English
   AU200183863-A   15 May 2002   G06T-011/00   200258      English
   EP1332473-A1   06 Aug 2003   G06T-011/00   200353      English
   JP2004512884-W   30 Apr 2004   A45D-044/00   200430   Pages: 31   Japanese
   US6959119-B2   25 Oct 2005   G06T-003/00   200570      English
UT DIIDW:2002518200
ER

PT P
PN EP897680-A2; JP11056469-A; KR99023537-A; TW360508-A; US6333985-B1; JP2004329965-A; JP3611007-B2; JP3927569-B2; EP897680-A3; KR443877-B1
TI Method of selecting a suitable hairstyle - analyses suitability with regard to the formation, balance and image of the user.
AB       NOVELTY - The selection method involves an analysis being performed to determine if the selected hairstyle is suitable with regard to the formation, balance and image of the user.
   DETAILED DESCRIPTION - The method of analysis involves five elements of balancing between upper and lower parts of a face, the profile, face line and balance between the head and face. The standard proportion between hair and face comprises the users face being imposed on a standard face which is compatible with the desired hair style. The ratio for proper balance is 1:1, this should apply between the eyes and the fringe and from the eyes to the jaw, and it should also be the ratio between length of forehead, distance from the base of the forehead to the tip of the nose, and from the nose to the jaw. An INDEPENDENT CLAIM is included for an image map for a hairstyle.
   USE -   Especially for women to select hairstyles which suit them.
   ADVANTAGE -   Lets women assess whether or not to go ahead with a chosen style.
   DESCRIPTION OF DRAWING(S) - The figure shows a woman with a chosen hairstyle. (1) woman.
PD EP897680-A2   24 Feb 1999   A45D-044/00   199912   Pages: 40   English
   JP11056469-A   02 Mar 1999   A45D-044/00   199919   Pages: 14   Japanese
   KR99023537-A   25 Mar 1999   G06K-009/46   200024      
   TW360508-A   11 Jun 1999   A45D-044/00   200027      Chinese
   US6333985-B1   25 Dec 2001   G06K-009/00   200206      English
   JP2004329965-A   25 Nov 2004   A45D-044/00   200477   Pages: 17   Japanese
   JP3611007-B2   19 Jan 2005   A45D-044/00   200507   Pages: 16   Japanese
   JP3927569-B2   13 Jun 2007   A45D-044/00   200740   Pages: 16   Japanese
   EP897680-A3   29 Dec 1999   A45D-044/00   201735      English
   KR443877-B1   28 Oct 2004   G06K-009/46   200516      
UT DIIDW:1999134432
ER

PT P
PN US9836642-B1
TI Computing device for detecting fraud by utilizing facial recognition, has processor denying access to computing device in response to detecting that evidence in image indicates artificial two dimensional representation presented to camera.
AB    NOVELTY - The device has a processor for detecting evidence in first image data indicative of an artificial two dimensional representation of a human face presented to a camera, where the evidence includes specular reflections caused by light emitted by an illumination device, and a pattern of the specular reflections comprising a characteristic of a flat surface. The processor denies an access to the computing device in response to detecting that the evidence in the image indicates the artificial two dimensional representations presented to the camera.
   USE - Computing device for detecting fraud by utilizing facial recognition. Uses include but are not limited to a mobile phone, a tablet computer, a smart phone, an electronic book reader, and personal computers, such as desktop or laptop computers.
   ADVANTAGE - The device can use image information to determine gestures or motions of a user, which enables the user to provide an input through a portable device without a need to contact or move the portable device, and prevents malicious or unauthorized users from attempting to gain access to the device by presenting an artificial representation e.g. picture, photograph, and mask, of an authorized user.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer implemented method for detecting fraud by utilizing facial recognition by a computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a process for utilizing facial recognition fraud detection.
   Step for capturing image by using digital camera embedded in device (501)
   Step for analyzing image captured by camera to match human face (502)
   Step for detecting evidence indicative of artificial representation of human face in image (503)
   Step for denying access to user in response to detecting evidence indicative of artificial representation of human face (504)
PD US9836642-B1   05 Dec 2017   G06K-009/00   201781   Pages: 19   English
UT DIIDW:201782294M
ER

PT P
PN CN107423707-A
TI Complex environment based human face emotion recognition method, involves combining reinforcement learning algorithm, back propagation algorithm and discarding algorithm for promoting accuracy and reliability of identified facial expression.
AB    NOVELTY - The method involves performing Bayesian network shaking and moving process under complicated condition. Variation inference is combined with a Markov chain. Face displaying process is performed in multi-face environment under noisy background condition using a deep convolutional neural network to obtain super-resolution resistance of a super resolution generative adversarial network neural network. Reinforcement learning algorithm, back propagation algorithm and discarding algorithm are combined for effectively promoting effect, accuracy and reliability of identified facial expression.
   USE - Complex environment based human face emotion recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a complex environment based human face emotion recognition method. '(Drawing includes non-English language text)'
PD CN107423707-A   01 Dec 2017   G06K-009/00   201801   Pages: 14   Chinese
UT DIIDW:201784877X
ER

PT P
PN CN107066983-A; WO2018192406-A1
TI Identity authentication method, involves determining reliability of object to be verified as living body according to target face image, and verifying object to be verified according to reliability of object and target face image.
AB    NOVELTY - The method involves issuing operation hint information to an object to be verified. A video stream data of the object to be verified is obtained. A continuous frame human face image is obtained by the video stream data when the object to be verified is based on operation prompt information. A target face image is determined according to the video stream data. A reliability of the object to be verified is determined as a living body according to the target face image. The object to be verified is verified according to the reliability and the target face image.
   USE - Identity authentication method.
   ADVANTAGE - The method enables effectively blocking a photo in a face recognition process, video and human model and various types of attacks in a simple manner with high safety.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an identity authentication device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating of an identity authentication method. '(Drawing includes non-English language text)'
PD CN107066983-A   18 Aug 2017   G06K-009/00   201766   Pages: 24   Chinese
   WO2018192406-A1   25 Oct 2018   G06K-009/00   201871      Chinese
UT DIIDW:201758026H
ER

PT P
PN CN106874347-A; CN106874347-B
TI Method for matching human characteristic or physical characteristic with media access control address, involves obtaining media access control record of mobile device, and obtaining matching degree of mobile device.
AB    NOVELTY - The method involves receiving a video image at monitoring range. A human face image and/or body image are obtained. Photography time is calculated to generate image data. The image data is stored in an image log database. Face characteristics or physical characteristics are extracted for face clustering or physical clustering. A media access control (MAC) record of a mobile device is obtained. A moving track of the mobile device is drawn according to a time sequence. A human motion track traversing result is obtained. Matching degree of the mobile device is obtained.
   USE - Method for matching a human characteristic or physical characteristic with an MAC address.
   ADVANTAGE - The method enables matching a human characteristic or physical characteristic with an MAC address, realizing association identification and obtaining a human identity corresponding to the MAC address.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for matching a human characteristic or physical characteristic with an MAC address.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for matching human characteristic or physical characteristic with MAC address. '(Drawing includes non-English language text)'
PD CN106874347-A   20 Jun 2017   G06F-017/30   201757   Pages: 16   Chinese
   CN106874347-B   01 Dec 2020   G06F-016/51   202000      Chinese
UT DIIDW:2017445134
ER

PT P
PN CN106503615-A; CN106503615-B
TI Multiple sensors based indoor human body detection tracking and identity recognition method, involves determining low rejection rate of classifiers set for performing human face image area cascading process.
AB    NOVELTY - The method involves performing acceleration rectangular image area detecting process. An accelerate Haar-like input characteristic of an image structure is obtained. A face and non-face creator classifier node is established by utilizing Adaboost algorithm. Weak classifier node screening process is performed for obtaining a Haar-like feature training decision tree of a human face. Area denial image amount is reduced in a face image region by a sub-optimal classifier. A low rejection rate of a classifiers set is determined for performing human face image area cascading process.
   USE - Multiple sensors based indoor human body detection tracking and identity recognition method.
   ADVANTAGE - The method enables improving human body identity recognition efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multiple sensors based indoor human body detection tracking and identity recognition method. '(Drawing includes non-English language text)'
PD CN106503615-A   15 Mar 2017   G06K-009/00   201724   Pages: 23   Chinese
   CN106503615-B   08 Oct 2019   G06K-009/00   201979      Chinese
UT DIIDW:201719047Y
ER

PT P
PN CN106384237-A
TI Face recognition based member authentication and management method, involves prompting consumption information to current consumer and adding member to prompt information while detecting that living body is not in designated area in scene.
AB    NOVELTY - The method involves monitoring a designated area in scene by an infrared camera device. A face image of a current consumer is extracted from a consumer image when the consumer image in the scene is monitored. Member identity information and consumption information are prompted to the current consumer while detecting that a living body is in the designated area in the scene. The consumption information is prompted to the current consumer and a member is added to the prompt information while detecting that a living body is not in the designated area in the scene.
   USE - Face recognition based member authentication and management method.
   ADVANTAGE - The method enables effectively discriminating consumers and photo or video without being influenced by lighting conditions.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a face recognition based member authentication and management device
   (2) a face recognition based member authentication and management system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a face recognition based member authentication and management method. '(Drawing includes non-English language text)'
PD CN106384237-A   08 Feb 2017   G06Q-020/40   201715   Pages: 17   Chinese
UT DIIDW:201711480B
ER

PT P
PN CN106096538-A; CN106096538-B
TI Sequential neural network model based human face identifying method, involves performing identified images characteristic expression similarity calculating process, and performing database known face image features extracting process.
AB    NOVELTY - The method involves reading an input image to be identified (S1). A human face position is detected in the image to be identified. Key point information and face position information are generated (S2) to perform image pre-processing operation. Neural network model image to be identified processing operation is performed (S3) to obtain a characteristic expression of the identified image. Identified images characteristic expression similarity calculating process is performed (S4). Database known face image features extracting process is performed to identify the image to be identified.
   USE - Sequential neural network model based human face identifying method.
   ADVANTAGE - The method reducing large calculation cost and providing a sequencing neural network structure for sequencing different characteristics to effectively reduce network parameter and save computing time.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a sequential neural network model based human face identifying device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a sequential neural network model based human face identifying method. '(Drawing includes non-English language text)'
   Step for reading input image to be identified (S1)
   Step for generating key point information and face position information to perform image pre-processing operation (S2)
   Step for performing neural network model image to be identified processing operation to obtain characteristic expression of identified image (S3)
   Step for performing identified images characteristic expression similarity calculating process (S4)
PD CN106096538-A   09 Nov 2016   G06K-009/00   201678      Chinese
   CN106096538-B   23 Aug 2019   G06K-009/00   201968      Chinese
UT DIIDW:201671631U
ER

PT P
PN CN105469065-A; CN105469065-B
TI Recursion neural network based discrete method for identifying emotion, involves designing support vector machine algorithm to classifier and using sequential coding in final expression of characteristics of emotional category forecast.
AB    NOVELTY - The method involves extracting video image signal for facial feature, detecting face and video image signal. The face region is obtained. The sequential coding is used with memory model of recurrent neural network. The obtained emotion representation vector is connected in series to obtain the final expression characteristic of the video data. The support vector machine algorithm is designed to support vector machine classifier and sequential coding is used in final expression of characteristics of emotional category forecast.
   USE - Recursion neural network based discrete method for identifying emotion.
   ADVANTAGE - The discrete emotion identification method can make use of the dynamic information in the process of emotion expression, so as to realize the accurate identification of the emotion of the participants in the video.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flowchart illustrating a recursion neural network based discrete method for identifying emotion. (Drawing includes non-English language text)
PD CN105469065-A   06 Apr 2016   G06K-009/00   201629   Pages: 13   English
   CN105469065-B   23 Apr 2019   G06K-009/00   201934      Chinese
UT DIIDW:2016238155
ER

PT P
PN US2016071111-A1; US9934504-B2
TI Computing device for capturing image information including user and performing facial recognition on captured image information, has processor for causing transaction to be completed once user is authenticated.
AB    NOVELTY - The device (104) has a processor for causing an image to be analyzed to attempt to recognize an identity of a person in the image, and for causing additional captured image information to be analyzed to verify whether the person in the image corresponds to a physical being. The processor authenticates a user (102) as a result of an identity of the person being recognized as the user associated with the identifying information and as a result of the person being verified to correspond to a physical being, and causes the transaction to be completed once the user is authenticated.
   USE - Computing device for capturing image information including a user and performing facial recognition on the captured image information. Uses include but are not limited to a smart phone, e-book reader, tablet computer, notebook computer, personal data assistant, video gaming console, portable media player, personal computer, cell phone and a laptop computer.
   ADVANTAGE - The device improves quality of image capture, so that the image can be fed to a facial recognition process, thus analyzing the image to attempt to locate a user's face and any landmarks or features of the face. The device determines presence of a user's head without requiring significant image processing, thus improving perspective determinations. The device provides user authentication without the user having to physically interact with the device, thus enabling a user to access information by looking at a camera and performing a simple gesture such as a smile.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer-implemented method for performing an image analysis for user authentication
   (2) a non-transitory computer-readable storage medium comprising a set of instructions for performing an image analysis for user authentication.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a computing device for capturing image information including a user and performing facial recognition on the captured image information.
   User (102)
   Computing device (104)
   Camera (106)
   Camera view (108)
PD US2016071111-A1   10 Mar 2016   G06Q-020/40   201620   Pages: 18   English
   US9934504-B2   03 Apr 2018   G06Q-020/40   201823      English
UT DIIDW:201615456F
ER

PT P
PN CN105354527-A
TI Negative expression recognition encouraging system, has characteristic base module for obtaining facial expression image and normalization stage of human face picture is determined according to returning result.
AB    NOVELTY - The system has a platform with a camera head that captures a human face picture. A basic expression system performs human face image preprocessing operation. Normalization stage of the human face picture is determined according to a returning result. The camera head is connected with a computer system. A characteristic base module obtains a facial expression image. An image enhancement unit is connected with an image normalization unit.
   USE - Negative expression recognition encouraging system.
   ADVANTAGE - The system is aesthetic in appearance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an operation of a negative expression recognition encouraging system. '(Drawing includes non-English language text)'
PD CN105354527-A   24 Feb 2016   G06K-009/00   201619   Pages: 8   English
UT DIIDW:201614187P
ER

PT P
PN US9176987-B1; CN105100894-A; CN105100894-B
TI Method for enabling automatic face annotation in video utilizing semi-supervised learning on social network data, involves labeling remaining unlabeled face tracks in input video, and outputting input video containing annotated face images.
AB    NOVELTY - The method involves generating a labeled database containing refined labeled images as training data. Exact frames containing face images are found and labeled in input video matching of refined labeled images in the labeled database based on the refined labeled images stored in the labeled database. Remaining unlabeled face tracks are labeled in an input video (302) by a semi-supervised learning algorithm to annotate the face images in the input video. The input video containing the annotated face images is output.
   USE - Method for enabling automatic face annotation in video utilizing semi-supervised learning on social network data by electronic devices. Uses include but are not limited to smart phones, tablets, PCs, and smart watches.
   ADVANTAGE - The method enables ignoring near-duplicated images in a camera after a camera registration operation by camera registration module, thus reducing throughput while maintaining an acceptable recognition accuracy. The method enables extracting temporal and spatial information by using camera take and shot boundary detection algorithms, thus formulating a semi-supervised learning problem with constraints, and hence dramatically increasing clustering accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an automatic face annotation system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of an automatic face annotation system in video consistent.
   Automatic face annotation system (300)
   Input video (302)
   Camera take detection module (304)
   Face matching module (310)
   Output module (312)
PD US9176987-B1   03 Nov 2015   G06K-009/00   201574   Pages: 12   English
   CN105100894-A   25 Nov 2015   H04N-021/44   201601      English
   CN105100894-B   05 May 2020   H04N-021/44   202041      Chinese
UT DIIDW:201567770T
ER

PT P
PN US9158962-B1; WO2015171397-A1
TI System for passive driver identification, has processor which assigns identifier that associates trip to first album.
AB    NOVELTY - The system has an input interface which receives a collection of face data from a vehicle event recorder (200). A processor (202) determines a set of face data of collection of face data that is associated with a trip. The processor determines a first album associated with trip. The set of face data associated with trip is similar to face data of other trips in first album. The set of face data associated with trip is dissimilar to face data of a set of trips in a second album. The processor assigns an identifier that associates trip to first album.
   USE - System for passive driver identification.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer program product for passive driver identification; and
   (2) a method for passive driver identification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a vehicle event recorder.
   Vehicle event recorder (200)
   Processor (202)
   Data storage (204)
   Wireless communications interface (206)
   Global positioning system (208)
PD US9158962-B1   13 Oct 2015   G06K-009/00   201570   Pages: 31   English
   WO2015171397-A1   12 Nov 2015   G06F-017/00   201575      English
UT DIIDW:2015614849
ER

PT P
PN US9129148-B1
TI Image processing system e.g. distributed computing system, for recognizing scene within image, selects subset of images based on image feature distances, determines scene type of image subset and assigns determined scene type to input image.
AB    NOVELTY - The system extracts a set of input image features from an input image and a set of image features corresponding to a distance metric from each image and computes an image feature distance between the input image features and each extracted image feature based on a set of image feature weights of the distance metric. The system selects a subset of images from a set of images based on computed image feature distances, determines a scene type of the subset of images and assigns the determined scene type to the input image.
   USE - Image processing system e.g. distributed computing system and cloud computing system (all claimed), for recognizing a scene within an image.
   ADVANTAGE - The system utilizes software application to determine facial feature position by selecting the facial feature position corresponding to local binary pattern feature template that corresponds to a maximum matching score for each facial feature in a detected face, so that learned metric derived from a distance metric learning process can significantly improve performance and accuracy in facial recognition in an efficient manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for recognizing a scene type of an image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a process for recognizing an image by an image processing computer.
   Step for receiving source scene image (1702)
   Step for selecting source scene image as input image (1710)
   Step for retrieving distance mettle from database (1712)
   Step for retrieve image features for each image categorized by image scene types (1716)
   Step for extracting input image features from input image (1717)
PD US9129148-B1   08 Sep 2015   G06K-009/00   201561   Pages: 49   English
UT DIIDW:2015521142
ER

PT P
PN CN104820675-A; EP3079082-A1; US2016300101-A1; WO2016161807-A1; KR2016132810-A; JP2017520809-W; MX2016004111-A1; BR112016011338-A2; IN201637008032-A; KR1820781-B1; RU2016126469-A; RU2647681-C2; US9953212-B2; MX359407-B; CN104820675-B; EP3079082-B1
TI Album displaying method for album display device, involves obtaining user photo by server, performing photo human face recognizing process, determining face category of photo, and sending human face identification information to server.
AB    NOVELTY - The method involves obtaining a user photo by using a server. Photo human face recognizing process is performed. Human face identification information is obtained corresponding to the photo human face recognizing process. A face category of the user photo is determined. Album face category identifying process is performed. Album user photo mark printing process is performed. Grid human face displaying process is performed. Album user photo searching process is performed. The human face identification information is sent to the server. A book number of the user photo is obtained.
   USE - Album displaying method for an album display device (claimed).
   ADVANTAGE - The method enables realizing album displaying process in an efficient manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an album display device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an album displaying method. '(Drawing includes non-English language text)'
PD CN104820675-A   05 Aug 2015   G06F-017/30   201569   Pages: 30   Chinese
   EP3079082-A1   12 Oct 2016   G06F-017/30   201667      English
   US2016300101-A1   13 Oct 2016   G06K-009/00   201668      English
   WO2016161807-A1   13 Oct 2016   G06F-017/30   201668      Chinese
   KR2016132810-A   21 Nov 2016   G06F-017/30   201679      
   JP2017520809-W   27 Jul 2017   G06Q-050/10   201750   Pages: 29   Japanese
   MX2016004111-A1   11 Apr 2017   G06F-017/30   201752      Spanish
   BR112016011338-A2   08 Aug 2017   G06F-017/30   201774      English
   IN201637008032-A   10 Nov 2017   G06F-017/30   201782      English
   KR1820781-B1   23 Jan 2018   G06F-017/30   201810      
   RU2016126469-A   12 Jan 2018   G06F-017/30   201812      Russian
   RU2647681-C2   16 Mar 2018   G06F-017/30   201821      Russian
   US9953212-B2   24 Apr 2018   G06K-009/00   201828      English
   MX359407-B   27 Sep 2018   G06K-009/62   201875      Spanish
   CN104820675-B   06 Nov 2018   G06F-017/30   201876      Chinese
   EP3079082-B1   28 Jul 2021   G06F-016/54   202162      English
UT DIIDW:201558185A
ER

PT P
PN WO2015020703-A1; AU2014304760-A1; KR2016041965-A; EP3028177-A1; CN105556508-A; JP2016532197-W; IN201647005202-A; EP3028177-A4; IL243910-A1; RU2016107189-A; BR112016002493-A2; JP6389888-B2; RU2668408-C2; CN105556508-B; CN110609617-A; AU2014304760-B2; IL243910-A; KR2266361-B1; KR2021073611-A
TI Computer implemented method for operating imaging and display system, involves opening user account and displaying thumbnail on monitor when user has existing account and setting temporary account when user has no account.
AB    NOVELTY - The method involves operating (1701) a display system in an idle mode such that an image captured by a camera is buffered to display on a monitor. The system is switched (1702) to virtual mirror mode when a presence of user is detected. The image is transformed to generate a mirror image. A user account is opened (1703) and a thumbnail corresponds to the account is displayed when the user has an existing account. A temporary account is set and user controls the account when the user has no existing account. The system is returned to idle mode when the user exits the field of view.
   USE - Computer implemented method for operating imaging and display system with monitor, camera and processor to display user image e.g. mirror-mimicking image on monitor used in retail and/or service environment, medical/home situation, video conferencing, gaming.
   ADVANTAGE - The regular virtual dressing provides reality of the item draped on the body since the folds, shading is transferred with the appropriate masks thus the user can sense how the item feels on the body, how the item effect and change his body shape. The combination of EyesMatch with the face recognition allows the operator to install a camera above a user height which allows the user to move freely below the camera.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a non-transitory computer-readable storage medium storing program for separating an object in a digital image;
   (2) a computer system programmed for performing a color change on an object within a digital image;
   (3) a method for operating a mirror-display system;
   (4) a computerized system to display a mirror-mimicking image on a monitor; and
   (5) a non-transitory computer-readable storage medium storing program to display a mirror-mimicking image on a monitor.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flowchart illustrating a computer implemented method for operating imaging and display system to display user image.
   Step for operating a display system in an idle mode (1701)
   Step for switching a system to a virtual mirror mode (1702)
   Step for opening an user account (1703)
   Step for allowing a user control (1704)
   Step for offering an incentive from a manufacturer (1706)
PD WO2015020703-A1   12 Feb 2015   G06F-017/00   201517   Pages: 79   English
   AU2014304760-A1   25 Feb 2016   G06F-017/00   201628      English
   KR2016041965-A   18 Apr 2016   G06T-019/00   201630      English
   EP3028177-A1   08 Jun 2016   G06F-017/00   201638      English
   CN105556508-A   04 May 2016   G06F-017/00   201639      Chinese
   JP2016532197-W   13 Oct 2016   G06T-019/00   201669   Pages: 54   Japanese
   IN201647005202-A   31 Aug 2016   G06F-017/00   201716      English
   EP3028177-A4   10 May 2017   G06T-011/00   201733      English
   IL243910-A1   31 Mar 2016   G06F-017/00   201761      English
   RU2016107189-A   07 Sep 2017   G06F-017/00   201772      Russian
   BR112016002493-A2   01 Aug 2017   G06F-017/00   201774      English
   JP6389888-B2   12 Sep 2018   G06T-019/00   201863   Pages: 42   Japanese
   RU2668408-C2   28 Sep 2018   G06F-017/00   201867      Russian
   CN105556508-B   16 Aug 2019   G06F-003/01   201965      Chinese
   CN110609617-A   24 Dec 2019   G06F-003/01   202002      Chinese
   AU2014304760-B2   16 Jan 2020   G06F-017/00   202006      English
   IL243910-A   25 Mar 2021   G06F-003/00   202135      English
   KR2266361-B1   16 Jun 2021   G06T-019/00   202152      
   KR2021073611-A   18 Jun 2021   G06T-019/00   202153      
UT DIIDW:201511415R
ER

PT P
PN CN103793697-A; CN103793697-B
TI Human face image marking method, involves identifying human face image based on marking option, and identifying picture appearance from returned pages, where human face image is identified by face identification module.
AB    NOVELTY - The method involves identifying human face image based on marking option. Picture appearance is identified from returned pages based on frequency level, where the human face image is identified by a face identification module based on human face technology platform. Picture identification result information is determined from a final dimensional picture determination unit and the face identification module. The picture identification result information is matched from an extraction filter. Training face images are marked based on a machine learning algorithm.
   USE - Human face image marking method.
   ADVANTAGE - The method enables improving image identification and marking efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for a human face image identification method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a human face image marking method. '(Drawing includes non-English language text)'
PD CN103793697-A   14 May 2014   G06K-009/00   201446   Pages: 9   Chinese
   CN103793697-B   01 May 2018   G06K-009/00   201831      Chinese
UT DIIDW:2014N14410
ER

PT P
PN CN103402203-A; CN103402203-B
TI Method for quickly accessing biometric identification characteristics of e.g. human face, involves containing biological characteristics authentication data with access network address, account number and password.
AB    NOVELTY - The method involves receiving user login trigger event data. Biological features of a current user are collected by a mobile terminal. Biological characteristics authentication data is contained with an access network address, an account number and a password. Biometric identification characteristics are formed as human face, iris, fingerprint and palm print characteristics. An authentication data access network is connected with the mobile terminal for receiving the account number of the authentication data. The account number is matched with the network address.
   USE - Method for quickly accessing biometric identification characteristics of human face, iris, fingerprint and palm print.
   ADVANTAGE - The method enables improving user convenience degree.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for quickly accessing biometric identification characteristics of human face, iris, fingerprint and palm print.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a method for quickly accessing biometric identification characteristics of human face, iris, fingerprint and palm print.'(Drawing includes non-English language text)'
PD CN103402203-A   20 Nov 2013   H04W-012/08   201406   Pages: 7   Chinese
   CN103402203-B   25 Aug 2017   H04W-012/08   201759      Chinese
UT DIIDW:2014B35185
ER

PT P
PN CN102982316-A
TI Driver abnormal driving behavior identifying device, has face pose detection module connected with control input end, and vehicle sensor module connected with input end of driving action analysis module that is linked to control module.
AB    NOVELTY - The device has a human face detection module for collecting and detecting a driver face. Human face characteristic points are calculated by a human face feature point locating module and matched with optimal human face feature shape. The human face detection module is connected with the human face feature point locating module. A face pose detection module is connected with a control input end that is connected with an output end of a vehicle sensor module. The vehicle sensor module is connected with an input end of a driving action analysis module that is connected with a control module.
   USE - Driver abnormal driving behavior identifying device.
   ADVANTAGE - The device ensures safe driving process, and has better working performance.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a driver abnormal driving behavior identifying method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a driver abnormal driving behavior identifying device.'(Drawing includes non-English language text)'
PD CN102982316-A   20 Mar 2013   G06K-009/00   201347   Pages: 15   Chinese
UT DIIDW:2013K43478
ER

PT P
PN US8331632-B1
TI Unknown face recognition algorithm modeling method for face recognition system used in e.g. laptop, involves using stress minimization with iterative majorization to determine set of coordinates from equivalent Euclidean matrix.
AB    NOVELTY - The method involves providing an unknown face recognition algorithm. A training set comprising multiple face images (70) is provided. A score matrix containing scores between the pairs of the face images of the training set produced by the unknown face recognition algorithm is provided, where each score is a distance between the pairs of the face images. The score matrix is converted to an equivalent Euclidean matrix (72). A stress minimization with iterative majorization is used to determine a set of coordinates for the training set from the equivalent Euclidean matrix.
   USE - Method for modeling an unknown face recognition algorithm for a face recognition system (FRS), which is used in a low-risk secure system e.g. laptop and cell phone, and a high-risk secure system e.g. military base and airport.
   ADVANTAGE - The method enables efficient modeling of the FRS, and reduction of face template comparisons in the FRS.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for indexing face templates in a recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram demonstrating modeling of a face recognition algorithm with a linear transformation.
   FRS (15)
   Face images (70)
   Equivalent Euclidean matrix (72)
   Model space (74)
   Linear model (75)
PD US8331632-B1   11 Dec 2012   G06K-009/00   201302   Pages: 23   English
UT DIIDW:2013A06708
ER

PT P
PN CN102609681-A; CN102609681-B
TI Dictionary learning model based human face identification method, involves mapping training and tested human face image to low dimensional space to obtain training signal collecting matrix.
AB    NOVELTY - The method involves mapping a training and tested human face image to low dimensional space to obtain a training signal collecting matrix. A dictionary learning model is divided into an irrelevant dictionary learning model and a free irrelevant dictionary learning model. A sparse vector is obtained based on a test sample image by a sparse expression algorithm. The sparse vector is input to a linear classifier to obtain a category tag of the test sample image. A dictionary related index value is added into the dictionary learning model.
   USE - Dictionary learning model based human face identification method.
   ADVANTAGE - The method enables solving pattern recognition and image classification problem. The method ensures high human face recognition accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a dictionary learning model based human face identification method.'(Drawing includes non-English language text)'
PD CN102609681-A   25 Jul 2012   G06K-009/00   201266   Pages: 12   Chinese
   CN102609681-B   30 Apr 2014   G06K-009/00   201440      Chinese
UT DIIDW:2012M57411
ER

PT P
PN US2011194732-A1; EP2357589-A2; JP2011165008-A; CN102147856-A; KR2011093659-A; EP2357589-A3; KR1280920-B1; CN102147856-B; US8559722-B2; CN103353933-A; JP5567853-B2; CN103353933-B; EP2357589-B1
TI Image recognition apparatus used for image capturing apparatus for detection of face of person has recognition unit to recognize whether object of input object image is same object as object of registered image information.
AB    NOVELTY - The recognition apparatus has recognition unit to recognize whether object of input object image (604-611) is same object as object of registered image (601-603) information based on coincidence degree between object recognizability states of input object image and registered image information and similarity between image features of input object image and registered image information. Objects of input object image and registered image information are different when similarity is lower than first threshold and coincidence degree is equal to or higher than second threshold.
   USE - Image recognition apparatus used for an image capturing apparatus, such as a video camera, for detection of face of a person. Can also be used for detection of face of an animal, such as a pet, and for recognition of an object, such as an automobile, and can also be used with a reproduction apparatus, such as a printer or display screen.
   ADVANTAGE - Image recognition apparatus accurately determines that an object detected from an image has not been registered in a database compared with simply comparing an input image with a database image. Processing efficiency is increased since a determination result obtained in a past frame by object recognition and determination of the status coincidence degree for the identical recognition target is inherited.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method of controlling an image recognition apparatus which recognizes an object in an input image; and
   (2) a non-transitory computer readable storage medium which stores a program for causing a computer to execute a method of controlling an image recognition apparatus.
   DESCRIPTION OF DRAWING(S) - The drawing shows the schematic diagram exemplifying the recognition processing.
   Registered images (601-603)
   Input object images (604-611)
   Object recognition execution result table for scene 1 (612)
   Object recognition execution result table for scene 2 (613)
PD US2011194732-A1   11 Aug 2011   G06K-009/62   201155   Pages: 15   English
   EP2357589-A2   17 Aug 2011   G06K-009/00   201155      English
   JP2011165008-A   25 Aug 2011   G06T-007/00   201156   Pages: 13   Japanese
   CN102147856-A   10 Aug 2011   G06K-009/00   201158      Chinese
   KR2011093659-A   18 Aug 2011   G06T-007/00   201159      
   EP2357589-A3   25 Jan 2012   G06K-009/00   201207      English
   KR1280920-B1   02 Jul 2013   G06T-007/00   201355      
   CN102147856-B   05 Jun 2013   G06K-009/00   201368      Chinese
   US8559722-B2   15 Oct 2013   G06K-009/00   201371      English
   CN103353933-A   16 Oct 2013   G06K-009/00   201402      Chinese
   JP5567853-B2   06 Aug 2014   G06T-007/00   201451   Pages: 14   Japanese
   CN103353933-B   28 Dec 2016   G06K-009/00   201704      Chinese
   EP2357589-B1   12 Jun 2019   G06K-009/00   201943      English
UT DIIDW:2011K15697
ER

PT P
PN US2011002510-A1; US8306279-B2
TI Operator interface system for hand-held or adjustable-mount iris recognition device of iris recognition system, has visible illuminator directed onto portion of face of subject, and infrared illuminator directed onto another portion of face.
AB    NOVELTY - The system has a visible illuminator (101) directed onto a portion of a face of a subject (102), and an infrared illuminator directed onto another portion of face. The former portion primarily does not include an eye region, where the latter portion primarily includes the region. A cheek region, two eyes, forehead and mouth are primarily included in the former portion. The visible illuminator is provided such that a position of the visible illuminator indicates whether a hand-held or adjustable-mount iris recognition device (100) is pointed in a correct direction to an operator (103).
   USE - Operator interface system for a hand-held or adjustable-mount iris recognition device of an iris recognition system.
   ADVANTAGE - The system has the infra-red illuminator directed to primarily illuminate the eye region when the device is pointed in a direction of the subject, where the visible illuminator is directed to primarily illuminate other regions including cheeks in order to avoid causing visual distraction or discomfort to the eyes of the subject. The operator can simultaneously look at the subject and at the illumination projected onto the face of the subject, without having to re-direct his/her gaze to a user feedback display or the illuminators mounted on the device. The system allows the operator to look at the subject continuously while operating the device.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for providing a feedback to an operator of an iris recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an iris recognition system in which a gaze of an operator is fixated on a face of a subject during iris recognition.
   Hand-held or adjustable-mount iris recognition device (100)
   Visible illuminator (101)
   Subject (102)
   Operator (103)
PD US2011002510-A1   06 Jan 2011   G06K-009/00   201109   Pages: 21   English
   US8306279-B2   06 Nov 2012   G06K-009/00   201274      English
UT DIIDW:2011A43344
ER

PT P
PN US2010310134-A1; US8325999-B2
TI Context driven assisted face recognition tagging system for digital images, has selection module selecting individual face image to present to user for manual identification based on context and adding user identified face image to database.
AB    NOVELTY - The system has a context driven pattern analyzer module to calculate probable identities of face images from a photo gallery of a user based upon a context of the face images in the gallery. A context driven sample selection module selects an individual face image to present to the user simultaneously for manual identification based upon the context. The sample selection module adds the user identified face image to a database of known face images associated with the user. A user-interface module delivers a user-supplied identification of the individual face image to the analyzer module.
   USE - Context driven assisted face recognition tagging (CDAFRT) system for digital images.
   ADVANTAGE - The system can perform context-driven face recognition to identify individual face images at a specified probability, so that the probability that the individual face images correctly identified can be increased than attempting to identify individual face images in isolation. The system utilizes context-driven analysis to determine the individual face image to present to the user for manual identification, thus decreasing number of user actions employed to correctly identify face images in the photo gallery and increasing possibility that the user can actually organize the photo gallery. The system detects face images in a set of photos of the user's photo gallery with minimized time required for searching the photos.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer-readable storage medium storing a set of instructions for performing a context-driven assisted face recognition tagging method of digital images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a context driven assisted face recognition tagging method.
PD US2010310134-A1   09 Dec 2010   G06K-009/00   201101   Pages: 17   English
   US8325999-B2   04 Dec 2012   G06K-009/00   201279      English
UT DIIDW:2010Q16326
ER

PT P
PN US7734070-B1
TI Method for inserting facial images of people into live video playback sequence, involves replacing replaceable actor image in video sequence with input images based on the demographic classifications.
AB    NOVELTY - The method involves receiving several input images and extracting one or more facial images (271) from the input images using a computer system. The demographic classifications are generated at the computer system, using the input images. A replaceable actor image in a video sequence is replaced with input images based on the demographic classifications. A movie (930) is created based on the replaced replaceable actor image.
   USE - Method for inserting facial images of people into live video playback sequence.
   ADVANTAGE - The newly created user-participated movie can be played with any movie player and shots of the immersed video images can be printed. The user can actively participate into movie content. The replacement of images can be automatically personalized and matched.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for apparatus for inserting facial images of people into live video playback sequence.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view explaining insertion of facial images.
   Imaging unit (110)
   Displaying unit (111)
   Facial images (271)
   Movie (930)
   Images (950)
PD US7734070-B1   08 Jun 2010   G06K-009/00   201038   Pages: 21   English
UT DIIDW:2010G39635
ER

PT P
PN CN101588443-A
TI TV audience rating statistical apparatus, has double cameras connected with infrared lamp and infrared light filter in parallel, and set-top box connected with background server by wireless or wired manner and connected with TV.
AB    NOVELTY - The apparatus has double cameras connected with an infrared lamp and an infrared light filter in parallel, and connected with a set-top box. The set-top box is connected with a background server in a wireless or wired manner, and is connected with a TV. The set-top box comprises a face detection module, a face identification module, a face tracing module, a number counting module, a data analysis and processing module and a wireless transmission module. The face detection module is connected with the face identification module, the face tracing module and the number counting module.
   USE - Statistical apparatus for rating TV audience based on face detection.
   ADVANTAGE - The apparatus counts the number of people watching specified advertisement within each time section, preference level, age, sex and standard of living of the watcher, and traces the faces detected in real time. The apparatus avoids repeated calculation and improves the accuracy of the audience rating.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a TV audience rating detection method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a TV audience rating statistical apparatus.'(Drawing includes non-English language text)'
PD CN101588443-A   25 Nov 2009   H04N-005/00   200982   Pages: 15   Chinese
UT DIIDW:2009S03128
ER

PT P
PN WO2009116049-A2; WO2009116049-A3; US2011182485-A1; US2013262588-A1; US8666198-B2; US2014072182-A1; US9064146-B2; US2015193471-A1; US9143573-B2; US2015294138-A1; US9275272-B2; US2016070954-A1; US9665765-B2; US2017220601-A1; US9984098-B2; US2018276245-A1; US10423656-B2; US10776418-B2
TI Persons'interpersonal relationship mapping method for social network, involves searching list of candidate persons based on priority to select one of candidate persons having predetermined relationship with person.
AB    NOVELTY - The method involves processing a set of images and contextual information relating to the images by creating and prioritizing a list of candidate persons having a predetermined relationship with a person connected to one of the image, where the list is created by utilizing multi-dimensional information including visually sensible information in the images and contextual information. The list of candidate persons is searched based on the priority to select one of the candidate persons having the predetermined relationship with the person.
   USE - Method for mapping interpersonal relationships between persons in a social network using an interpersonal relationship mapping system (claimed).
   ADVANTAGE - The method enables providing positive/negative feedback regarding whether a recognized person is the named person by effectively mapping interpersonal relationships between the persons in the social network.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an interpersonal relationship mapping system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic representation illustrating relationship mapping functionality by employing multi-dimensional context including facial recognition.
PD WO2009116049-A2   24 Sep 2009   G06T-007/20   200964   Pages: 36   English
   WO2009116049-A3   11 Mar 2010   G06K-009/00   201019      English
   US2011182485-A1   28 Jul 2011   G06K-009/68   201151      English
   US2013262588-A1   03 Oct 2013   H04L-029/08   201365      English
   US8666198-B2   04 Mar 2014   G06K-009/60   201417      English
   US2014072182-A1   13 Mar 2014   G06K-009/00   201420      English
   US9064146-B2   23 Jun 2015   G06K-009/00   201543      English
   US2015193471-A1   09 Jul 2015   G06F-017/30   201545      English
   US9143573-B2   22 Sep 2015   G06K-009/60   201563      English
   US2015294138-A1   15 Oct 2015   G06K-009/00   201569      English
   US9275272-B2   01 Mar 2016   G06K-009/60   201618      English
   US2016070954-A1   10 Mar 2016   G06K-009/00   201619      English
   US9665765-B2   30 May 2017   G06K-009/60   201737      English
   US2017220601-A1   03 Aug 2017   G06F-017/30   201752      English
   US9984098-B2   29 May 2018   G06K-009/00   201837      English
   US2018276245-A1   27 Sep 2018   G06F-017/30   201866      English
   US10423656-B2   24 Sep 2019   G06K-009/60   201974      English
   US10776418-B2   15 Sep 2020   G06F-016/583   202076      English
UT DIIDW:2009P03403
ER

PT P
PN US2009196509-A1; CN101505372-A; TW200939136-A; US8111942-B2; CN101505372-B; TW419062-B1
TI Electronic system e.g. facial recognition system, for use on computer system, has comparator coupled to image capturing device for generating feedback signal and for adjusting setting of capturing device by feedback signal.
AB    NOVELTY - The system has an image capturing device e.g. camera (202), for capturing a live image, and a comparator coupled to the image capturing device for generating a feedback signal by comparing the live image with a reference image and for adjusting a setting e.g. image brightness setting and image contrast setting, of the image capturing device by the feedback signal. The comparator determines a direction of adjustment of the setting of the image capturing device based on matching percentages.
   USE - Electronic system e.g. facial recognition system and video conference system, for use on a computer system (all claimed). Can also be used for on-line instant messenger.
   ADVANTAGE - The system is designed such that the camera settings are automatically optimized by the feedback signal generated by the comparator based on the live image and the reference image, thus enabling the system to reproduce original conditions e.g. lighting and/or color conditions of the reference image, and to optimize the recognition probability, thus improving the recognition process.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer system comprising a processor coupled to a storage system
   (2) a method for optimizing a setting of an image capturing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block representation of a computer system.
   Camera (202)
   Reference image database (204)
   Computer system (300)
   Processor (302)
   Storage system (304)
   Machine-readable instructions (308)
   Data interface (310)
PD US2009196509-A1   06 Aug 2009   G06K-009/62   200953   Pages: 9   English
   CN101505372-A   12 Aug 2009   H04N-005/232   200956      Chinese
   TW200939136-A   16 Sep 2009   G06K-009/78   201015      Chinese
   US8111942-B2   07 Feb 2012   G06K-009/40   201211      English
   CN101505372-B   11 Jul 2012   H04N-005/232   201273      Chinese
   TW419062-B1   11 Dec 2013   G06K-009/78   201423      Chinese
UT DIIDW:2009M27569
ER

PT P
PN CN101436348-A; CN101436348-B
TI Electronic police system for providing real punishment to rule breaking driver of vehicle, has license plate identifying unit for identifying license plate number represented by license plate image in each image group.
AB    NOVELTY - The system has a driver identifying unit that is internally stored with a set of driver images of an image group from a set of image groups and driving license information of drivers. The driver identifying unit identifies one driver image in each image group from the set of driver images, and acquires driving license information corresponding to the driver image, using a face detection and identification technology. A license plate identifying unit identifies license plate number represented by the license plate image in each image group, using a license plate identification technology.
   USE - Electronic police system for providing real punishment to rule breaking driver of a vehicle (claimed).
   ADVANTAGE - The configuration of the electronic police system identifies the driver image of the driver actually breaking the rules, and acquires driving license information of the driver actually breaking the rules when the holographic image is matched with the license plate image and the driver image in the same image group, thus ensuring real punishments for rule breaking drivers of the vehicle.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an information identifying method for an electronic police system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an electronic police system.'(Drawing includes non-English language text)'
PD CN101436348-A   20 May 2009   G08G-001/01   200938   Pages: 18   Chinese
   CN101436348-B   23 Apr 2014   G06K-009/00   201439      Chinese
UT DIIDW:2009K05097
ER

PT P
PN US2009060287-A1; CN101383859-A; KR2009025176-A; KR2009025177-A; JP2009171544-A
TI Physiological condition measuring device for e.g. communication transfer, has sensing system for measuring physiological condition through manipulation of output of device and analysis of user response.
AB    NOVELTY - The device (100) has a sensing system for measuring a physiological condition through manipulation of an output of the device and analysis of a user response. A communication device i.e. cellular telephone (102), comprises a processing unit enclosed by a housing (110). An image capture device i.e. camera (132), for capturing an image e.g. still image, is coupled to the processing unit, where the communication device is provided for measurement of physiological condition by analysis of the image. The processing unit is provided to recognize facial features (136).
   USE - Physiological condition measuring device for communication transfer and audio/video playback.
   ADVANTAGE - The device effectively provides the feedback by presenting data to the user in visual via a display and in audible via a speaker. The communication device effectively provides transmission of speech data between the user and another individual by converting speech to an electric signal for transmission from one party to another.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a cellular telephone that recognizes the facial features.
   Physiological condition measuring device (100)
   Cellular telephone (102)
   Housing (110)
   Camera (132)
   Facial features (136)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The communication device transmits and receives information utilizing a variety of technologies including frequency division multiple access (FDMA), Time Division Multiple Access and code division multiple access.
PD US2009060287-A1   05 Mar 2009   G06K-009/00   200921   Pages: 22   English
   CN101383859-A   11 Mar 2009   H04M-001/21   200923      Chinese
   KR2009025176-A   10 Mar 2009   A61B-005/04   200934      
   KR2009025177-A   10 Mar 2009   A61B-005/04   200934      
   JP2009171544-A   30 Jul 2009   H04M-001/00   200953   Pages: 24   Japanese
UT DIIDW:2009F60966
ER

PT P
PN US2009052748-A1; US7646909-B2
TI Computer-implemented method for generating three-dimensional (3D) image of object, by calculating parameters that map located feature points of two-dimensional (2D) image to feature points of 2D representation of 3D image of standard object.
AB    NOVELTY - The method involves: providing a three-dimensional (3D) model of an object, the 3D model including a 3D image of a standard object and parameters indicating variations of an individual object; receiving a 2D image of the object; locating feature points of the 2D image of the object; calculating parameters that map the located feature points of the 2D image to feature points of a 2D representation of the 3D image of the standard object; and setting points of the 3D image of the object using the provided 3D model and the calculated parameters.
   USE - Computer-implemented method for generating three-dimensional image of object, such as face, in various applications such as security and processing of digital photographs.
   ADVANTAGE - Provides recognition system which can automatically generate 2D images of faces under various image conditions and use those 2D images to recognize a target face in a 2D image. Reduces computational complexity by more compactly representing the image of the standard face.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer-readable medium containing instructions for controlling a computer system to generate a 3D image of a face.
   DESCRIPTION OF DRAWING(S) - The drawing illustrates the processing of a recognition system.
   Image alignment step (102)
   Step of constructing a 3D image (103)
   Step of texture-mapping a 2D image (104)
   Step of generating additional 3D images (105)
   Face recognition step (108)
PD US2009052748-A1   26 Feb 2009   G06K-009/00   200918   Pages: 12   English
   US7646909-B2   12 Jan 2010   G06K-009/00   201005      English
UT DIIDW:2009F53726
ER

PT P
PN US2008267456-A1; WO2008134392-A1; GB2460989-A; CN101681228-A; GB2460989-B; US8063889-B2; CN101681228-B
TI Biometric data collection system for collecting biometric user information in e.g. international airport, has keys on touch screen and sensors e.g. biometric sensors located behind key of multiple keys.
AB    NOVELTY - The system has keys (14) on a touch screen (12). Sensors (15, 26) e.g. biometric sensors, are located behind the key of the multiple keys. The sensor is located behind at the key is for obtaining a finger print of a user touching the key. A camera is situated behind the touch screen for obtaining an image of the user. A sound sensor (24) obtains voice sounds of the user. A copier obtains images of the user's documents possibly relating to an identity of the user. The camera behind the touch screen is for obtaining the image of an iris of the user and for obtaining the image of the user's face.
   USE - Biometric data collection system for collecting biometric user information with touch screen and detection technology in e.g. international airports, customs, port authorities, border crossings, money-supplying bank kiosks and admission to secure areas.
   ADVANTAGE - The system detects the user who is attempting to provide false identification or information.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for collecting biometric information
   (2) a system for obtaining biometric data comprising an instrument.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a user interface for a biometric collection system.
   Biometric data collection instrument (11)
   Touch screen (12)
   Keys (14)
   Sensors (15, 26)
   Sound sensor (24)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The system conforms to communication networks e.g. global system for mobile communication for information transfer.
PD US2008267456-A1   30 Oct 2008   G06K-009/00   200876   Pages: 6   English
   WO2008134392-A1   06 Nov 2008   G06F-003/048   200878      English
   GB2460989-A   23 Dec 2009   G06F-003/048   201001      English
   CN101681228-A   24 Mar 2010   G06F-003/048   201024      Chinese
   US8063889-B2   22 Nov 2011   G06F-003/041   201177      English
   CN101681228-B   29 Apr 2015   G06F-003/0488   201543      Chinese
UT DIIDW:2008M99752
ER

PT P
PN CN101021899-A; CN100580691-C
TI The interactive face recognition system and method synthetically using both face and figure supplement information.
AB    NOVELTY - The invention relates to the image recognition system and method, it is the interactive face recognition system and method synthetically using both face and figure supplement information. It comprises client, server, and the communication line between them, the client comprises face detection module, feature extraction module, user validation module. The server comprises native face data module, recognition engine module, adjusting recognition engine module, or the server comprises network face data module. The client comprises face detection module, feature extraction base module, recognition engine module, user validation module for validating the recognition result, and the adjusting recognition engine module which adjusts recognition engine module according to the recognition result. It is interactive face recognition system which advances discriminating precision. Otherwise, the invention can apply to the internet, and large-scale face recognition system is formed based on client and server.
PD CN101021899-A   22 Aug 2007   G06K-009/00   200810      Chinese
   CN100580691-C   13 Jan 2010      201024      Chinese
UT DIIDW:2008B40297
ER

PT P
PN CN1924894-A; CN100426317-C
TI Multiple attitude human face detection and track system and method.
AB    NOVELTY - This invention discloses one method and system for different human faces to test multiple faces in the test sequence for continuous tracing in the images, which comprises the following steps: separately getting human face front and side test mode through sample training to determining AAM human face mode; using the above modes to test the input visual image to determine whether one frame of imageis stored in the human face, if testing the face and then tracing and validating the human face in the back frame.
PD CN1924894-A   07 Mar 2007   G06K-009/00   200749      Chinese
   CN100426317-C   15 Oct 2008   G06K-009/00   200907      Chinese
UT DIIDW:2007497602
ER

PT P
PN CN1731416-A
TI Method of quick and accurate human face feature point positioning.
AB    NOVELTY - The invention elates to a human face character position method in the field of image processing technology. It first uses human face detecting method to find the human face area in the image, and thenit detects eyes of human face area to fide the location of two eyes, then it according to the middle location of the two eyes, the distance between the two eyes and the angle of the two eyes to do imitate-injecting transformation to the initial ASM module so that the initial location of ASM module is near to the module which is formed by real character points, at last it dose ASM searching to theinitial location after imitate-injecting transformation so as to obtain the location of human face character point.
PD CN1731416-A   08 Feb 2006   G06K-009/00   200645      Chinese
UT DIIDW:2006434277
ER

PT P
PN US2005224573-A1; JP2005301539-A; CN1680971-A; US7258272-B2; CN101447110-A; CN100489901-C
TI Customer identification system used in financial institution e.g. bank, authenticates customer based on customer's face image recorded by camera connected to computer in financial institution and registered face image data of customer.
AB    NOVELTY - An authentication server (21) includes a database (21a) that stores registered face image data of a customer. A camera connected to a computer arranged at the counter of a financial institution, records the face image of the customer. The customer is authenticated based on the recorded face image and the registered face image data which is displayed on computer.
   USE - For use in financial institution such as bank, credit association, post office, while performing commercial transactions using automatic teller machine (ATM) and cash dispenser (CD).
   ADVANTAGE - Enables authentication of the customer, in a short period of time. Also reduces the cost of system configuration.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for transaction facility.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic diagram of the identification system.
   camera (13)
   authentication server (21)
   database (21a)
   customer (31)
   card (33)
PD US2005224573-A1   13 Oct 2005   G06F-017/60   200574   Pages: 40   English
   JP2005301539-A   27 Oct 2005   G06F-017/60   200574   Pages: 32   Japanese
   CN1680971-A   12 Oct 2005   G06K-009/00   200612      Chinese
   US7258272-B2   21 Aug 2007   G06K-005/00   200755      English
   CN101447110-A   03 Jun 2009   G07F-019/00   200939      Chinese
   CN100489901-C   20 May 2009   G07C-009/00   200969      Chinese
UT DIIDW:2005722994
ER

PT P
PN EP1583024-A2; JP2005284348-A; US2005220336-A1; EP1583024-B1; DE602005012011-E; US7657085-B2; JP4482796-B2; EP1583024-A3
TI Information processing apparatus for detecting face image, calculates data weights based on learning samples that have not been removed using reference value, for selecting weak classifier in next iteration of learning.
AB    NOVELTY - A calculation unit calculates a reference value according to cumulative sums calculated based on accumulating values obtained by weighting results of classification of respective learning samples by weak classifier. Another calculation unit calculates data weights based on learning samples that have not been removed using reference value, for selecting weak classifier in a next iteration of learning.
   USE - For detecting face image from complex video scenes.
   ADVANTAGE - The face image is detected quickly.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) information processing method;
   (2) recorded medium storing information processing program; and
   (3) information processing program.
   DESCRIPTION OF DRAWING(S) - The figure shows a functional block diagram of an object detecting apparatus.
   object detecting apparatus (1)
PD EP1583024-A2   05 Oct 2005   G06K-009/62   200572   Pages: 61   English
   JP2005284348-A   13 Oct 2005   G06T-007/00   200572   Pages: 41   Japanese
   US2005220336-A1   06 Oct 2005   G06K-009/00   200572      English
   EP1583024-B1   31 Dec 2008   G06K-009/00   200904      English
   DE602005012011-E   12 Feb 2009   G06K-009/00   200917      German
   US7657085-B2   02 Feb 2010   G06K-009/62   201010      English
   JP4482796-B2   16 Jun 2010   G06T-007/00   201040   Pages: 42   Japanese
   EP1583024-A3   19 Apr 2006   G06K-009/00   201738      English
UT DIIDW:2005691866
ER

PT P
PN JP2004265353-A; JP4296008-B2
TI Personal identification system for managing entry/exit of person with respect to facility gate, utilizes similarity of face image of person to be authenticated and registered perspective image of viscerocranium, to perform authentication.
AB    NOVELTY - A sensor (22) acquires the facial image of person to be authenticated, and an extraction unit (24) extracts the characteristics such as eye and nose of acquired face image. An authentication unit (26) utilizes similarity of acquired face image and face image registered with a registration unit (18), and similarity of face image and registered perspective image viscerocranium, to perform personal authentication.
   USE - For personal identification, for managing entry and exit of person with respect to gate in facility.
   ADVANTAGE - By considering cranial bones as one of the comparison collation, the reliability of personal identification is improved.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the personal identification system. (Drawing includes non-English language text).
   urface shape measurement sensors (12,22)
   bone shape measurement sensor (14)
   urface features information extraction unit (16,24)
   registration unit (18)
   bone characteristics information extraction unit (20)
   authentication unit (26)
   processing unit (28)
PD JP2004265353-A   24 Sep 2004   G06T-007/00   200467   Pages: 11   Japanese
   JP4296008-B2   15 Jul 2009   G06T-007/00   200946   Pages: 10   Japanese
UT DIIDW:2004681410
ER

PT P
PN EP1434164-A2; JP2004209244-A; US2004151375-A1; CN1516074-A; KR2004059313-A; JP3753722-B2; CN1301489-C; EP1434164-B1; DE60327156-E; EP1434164-A3; KR480781-B1
TI Teeth image area extracting method for personal identification, involves defining interline between upper and lower teeth portion areas based on inner mouth area, and extracting upper and lower teeth areas based on interline.
AB    NOVELTY - The method involves separating an inner mouth area from a teeth area of a teeth image of a person to be identified. An interline is defined between an upper teeth portion and a lower teeth portion of the teeth area on the basis of the inner mouth area. An upper teeth area and a lower teeth area are extracted by teeth image processing unit (13) on the basis of the interline.
   USE - Used for extracting an upper teeth area and a lower teeth area from a teeth image (claimed) to perform personal identification during depositing or withdrawal at an automatic teller machine (ATM) and during an electronic payment through a personal mobile communication equipment e.g. cellular phone or personal digital assistant.
   ADVANTAGE - The method provides a simple recognition of the personal by extracting shapes of teeth, thereby increasing the speed of personal identification and decreasing the cost for implementation without the need to perform complex algorithm as required for face recognitions, finger print recognition utilized in identifying a person.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) a personal identification method
   (b) a computer readable recording medium on which is recorded a program for performing the personal identification method
   (c) a personal identification apparatus using a teeth image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a personal identification apparatus using a teeth image.
   Teeth image acquisition unit (11)
   Teeth image processing unit (13)
   Teeth image database (15)
PD EP1434164-A2   30 Jun 2004   G06K-009/00   200447   Pages: 19   English
   JP2004209244-A   29 Jul 2004   A61B-005/117   200450   Pages: 15   Japanese
   US2004151375-A1   05 Aug 2004   G06K-009/00   200452      English
   CN1516074-A   28 Jul 2004   G06T-005/00   200469      Chinese
   KR2004059313-A   05 Jul 2004   G06K-009/46   200472      
   JP3753722-B2   08 Mar 2006   A61B-005/117   200618   Pages: 15   Japanese
   CN1301489-C   21 Feb 2007   G06T-005/00   200749      Chinese
   EP1434164-B1   15 Apr 2009   G06K-009/00   200926      English
   DE60327156-E   28 May 2009   G06K-009/00   200935      German
   EP1434164-A3   17 Aug 2005   G06K-009/00   201731      English
   KR480781-B1   06 Apr 2005   G06K-009/46   200568      
UT DIIDW:2004490256
ER

PT P
PN EP1217572-A2; US2002106112-A1; JP2002230547-A; US6920237-B2; EP1217572-A3
TI Digital image processing method for detecting human irises in a digital image formed from pixels, uses the red intensity of the pixels in the image in combination iris and non-iris statistical models.
AB    NOVELTY - Colour histogram equalization (22), skin colour detection (24) and oval region extraction (26), iris colour detection step (28) determines whether a pixel is an iris or not by measuring the red intensity of the pixel and applying probability analysis using a statistical model and a non-iris statistical model. Bayes model is used to ensure that the iris colour pixels are associated with an eye.
   USE - For detecting human irises in a digital image, which may also help locate other features and to determine orientation of a human face in an image.
   ADVANTAGE - The detection of red intensity and iris/non-iris statistical models reduce the region of the image that must be searched, thus greatly reducing the computation required to locate an eye and reducing incidence of eye detection false positives and providing an effective means of obtaining iris positions in a frontal face image.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer program product for detecting human irises and eyes in a digital image.
   DESCRIPTION OF DRAWING(S) - The figure is a flowchart illustrating a method of detecting an eye in a digital image.
PD EP1217572-A2   26 Jun 2002   G06K-009/00   200253   Pages: 13   English
   US2002106112-A1   08 Aug 2002   G06K-009/00   200254      English
   JP2002230547-A   16 Aug 2002   G06T-007/00   200269   Pages: 9   Japanese
   US6920237-B2   19 Jul 2005   G06K-009/00   200547      English
   EP1217572-A3   14 Jan 2004   G06K-009/00   201731      English
UT DIIDW:2002492383
ER

PT P
PN EP1041506-A2; EP1041506-A3
TI Self service terminal method of biometrics recognition for recognizing user based on one or more of user's facial characteristics displays information on panel and takes camera image of user through panel.
AB    NOVELTY - The self service terminal records an image, using a camera (16), of the user (14) through a display (12), where the display is operating and displaying visual information to the user. The recorded image is processed and the processed image is compared with a database of stored processed images to obtain a match and thereby recognize the user.
   USE - For using a method of biometrics recognition for recognizing a user based on one or more of the user's facial characteristics.
   ADVANTAGE - Does away with the disadvantages of existing self service terminals operating a recognition system.
   DETAILED DESCRIPTION - An independent claim describes a self service terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic diagram of the self service terminal.
   the camera taking the image of the user (16)
   the user using the terminal (14)
   the display (12)
PD EP1041506-A2   04 Oct 2000   G06K-009/00   200062   Pages: 8   English
   EP1041506-A3   21 Apr 2004   G06K-009/00   201731      English
UT DIIDW:2000640099
ER

PT P
PN JP2000235648-A
TI Eye position extractor for blink detector, compares pattern images based on preset evaluation characteristics and recognizes pattern image with highest evaluating point.
AB    NOVELTY - An image processor (8) extracts pattern image containing the position of eye. A recognition unit (9) recognizes the pattern image with highest evaluating point by comparing pattern images with preset evaluation item which shows characteristics of eye.
   USE - For blink detector to extract position of eye from facial image of driver of vehicle to detect napping condition.
   ADVANTAGE - Enables to extract eye position correctly by removing the influence of spectacles, eyebrows, etc. Enables to detect neck swing of driver and napping.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for blink detector.
   DESCRIPTION OF DRAWING(S) - The figure shows theoretical diagram of blink detector.
   Image processor (8)
   Pattern image recognition unit (9)
PD JP2000235648-A   29 Aug 2000   G06T-007/00   200055   Pages: 6   Japanese
UT DIIDW:2000583837
ER

PT P
PN JP2000030065-A; JP4087953-B2
TI Facial image pattern recognition apparatus for individual identification, projects input and dictionary partial space or restriction partial space and thereby identifies object using input and dictionary patterns.
AB       NOVELTY - From an input pattern, input partial space is computed by a calculator (19). Dictionary partial space calculator and restriction partial space are computed based on the dictionary pattern. A projection unit (18) projects dictionary and input partial space, or restriction space, thereby identifying the object using input and dictionary pattern.
   USE -   For individual identification by facial image pattern recognition.
   ADVANTAGE -   High robust property opposing to a high deformation absorbing ability and the illumination variation is realizable. By eliminating unnecessary pattern change for individual identification, the influence of variation components is suppressed to minimum extent. DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of facial image recognition apparatus. (18) Partial space projection unit; (19) Partial space calculator.
PD JP2000030065-A   28 Jan 2000   G06T-007/00   200017   Pages: 13   Japanese
   JP4087953-B2   21 May 2008   G06T-007/00   200836   Pages: 22   Japanese
UT DIIDW:2000186977
ER

PT P
PN GB2328504-A; JP11066317-A; KR99016896-A; KR99023570-A; US6072892-A; GB2328504-B; JP4004634-B2
TI Eye position detection apparatus - uses block histogram analysis of grey levels with selection of largest areas that define possible eye characteristics.
AB       The eye position detection apparatus [110] receives a scanned facial image and using an A/D convertor converts the image in to a digitised signal. The digitised facial image is filtered by a median filter to remove noises such as impulse noise, with filtering occurring on a pixel by pixel basis. The filtered image is fed to a window generator [120] that sequentially generates search windows in the filtered image to a histogram extractor [130].
   The window extractor takes the rectangular search windows and scans the entire image sing a raster scanning method. Position vectors are formed to indicate positions of search windows. A histogram of the search window is obtained by plotting frequencies of pixels as a function of grey level values. The histogram is fed to the peak detector [140] which identifies three levels of peaks associated with the flesh, white of the eye, and pupil all in combination. The largest areas [150] related to the combination are selected [170] and the eye position is determined.
   ADVANTAGE -   Provides recognition of eye without the requirement for binarizaion of the image.
PD GB2328504-A   24 Feb 1999   G06T-007/00   199910   Pages: 23   English
   JP11066317-A   09 Mar 1999   G06T-007/00   199920   Pages: 10   Japanese
   KR99016896-A   15 Mar 1999   G06T-001/00   200020      
   KR99023570-A   25 Mar 1999   G06T-005/40   200024      
   US6072892-A   06 Jun 2000   G06K-009/00   200033      English
   GB2328504-B   19 Dec 2001   G06T-007/00   200203      English
   JP4004634-B2   07 Nov 2007   G06T-007/60   200774   Pages: 9   Japanese
UT DIIDW:1999109085
ER

PT P
PN US2018053056-A1; WO2018039269-A1; CA3034644-A1; IN201947008003-A; AU2017317599-A1; KR2019041504-A; EP3500911-A1; CN109923500-A; US10402649-B2; JP2019532392-W; EP3500911-A4; IL264820-A; AU2017317599-B2; CN109923500-B; JP7002536-B2; CN114253400-A
TI Head mounted display system for training a hydra neural network for performing different types of functions, comprises multiple sensors for capturing different types of sensor data and deep neural network is provided with an input layer.
AB    NOVELTY - The head mounted display system comprises multiple sensors for capturing different types of sensor data. The sensors are disposed on a frame of the head mounted display system. A frame is configured to be worn on the head of a user and to position a display system in front of the eyes of the user. Multiple sensors are provided with an outward-facing camera configured to obtain face images. A deep neural network is provided with an input layer for receiving input of the deep neural network, lower layer and a middle layer.
   USE - Head mounted display system for training a hydra neural network for performing different types of functions, such as face recognition, location and mapping, object detection and depth estimation.
   ADVANTAGE - The size of the coils generating the magnetic field at the electromagnetic field emitter is reduced. The performance of the head mounted display system is improved.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an augmented reality scenario.
   Avatar character (2)
   Reality scene (4)
   Setting (6)
   Robot statue (1110)
   Platform (1120)
PD US2018053056-A1   22 Feb 2018   G06K-009/00   201816   Pages: 92   English
   WO2018039269-A1   01 Mar 2018   G06F-003/01   201816      English
   CA3034644-A1   01 Mar 2018   G06F-003/01   201918      English
   IN201947008003-A   08 Mar 2019   G06F-003/01   201920      English
   AU2017317599-A1   21 Mar 2019   G06F-003/01   201921      English
   KR2019041504-A   22 Apr 2019   G06F-003/01   201931      
   EP3500911-A1   26 Jun 2019   G06F-003/01   201948      English
   CN109923500-A   21 Jun 2019   G06F-003/01   201950      Chinese
   US10402649-B2   03 Sep 2019   G06F-001/16   201967      English
   JP2019532392-W   07 Nov 2019   G06F-003/01   201986   Pages: 78   Japanese
   EP3500911-A4   29 Apr 2020   G06F-003/01   202037      English
   IL264820-A   25 Mar 2021   A63F-013/00   202135      English
   AU2017317599-B2   23 Dec 2021   G06F-003/01   202104      English
   CN109923500-B   04 Jan 2022   G06F-003/01   202207      Chinese
   JP7002536-B2   20 Jan 2022   B65H-003/12   202229      Japanese
   CN114253400-A   29 Mar 2022   G06F-003/01   202231      Chinese
UT DIIDW:201813739G
ER

PT P
PN WO2017198014-A1; CN107404381-A; TW201741921-A; HK1247461-A0; CA3024565-A1; AU2017266971-A1; IN201847047132-A; KR2019009361-A; SG11201810131-A1; BR112018073635-A2; EP3460697-A1; US2019102531-A1; EP3460697-A4; PH12018502437-A1; VN63169-A; JP2019522840-W; ZA201807860-A; ID201905958-A; RU2018144787-A; MX2018014147-A1; US10789343-B2; RU2738325-C2; KR2196686-B1; AU2017266971-B2; TW706268-B1; SG11201810131-B; JP2021182420-A; EP3460697-B1
TI Method for performing identity authentication by intelligent mobile phone, involves determining that target subject passes authentication if comparing of physiological characteristics of target subject satisfy identification conditions.
AB    NOVELTY - The method involves acquiring template physiological characteristics corresponding to a subject identifier from subject registration information if the information comprises the subject identifier. Physiological identification of a video stream is implemented to obtain physiological characteristics of a target subject. The physiological characteristics of the target subject are compared with the template physiological characteristics to obtain comparison results. Determination is made that the target subject passes authentication if the comparison results satisfy identification conditions.
   USE - Method for performing identity authentication by an intelligent mobile phone.
   ADVANTAGE - The method enables improving efficiency and reliability of identity authentication.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an identity authentication device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an identity authentication method. '(Drawing includes non-English language text)'
   Step for acquiring audio-video stream to be registered of a target subject (101)
   Step for determining that lip movement and speech are consistent (102)
   Step for setting speech recognition content as subject identifier (103)
   Step for implementing voiceprint recognition to obtain voiceprint characteristics (104)
   Step for implementing facial recognition to obtain facial characteristics (105)
PD WO2017198014-A1   23 Nov 2017   G06F-021/32   201779   Pages: 30   Chinese
   CN107404381-A   28 Nov 2017   H04L-009/32   201781      Chinese
   TW201741921-A   01 Dec 2017   G06F-021/32   201817      Chinese
   HK1247461-A0   21 Sep 2018   H04L-000/00   201867      Chinese
   CA3024565-A1   23 Nov 2017   G06F-021/32   201880      English
   AU2017266971-A1   06 Dec 2018   G06F-021/32   201881      English
   IN201847047132-A   11 Jan 2019   G06F-021/32   201906      English
   KR2019009361-A   28 Jan 2019   G06F-021/32   201909      
   SG11201810131-A1   28 Dec 2018   G06F-021/32   201911      English
   EP3460697-A1   27 Mar 2019   G06F-021/32   201922      English
   US2019102531-A1   04 Apr 2019   G06F-021/32   201925      English
   EP3460697-A4   08 May 2019   G06F-021/32   201934      English
   PH12018502437-A1   15 May 2019   G06F-021/32   201944      English
   JP2019522840-W   15 Aug 2019   G06F-021/32   201962   Pages: 26   Japanese
   ZA201807860-A   28 Aug 2019   G06F-000/00   201982      English
   ID201905958-A   16 Aug 2019   H04L-029/06   202033      
   RU2018144787-A   19 Jun 2020   G06F-021/32   202058      Russian
   MX2018014147-A1   12 Aug 2019   G06F-021/32   202060      Spanish
   US10789343-B2   29 Sep 2020   H04L-009/32   202079      English
   RU2738325-C2   11 Dec 2020   G06F-021/32   202003      Russian
   KR2196686-B1   31 Dec 2020   G06F-021/32   202104      
   AU2017266971-B2   13 May 2021   G06F-021/32   202142      English
   TW706268-B1   01 Oct 2020   G06F-021/32   202150      Chinese
   SG11201810131-B   12 May 2021   G06F-021/32   202166      English
   JP2021182420-A   25 Nov 2021   G06F-021/32   202100   Pages: 22   Japanese
   EP3460697-B1   08 Dec 2021   G06F-021/32   202100      English
UT DIIDW:201779807T
ER

PT P
PN CN107071580-A
TI Data processing method, involves determining live video data and specific object, and transmitting video specific data to live broadcast service terminal for live broadcast service, where identifying target object in video live data.
AB    NOVELTY - The method involves determining live video data and specific object information. Target object information in the video live data is identified. The specific object information and the target object are integrated to obtain video specific data. The video specific data is transmitted to a live broadcast service terminal for a live broadcast service. The live broadcast service terminal transmitted is received. The specific object presenting information is displayed. The video live data is utilized for face recognition processing to determine human face in each image frame.
   USE - Data processing method.
   ADVANTAGE - The method enables improving the broadcast effect.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a data processing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a data processing method. '(Drawing includes non-English language text)'
PD CN107071580-A   18 Aug 2017   H04N-021/475   201769   Pages: 22   Chinese
UT DIIDW:201758469P
ER

PT P
PN CN106845421-A; CN106845421-B
TI Method for recognizing human face characteristic based on multi-area feature and metric learning, involves obtaining corresponding position and size, and carried outface feature dimension reduction is carried out by using Euclidean distance.
AB    NOVELTY - The method involves obtaining corresponding position and size of a convolutional neural network parameter by training multiple scale face area. A feature of a human face corresponding area is extracted according to a convolutional neural network parameter. A high-dimensional facial feature is obtained. A metric learning process is performed according to the high-dimensional facial feature. The feature dimension reduction process is performed. A loss function training network model is established. A face feature dimension reduction is carried out by using Euclidean distance is identified.
   USE - Method for recognizing human face characteristic based on multi-area feature and metric learning.
   ADVANTAGE - The method enables improving expression capacity of characteristic and expression efficiency, and effectively ensuring the accuracy of face recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for recognizing human face characteristic based on multi-area feature and metric learning.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for recognizing human face characteristic based on multi-area feature and metric learning. '(Drawing includes non-English language text)'
PD CN106845421-A   13 Jun 2017   G06K-009/00   201755   Pages: 21   Chinese
   CN106845421-B   24 Nov 2020   G06K-009/00   202098      Chinese
UT DIIDW:201742345A
ER

PT P
PN CN106778525-A; CN106778525-B
TI User identity authentication method, involves obtaining document image of person to-be authenticated, and determining identity authentication of person as legal according to certificate authentication result and living body detection result.
AB    NOVELTY - The method involves obtaining a document image of a person to-be authenticated. Judgment is made to check whether the document image is obtained from authentication certificate. A human face image of the person to-be authenticated is obtained. Living body detection result is obtained from the human face image. Identity authentication of a person is determined as legal according to certificate authentication result and the living body detection result. Character recognition of the document image is performed for obtaining text information of the document image.
   USE - User identity authentication method.
   ADVANTAGE - The method enables performing identity authentication in a more accurate manner, and improving security of user authentication, so as to guarantee rights and interests of the user in an effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a user identity authentication device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a user identity authentication method. '(Drawing includes non-English language text)'
PD CN106778525-A   31 May 2017   G06K-009/00   201748   Pages: 41   Chinese
   CN106778525-B   10 Aug 2021   G06K-009/00   202168      Chinese
UT DIIDW:201740659N
ER

PT P
PN US2017076145-A1; WO2017044782-A1; TW201712580-A; US9721150-B2; AU2016319775-A1; KR2018057646-A; SG11201802003-A1; IN201817010082-A; EP3347853-A1; CN108351961-A; JP2018528543-W; AU2016319775-B2; PH12018500541-A1; MX2018003051-A1; JP6416438-B2; VN58773-A; BR112018004755-A2; AU2018256555-A1; JP2018206442-A; JP6449516-B2; KR1967123-B1; KR2019038685-A; AU2018256555-B2; HK1251933-A0; JP2019079546-A; RU2691195-C1; RU2019116007-A; AU2019204639-A1; ID201811305-A; BR122018007964-A2; BR122018007961-A2; CN108351961-B; AU2019204639-B2; RU2711050-C2; KR2020007085-A; KR2067947-B1; CN110852160-A; SG11201802003-B; SG10202001382-A1; SG10202001380-A1; KR2131104-B1; HK1251933-A1; HK40016785-A0; JP6778247-B2; TW687832-B1; TW706272-B1; TW706266-B1; TW202029031-A; TW202029035-A; SG10202001382-B; SG10202001380-B
TI Computer implemented method for biometric authentication, involves generating feature descriptor based on combination of patterned histogram feature descriptors and storing generated feature descriptors in biometric template.
AB    NOVELTY - The method involves receiving an image of a facial region of a user (510). The image is processed to define an ocular image region including a portion of the eye in the image of facial region and defines periocular image regions each including a portion of the area surrounding the eye in the image of facial region. The points of interest are identified in the ocular image region and the periocular image regions. The feature descriptor is generated based on a combination of patterned histogram feature descriptors. The generated feature descriptors is stored in a biometric template.
   USE - Computer implemented method for biometric authentication.
   ADVANTAGE - The standardizing of the region of interest can minimize registration issues, and the accuracy of biometric system is improved, while the periocular region has no hard-defined boundaries. The image quality metric acts as a match-predictive quality metric and fuses into a final match score to improve biometric system performance.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system for biometric authentication.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the system for performing biometric scanning and analysis.
   User device (500)
   User (510)
   Image sensor (530)
   Processor (540)
   Memory (550)
PD US2017076145-A1   16 Mar 2017   G06K-009/00   201721   Pages: 20   English
   WO2017044782-A1   16 Mar 2017   G06K-009/00   201722      English
   TW201712580-A   01 Apr 2017   G06F-021/31   201747      Chinese
   US9721150-B2   01 Aug 2017   G06K-009/00   201752      English
   AU2016319775-A1   12 Apr 2018   G06K-009/00   201826      English
   KR2018057646-A   30 May 2018   G06K-009/00   201838      
   IN201817010082-A   06 Jul 2018   G06K-009/00   201847      English
   EP3347853-A1   18 Jul 2018   G06K-009/00   201848      English
   CN108351961-A   31 Jul 2018   G06K-009/00   201853      Chinese
   JP2018528543-W   27 Sep 2018   G06T-007/00   201865   Pages: 61   Japanese
   PH12018500541-A1   24 Sep 2018   G06K-009/00   201867      English
   MX2018003051-A1   08 Jun 2018   G06K-009/00   201871      Spanish
   JP6416438-B2   31 Oct 2018   G06T-007/00   201873   Pages: 43   Japanese
   VN58773-A   27 Aug 2018   G06K-009/00   201873      English
   BR112018004755-A2   25 Sep 2018   G06K-009/00   201876      English
   AU2018256555-A1   22 Nov 2018   G06K-009/00   201880      English
   JP2018206442-A   27 Dec 2018   G06T-007/00   201903   Pages: 44   Japanese
   JP6449516-B2   09 Jan 2019   G06T-007/00   201905   Pages: 44   Japanese
   KR1967123-B1   08 Apr 2019   G06K-009/00   201928      
   KR2019038685-A   08 Apr 2019   G06K-009/00   201928      
   HK1251933-A0   03 May 2019   G06K-000/00   201935      English
   JP2019079546-A   23 May 2019   G06T-007/00   201939   Pages: 45   Japanese
   RU2691195-C1   11 Jun 2019   G06K-009/00   201947      Russian
   RU2019116007-A   11 Jun 2019   G06K-009/00   201953      Russian
   AU2019204639-A1   18 Jul 2019   G06K-009/00   201955      English
   ID201811305-A   19 Oct 2018   G06K-009/00   201972      
   BR122018007964-A2   10 Sep 2019   G06K-009/00   201975      English
   CN108351961-B   01 Nov 2019   G06K-009/00   201986      Chinese
   RU2711050-C2   14 Jan 2020   G06K-009/00   202008      Russian
   KR2020007085-A   21 Jan 2020   G06K-009/00   202009      
   KR2067947-B1   17 Jan 2020   G06K-009/00   202015      
   SG10202001382-A1   29 Apr 2020      202049      English
   HK1251933-A1   04 Sep 2020   G06K-009/00   202079      Chinese
   HK40016785-A0   18 Sep 2020   G06K-009/00   202080      Chinese
   JP6778247-B2   28 Oct 2020   G06T-007/00   202088   Pages: 44   Japanese
   TW687832-B1   11 Mar 2020   G06F-021/31   202142      Chinese
   TW706272-B1   01 Oct 2020   G06F-021/32   202151      Chinese
   TW202029031-A   01 Aug 2020   G06F-021/31   202151      Chinese
   TW202029035-A   01 Aug 2020   G06F-021/32   202151      Chinese
   SG10202001382-B   16 Jul 2021   G06F-021/32   202181      English
   SG10202001380-B   16 Jul 2021   G06K-009/46   202181      English
UT DIIDW:201718580P
ER

PT P
PN CN106503669-A; CN106503669-B
TI Deep learning network based multiple tasks training method, involves obtaining human face area of training set of human face image, and performing loss function of age recognition task and gender identification task by soft max function.
AB    NOVELTY - The method involves obtaining human face area of training set of human face image. Key point detection is performed for obtaining the key characteristic point position of the human face area. The human face image is inputted to a multi-task deep learning network to obtain a multi-task deep learning network model. Loss function of a human face recognition task is performed by triplet function. Loss function of age recognition task is performed by soft max function. Loss function of a gender identification task is performed by soft max function.
   USE - Deep learning network based multiple tasks training method.
   ADVANTAGE - The method enables improving efficiency of multi-task deep learning network training and recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning network based multiple tasks training system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning network based multiple tasks training method. '(Drawing includes non-English language text)'
PD CN106503669-A   15 Mar 2017   G06K-009/00   201723   Pages: 16   Chinese
   CN106503669-B   10 Dec 2019   G06K-009/00   201997      Chinese
UT DIIDW:2017190407
ER

PT P
PN CN106203305-A; CN106203305-B
TI Living human face detecting method, involves transmitting multi-channel image to trained convolutional neural network, and performing living human face detecting process based on output of trained convolutional neural network.
AB    NOVELTY - The method involves acquiring a human face image for testing an object base on specific condition. Light source brightness approach process is performed for collection multiple light supplementing images based on an un-lighting image. An optical image highlight free image is obtained. Multiple high-light images are combined into a multi-channel image. The multi-channel image is transmitted to a trained convolutional neural network. Living human face detecting process is performed based on output of the trained convolutional neural network.
   USE - Living human face detecting method.
   ADVANTAGE - The method enables increases mask screen optical characteristics efficiency and realizing actual living face detecting process by the mask screen.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a living human face detection device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a front view illustrating a living human face detecting method. '(Drawing includes non-English language text)'
PD CN106203305-A   07 Dec 2016   G06K-009/00   201703   Pages: 20   Chinese
   CN106203305-B   04 Feb 2020   G06K-009/00   202013      Chinese
UT DIIDW:201681309D
ER

PT P
PN CN105740758-A
TI Deep learning technique based internet video human face identification method, involves marking face location frame and name of person to establish face image database, and analyzing face identification result to determine name label.
AB    NOVELTY - The method involves obtaining human face image from internet, and marking human face location frame and name of person to establish human face image database. Human face data convolution trained neural network is established. Confidence coefficient degree is determined. Face image is extracted using face detection and tracking algorithm, and internet video frame showing the position of human face is determined. Position of face frame is inputted via neural network, and human face frame identification result is obtained. Identification result is analyzed to determine the name label.
   USE - Deep learning technique based internet video human face identification method.
   ADVANTAGE - The human face image quality is evaluated easily. The human face image is analyzed so as to discard low quality image and to retain only the high quality face images, such that the human face identification reliability can be improved. The statistical parameter to determine the overall trajectory of name label is determined easily. The impact of video quality on the identification accuracy can be prevented effectively.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the human face identification process. (Drawing includes non-English language text)
PD CN105740758-A   06 Jul 2016   G06K-009/00   201652   Pages: 8   Chinese
UT DIIDW:201644091H
ER

PT P
PN CN105426875-A
TI Depth convolution nerve network-based human face identification method, involves collecting staff personal information from user management server, collecting human face samples, and storing user attendance record in user management server.
AB    NOVELTY - The method involves collecting staff personal information from a user management server. A human face samples are collected. The staff personal information and the human face sample are transmitted to a center server by a user management server. A human face identification module is established. The center server is established according to a human face identification module and a depth convolution nerve network. A convolution nerve network training result is stored in the center server. A user attendance record is detected. The user attendance record is stored in the user management server.
   USE - Depth convolution nerve network-based human face identification method.
   ADVANTAGE - The method enables reducing traditional manual extraction caused by incomplete characterization and uncertainty issues. The method enables increasing human face recognition rate, precision rate and attendance rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a depth convolution nerve network-based human face identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a depth convolution nerve network-based human face identification method. '(Drawing includes non-English language text)'
PD CN105426875-A   23 Mar 2016   G06K-009/00   201624   Pages: 11   English
UT DIIDW:2016194991
ER

PT P
PN WO2016034069-A1; CN105468950-A; KR2017047255-A; HK1221795-A0; US2017180362-A1; EP3190534-A1; IN201717007318-A; SG11201701497-A1; JP2017530457-W; EP3190534-A4; SG10201901818-A1; EP3190534-B1; KR1997371-B1; EP3540621-A1; US10601821-B2; CN105468950-B; EP3540621-B1; CN111898108-A; JP6820062-B2; ES2810012-T3; SG10201901818-B
TI Method for authenticating identity of user, involves receiving dynamic facial authentication prompt information sent by server, and obtaining pose identification information of dynamic facial authentication prompt information.
AB    NOVELTY - The method involves receiving dynamic facial authentication prompt information sent by a server (201) when a user performs identity authentication. Pose identification information of the dynamic facial authentication prompt information is obtained (202) by identifying a facial pose presented by the user. The pose identification information is sent to the server (203) such that the server determines that the user passes the identity authentication when the server verifies the consistency between the pose identification information and the dynamic facial authentication prompt information.
   USE - Method for authenticating an identity of a user.
   ADVANTAGE - The method enables authenticating the identity of the user with high security by dynamic facial authentication, such that authentication information cannot be thieved by a malicious third party compared with the existing authentication mode of using an authentication password, so that authentication reliability is improved, and the user can be identified as a live user by dynamic facial authentication, thus improving accuracy of identity authentication and reducing security risks during the authentication process.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an identity authentication device
   (2) a terminal
   (3) a server.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for authenticating an identity of a user. '(Drawing includes non-English language text)'
   Step for receiving dynamic facial authentication prompt information sent by server (201)
   Step for obtaining pose identification information of dynamic facial authentication prompt information (202)
   Step for sending pose identification information to server (203)
PD WO2016034069-A1   10 Mar 2016   G06F-021/32   201620   Pages: 35   Chinese
   CN105468950-A   06 Apr 2016   G06F-021/32   201626      English
   KR2017047255-A   04 May 2017   G06F-021/32   201733      
   HK1221795-A0   09 Jun 2017   G06F-000/00   201741      Chinese
   US2017180362-A1   22 Jun 2017   H04L-029/06   201742      English
   EP3190534-A1   12 Jul 2017   G06F-021/32   201747      English
   IN201717007318-A   30 Jun 2017   G06F-021/32   201747      English
   SG11201701497-A1   30 Mar 2017   G06F-021/32   201749      English
   JP2017530457-W   12 Oct 2017   G06T-007/00   201768   Pages: 29   Japanese
   EP3190534-A4   21 Mar 2018   G06F-021/32   201820      English
   SG10201901818-A1   28 Mar 2019      201938      English
   EP3190534-B1   26 Jun 2019   G06F-021/30   201948      English
   KR1997371-B1   05 Jul 2019   G06F-021/32   201956      
   EP3540621-A1   18 Sep 2019   G06F-021/32   201971      English
   US10601821-B2   24 Mar 2020   H04L-029/06   202025      English
   CN105468950-B   30 Jun 2020   G06F-021/32   202056      Chinese
   EP3540621-B1   29 Jul 2020   G06F-021/32   202062      English
   CN111898108-A   06 Nov 2020   G06F-021/32   202094      Chinese
   JP6820062-B2   27 Jan 2021   G06T-007/00   202110   Pages: 21   Japanese
   ES2810012-T3   08 Mar 2021   G06F-021/32   202152      Spanish
   SG10201901818-B   08 Jun 2021   G06F-021/30   202165      English
UT DIIDW:201614932K
ER

PT P
PN CN105138993-A; CN105138993-B
TI Human face identification model establishing method, involves obtaining image dimension characteristic according to nerve network CNN model, and determining LDA projection matrix by human face identification module.
AB    NOVELTY - The method involves obtaining a human face image by performing size normalization processing operation. PCA analysis process is performed by a normalization main component. PCA projection matrix regularization process is performed according to the human face image. Human face image processing operation is performed according to PCA projection matrix. A convolution nerve network CNN model is established according to human face image. Image dimension characteristic is obtained according to the nerve network CNN model. A LDA projection matrix is determined by a human face identification module.
   USE - Human face identification model establishing method.
   ADVANTAGE - The method enables increasing face identification model establishing efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification model establishing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face identification model establishing method. '(Drawing includes non-English language text)'
PD CN105138993-A   09 Dec 2015   G06K-009/00   201601   Pages: 28   English
   CN105138993-B   27 Jul 2018   G06K-009/00   201853      Chinese
UT DIIDW:2015811151
ER

PT P
PN CN105117688-A; CN105117688-B
TI Texture characteristic fusion based support vector machine human face identification method, involves carrying out face characteristic matrix dimension reduction operation, and obtaining face image classifier recognition result.
AB    NOVELTY - The method involves performing human face image normalization process. A human face database extracting uniform local NSCT binary pattern (ULNBH) characteristic vector value and a sample Gabor characteristic vector value are obtained. Gabor ULNBH fusion characteristic normalization operation is carried out. Human face characteristic vector training sample testing process is performed. Human face characteristic matrix dimension reduction operation is carried out. A human face image category is divided into multi-image classes. Human face image classifier recognition result is obtained.
   USE - Texture characteristic fusion based support vector machine human face identification method.
   ADVANTAGE - The method enables simplifying identification of support vector machine human face so as to ensure better gesture robustness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a texture characteristic fusion based support vector machine human face identification method. '(Drawing includes non-English language text)'
PD CN105117688-A   02 Dec 2015   G06K-009/00   201601   Pages: 14   English
   CN105117688-B   28 Aug 2018   G06K-009/00   201860      Chinese
UT DIIDW:2015805246
ER

PT P
PN CN105069400-A; CN105069400-B
TI Stacked sparse code based human face image gender identification method, involves performing logistic regression operation by gender classifier based on gender characteristic, and performing source image processing process.
AB    NOVELTY - The method involves selecting a human face image according to training sample data (S1). A grey image is stored (S2a) in a human face standard library. Human face image location and geometry correction operations are performed (S2b) by using DAM algorithm. Image normalization process is performed. Human face gender characteristic is obtained (S3a) by an automatic building sparse stacked coding module. Logistic regression operation is performed (S3b) by a gender classifier based on the gender characteristic. Source image processing process is performed.
   USE - Stacked sparse code based human face image gender identification method.
   ADVANTAGE - The method increasing gender identification accuracy rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a stacked sparse code based human face image gender identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a stacked sparse code based human face image gender identification method. '(Drawing includes non-English language text)'
   Step for selecting a human face image according to training sample data (S1)
   Step for storing grey image in human face standard library (S2a)
   Step for performing human face image location and geometry correction operations by using DAM algorithm (S2b)
   Step for obtaining human face gender characteristic by automatic building sparse stacked coding module (S3a)
   Step for performing logistic regression operation by gender classifier based on gender characteristic (S3b)
PD CN105069400-A   18 Nov 2015   G06K-009/00   201581   Pages: 15   English
   CN105069400-B   25 May 2018   G06K-009/00   201838      Chinese
UT DIIDW:201575296F
ER

PT P
PN US9111134-B1
TI Computer implemented method for generating non-transient record embodying measure of location of feature of face of person for behavior analysis involves extracting feature location based on feature update model.
AB    NOVELTY - The method involves generating a set of facial feature displacements for each training image with associated image sample vectors. A regularized linear regression is trained which maps from image sample vectors to displacement vectors, the regularized linear regression is characterized by regularization. A feature location (301) is extracted based on the feature update model, characterizing the location of the feature of the face of the specified individual person. The feature location is stored to the non-transient record.
   USE - Computer implemented method for generating non-transient record embodying measure of location of feature of face of person used in applications such as performance driven facial animation, behavior analysis or understanding a person's response to advertising or other visual information, photographic and video manipulation, facial recognition, gaze analysis, product design analytics and attention measurement.
   ADVANTAGE - The use of this calibration information results in increased robustness and improved accuracy of the feature location and expression measurements in a subsequent unconstrained facial performance. Due to the robust and generic characteristics of the measurement system, this animation production process is deployed in an unrestricted consumer environment targeting a large proportion of the population. The specificity of the calibration tracker to neutral expressions enables it to extract reliable feature locations at this expression for a far wider set of identities than the generic feature locator. The improvement in accuracy is achieved without introducing extra computational cost when analyzing frames after the calibration process.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a non-transitory computer readable medium storing program for.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flow diagram illustrating a computer implemented method for generating non-transient record embodying measure of location of feature of face of person.
   Calibrator feature locator (300)
   Feature location (301)
   Calibrator feature locator training date (500)
   Generic subject feature locator training date (501)
   Train feature locator (502)
PD US9111134-B1   18 Aug 2015   G06K-009/46   201556   Pages: 33   English
UT DIIDW:2015475593
ER

PT P
PN US2015131872-A1; WO2015070320-A1; CA2930322-A1; IN201627020004-A; EP3069301-A1; CN105917360-A; US2016292494-A1; US9639740-B2; EP3069301-A4
TI Method for face and portrait extraction using face detection and recognition, involves storing area co-ordinates of location of detected face in digital image, and applying transformation to detected face to create portrait of detected face.
AB    NOVELTY - The method involves detecting a face in a digital image and determining and storing area co-ordinates of a location of the detected face in the digital image. The transformation is applied to the detected face to create portrait of the detected face. The portrait is rotated until the portrait is shown in a vertical orientation and a pair of eyes of the one face shown in the portrait are positioned on a horizontal plane. The rotated portrait is stored.
   USE - Method for face and portrait extraction using face detection and recognition.
   ADVANTAGE - The transformation is applied to the detected face to create portrait of the detected face, thus enhances the texture-based face detection and facilitates an optimal training by more than one face signature associated with a person, and thus improves accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a system that has a computer that has a processor and a non-transitory computer readable memory that has processing instructions.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic representation of a face recognition method.
   Desktop computer system (15)
   Queuing images (29)
   Automatically detecting face and eye locations (31)
   Translating face into a signature (33)
   Users (36)
PD US2015131872-A1   14 May 2015   G06K-009/00   201535   Pages: 78   English
   WO2015070320-A1   21 May 2015   G06K-009/46   201535      English
   CA2930322-A1   21 May 2015   G06K-009/46   201636      English
   IN201627020004-A   26 Aug 2016   G06K-009/46   201660      English
   EP3069301-A1   21 Sep 2016   G06K-009/46   201662      English
   CN105917360-A   31 Aug 2016   G06K-009/46   201663      Chinese
   US2016292494-A1   06 Oct 2016   G06K-009/00   201666      English
   US9639740-B2   02 May 2017   G06K-009/00   201730      English
   EP3069301-A4   18 Oct 2017   G06T-011/60   201769      English
UT DIIDW:201530259C
ER

PT P
PN CN104346607-A; CN104346607-B
TI Human face identification method, involves electing ideal human face image, which is input as convolution neural network entering into input layer, and fixing mapping layer without modification of each plane.
AB    NOVELTY - The method involves selecting an ideal human face image, which is an input as a convolution neural network entering into an input layer. A component edge direction is different in the input image as characteristic extraction of a first time and an output to a characteristic mapping layer of a first layer, where the characteristic mapping layer is fixed without a modification of each characteristic mapping plane.
   USE - Human face identification method.
   ADVANTAGE - The method enables increasing a rate of complex scene human face.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a human face identification process. '(Drawing includes non-English language text)'
PD CN104346607-A   11 Feb 2015   G06K-009/00   201524   Pages: 10   Chinese
   CN104346607-B   22 Dec 2017   G06K-009/00   201802      Chinese
UT DIIDW:201521400P
ER

PT P
PN CN104091162-A; CN104091162-B
TI Characteristic point three-dimensional human face identifying method, involves performing smooth de-noising process, processing library-concentrated face model, and obtaining identity authentication of three-dimensional face model.
AB    NOVELTY - The method involves establishing a human face model and a library-concentrated face model. Three-dimensional (3D) human face point cloud data conversion operation is performed by a 3D grid. Smooth de-noising process is performed. Point cloud coordinate information is stored in a library collection face model. A feature point is determined by the library collection face model. A left and right inner eye corner point is determined. A contour line is fixed with a sample. The library-concentrated face model is processed. Identity authentication of the 3D face model is obtained.
   USE - Characteristic point 3D human face identifying method.
   ADVANTAGE - The method enables effectively realizing characteristic point 3D human face identifying process with high location property and better robustness effect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a characteristic point 3D human face identifying method.'(Drawing includes non-English language text)'
PD CN104091162-A   08 Oct 2014   G06K-009/00   201502   Pages: 16   Chinese
   CN104091162-B   23 Jun 2017   G06K-009/00   201744      Chinese
UT DIIDW:2015007276
ER

PT P
PN CN104063683-A; CN104063683-B
TI Expression input method based on human face identification, involves obtaining theme for expression of emoticon label based on correspondence relationship between emotion label and expression of subject.
AB    NOVELTY - The method involves starting (110) input process, so that a picture is taken (120) by the user. The facial expression in a picture is determined (130) corresponding to emotion label, using human face expression identification model. A theme is obtained (140) for expression of emoticon label based on correspondence relationship between emotion label and expression of subject. The expression of each subject and a candidate item are displayed (150) in a client display unit.
   USE - Expression input method based on human face identification.
   ADVANTAGE - The expression input of user is matched conveniently according to the picture of user, so that the expression accuracy and expression resource range are improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an expression input device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the expression input process. (Drawing includes non-English language text)
   Step for starting input process (110)
   Step for taking picture by user (120)
   Step for determining facial expression in picture corresponding to emotion label (130)
   Step for obtaining theme for expression of emoticon label (140)
   Step for displaying expression of each subject and candidate item in client display unit (150)
PD CN104063683-A   24 Sep 2014   G06K-009/00   201501   Pages: 25   Chinese
   CN104063683-B   17 May 2017   G06K-009/00   201737      Chinese
UT DIIDW:2014W46524
ER

PT P
PN WO2014093931-A1; US2014168071-A1; US2014168344-A1; US2014168453-A1; US2014173675-A1; US2014359647-A1; US8914837-B2; US2014375752-A1; US2015026708-A1; US2015070516-A1; US2015243163-A1; US2015244807-A1; US2015334344-A1; US9253520-B2; US9300910-B2; US9485459-B2; US9654563-B2; US2017223109-A1
TI Method for remotely accessing and controlling image capture device e.g. video camera, involves controlling image capture devices over network, where image capture devices include storage medium in communication with processor.
AB    NOVELTY - The method involves receiving user input from a user. Image capture devices (ICDs) (105a-105c) e.g. video cameras, are controlled over a network (115) e.g. Internet, based on the user input received from the user, where the ICDs include an audio output interface to provide audio output to an audio receiver, a video capture device to capture image data or video data, an audio capture device to capture audio data and a storage medium in communication with processor. A master account for the user is registered. The ICDs are assigned to the master account.
   USE - Method for remotely accessing and controlling an ICD or presence detection device (PDD) e.g. video camera, video calling device and security camera.
   ADVANTAGE - The method enables sending user's biometric/physiological information to PDD and to a data center local to the PDD so as to reduce comparison/matching time for identification/authentication of the user at the PDD. The method enables detection of an individual to be fully automatic and to require no user interaction. The method enables characterizing an individual's facial features automatically, detecting presence of a secondary device and characterizing an individual's voice print automatically, thus reducing errors in the detection process. The method enables imaginations of game developers and players for enhancing multiplayer/multitasking game play. The method enables audio challenge/response verification with analysis of sub-vocal responses from person challenged to prevent undesired casual overhearing of audio passwords and audio keyphrases.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an apparatus for remotely accessing and controlling an ICD
   (2) an ICD
   (3) a system for remotely accessing and controlling an ICD.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a system for implementing remote control of image and video capture devices, and for implementing remote control of processing and sharing functions associated with captured image and video data.
   ICDs (105a-105c)
   Control server (110)
   Network (115)
   User interface (120)
   Web server (125)
   Cloud storage system (130)
PD WO2014093931-A1   19 Jun 2014   H04N-021/63   201444   Pages: 90   English
   US2014168071-A1   19 Jun 2014   G06F-003/00   201444      English
   US2014168344-A1   19 Jun 2014   H04N-007/14   201444      English
   US2014168453-A1   19 Jun 2014   H04N-005/232   201444      English
   US2014173675-A1   19 Jun 2014   H04N-021/2743   201444      English
   US2014359647-A1   04 Dec 2014   H04N-021/442   201480      English
   US8914837-B2   16 Dec 2014   H04N-007/173   201501      English
   US2014375752-A1   25 Dec 2014   H04N-007/14   201502      English
   US2015026708-A1   22 Jan 2015   H04N-021/2668   201508      English
   US2015070516-A1   12 Mar 2015   H04N-005/232   201519      English
   US2015243163-A1   27 Aug 2015   G08C-023/02   201557      English
   US2015244807-A1   27 Aug 2015   H04L-029/08   201557      English
   US2015334344-A1   19 Nov 2015   H04N-007/14   201576      English
   US9253520-B2   02 Feb 2016   H04N-005/225   201611      English
   US9300910-B2   29 Mar 2016   H04N-007/16   201623      English
   US9485459-B2   01 Nov 2016   H04H-060/33   201672      English
   US9654563-B2   16 May 2017   H04N-007/16   201734      English
   US2017223109-A1   03 Aug 2017   H04L-029/08   201753      English
UT DIIDW:2014L94742
ER

PT P
PN CN103690149-A; CN103690149-B
TI Method for realizing health condition through face shooting by e.g. mobile terminal, involves displaying human face characteristic based on health condition information, and searching corresponding health guidance information.
AB    NOVELTY - The method involves establishing (S100) human face database and each portion status of body health condition. Human face information is obtained (S200). The face portion of human is recognized. A database in a corresponding thinning characteristic of human face of the health condition information is searched (S300) according to recognition of face of each portion. The human face characteristic corresponding to thinning of the health condition information is displayed. The corresponding health guidance information of a prompt is searched (S400).
   USE - Method for realizing health condition through face shooting by mobile terminal such as mobile terminal.
   ADVANTAGE - The health condition can be realized through face shooting by mobile terminal effectively.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the method for realizing health condition through face shooting by mobile terminal. (Drawing includes non-English language text)
   Step to establish human face database and each portion status of body health condition (S100)
   Step to obtain human face information (S200)
   Step to search database in corresponding thinning characteristic of human face of health condition information (S300)
   Step to search corresponding health guidance information of prompt (S400)
PD CN103690149-A   02 Apr 2014   A61B-005/00   201434   Pages: 13   Chinese
   CN103690149-B   17 Aug 2016   A61B-005/00   201660      Chinese
UT DIIDW:2014K24556
ER

PT P
PN US2013243260-A1; US9008370-B2
TI Method for tracking customer history data in e.g. grocery stores, involves applying license plate information in association with facial information as unique customer identifier with respect to history data.
AB    NOVELTY - The method involves capturing a license plate image with respect to a customer vehicle and facial image to obtain license plate information and facial information. The license plate information in association with facial information as a unique customer identifier is applied with respect to customer history data stored in a database, if customer places an order through transaction approach. The order information is associated with customer history data identified from license plate information.
   USE - Method for tracking customer history data in grocery stores, departmental stores and enterprises.
   ADVANTAGE - The degree of difficulty of the face matching/recognition problem can be reduced effectively. Two facial images of the customer can be taken on the same day with similar lighting and attire and forward facing pose. The parameters of the facial recognition can be tuned to bias towards lower yield and extremely high accuracy results. The customer history data utilizing vehicle and facial information can be tracked effectively.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a system for tracking customer history data; and
   (2) a processor-readable medium storing program for tracking customer history data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of the computer software system.
   Computer software system (150)
   Operating system (151)
   Customer history data tracking module (152)
   Interface (153)
   Software application (154)
PD US2013243260-A1   19 Sep 2013   G06K-009/00   201365   Pages: 14   English
   US9008370-B2   14 Apr 2015   G06K-009/00   201526      English
UT DIIDW:2013N51680
ER

PT P
PN CN103268549-A
TI Human face characteristics based mobile payment and password verification system, has matching failure server performing program re-checking process if authentication information selecting process is performed.
AB    NOVELTY - The system has a face information registration binding part comprising a camera terminal device and a server. A camera obtains registered user information of an image. An information identification verifying part comprises another camera terminal device and a face image detecting and extracting uploading server. Another camera obtains a present user face information image from the face image detecting and extracting uploading server. A matching failure server performs program re-checking process if authentication information selecting process is performed.
   USE - Human face characteristics based mobile payment and password verification system.
   ADVANTAGE - The system effectively prevents an account password from being stolen.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating operations performed by a human face characteristics based mobile payment and password verification system.'(Drawing includes non-English language text)'
PD CN103268549-A   28 Aug 2013   G06Q-020/40   201377   Pages: 6   Chinese
UT DIIDW:2013V38544
ER

PT P
PN KR1242390-B1; WO2013100699-A1; US2014165187-A1; EP2798563-A1; CN104169933-A
TI Method for authenticating user using smart phone, involves forming operation pattern of face image based on rotational direction of blinking collection of eye for authenticating user.
AB    NOVELTY - The method involves forming the operation pattern of face image based on the rotational direction of the blinking collection of eye for authenticating the user. The feature information of eye is extracted from the face images of the fixed number. The blinking collection of eye is detected by tracking the feature information of eye. The rotational direction of head is detected by tracking the feature information of the face images of the fixed number.
   USE - Method for authenticating user using smart phone and tablet personal computer (PC).
   ADVANTAGE - The operation of user face can be clearly recognized, since the face image is obtained from the stopped image including the photo etc. The face image can be easily coincided with the other image based on the updated registered operation pattern, when the password is registered by storage with respect to each user. The facial cognitive rate of the user can be improved, by the face cognition motion performed in the face authentication using the recent face operation of the user.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) apparatus for authenticating user; and
   (2) computer readable medium storing program for authenticating user.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of the system for authenticating user.(Drawing includes non-English language text)
   User authentication system (100)
   Display section (110)
   Camera (120)
   Storage (130)
   Transceiver (140)
PD KR1242390-B1   12 Mar 2013   G06F-021/20   201338   Pages: 23   
   WO2013100699-A1   04 Jul 2013   G06F-021/20   201344      English
   US2014165187-A1   12 Jun 2014   G06F-021/32   201453      English
   EP2798563-A1   05 Nov 2014   G06F-021/30   201473      English
   CN104169933-A   26 Nov 2014   G06F-021/32   201506      Chinese
UT DIIDW:2013G29742
ER

PT P
PN US2012250951-A1; WO2012135156-A2; WO2012135156-A3; TW201303612-A; US8744143-B2; TW522814-B1
TI Computer-implemented method for consent-based sharing photograph in social network e.g. Facebook, involves unblurring portion of online photograph according to permission received from corresponding individual.
AB    NOVELTY - The method involves identifying an individual in online photograph. The portion of online photograph is blurred temporarily. The portion of online photograph is unblurred according to the permission received from individual. The upload request to upload online photograph is received from individual. The tag on online photograph is received by corresponding individual. The corresponding individual is identified using facial recognition. A training photograph of the individual is received prior to identify the corresponding individual.
   USE - Computer-implemented method for consent-based sharing photograph in social network such as Facebook (RTM: social networking service created by Mark Zuckerberg), Yahoo (RTM: global internet service company), Pulse (RTM: interbank electronic funds transfer network), Twitter (RTM: social networking service developed by Atebits) and Linkedln (RTM: social networking website for people in professional occupations).
   ADVANTAGE - The portion of the photograph is temporarily blurred, so that the easy recognizing of individuals can be prevented effectively. The consent-based technique is provided, such that the privacy for online photographs can be ensured. Individuals can be identified automatically from facial detection technique. The user's privacy can be maintained reliably.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for computer program product stored on non-transitory computer readable medium storing program for sharing online photographs.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the process for consent-based sharing online photographs.
   Step for starting online photograph sharing process (105)
   Step for uploading photograph to online community (110)
   Step for protecting privacy of individuals in online photograph (120)
   Step for publishing online photograph according to permissions (130)
   Step for ending online photograph sharing process (195)
PD US2012250951-A1   04 Oct 2012   G06K-009/00   201267   Pages: 12   English
   WO2012135156-A2   04 Oct 2012   G06Q-050/30   201267      English
   WO2012135156-A3   31 Jan 2013   G06Q-050/30   201309      English
   TW201303612-A   16 Jan 2013   G06F-015/16   201328      Chinese
   US8744143-B2   03 Jun 2014   G06K-009/54   201436      English
   TW522814-B1   21 Feb 2016   G06F-015/16   201637      Chinese
UT DIIDW:2012M72841
ER

PT P
PN WO2012126135-A1; CN103430218-A; EP2689396-A1; US2014043329-A1; EP2689396-A4
TI Augmented makeover method with three-dimensional (3D) face modeling and landmark alignment involves generating personalized facial components in real time based in part on two-dimensional (2D) landmark points registered to generic 3D model.
AB    NOVELTY - The method involves capturing 2D images (102) of a scene by a camera, detecting the user's face in the 2D image and detecting 2D landmark points of the user's face in the 2D image. Each of the 2D landmark points are registered to a generic 3D face model (104). Personalized facial components (106) representing the user's face mapped to the generic 3D face model are generated in real time to form the personalized 3D morphable model based in part on the 2D landmark points registered to the generic 3D face model.
   USE - Augmented makeover method with 3D face modeling and landmark alignment, for augmented reality applications executed by a processor in an image processing system for personalizing facial images.
   ADVANTAGE - Enables real time face tracking and personalized avatar manipulation by using principle component analysis to transform the mapping of typically thousands of vertices and triangles into a mapping of tens of parameters, making the computational complexity feasible if augmented reality component is executed on a processing system comprising an embedded platform with limited computational capabilities. Provides for an immersive experience for the user by combining all four capabilities, i.e. facial features, 2D face detection, face tracking in video sequences, landmark detection for alignment and 3D face animation, in a single processing system.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system to generate a personalized 3D morphable model representing a user's face;
   (2) machine-readable instructions used to implement the augmented makeover method and the system to generate a personalized 3D morphable model representing a user's face; and
   (3) a machine-readable storage storing the machine-readable instructions used to implement the augmented makeover method.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of an augmented reality component.
   2D images (102)
   Generic 3D face model (104)
   Personalized facial components (106)
   Landmark points detection component (108)
   3D landmark points registration component (110)
   Personalized avatar generation component (112)
PD WO2012126135-A1   27 Sep 2012   G06T-017/00   201265   Pages: 61   English
   CN103430218-A   04 Dec 2013   G06T-017/00   201407      Chinese
   EP2689396-A1   29 Jan 2014   G06T-017/00   201409      English
   US2014043329-A1   13 Feb 2014   G06T-017/10   201412      English
   EP2689396-A4   03 Jun 2015   G06T-017/00   201769      English
UT DIIDW:2012M61532
ER

PT P
PN CN102592143-A; CN102592143-B
TI Driver holding telephone illegal behavior detection method, involves collecting driver head image, and performing image normalization process in rectangular ear part locating region to obtain size of rectangle left side ear image.
AB    NOVELTY - The method involves collecting a driver head image. A personal face area queue is provided by a face detection algorithm to detect a human face from frame head images. A human face rectangular area is located in a rectangular characteristic human face area. Judgment is made whether a human face area queue space continuously detects a human face. The rectangular characteristic human face area is placed in a left upper corner as an origin image. An image normalization process is performed in a rectangular ear part locating region to obtain size of a rectangle left side ear image.
   USE - Driver holding telephone illegal behavior detection method.
   ADVANTAGE - The method enables detecting illegal behavior in real time with better performance and high recognition precision. The method enables providing pre-warning to a driver so as to effectively reduce answering a telephone call, thus reducing traffic accident, hence improving driving safety.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a driver holding telephone illegal behavior detection method. '(Drawing includes non-English language text)'
PD CN102592143-A   18 Jul 2012   G06K-009/62   201263   Pages: 13   Chinese
   CN102592143-B   23 Oct 2013   G06K-009/62   201402      Chinese
UT DIIDW:2012L86526
ER

PT P
PN CN102567686-A
TI Safety authentication method for mobile terminal application software based on human body stable characteristics, involves providing mobile terminal with intelligent transflash card and human body stable characteristic identifying module.
AB    NOVELTY - The method involves providing a mobile terminal (1) with an intelligent transflash (TF) card (3) and a human body stable characteristic identifying module. The human body stable characteristic identifying module is provided with a face identifying module, biometrics, iris identification module, fingerprint identification module, palm scanning identification module, gait identifying module and a voice recognition module.
   USE - Safety authentication method for mobile terminal application software based on human body stable characteristics.
   ADVANTAGE - The convenience can be improved. The cost is reduced. The authentication can be performed safely and effectively.
   DESCRIPTION OF DRAWING(S) - The drawing shows a front view of the mobile terminal.
   Mobile terminal (1)
   Display screen (2)
   TF card (3)
   Fingerprint identification module (4)
PD CN102567686-A   11 Jul 2012   G06F-021/22   201259   Pages: 7   Chinese
UT DIIDW:2012L20595
ER

PT P
PN WO2012020927-A1; KR2012014480-A; KR2012039146-A; US2013136363-A1; CN103119593-A; KR2016129823-A; CN103119593-B; ID201304102-A; KR2016149185-A; KR1690264-B1; US9576195-B2; US2017075885-A1; KR1729938-B1; KR1756981-B1; CN107092614-A; KR1784287-B1; ID201708574-A; US10380170-B2; CN107092614-B
TI Object i.e. human image searching system for use in online shopping mall, has decoder performing auto driving of image corresponding to type of object, and photographic unit taking photograph of object based on search request.
AB    NOVELTY - The system has an image detecting apparatus (300) for providing additional information related to an object. A terminal (100) is connected to the image detecting apparatus through a communication network (200) for requesting to search the additional information about the object. The terminal transmits object information read by the image detecting apparatus and receives a search result. A decoder performs auto driving of the image corresponding to the type of the object, and a photographic unit (120) takes a photograph of the object based on a search request.
   USE - Object i.e. human image searching system for use in an online shopping mall.
   ADVANTAGE - The system automatically performs image search regardless of the type of the object in an efficient manner, and conveniently performs the retrieval service.
   DETAILED DESCRIPTION - The decoder is selected from a bar code decoder, a quick response code decoder, a near field communication (NFC) decoder, a radio frequency identification (RFID) decoder, an optical character recognition (OCR) decoder, a face recognition decoder and a vision recognition decoder. An INDEPENDENT CLAIM is also included for an object image searching method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic diagram of an object image searching system.'(Drawing includes non-English language text)'
   Terminal (100)
   Photographic unit (120)
   Communication network (200)
   Image detecting apparatus (300)
   Additional service apparatus (400)
PD WO2012020927-A1   16 Feb 2012   G06F-017/30   201214   Pages: 25   
   KR2012014480-A   17 Feb 2012      201216      
   KR2012039146-A   25 Apr 2012   G06K-007/10   201230      
   US2013136363-A1   30 May 2013   G06K-009/00   201337      English
   CN103119593-A   22 May 2013   G06F-017/30   201367      Chinese
   KR2016129823-A   09 Nov 2016   G06K-007/14   201676      
   CN103119593-B   30 Nov 2016   G06F-017/30   201681      Chinese
   ID201304102-A   14 Nov 2013   G06F-017/30   201701      
   KR2016149185-A   27 Dec 2016   G06F-017/30   201705      
   KR1690264-B1   09 Jan 2017   G06K-007/14   201707      
   US9576195-B2   21 Feb 2017   G06K-009/46   201716      English
   US2017075885-A1   16 Mar 2017   G06F-017/30   201720      English
   KR1729938-B1   26 Apr 2017   G06F-017/30   201732      
   KR1756981-B1   12 Jul 2017   G06K-007/14   201753      
   CN107092614-A   25 Aug 2017   G06F-017/30   201758      Chinese
   KR1784287-B1   11 Oct 2017   G06F-017/30   201770      
   ID201708574-A   11 Aug 2017   G06F-017/30   201852      
   US10380170-B2   13 Aug 2019   G06K-009/62   201961      English
   CN107092614-B   20 Apr 2021   G06K-009/00   202138      Chinese
UT DIIDW:2012C14305
ER

PT P
PN CN101950550-A; CN101950550-B
TI Picture angle displaying device for watching circumstance of single person, has computer controlling system for outputting pictures or videos, where beta is set as rotary angle between watcher and center line of display screen.
AB    NOVELTY - The device has a computer controlling system for pre-storing a set of pictures or videos shot under different visual angles in same scene. A computer locates a face candidate, and detects positions of eyes of a watcher by applying eye detection arithmetic. The computer calculates the visual angles between the watcher and a camera. The system outputs the pictures or videos corresponding to the visual angles to a display screen, where beta is set as rotary angle between the watcher and a center line of the display screen.
   USE - Picture angle displaying device for displaying an angle of a picture for watching circumstance of a single person (claimed).
   ADVANTAGE - The device quickly locks position of the watcher, and calculates the visual angles of the watcher, thus displaying the visual angles of the pictures according to the visual angles of the watcher, and hence providing better visual experience.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a kinematic analysis method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the process involved in a picture angle displaying device. '(Drawing includes non-English language text)'.
PD CN101950550-A   19 Jan 2011   G09G-005/00   201123   Pages: 10   Chinese
   CN101950550-B   29 May 2013   G09G-005/00   201367      Chinese
UT DIIDW:2011C80880
ER

PT P
PN CN101825947-A; WO2011137627-A1
TI Method for intelligently controlling mobile terminal, involves transferring corresponding direction navigation order according to moving direction of face of mobile terminal for controlling mobile terminal.
AB    NOVELTY - The method involves collecting face operational information in real time, and acquiring and recording position information of a feature point of an operator according to the face operational information. Current position information is compared with previous position information, and a compared result is generated. A moving direction of a face of a mobile terminal is acquired according to the compared result. A corresponding direction navigation order is transferred according to the moving direction of the face of the mobile terminal for controlling the mobile terminal.
   USE - Method for intelligently controlling a mobile terminal (claimed).
   ADVANTAGE - The method enables controlling the mobile terminal conveniently in a sense control manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for intelligently controlling a mobile terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a method for intelligently controlling a mobile terminal. '(Drawing includes non-English language text)'
PD CN101825947-A   08 Sep 2010   G06F-003/01   201070   Pages: 18   Chinese
   WO2011137627-A1   10 Nov 2011   G06K-009/00   201174      Chinese
UT DIIDW:2010M42955
ER

PT P
PN US2010014721-A1; US8199979-B2
TI Digital image processing system for use in image capture appliance i.e. digital camera, has face recognition module for extracting values of face classifier parameters from normalized face region.
AB    NOVELTY - The system has a face detection module for identifying a group of pixels corresponding to face regions (FR1-FR3) within digital image data acquired by an acquisition device. A normalization module generates a normalized face region from the face regions. A face recognition module extracts a set of values of face classifier parameters from the normalized face region. A workflow module associates a faceprint, and a database module is utilized for archiving the data within digital data storage media.
   USE - Digital image processing system for use in an image capture appliance i.e. digital camera.
   ADVANTAGE - The system improves quality of the digital image, improves the accuracy and the reliability of the recognition process, improves the response time of the face recognition module, and greatly enhances the value of the images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a photograph of three face regions mapped to face prints in a three-component face space.
   Face regions (FR1-FR3)
PD US2010014721-A1   21 Jan 2010   G06K-009/00   201009   Pages: 56   English
   US8199979-B2   12 Jun 2012   G06K-009/00   201239      English
UT DIIDW:2010A89246
ER

PT P
PN CN101419671-A; CN101419671-B
TI Human face gender identifying method for e.g. monitoring system, involves determining input sample to be male/female if input human face image is divided into upper half parts by classification plane equation of fuzzy support vector.
AB    NOVELTY - The method involves performing pre-treatment to image in human face training storehouse and collected image of human face images, and performing human face feature extraction and feature selection to two groups of pre-treated images. The gender identification of human face is performed using human face feature extraction and feature selection result of the collected images of the human face. An input sample is determined to be male/female if input human face image is divided into upper half parts by a classification plane equation of a fuzzy support vector.
   USE - Method for identifying gender of a human face in a monitoring system and an information collecting system.
   ADVANTAGE - The method enables providing high environment adaptive capacity and maintaining high robustness at different illuminations, postures and expressions.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face gender identifying process.'(Drawing includes non-English language text)'
PD CN101419671-A   29 Apr 2009   G06K-009/00   200934      Chinese
   CN101419671-B   18 May 2011   G06K-009/00   201169      Chinese
UT DIIDW:2009J38771
ER

PT P
PN CN101329724-A; CN101329724-B
TI Optimal face recognition method, involves centralizing training sample set of face image, and calculating characteristic vectors corresponding to maximal characteristic values of covariance matrix.
AB    NOVELTY - The method involves centralizing a training sample set of a face image, and calculating characteristic vectors corresponding to maximal characteristic values of a covariance matrix. A face image space is transformed into dimensionality reduced characteristic face space by use of a main component analysis projection matrix to obtain optimal description characteristics of the face image. A testing sample and the training sample are projected to a transformation matrix to obtain respective projection coefficient.
   USE - Method for recognizing an optimal face.
   ADVANTAGE - The method reduces the dimension of the face characteristic space, thus improving the recognition rate of the face image.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an optimal face recognition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of an optimal face recognition method. '(Drawing includes non-English language text)'
PD CN101329724-A   24 Dec 2008   G06K-009/00   200907   Pages: 16   Chinese
   CN101329724-B   09 Jun 2010   G06K-009/00   201058      Chinese
UT DIIDW:2009B12353
ER

PT P
PN US2008080743-A1; US7881505-B2
TI Video data processing method for use in video search and retrieval system, involves enabling user to view face-specific video segments in video data based on identified face-specific set of video frames.
AB    NOVELTY - The method involves detecting human faces in video frames in video data (12), and identifying a face-specific set of video frames, for one detected human face, irrespective of whether the detected human face is present in the face-specific set of video frames in a substantially temporally continuous manner. A user is enabled to view face-specific video segments in the video data based on the identified face-specific set of video frames.
   USE - Method for processing video data in a video search and retrieval system.
   ADVANTAGE - The method enables to automatically create an index of human faces or objects-of-interest that appear in the video, and allows a user to selectively view video segments associated with a specific human face or object without performing a time-consuming search of the entire video for those video segments.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a data storage medium containing a program code for processing video data
   (2) a system for processing video data, comprising a computing unit
   (3) a computer system configured to process video data.
   DESCRIPTION OF DRAWING(S) - The drawing shows an overall schematic diagram of major components in a system implementing the face mining application.
   Video data (12)
   Face detection module (26)
   Face association module (28)
   Face recognition module (30)
   Database (31)
   Face mapping module (33)
   Interactive graphical display (35)
PD US2008080743-A1   03 Apr 2008   G06K-009/00   200833   Pages: 20   English
   US7881505-B2   01 Feb 2011   G06K-009/00   201110      English
UT DIIDW:2008E81939
ER

PT P
PN US2008037836-A1; US7751599-B2
TI Virtual facial expression driving method for use in e.g. digital camcorder, involves detecting positions of face, eyes, and mouth in target image captured by capturing device, and converting detected positions into standard image.
AB    NOVELTY - The method involves automatically detecting positions of a face, eyes, nose, and mouth in a target image captured by a digital image capturing device e.g. digital camcorder. The detected positions are converted into a standard image for a virtual front face. Initial positions of all related key points of eyes, nose, and mouth of the target image on the standard image are calculated. Exact positions of the key points of the target image on the standard image are calculated. The exact positions of all key points of the standard image are inversely aligned onto the target image.
   USE - Method for driving virtual facial expression of a face image, in a digital image capturing device e.g. digital camera and digital camcorder, with computer device e.g. digital personal assistant and notebook computer, and communications device e.g. mobile phone and video-conferencing phone, for conducting videoconference or a chatting over a narrowband network environment.
   ADVANTAGE - The method automatically captures, detects and dynamically tracks the image of the face and facial features, thus obtaining exact position of each key point, and hence allowing a user to transmit the virtual face image with facial expressions to the opposite party without the need of transmitting a real image having a large number of data. The method assures the user`s privacy, and efficiently allows the users to conduct the videoconference or the chatting over the narrowband network environment.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow representation of processing a face detection block.
PD US2008037836-A1   14 Feb 2008   G06K-009/00   200815   Pages: 16   English
   US7751599-B2   06 Jul 2010   G06K-009/00   201044      English
UT DIIDW:2008C05690
ER

PT P
PN CN101059838-A; CN100481112-C
TI Human face recognizing system, has complex programmable logic device address decoder connected with exterior program memorizer NAND Flash, and power supply module for supplying electric power source.
AB    NOVELTY - The system has an exterior program memorizer NAND Flash (5) for memorizing recognition programs. A network interface module (1) and a dual-core chip substrate on chip (SOC) module (2) are interconnected. The dual-core chip module and a high-speed calculation memory (4) are interconnected via a data bus. A complex programmable logic device (CPLD) address decoder (3) is linked to the flash via an external memory interface (EMIF) bus. The decoder is linked to an integrated drive electronics (IDE) hard disc (6). A streaming data is transmitted by a network, and acquires digital image sequence.
   USE - Human face recognizing system.
   ADVANTAGE - The system facilitates a high recognition rate, high recognition speed and strong data processing capability. The system is compatible with current monitoring network, stable to work, and easy to be updated and maintained. The system is manufactured in a cost effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a recognizing method for a human face recognizing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face recognizing system. `(Drawing includes non-English language text)`
   Network interface module (1)
   Dual-core chip substrate on chip module (2)
   Complex programmable logic device address decoder (3)
   High speed calculation memory (4)
   Exterior program memorizer NAND Flash (5)
   Integrated drive electronics hard disc (6)
   Power supply module (7)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The streaming media data carries on the compressed video decoding through video formats such as MPEG2, MPEG4, H.263 and H.264.
PD CN101059838-A   24 Oct 2007   G06K-009/00   200833   Pages: 13   Chinese
   CN100481112-C   22 Apr 2009   G06K-009/00   200970      Chinese
UT DIIDW:2008E72469
ER

PT P
PN KR2006081380-A; WO2007081122-A1; KR729280-B1; US2008292144-A1; IN200805272-P1; CN101366047-A; JP2009522683-W; CN101366047-B; US8085994-B2
TI System and method for performing face and/or iris recognition using mobile terminal equipped with stereo camera.
AB    NOVELTY - A system and a method for recognizing an iris using a mobile terminal equipped with a stereo camera are provided to correct deviation of an image size according to a distance by calculating the distance between the stereo camera and a face, as the stereo camera of the mobile terminal captures the image fit for face and/or iris recognition.
   DETAILED DESCRIPTION - The stereo camera (10) captures an iris image by being installed to the mobile terminal. A digital camera chip (20) converts an analog image captured in the stereo camera into a digital image. A controller (40) performs the iris and/or face recognition by receiving the digital image from the digital camera chip through a video input unit for controlling video input. The stereo camera obtains more than two images through one image sensor by using more than two cameras or using more than two optical sensors to one image sensor.
PD KR2006081380-A   12 Jul 2006   G06K-009/00   200712      
   WO2007081122-A1   19 Jul 2007   G06K-009/00   200749      English
   KR729280-B1   15 Jun 2007   G06K-009/00   200833      
   US2008292144-A1   27 Nov 2008   G06K-009/00   200881      English
   IN200805272-P1   24 Oct 2008   G06K-009/00   200903      English
   CN101366047-A   11 Feb 2009   G06K-009/00   200917      Chinese
   JP2009522683-W   11 Jun 2009   G06T-007/00   200939   Pages: 13   Japanese
   CN101366047-B   19 Oct 2011   G06K-009/00   201177      Chinese
   US8085994-B2   27 Dec 2011   G06K-009/00   201203      English
UT DIIDW:2007119338
ER

PT P
PN US2005105780-A1; JP2005149506-A; US7596247-B2; JP4543423-B2
TI Object verifying or recognizing method for face recognition, involves applying extracted features to previously-determined additive Gaussian model to determine whether object of interest belongs to existing class.
AB    NOVELTY - The method involves detecting an object of interest in a digital image data for normalizing the object of interest to generate a normalized object representation. A set of features is extracted from the representation. Each extracted feature is applied to a previously-determined additive probability model to determine whether the object of interest belongs to an existing class, where the model is an additive Gaussian model.
   USE - Used for recognizing or verifying an object in a digital image for face recognition.
   ADVANTAGE - The method utilizing the additive Gaussian model provides a powerful way for face recognition using even a single example of each face. The method allows multiple examples to be combined in a principled way into a more accurate face model which is robust and applies to both frontal and non-frontal faces.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an apparatus for recognizing or verifying objects in a digital image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of operations performed to classify faces using probability models.
PD US2005105780-A1   19 May 2005   G06K-009/00   200539   Pages: 15   English
   JP2005149506-A   09 Jun 2005   G06T-007/00   200539   Pages: 18   Japanese
   US7596247-B2   29 Sep 2009   G06K-009/00   200964      English
   JP4543423-B2   15 Sep 2010   G06T-007/00   201061   Pages: 18   Japanese
UT DIIDW:2005384250
ER

PT P
PN US2005025360-A1; US7346196-B2
TI Rotatable bay window switch box surveillance camera and illuminator system for facial recognition, has partially rotatable bay window inner frame within switch box, where frame is pivoted to orient camera towards entrance to room.
AB    NOVELTY - The system has a partially rotatable bay window inner frame mounted within a switch box (3). The frame is pivoted back and forth to orient a camera towards an entrance to a room in which the switch box is mounted. A bay window switch box cover plate mounted over the frame, engages and locks the frame in a selected position by attachment of the cover plate to the box, when the frame is rotated to a desired position.
   USE - Used for facial recognition.
   ADVANTAGE - The partially rotatable bay window inner frame within the switch box, is pivoted back and forth to orient the camera, thus giving an excellent face-shot surveillance.
   DESCRIPTION OF DRAWING(S) - The drawing shows an isometric view of a camera and an illuminator mounted in respective upper and lower stages of a partially rotatable bay window inner frame.
   Camera (1)
   Illuminator (2)
   Switch box (3)
   Inner frame notch (32)
   Lens (38)
PD US2005025360-A1   03 Feb 2005   H04N-007/18   200520   Pages: 10   English
   US7346196-B2   18 Mar 2008   G06K-009/00   200822      English
UT DIIDW:2005194058
ER

PT P
PN US2004184667-A1; JP2004288185-A; US7103227-B2; JP4364686-B2
TI Naturally illuminated scene image e.g. still image, enhancement method for use in e.g. digital camera, involves determining intensity gradients for group of images, and weighting each image with weights based on gradients.
AB    NOVELTY - The method involves acquiring a group of images of a scene, where each image has a different uncontrolled illumination. An intensity gradient for each image is determined. Each image is weighted with weights based on the intensity gradients, where the weights are based on spatial or temporal intensity gradient. The weighted images are combined to construct an enhanced image.
   USE - Used in a digital camera, infrared camera, laser range scanner and radar, for enhancing an image e.g. still image or video image of a naturally illuminated scene for highlighting or clarifying selected feature in a technical illustration or high-quality thumbnail image, for illustrating complex mechanical part, for image simplification, for photo-booth and video surveillance e.g. determining a part of a building near where a person is standing, determining what a person`s hand is hiding, identifying reflections in dark areas such as headlights reflecting from windows of dark buildings, and identifying a blinking light at night, and for computer vision and image processing application e.g. stereo reconstruction, visual hull construction, face recognition, and cartoons.
   ADVANTAGE - The method efficiently acquires multiple images and renders stylized images of scenes such as scenes with low contrast, geometrically complex scenes, and action scenes.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a method for generating a stylized image.
PD US2004184667-A1   23 Sep 2004   G06T-005/00   200469   Pages: 19   English
   JP2004288185-A   14 Oct 2004   G06T-003/00   200469   Pages: 58   Japanese
   US7103227-B2   05 Sep 2006   G06K-009/40   200660      English
   JP4364686-B2   18 Nov 2009   G06T-003/00   200976   Pages: 21   Japanese
UT DIIDW:2004707788
ER

PT P
PN EP1441276-A2; JP2004227589-A; US2004164848-A1; KR2004067122-A; US7161468-B2; EP1441276-A3; KR528328-B1
TI User authentication method for use in door lock/unlock system, involves finding authenticated biometric with registered biometrics and threshold values that are set depending on matching of input password with registered password.
AB    NOVELTY - The method involves determining whether a password is input. One threshold value is set, if an input password matches with a registered password and another threshold value is set, if the input password not match with the registered password. Authenticated biometric information is determined with the registered biometrics information and the two threshold values.
   USE - Used for authenticating a user based on a password input by the user biometrics information (claimed) e.g. fingerprint, iris, face recognition in door lock/unlock system, and entrance/exit control system.
   ADVANTAGE - The method adopts a combination password authentication and biometrics, thereby reducing a false acceptance rate (FAR) and a false rejection rate (FRR).
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) an user authentication apparatus that authenticates an user based on a password input by the user and the users biometrics information
   (b) a computer-readable medium which contains a program that enables the user authentication method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a user authentication method.
PD EP1441276-A2   28 Jul 2004   G06F-001/00   200453   Pages: 15   English
   JP2004227589-A   12 Aug 2004   G06F-015/00   200453   Pages: 14   Japanese
   US2004164848-A1   26 Aug 2004   G06F-007/04   200457      English
   KR2004067122-A   30 Jul 2004   G06K-009/00   200475      
   US7161468-B2   09 Jan 2007   H04Q-009/00   200705      English
   EP1441276-A3   15 Feb 2006   G06F-001/00   201731      English
   KR528328-B1   15 Nov 2005   G06K-009/00   200682      
UT DIIDW:2004545690
ER

PT P
PN WO2004044689-A2; AU2003286922-A1; EP1559060-A2; US2006013505-A1; AU2003286922-A8; KR2005084991-A; IN200500884-P2; JP2006518886-W; CN1781111-A; KR819960-B1; WO2004044689-A3; EP1559060-A4
TI Geometric surfaces analysis method e.g. for human face, involves comparing Gaussian and mean curvatures of mapped brain map surfaces for determining whether they exceed predetermined value.
AB    NOVELTY - A set of mesh representations of two surfaces of a brain map is received, and is conformably mapped. A set of Gaussian curvature and mean curvature of the mapped surfaces is computed. The computed two levels sets are compared to determine whether they exceed a predetermined value. A match between the two surfaces is declared when it is above predetermined value, else mismatch is declared.
   USE - For analyzing, classifying and recognizing geometric surfaces of human face. Also used in medical e.g. brain, imaging.
   ADVANTAGE - Provides a general way to classify surfaces effectively. Also provides efficient and achievable computation that is both numerically stable and accurate.
   DESCRIPTION OF DRAWING(S) - The figure shows a conformal mapping between human face and square.
PD WO2004044689-A2   27 May 2004   G06F-000/00   200441   Pages: 56   English
   AU2003286922-A1   03 Jun 2004   G06F-000/00   200470      English
   EP1559060-A2   03 Aug 2005   G06K-009/00   200551      English
   US2006013505-A1   19 Jan 2006   G06K-009/36   200607      English
   AU2003286922-A8   03 Nov 2005   G06K-009/00   200629      English
   KR2005084991-A   29 Aug 2005   G06K-009/00   200644      
   IN200500884-P2   09 Jun 2006   G06F-000/00   200648      English
   JP2006518886-W   17 Aug 2006   G06T-015/00   200654   Pages: 47   Japanese
   CN1781111-A   31 May 2006   G06K-009/00   200663      Chinese
   KR819960-B1   07 Apr 2008   G06K-009/00   200869      
   WO2004044689-A3   02 Sep 2004   G06K-009/00   201213      English
   EP1559060-A4   13 Jun 2007   G06K-009/00   201747      English
UT DIIDW:2004440694
ER

PT P
PN US2004034611-A1; JP2004078959-A; KR2004015613-A; US7295687-B2; KR442835-B1
TI Face recognition apparatus for credit card verification, has neural networks, each corresponding to selected eigenpaxels to output face recognition result, and determination unit to output final face recognition result.
AB    NOVELTY - The apparatus has an eigenpaxel selection unit (200) to generate eigenpaxels representing characteristic patterns of a face and to select a predetermined number of eigenpaxels. Neural networks (240), each corresponding to a selected eigenpaxels receives a filtered image signal, and output a face recognition result. A determination unit (250) receives the recognition result and outputs a final face recognition result.
   USE - Used in verification of credit card, cash card and electronic resident registration certificate.
   ADVANTAGE - The apparatus reduces the amount of computation required in neural network learning, thereby enabling high speed convergence of neural network, and improving face recognition with a large amount of data.
   DETAILED DESCRIPTION - An eigenfiltering unit filters an input facial image with selected eigenpaxels. INDEPENDENT CLAIMS are also included for the following:
   (a) a face recognizing method
   (b) a computer readable medium with a computer program for a face recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a face recognition apparatus.
   Eigenpaxel selection unit (200)
   Input image normalization unit (210)
   Eigenfiltering unit (220)
   Sun-sampling unit (230)
   Neural networks (240)
   Determination unit (250)
PD US2004034611-A1   19 Feb 2004   G06K-009/00   200421   Pages: 18   English
   JP2004078959-A   11 Mar 2004   G06T-007/00   200421   Pages: 19   Japanese
   KR2004015613-A   19 Feb 2004   G06K-009/00   200439      
   US7295687-B2   13 Nov 2007   G06K-009/00   200776      English
   KR442835-B1   02 Aug 2004   G06K-009/00   200480      
UT DIIDW:2004225145
ER

PT P
PN US2004022442-A1; EP1388804-A2; JP2004054956-A; KR2004008792-A; US7447338-B2; EP1388804-A3; KR442834-B1
TI Face detecting method, involves training each pattern classifier corresponding to sub-class using face and near-face feature vectors, and determining whether input image is face image using classifier.
AB    NOVELTY - The method involves classifying face and near-face feature vectors into predetermined sub-classes, and training each pattern classifier corresponding to a sub-class using face and near-face feature vectors. A feature vector of an image, which is input for detection, is applied to a classifier corresponding to a sub-class including the feature vector. The classifier is used to determine whether the input image is a face image.
   USE - Used for detecting a face using a pattern classifier in a face recognition system.
   ADVANTAGE - The method improves the reliability of the pattern classifier in detecting a face and improves the face detection speed.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS also included for the following:
   (a) a recording medium on which the method of detecting a face using a pattern classifier for learning face images and near face images is recorded
   (b) a system of detecting a face using a pattern classifier for learning face images and near face images.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a face detection system.
   Face basis vector database (118)
   Near-face basis vector database (119)
   Feature vector extractor (122)
   Clustering unit (130b)
   Face determiner (160)
PD US2004022442-A1   05 Feb 2004   G06K-009/62   200415   Pages: 18   English
   EP1388804-A2   11 Feb 2004   G06K-009/62   200415      English
   JP2004054956-A   19 Feb 2004   G06T-007/00   200415   Pages: 17   Japanese
   KR2004008792-A   31 Jan 2004   G06K-009/46   200435      
   US7447338-B2   04 Nov 2008   G06K-009/00   200875      English
   EP1388804-A3   06 Apr 2005   G06K-009/62   201731      English
   KR442834-B1   02 Aug 2004   G06K-009/46   200480      
UT DIIDW:2004156155
ER

PT P
PN US2003108225-A1; US7239726-B2
TI Image analysis system for face recognition, has facial feature detector to determine coordinates of eye, eyebrow, nose, mouth in image, based on vertical projection procedure.
AB    NOVELTY - A facial-feature detector performs vertical projection procedure on eye (418) and eyebrow (422) in image, by summing gray-scale values of pixels within the region of a window (414), to determine the coordinates of eye and eyebrow. Similarly, the gray-scale values of pixels within the region of a window (714) on the mouth (718) and nose (722) are totaled, to determine the coordinates of mouth and nose, respectively.
   USE - For use in face recognition.
   ADVANTAGE - Enables extracting facial feature information from image and recognizing face, effectively.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) method for analyzing image; and
   (2) computer-readable medium for storing image analysis program.
   DESCRIPTION OF DRAWING(S) - The figures respectively show the block diagrams illustrating the vertical projection procedure carried out in eye and mouth.
   windows (414,712)
   eye (418)
   eyebrow (422)
   histograms (426,726)
   mouth (718)
   nose (722)
PD US2003108225-A1   12 Jun 2003   G06K-009/00   200362   Pages: 17   English
   US7239726-B2   03 Jul 2007   G06K-009/00   200746      English
UT DIIDW:2003659040
ER

PT P
PN EP1216658-A1; CA2364874-A1; FR2818529-A1; US2002090123-A1; JP2002304620-A; JP2006048729-A; US7006657-B2
TI Remote determination of a physical cosmetic facial/body characteristic of Internet user in which sequence of images is displayed to Internet user who chooses nearest likeness.
AB    NOVELTY - The image sequence corresponds to different characteristic degrees of facial/body type following a continuous or sensibly continuous progression. The animated image sequences (21,22) are generated on personal computer from server, or on a console at a shop. The image sequences are displayed as function of the observers actions, using a cursor (24) moving over a bar (25). The observer can select an image for which data relating to the selection will be displayed
   USE - For selection of cosmetic treatment over the Internet.
   ADVANTAGE - Designed to remotely determine physical characteristics of a person who has access to the Internet.
   DETAILED DESCRIPTION - Method of determining the effective and/or wanted degree of a cosmetic facial characteristic of an individual in which:(a) a sequence of images (21,22,23) is generated expressing the characteristic with a variable progressive degree; (b) identification from the sequence of at least an image or series of images corresponding to the wanted characteristic. The number of image sequences is preferably greater or equal to 20
   DESCRIPTION OF DRAWING(S) - The drawings shows generation of an image sequence
   image sequence (21,22)
   cursor (24)
   election bar (25)
   alphanumeric data. (26)
PD EP1216658-A1   26 Jun 2002   A61B-005/103   200254   Pages: 22   French
   CA2364874-A1   21 Jun 2002   A61B-005/103   200254      French
   FR2818529-A1   28 Jun 2002   A61B-005/00   200254      French
   US2002090123-A1   11 Jul 2002   G06K-009/00   200254      English
   JP2002304620-A   18 Oct 2002   G06T-001/00   200301   Pages: 59   Japanese
   JP2006048729-A   16 Feb 2006   G06T-011/80   200614   Pages: 25   Japanese
   US7006657-B2   28 Feb 2006   G06K-009/00   200616      English
UT DIIDW:2002502164
ER

PT P
PN US2001031073-A1; JP2001283224-A; JP3575679-B2; US7177450-B2
TI Face recognition method involves comparing referential image generated using stored three-dimensional shape data and color image data according to shooting conditions with enquiry image.
AB    NOVELTY - The shooting conditions (8) of an enquiry face image (6) are estimated. A graphics unit (10) generates a referential face image (11) using stored 3D shape data (3) and color image data (4) according to estimated shooting conditions. The referential image is compared with the enquiry image and if the difference between the face images is small, the person of the enquiry face image is identified as the person of referential face image.
   USE - For identifying a person's face in face recognition system. Also, for identifying objects such as buildings, historic sites, mountains, rivers, sear, animals, plants, etc.
   ADVANTAGE - Enables executing precise and effective face collation, as large number of three-dimensional shapes of the face images and surface images are stored in database.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) Object recognition method;
   (b) Face recognition device;
   (c) Object recognition device;
   (d) Recording medium storing face recognition program;
   (e) Recording medium storing object recognition program
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of face image recognition system.
   Three-dimensional shape data (3)
   Color image data (4)
   Database (5)
   Enquiry face image (6)
   Shooting conditions (8)
   Graphics unit (10)
   Referential face image (11)
PD US2001031073-A1   18 Oct 2001   G06K-009/00   200230   Pages: 15   English
   JP2001283224-A   12 Oct 2001   G06T-007/00   200230   Pages: 10   Japanese
   JP3575679-B2   13 Oct 2004   G06T-007/00   200467   Pages: 12   Japanese
   US7177450-B2   13 Feb 2007   G06K-009/00   200714      English
UT DIIDW:2002254636
ER

PT P
PN KR2000050399-A; US6542625-B1; KR361497-B1
TI Method for extracting facial area.
AB    NOVELTY - The method involves detecting first image difference (101) between first pair of two frames with first time difference to obtain first candidate region of a specific object i.e. face (102), and detecting second image difference (104) between second pair of two frames with second time difference to obtain second candidate region of the specific object, where the first time difference is greater than the second time difference. A final region of the specific object is determined utilizing the first candidate region and the second candidate region.
   USE - Method for detecting a specific object i.e. face (claimed), in an image signal related to a moving picture file of a movie or drama for moving picture search assisted image editing applications and face recognition based security applications.
   ADVANTAGE - The method enables combining movement of the object to be detected and movement of the object feature so as to detect the object more accurately and efficiently in a rapid manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of images of two frames of relatively small time difference.
   Image differences between frames (101, 104)
   Face (102)
   Eye (103)
PD KR2000050399-A   05 Aug 2000   G06K-009/00   200120      
   US6542625-B1   01 Apr 2003   G06K-009/00   200324   Pages: 10   English
   KR361497-B1   18 Nov 2002      200332      
UT DIIDW:2001200581
ER

PT P
PN US2018218371-A1; US10108961-B2
TI Mobile phone for image analysis for user authentication, comprises a processor, and memory storing instructions that, when executed by the processor, cause the mobile phone to e.g. obtain a request to authorize a transaction.
AB    NOVELTY - Mobile phone comprises a processor; and memory storing instructions that, when executed by the processor, cause the mobile phone to: obtain a request to authorize a transaction; capture, using a camera (106) on the mobile phone, a two dimensional infrared image (120) of portion of a face of a user (102) interacting with the mobile phone; project a pattern of infrared dots from the mobile phone onto a face of the user; obtain a reflection of a subset of the infrared dots using an infrared detector; analyze the reflection of the infrared dots to verify that the user is in proximity of the mobile phone; identify the user based in portion on the two dimensional infrared image and the reflection of a subset of the infrared dots; authenticate the user as a result of the identity of the user being recognized as an authorized user and as a result of the user's face (122) being in the proximity of mobile phone; and allow a transaction to be performed as a result of user being authenticated.
   USE - The mobile phone is useful for image analysis for user authentication.
   ADVANTAGE - The method can improve security by adding biometric identification; and can provide a further level of accuracy for image-based user authentication.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:
   (1) a computer-implemented method of authenticating a user of a mobile device, which involves capturing, using an image sensor associated with the mobile device, an image of portion of a face of the user of the mobile device; projecting a pattern of infrared dots from the mobile device onto the face of the user; obtaining a reflection of a subset of the infrared dots using an infrared detector; determining, based in portion on the reflection, that the user corresponds to a physical being within proximity of the mobile device; determining an identity of the user by analyzing a portion of the image and the reflection; and authenticating the user as a result of the identity of the user being recognized as an authorized user and as a result of the user corresponding to a physical being in the proximity of the mobile device; and
   (2) a non-transitory computer-readable storage medium storing instructions that, as a result of being executed by a processor on a mobile computing device (104) cause the mobile computing device to: capture, using an infrared camera on the mobile device, an infrared image of portion of a face of a user of the mobile device; project a pattern of infrared dots from the mobile device onto the face of the user; obtain a reflection of a subset of the projected infrared dots using the infrared camera on the mobile device; determine, using the reflection of the subset of the projected infrared dots, that the user represented in the image matches a three-dimensional model of an authorized user stored in a memory on the mobile device; and authorize a request as a result of having determined that the user is the authorized user.
   DESCRIPTION OF DRAWING(S) - The drawings show schematic views of computing device configured to capture image information including a user and perform facial recognition on the captured image information.
   User (102)
   Computing device (104)
   Camera (106)
   Image (120)
   User's face (122)
TF TECHNOLOGY FOCUS - IMAGING AND COMMUNICATION - Preferred Parameters: The reflection of the infrared dots is used to generate a three-dimensional model of a face of the user. The instructions further cause the mobile phone to: obtain identifying information for an authorized user in association with enrollment in a visual authentication process; and authenticate the user as a result of the identity of the user being recognized as the authorized user associated with the identifying information. The instructions also cause the mobile phone to direct the user to look at the mobile phone to facilitate an authentication process. The instructions when executed further cause the mobile device to: as portion of a visual authentication enrollment process, project an additional set of infrared dots on to a face of an authorized user; capture a reflection of the additional set of infrared dots using the infrared camera; and generate a depth model of the authorized user based in portion on the reflection of the additional set of infrared dots. The instructions when executed further cause the mobile device to determine that the gaze of the user is directed at the device. Further, the instructions when executed cause the mobile device to illuminate the face of the user with an infrared light emitting diode on the mobile device. Preferred Device: An infrared LED on the mobile phone illuminates the user. The image sensor is operable to capture light in the infrared spectrum, where the image is an infrared image. Preferred Processes: The portion of the image is analyzed using a facial recognition algorithm; facial data of the authorized user is collected as portion of a visual authentication enrollment process; and the identity of the user is determined based in portion on the facial data. The computer-implemented method further involves generating a three-dimensional model of the face of the user by analyzing the reflection of a subset of the infrared dots; and storing the three-dimensional model as portion of a visual authentication enrollment process. It also involves illuminating the user with an infrared light source. It further involves detecting that the user is looking at the image sensor; and initiating an authentication process as a result of detecting that the user is looking at the image sensor. A 2nd verification process is utilized when a 1st verification process is unable to verify that the user contained in the image corresponds to a physical being with a minimum level of confidence. The computer-implemented method also involves presenting a signal to the user indicating that that the user should look at the device to perform an authentication process. It further involves receiving a request from a user to utilize an authentication process; obtaining image information of the face of the user; generating a three-dimensional model corresponding to the face of the user; and storing the model to a memory on the mobile device. The three-dimensional model and the image information are further made available to other computing device accessible by the user.
PD US2018218371-A1   02 Aug 2018   G06Q-020/40   201854   Pages: 18   English
   US10108961-B2   23 Oct 2018   G06Q-020/40   201871      English
UT DIIDW:2018602794
ER

PT P
PN EP3309727-A1; CA2981673-A1; US2018107999-A1; US10289990-B2
TI Store profile generation system has master control unit with processor that generates full planogram image of product display units to indicate presence or absence of products.
AB    NOVELTY - The store profile generation system (10) has a master control unit (24) operably associated with a mobile base (20), high-resolution (HR) image capture devices (40,42) and low-resolution (LR) image capture device (44). The master control unit has a processor that processes acquired HR images to identify barcodes associated with tags and generate a store profile. The processor generates a full planogram image of product display units. The full planogram image includes a display facing profile indicating presence or absence of the products and their respective locations in a retail environment.
   USE - Store profile generation system.
   ADVANTAGE - Minimizes cost, power consumption, transmission time, computation process and file handling requirements in real retail environments. Separates and optimizes image handling and analysis process for high resolution versus low resolution image analysis to maximize throughput and efficiency when all analytics modes are run during same mission.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a store profile generation method.
   DESCRIPTION OF DRAWING(S) - The drawing shows the schematic elevational view of a store profile generation system.
   Store profile generation system (10)
   Mobile base (20)
   Master control unit (24)
   HR image capture devices (40,42)
   LR image capture device (44)
PD EP3309727-A1   18 Apr 2018   G06Q-010/08   201829   Pages: 64   English
   CA2981673-A1   17 Apr 2018   G06Q-030/00   201830      English
   US2018107999-A1   19 Apr 2018   G06Q-020/20   201830      English
   US10289990-B2   14 May 2019   G06Q-020/20   201940      English
UT DIIDW:201829805P
ER

PT P
PN CN107066942-A
TI Living human body face identification method, involves collecting camera shooting video frame image, identifying video frame image of living human face by using convolutional neural network, and collecting living body human face image.
AB    NOVELTY - The method involves collecting (S100) a camera shooting video frame image. The video frame image of a living human face is identified by using a convolutional neural network (S200). A living body human face image is collected by using a camera for classifying human face image samples. A living body human face image sample is obtained through the convolutional neural network by using a picture human face identification model. An image process of the video frame image is performed. Judgment is made to check whether the frame image is included with a face after performing face detection process.
   USE - Living human body face identification method.
   ADVANTAGE - The method enables realizing a human learning neural network simulation process by using the neural network, and improving identification accuracy in an effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a living human body face identifying system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a living human body face identification method. '(Drawing includes non-English language text)'
   Step for collecting camera shooting video frame image (S100)
   Step for identifying video frame image of living human face by using convolutional neural network (S200)
PD CN107066942-A   18 Aug 2017   G06K-009/00   201769   Pages: 11   Chinese
UT DIIDW:201758865L
ER

PT P
PN CN106682616-A; CN106682616-B
TI Double-channel feature deep learning based neonatal pain expression recognition method, involves training double-channel convolution neural network model to identify input pain recognizing expression using network model trained test sample.
AB    NOVELTY - The method involves collecting a face image. The face image is divided into class expression according to pain degree. A facial expression image base is established. A sample in the facial expression image base is pre-processed to obtain image pixel size. A local binary pattern (LBP) characteristic map is extracted. A double-channel convolutional neural network is constructed. A trained network model is established. A double-channel convolution neural network model is trained to identify input pain recognizing expression using a neural network model trained test sample.
   USE - Double-channel feature deep learning based neonatal pain expression recognition method.
   ADVANTAGE - The method enables identifying calm, crying, mild pain, severe pain, expression and illumination of born facial image, avoiding noise shielding problems and increasing robustness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a double-channel feature deep learning based neonatal pain expression recognition method. '(Drawing includes non-English language text)'
PD CN106682616-A   17 May 2017   G06K-009/00   201742   Pages: 13   Chinese
   CN106682616-B   21 Apr 2020   G06K-009/00   202036      Chinese
UT DIIDW:2017329603
ER

PT P
PN CN106228628-A; CN106228628-B
TI Human face recognition based registration system for embedded face recognition processing device, has main body for performing target registered user sign-in confirming process when face feature is contained with target registered user data.
AB    NOVELTY - The system has a main body provided with a camera and an embedded face identification processing device. The embedded face recognition processing device is provided with a CPU, a graphic processing unit (GPU), a memory unit and an input/output interface. The camera collects user monitoring video stream and transmits the user monitoring video stream to the embedded face identification processing device through the input/output interface. The main body performs target registered user sign-in confirming process when registering face feature is contained with target registered user data.
   USE - Human face recognition based registration system for an embedded face recognition processing device (claimed).
   ADVANTAGE - The system increases lifting sign-in attendance checking efficiency and face recognition accelerating efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an embedded face recognition processing device
   (2) a human face recognition based registration method
   (3) a human face recognition based registration device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face recognition based registration method. '(Drawing includes non-English language text)'
PD CN106228628-A   14 Dec 2016   G07C-001/10   201704   Pages: 24   Chinese
   CN106228628-B   26 Mar 2021   G07C-001/10   202128      Chinese
UT DIIDW:201680106G
ER

PT P
PN CN105957001-A
TI Video based privacy protecting method, involves performing video mosaic processing and video acquisition processing operations, and performing video compressing and storing operations based on distribution process.
AB    NOVELTY - The method involves obtaining a reference face (S101). Determination is made to check whether the reference face is compared with a human face. Video scanning process is performed (S102) according to face recognition and detection algorithm. A processing face is obtained. Determination is made to check whether the reference face is matched (S103) with the processing face. Video mosaic processing and video acquisition processing operations are performed. Video compressing and storing operations are performed (S104) based on distribution process.
   USE - Video based privacy protecting method.
   ADVANTAGE - The method enables realizing privacy protecting process in a simple manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a video based privacy protecting device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a video based privacy protecting method. '(Drawing includes non-English language text)'
   Step for obtaining reference face (S101)
   Step for performing video scanning process according to face recognition and detection algorithm (S102)
   Step for determining whether the reference face is matched with the processing face (S103)
   Step for performing video compressing and storing operations based on distribution process (S104)
PD CN105957001-A   21 Sep 2016   G06T-003/40   201671   Pages: 11   Chinese
UT DIIDW:201662891S
ER

PT P
PN CN105760859-A; CN105760859-B
TI Multiple-task convolution nerve network based fingerprint human face image recognition method, involves recovering identified clear human face image using trained convolution nerve network model, and identifying clear human face image.
AB    NOVELTY - The method involves collecting a fingerprint human face image through a network. An image is marked through a threshold value to obtain an indicator position of a pattern. A residual image is obtained by a return value of a clear human face image and a screen print human face image. An auxiliary task is performed to obtain a prediction network fingerprint image. A trained convolution nerve network model is established. An identified clear human face image is recovered using the trained convolution nerve network model. The clear human face image is identified by human face identification.
   USE - Multiple-task convolution nerve network based fingerprint human face image recognition method.
   ADVANTAGE - The method effectively increasing convergence speed, improving generalization ability and image recovery performance, increasing identification accuracy rate of face image of a fingerprint people.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multiple-task convolution nerve network based fingerprint human face image recognition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a multiple-task convolution nerve network based fingerprint human face image recognition device. '(Drawing includes non-English language text)'
PD CN105760859-A   13 Jul 2016   G06K-009/00   201651   Pages: 10   Chinese
   CN105760859-B   21 Dec 2018   G06K-009/00   201903      Chinese
UT DIIDW:201645049C
ER

PT P
PN CN105096241-A
TI Human face image landscaping device, has identification module for identifying human face characteristic parameter of human face image, and landscaping module for performing landscaping process of human face image based on landscaping data.
AB    NOVELTY - The device has an identification module utilized for identifying human face characteristic parameter of a human face image. An obtaining module is utilized for obtaining landscaping data i.e. XML data, according to the human face characteristic parameter. A landscaping module is utilized for performing landscaping process of the human face image based on the landscaping data. The human face characteristic parameter is provided with facial characteristic parameter and human face outline characteristic parameter, which is provided with human face outline size and position parameter.
   USE - Human face image landscaping device.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face image landscaping method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face image landscaping method. '(Drawing includes non-English language text)'
PD CN105096241-A   25 Nov 2015   G06T-003/00   201601   Pages: 20   English
UT DIIDW:2015806355
ER

PT P
PN CN104880835-A; WO2016180285-A1; EP3296802-A1; EP3296802-A4; US2018120594-A1; JP2018525651-W
TI Intelligent glasses, have identify device connected to detection device for recognition and obtaining identity information and language information of face image and foreign language image, and input device connected to detection device.
AB    NOVELTY - The glasses have an image obtaining device fixed on a glasses body. The image obtaining device is used for obtaining an environment image. The image obtaining device is connected to a detection device. The detection device is used for detecting and extracting the image of a face image and a foreign language image. An identify device is connected to the detection device for recognition and obtaining identity information and language information of the face image and the foreign language image. An information input device is connected to the detection device.
   USE - Intelligent glasses.
   ADVANTAGE - The glasses identify and translate stranger and foreign language in a better manner.
   DETAILED DESCRIPTION - The image obtaining device is provided with a communication module. The communication module is based on Bluetooth protocol, WIFI protocol, 3G protocol or 4G protocol.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of intelligent glasses.
PD CN104880835-A   02 Sep 2015   G02C-011/00   201572   Pages: 16   Chinese
   WO2016180285-A1   17 Nov 2016   G02C-011/00   201676      Chinese
   EP3296802-A1   21 Mar 2018   G02C-011/00   201820      English
   EP3296802-A4   25 Apr 2018   G02C-011/00   201829      English
   US2018120594-A1   03 May 2018   G02C-011/00   201830      English
   JP2018525651-W   06 Sep 2018   G02C-011/00   201860   Pages: 18   Japanese
UT DIIDW:201564735A
ER

PT P
PN CN204463162-U
TI Intelligent human face recognition system, has data processing module connected with door lock control module and alarm module, and power source module for preventing turning-on of door lock.
AB    NOVELTY - The utility model is new type relates to one kind intelligent human face recognition system, including data processing module, data processing module and connected with the human face image collecting device, used for storing human face image and storing module, wherein also comprises data processing module and connected with the door lock control module, alarm module, key module, switch circuit, light emitting and light sensing element, above switch circuit connected with electric source module, power supply and infrared probe for future use, storing module orderly connected with communication module and server. The safety and stability of the whole intelligent human face identification intelligent recognition system high performance, low power consumption, collecting to the human face image is clear, and using human face identification system and intelligent communication service management phase of combined method, increase door lock applied, using suitable personnel management.
PD CN204463162-U   08 Jul 2015   G06K-009/00   201562   Pages: 6   Chinese
UT DIIDW:2015533681
ER

PT P
PN CN104731964-A
TI Human face identification based video abstracting method, involves generating original video and different human face image list, detecting human face by scanning original video, and extracting human face characteristic from input data.
AB    NOVELTY - The method involves generating an original video and a different human face image list. A human face is detected by scanning the original video. Human face characteristic is extracted from input data and stored in a human face area. The human face characteristic is clustered for obtaining a human face image. The human face characteristic is detected from a video frame. A human face abstract image is generated corresponding to an input video. An eye angle and a nozzle angle are obtained from a characteristic point of the human face.
   USE - Human face identification based video abstracting method.
   ADVANTAGE - The method enables improving video abstracting process efficiency and human face identity recognition precision.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification based video abstracting device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face identification based video abstracting device. '(Drawing includes non-English language text)'
PD CN104731964-A   24 Jun 2015   G06F-017/30   201564   Pages: 11   Chinese
UT DIIDW:201551382Q
ER

PT P
PN WO2015074476-A1; CN104657974-A
TI Method for processing human face image, involves acquiring source human face image, computing feature point set of source human face image, computing cutout area image in source human face image, and computing target human face image.
AB    NOVELTY - The method involves acquiring a source human face image (S101). A feature point set of the face image is computed. A cutout area image comprising the set of the face image is computed in the face image. A target human face image is computed in a target image (S102). A feature point set of the target human face image is computed. A paste area image which comprises the feature point set of the target human face image is computed in the target human face image. The cutout area image is adjusted (103) according to an image parameter of the feature point set of the target human face image.
   USE - Method for processing a human face image. Uses include but are not limited to flat computer, mobile phone, electronic reader, remote controller, personal computer, notebook computer, vehicle equipment, network TV, wearing equipment and intelligent network device.
   ADVANTAGE - The method enables processing the human face image, thus replacing digging image area image and maintaining vision consistency in an effective manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image processing device
   (2) a storage medium comprising a set of instructions for processing a human face image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for processing image.'(Drawing includes non-English language text)'
   Step for acquiring a source human face image (S101)
   Step for computing target human face image in target image (S102)
   Step for adjusting cutout area image (103)
   Step for replacing paste area image (104)
PD WO2015074476-A1   28 May 2015   G06T-007/00   201538   Pages: 27   Chinese
   CN104657974-A   27 May 2015   G06T-007/00   201553      Chinese
UT DIIDW:201531653W
ER

PT P
PN CN104301669-A
TI Suspicious target synergistic double-camera detection tracking and recognition method, involves collecting target face characteristic human face identification process by close-up tracking camera, and sending abnormal alarm to database.
AB    NOVELTY - The method involves collecting a video image signal using panoramic monitor camera. A movement target block basic parameter is detected and extracted. Kalman filter specific moving target estimating block motion track is utilized. A panoramic monitor camera space coordinate system, close-up monitoring camera space coordinate system and a world coordinate system between conversion and correction are utilized. A target face characteristic human face identification process is collected by a close-up tracking camera. An abnormal alarm is sent to a database.
   USE - Suspicious target synergistic double-camera detection tracking and recognition method.
   ADVANTAGE - The method enables utilizing double-camera and tracking monitoring policy, assisting working staff in a specific target, utilizing tracking algorithm in good real time character and performing the double-camera detection tracking and recognition in simple, rapid and reliable manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a suspicious target synergistic double-camera detection tracking and recognition method. '(Drawing includes non-English language text)'
PD CN104301669-A   21 Jan 2015   H04N-007/18   201520   Pages: 13   Chinese
UT DIIDW:2015154932
ER

PT P
PN CN103914683-A
TI Human face image based gender identification method, involves performing face image preprocessing process, determining pixel point of gray scale value, carrying out face image cutting process, and determining position coordinate point.
AB    NOVELTY - The method involves performing face image preprocessing process. Multi-scale LBP characteristic extraction process is carried out. A pixel point of a gray scale value is determined. Histogram area size is measured. The gray scale value is obtained. Different ratios of three-primary color are determined. Human face image rotating process is performed. Face image cutting process is carried out. A position coordinate point is determined. Gender identification process of a human face image is carried out.
   USE - Human face image based gender identification method.
   ADVANTAGE - The method enables increasing accuracy of human face image based gender identification.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face image based gender identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face image based gender identification method.'(Drawing includes non-English language text)'
PD CN103914683-A   09 Jul 2014   G06K-009/00   201465   Pages: 19   Chinese
UT DIIDW:2014R98639
ER

PT P
PN WO2014068567-A1; IL238574-A; US2015242707-A1; EP2915101-A1; EP2915101-A4; US10019653-B2; US2019005359-A1
TI Method for predicting personality characteristic from face image of person, has image-based classifier that is applied to image of subject person for outputting prediction of human personality characteristic of subject person.
AB    NOVELTY - The method involves collecting (110) training images associated with metadata characteristics of multiple persons for training purpose. The collected training images are grouped into training groups according to the associated metadata, either according to same metadata or similar metadata. One image-based classifier is trained to predict at least one characteristics of human personality from image of second person. One image-based classifier is applied to image of subject person for outputting prediction of human personality characteristic of subject person.
   USE - Method for predicting personality characteristic from one image such as face image of person, used for facilitating personal advertising (claimed), smart customer relationships management (CRM), negotiation tool , human resource (HR) recruitment, teaching aid, criminology analysis, personal gaming, meeting intelligence, and career guidance.
   ADVANTAGE - The personality traits, capabilities and suggested interactions of the subject person can be predicted efficiently and automatically from face image, thus the personal advertising can be facilitated efficiently by providing adaptive message to subject person according to the classifications.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a computer-readable medium storing program for predicting personality characteristic from one image of person.
   DESCRIPTION OF DRAWING(S) - The drawing shows block diagram illustrating the system for extracting personality traits from face images.
   Server (100)
   Images source (101)
   Face gathering module (110)
   Face normalization module (130)
   Face region segmentation module (140)
PD WO2014068567-A1   08 May 2014   G06K-009/46   201433   Pages: 72   English
   IL238574-A   30 Jun 2015   G06K-009/46   201554      English
   US2015242707-A1   27 Aug 2015   G06K-009/62   201557      English
   EP2915101-A1   09 Sep 2015   G06K-009/46   201559      English
   EP2915101-A4   11 Jan 2017   G06K-009/46   201706      English
   US10019653-B2   10 Jul 2018   G06K-009/00   201846      English
   US2019005359-A1   03 Jan 2019   G06K-009/62   201903      English
UT DIIDW:2014H95061
ER

PT P
PN US8526677-B1
TI Computer-implemented method for detecting guide dog and location by using e.g. mobile phone, involves storing voice recording annotation at memory of mobile device, and generating output based on current location of mobile device.
AB    NOVELTY - The method (900) involves determining distance to an object at a mobile device. A hap-tic feedback is generated at the mobile device. A map of a surrounding environment is compared with a location of the object in the environment at the mobile device. A current location of the mobile device is determined on the map of the environment based on comparing. Voice recording annotation is received from a user. The voice recording annotation is stored at a memory of the mobile device. An output based on the current location of the mobile device is generated at the mobile device.
   USE - Computer-implemented method for detecting an object e.g. person and service animal such as guide dog, and a location by using a mobile device (claimed) e.g. mobile phone, tablet computer and personal digital assistance, using image analysis techniques e.g. object recognition algorithms, edge and contour recognition algorithms and face recognition algorithms, to analyze 3-dimensional (3D) image data e.g. store front signs and street signs, of a surrounding environment.
   ADVANTAGE - The method enables increasing computing capability. The method enables proving convenient at times to refer to arrangements of operations as modules or by functional names without loss of generality.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a mobile device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a computer-implemented method for detecting an object and a location.
   Computer-implemented method for detecting object and location (900)
   Step for obtaining 3D image data of environment from stereoscopic camera (904)
   Step for determining locations and shapes of objects in environment from 3D image data (906)
   Step for estimating distance to object in view (908)
   Step for comparing distance with threshold (910)
PD US8526677-B1   03 Sep 2013   G06K-009/00   201360   Pages: 23   English
UT DIIDW:2013M97534
ER

PT P
PN CN103106393-A; CN103106393-B
TI Embedded system based human face identifying intelligent identity authentication system for restricting work area of robot platform, has detection platform for obtaining unique features of human face according to intelligent identification.
AB    NOVELTY - The system has a detection platform for obtaining unique features of a human face according to intelligent identification based on person's identity authentication technology. A human face register is utilized for collecting a face image, pre-processing the human face and extracting a characteristic point. A network video coder is utilized for converting an analog video into a digital video. A camera is provided with an image capture platform image pickup device, an active light source, a Y-direction rotating table, a rotating platform, a base and a human face capturing camera.
   USE - Embedded system based human face identifying intelligent identity authentication system for a restricting work area of a robot platform.
   ADVANTAGE - The system can identify the human face and face characteristics in a better manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an embedded system based human face identifying intelligent identity authentication system.'(Drawing includes non-English language text)'
PD CN103106393-A   15 May 2013   G06K-009/00   201366   Pages: 17   Chinese
   CN103106393-B   17 Aug 2016   G06K-009/00   201660      Chinese
UT DIIDW:2013Q75392
ER

PT P
PN CN102799870-A; CN102799870-B
TI Single training sample face identifying method, involves calculating one pixel radius for each area adjacent to same LBP, and connecting LBP histogram of sixteen sub-areas to column vector.
AB    NOVELTY - The method involves blocking a histogram of a human face image according to a certain format. The human face image is divided into 4 equal parts. A one pixel radius for each area adjacent to the same LBP is calculated. A LBP histogram of 16 sub-areas is connected to a column vector. A test image is put according to imaging of a training set. A phase ratio of traditional algorithm of feature extraction and classification is obtained.
   USE - Single training sample face identifying method.
   ADVANTAGE - The method enables extracting structure information of human face, and providing high recognition rate and robustness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a single training sample face identifying method. '(Drawing includes non-English language text)'
PD CN102799870-A   28 Nov 2012   G06K-009/00   201320   Pages: 11   Chinese
   CN102799870-B   29 Jul 2015   G06K-009/00   201565      Chinese
UT DIIDW:2013D21326
ER

PT P
PN US2012242819-A1; WO2012135018-A2; WO2012135018-A3; EP2688764-A2; CN103442925-A; JP2014515847-W; US9041789-B2; CN103442925-B; EP2688764-A4
TI Driver alertness detection system for use with vehicle, has warning unit for determining whether driver is in alert state or non-alert state to output warning signal to driver while determining that driver is in non-alert state.
AB    NOVELTY - The system has an imaging unit for imaging an area in a vehicle compartment in which a driver's head is located. An image processing unit receives the image from the imaging unit to determine positions of the driver's head and eyes. A warning unit determines whether the driver is in an alert state or a non-alert state to output a warning signal to the driver while determining that the driver is in the non-alert state, based on determined position of the driver's head and eyes as output by the processing unit. A disabling unit temporarily disables the system when an alert signal is received.
   USE - Driver alertness detection system for use with a vehicle.
   ADVANTAGE - The system uses a principal axis transform to align the driver's silhouette and facial control points to a camera's coordinate grid and normalize facial geometry for simplifying the search for related facial features, thus warning the driver when the driver is detected to be in the non-alert state.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for detecting driver alertness
   (2) a non-transitory computer readable medium comprising a set of instructions for performing a method for detecting driver alertness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a driver alertness system defining camera placement and coverage.
PD US2012242819-A1   27 Sep 2012   G08B-021/06   201266   Pages: 15   English
   WO2012135018-A2   04 Oct 2012   B60K-028/06   201266      English
   EP2688764-A2   29 Jan 2014   B60K-028/06   201409      English
   CN103442925-A   11 Dec 2013   B60K-028/06   201411      Chinese
   JP2014515847-W   03 Jul 2014   G08G-001/16   201443   Pages: 17   Japanese
   US9041789-B2   26 May 2015   H04N-009/47   201535      English
   CN103442925-B   17 Aug 2016   B60K-028/06   201660      Chinese
   EP2688764-A4   12 Nov 2014   B60K-028/06   201770      English
UT DIIDW:2012M54303
ER

PT P
PN EP2447883-A1; US2012105614-A1; KR2012003165-U; CN102467660-A; US8259168-B2; KR462271-Y1; CN102467660-B; EP2447883-B1; EP2447883-B8
TI Optical system for recognizing fingerprint light ray pattern of finger, has micro-structures guiding ray uniformly distributed in board such that ray is reflected to element for capturing ray on finger to recognize pattern of finger.
AB    NOVELTY - The system has an image capturing element (20) separated from a finger board (10) corresponding to a plate face (12) and a capturing fingerprint light ray pattern of a finger (C). A light emitting element (30) is separated from the board for distance corresponding to the plate face. The light emitting element emits incident light ray toward the board. Micro-structures (13) guide the ray uniformly distributed in the board such that the ray is reflected to the capturing element by the finger. The capturing element captures the ray applied on the finger to recognize the pattern of the finger.
   USE - Optical system for recognizing a fingerprint light ray pattern of a finger.
   ADVANTAGE - The ray is uniformly distributed on the board to make the ray irradiated on the finger uniformly and intensively, thus improving fingerprint recognition rate in an effective manner, and hence simplifying assembling process, reducing cost and improving installation position and the light emitting element in an effective manner. The ray is converted into a surface light source by a light diffusion plate, so that the surface light source uniformly overlaps an entire area of the board, thus ensuring the surface light source to cover the surface of the finger board in an effective manner. The ray is emitted out along a side wall of a light guide element, thus forming uniform light field in a cavity.
   DETAILED DESCRIPTION - The capturing element is a charge coupled device (CCD) and a complementary metal oxide semiconductor (CMOS) element.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an optical fingerprint recognition system.
   Finger (C)
   Finger board (10)
   Plate faces (11, 12)
   Micro-structures (13)
   Image capturing element (20)
   Light emitting element (30)
TF TECHNOLOGY FOCUS - POLYMERS - The board is made of thermoplastic polyurethane (TPU), polycarbonate (PC), silicone plastic (SP), polymethyl methacrylate (PMMA) and polyethylene terephthalate (PET).
PD EP2447883-A1   02 May 2012   G06K-009/00   201231   Pages: 12   English
   US2012105614-A1   03 May 2012   H04N-007/18   201231      English
   KR2012003165-U   08 May 2012   G06K-009/20   201233      
   CN102467660-A   23 May 2012   G06K-009/00   201239      Chinese
   US8259168-B2   04 Sep 2012   H04N-009/47   201259      English
   KR462271-Y1   03 Sep 2012   G06K-009/20   201260      
   CN102467660-B   06 Nov 2013   G06K-009/00   201404      Chinese
   EP2447883-B1   19 Jul 2017   G06K-009/00   201749      English
   EP2447883-B8   20 Sep 2017   G06K-009/00   201763      English
UT DIIDW:2012F04436
ER

PT P
PN CN102164113-A
TI Human face identification log-in method, involves picking up human face at log-in terminal through camera, sending picked up human face image to server, and comparing human face image with human face image stored in server database.
AB    NOVELTY - The method involves picking up a human face at a log-in terminal through a camera. The picked up human face image is sent to a server. The human face image is compared with the human face image stored in a server database, where a log-in is successful if the human face images are the same, and the log- in is not successful if the human face images are different. A user name and password of the user are input. A user name and password data information are resent to the server. Data information is compared with the data information stored in the server database.
   USE - Human face identification log-in method.
   ADVANTAGE - The method enables avoiding password from leaking to affect security of the individual account number. The method ensures that a user logs in only by the human face in a convenient and practical manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification log-in system, comprising a target server.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a process for identifying a human face log-in. '(Drawing includes non-English language text)'
PD CN102164113-A   24 Aug 2011   H04L-029/06   201164   Pages: 8   Chinese
UT DIIDW:2011L78608
ER

PT P
PN CN102103690-A
TI Automatic splitting hair area method for computer vision field, involves utilizing image splitting module for splitting hair area and outputting hair area according to target background mark module.
AB    NOVELTY - The method involves utilizing a face detection module to detect a face position from an input face picture by a well trained cascade classifier. A target background marking module is utilized for line out of an interested area. A possible target mark and a background mark are found out according to position and color characteristic. The hair area is splitted and output by an image splitting module according to the target background mark module. The optimized weak classifier is selected through an adaBoost algorithm to iterate.
   USE - Method for automatically splitting hair area in a computer vision field.
   ADVANTAGE - The method enables utilizing an image processing technique to perform face detection, target background mark, image splitting on an input image, outputting an area of hair in the image and accurately detecting the hair area under complicated background condition, thus performing identity recognition, gender and age estimation and image retrieval in a better manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of an automatic splitting method for hair area in a computer vision field. '(Drawing includes non-English language text)'
PD CN102103690-A   22 Jun 2011   G06K-009/00   201152   Pages: 13   Chinese
UT DIIDW:2011J62787
ER

PT P
PN US7929733-B1
TI Method for identifying unknown individual from several enrolled individuals, establishing link between enrolled individuals based upon enrolled biometric parameters.
AB    NOVELTY - The method involves comparing biometric parameters of the unknown individual to enrolled biometric parameters of enrolled individuals to determine a score. A list including enrolled individuals determined by score is displayed. A link between enrolled individuals is established based upon the enrolled biometric parameters. The link is determined based upon face biometrics of enrolled individuals. The link is established as a recognized affiliation.
   USE - Method for identifying unknown individual from several enrolled individuals.
   ADVANTAGE - The identification probability is improved so that ordered list is provided. The information necessary for identification process can be collected without causing inconvenience to the individual.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for system for identifying unknown individual from several enrolled individuals.
   DESCRIPTION OF DRAWING(S) - The drawing shows a system diagram for low-level fusion system.
   Fusion system (570)
   Voice inputs (572)
   Face inputs (574)
   Bio-demographic inputs (576)
   Low-level fusion layer (580)
PD US7929733-B1   19 Apr 2011   G06K-009/00   201129   Pages: 14   English
UT DIIDW:2011E11679
ER

PT P
PN US2010266213-A1; US8600100-B2
TI Individual assessment method through facial muscle activity and expressions involves analyzing at least one of action unit, combination of action units or at least one emotion to assess one or more characteristics of individual.
AB    NOVELTY - The individual assessment method involves coding contemporaneously detected and recorded expressional repositionings to at least one of action unit, combination of action units, or at least one emotion. At least one of the action unit, combination of action units, or at least one emotion is/are analyzed to assess one or more characteristics of the individual to develop a profile of the individual's personality in relation to the objective for which the individual is being assessed.
   USE - Individual assessment method through facial muscle activity and expressions.
   ADVANTAGE - Develop a profile of the individual's personality in relation to the objective for which the individual is being assessed. Evaluates people's personality type, behavioral tendencies, credibility, motivations and other such insights. Enables use of non-verbal language to gain a better understanding of people's personality type, behavioral tendencies, credibility, motivations and other insights related to applications including but not limited to personnel hiring, career development, training, Internet dating, and the analysis of people involved in law suits as witnesses or in actual or mock/shadow juries.
   DESCRIPTION OF DRAWING(S) - The drawing illustrates various facial muscles useful to detect emotions.
PD US2010266213-A1   21 Oct 2010   G06K-009/68   201074   Pages: 32   English
   US8600100-B2   03 Dec 2013   G06K-009/00   201380      English
UT DIIDW:2010N28914
ER

PT P
PN US2010205667-A1; WO2010101697-A2; WO2010101697-A3; AU2010221722-A1; EP2394235-A2; US10282563-B2
TI System for regulating interaction between user and computer, has processor which controls operation of data input/output device in response to facial recognition data and security parameters associated with user.
AB    NOVELTY - An input-output device has an image sensor (1400) which collects the facial recognition data proximate to computer (1050) and communicates the data to security processor. The user security parameter database including data encoding security parameters is made to communicate with security processor. The security processor receives facial recognition data and security parameters associated with user (1300), and control the operation of data input/output device in response to facial recognition data and security parameters associated with user.
   USE - System for regulating interaction between user and computers such as desktop computer, laptop computer, notebook computer, tablet computer. Can also be used in regulating interaction between user and electronic devices such as personal digital assistant (PDA), smart phone, picture and video players, music players such as MPEG audio layer-3 player, etc.
   ADVANTAGE - Provides robust solution for privacy and other problems. The data on the computer-controlled display can be viewed with greater security even in unsecured environments. The security processor displays the warning to user or modifies the operation of video display device of microphone connected to computer, in response of detecting the presence of third party.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) method of regulating interaction between computer and user of computer; and
   (2) computer-readable medium storing program for regulating interaction between computer and user of computer.
   DESCRIPTION OF DRAWING(S) - The drawing shows an explanatory view of the components for providing content privacy for single authorized user with image sensor.
   Computer (1050)
   Keyboard (1100)
   Computer-driven display system (1200)
   User (1300)
   Image sensor (1400)
PD US2010205667-A1   12 Aug 2010   G06F-021/00   201055   Pages: 38   English
   WO2010101697-A2   10 Sep 2010   G06F-021/00   201060      English
   WO2010101697-A3   28 Oct 2010   G06K-009/00   201071      English
   AU2010221722-A1   18 Aug 2011   G06K-009/00   201160      English
   EP2394235-A2   14 Dec 2011   G06K-009/00   201181      English
   US10282563-B2   07 May 2019   G06F-003/01   201934      English
UT DIIDW:2010K30052
ER

PT P
PN US2010054550-A1; EP2161678-A2; JP2010061465-A; CN101667248-A; JP4720880-B2; US8311293-B2; CN101667248-B; EP2161678-A3
TI Image processing apparatus e.g. cellular phone, has similarity calculating unit selecting feature quantity relating to subjects stored in subject information storage unit based on determined attributes.
AB    NOVELTY - The apparatus has a subject detecting unit (310) detecting a subject included in an image, and an attribute determining unit (330) determining attributes of the detected subject. A feature quantity extracting unit (350) extracts a feature quantity relating to the detected subject. A similarity calculating unit (360) selects the feature quantity relating to subjects stored in a subject information storage unit based on the determined attributes to calculate similarity between a subject according to the selected feature quantity and the detected subject.
   USE - Image processing apparatus e.g. cellular phone and personal computer.
   ADVANTAGE - The similarity calculating unit selects the feature quantity relating to the subjects stored in the subject information storage unit based on the determined attributes to calculate similarity between the subject according to the selected feature quantity and the detected subject, so that similarity of same person is prevented from being lost according to the differences of the face attributes.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for processing an image
   (2) a program comprising a set of instructions for performing a method for processing an image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an individual identifying unit.
   Subject detecting unit (310)
   Attribute determining unit (330)
   Feature point calculating unit (340)
   Feature quantity extracting unit (350)
   Similarity calculating unit (360)
PD US2010054550-A1   04 Mar 2010   G06K-009/46   201018   Pages: 61   English
   EP2161678-A2   10 Mar 2010   G06K-009/00   201018      English
   JP2010061465-A   18 Mar 2010   G06T-001/00   201020   Pages: 53   Japanese
   CN101667248-A   10 Mar 2010   G06K-009/00   201021      Chinese
   JP4720880-B2   13 Jul 2011   G06T-001/00   201145   Pages: 52   Japanese
   US8311293-B2   13 Nov 2012   G06K-009/00   201275      English
   CN101667248-B   27 Mar 2013   G06K-009/00   201340      Chinese
   EP2161678-A3   14 Aug 2013   G06K-009/00   201354      English
UT DIIDW:2010C36726
ER

PT P
PN WO2009134482-A2; WO2009134482-A3; US2011064302-A1; CN101965588-A; US8406525-B2
TI Method for recognition of high-dimensional data e.g. image of human face in presence of occlusion, involves identifying class of target data through linear superposition of sampled training data files by C1 minimization method.
AB    NOVELTY - The target data of test image (154) of an unknown class provided with occlusion and a known object is received and sampled with training image data files (150) comprising distinct classes of the same object as that of the target data. The class of the target data is identified through linear superposition of the sampled training data files by C1 minimization method with sparsest number of coefficients to identify the class of target data.
   USE - Method for recognition of high-dimensional data e.g. image of human face in presence of occlusion.
   ADVANTAGE - The system for recognition of human face is relatively simple in implementation and hence the results are relatively easy to reproduce. The proposed algorithm is scalable both in terms of computational complexity and recognition performance. The system is directly compatible with off-the-shelf face detectors and achieves stable performance under a wide range of variations in illumination, misalignment, pose, and occlusion. Very good recognition performance is achieved on large-scale tests with public datasets and the practical face images as acquired by the image acquisition system using only frontal two-dimensional images in the training without any explicit three-dimensional face model.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) system for recognition of high dimensional data in presence of occlusion;
   (2) system for acquiring set of training images of subject;
   (3) method for acquiring set of training images of subject;
   (4) method for alignment of subject; and
   (5) system for alignment of subject.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of the system for capture, alignment and recognition of object in partially occluded test image.
   Server (110)
   Memory (114)
   Processor (118)
   Training image data files (150)
   Test image (154)
PD WO2009134482-A2   05 Nov 2009   G06T-007/00   200975   Pages: 106   English
   WO2009134482-A3   14 Jan 2010   G06T-007/00   201005      English
   US2011064302-A1   17 Mar 2011   G06K-009/62   201120      English
   CN101965588-A   02 Feb 2011   G06T-007/00   201123      Chinese
   US8406525-B2   26 Mar 2013   G06K-009/66   201322      English
UT DIIDW:2009Q83352
ER

PT P
PN US7508960-B1
TI Biometric measurements or comparisons making system, has pattern comparison device rejecting identification, if pattern is positioned upon object that does not match reference pattern.
AB    NOVELTY - The system (100) has a device for providing a pattern (101), where the device comprises a light source (102) e.g. LED. An imaging device (106) is positioned at different locations from the light source. A pattern comparison device (112) compares details of the pattern positioned upon an object (108) e.g. human face, and details of a reference pattern. A biometric comparison device (114) compares an image of the object, where the pattern comparison device rejects the identification, if the pattern is positioned upon the object that does not match the reference pattern.
   USE - System for making biometric measurements or comparisons.
   ADVANTAGE - The system analyzes the shape of the image on the object to easily determine whether the object is a true human object or a replica that has a different shape of the image, and avoids the use of expensive computational operations. The system efficiently makes the statistical measurements of the object to make the appropriate comparison of the authenticity of the object.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic diagram of a system for making biometric measurements or comparisons.
   Biometric measurements or comparisons making system (100)
   Pattern (101)
   Light source (102)
   Imaging device (106)
   Object (108)
   Pattern comparison device (112)
   Biometric comparison device (114)
PD US7508960-B1   24 Mar 2009   G06K-009/00   200923   Pages: 7   English
UT DIIDW:2009G29563
ER

PT P
PN US2008170078-A1; US8130225-B2
TI Object e.g. actor, identifying method for motion capture system, involves comparing surface feature to motion model, identifying representation of object from model, and adjusting animation mesh to incorporate representation.
AB    NOVELTY - The method involves comparing a surface feature to a motion model, where the surface feature represents a portion of an object e.g. actor, in an image, and identifying a representation of the object from the motion model based upon the comparison. An animation mesh (400) is adjusted to incorporate the representation of the object, where the surface feature is compared to the motion model by processing decomposed data of the motion model to substantially match the surface feature, and the surface feature represents a natural feature on a surface of the object.
   USE - Computer-implemented method for identifying an object e.g. actor, in a motion capture system (claimed).
   ADVANTAGE - The method enables to recognize the actor's facial expression and other type of performance mannerism, characteristic or motion from a reduced set of identifying surface features.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a motion capture system comprising a data comparer
   (2) a computer program product for identifying an object
   (3) an expression identification system comprising a device to capture an image of an object.
   DESCRIPTION OF DRAWING(S) - The drawing shows a diagram that illustrates transferring of the motion information of motion meshes to an animation mesh using shape meshes.
   Animation mesh (400)
   Motion meshes (500a-500d)
   Shape meshes (502a-502d)
PD US2008170078-A1   17 Jul 2008   G06K-009/00   200850   Pages: 20   English
   US8130225-B2   06 Mar 2012   G06T-015/70   201217      English
UT DIIDW:2008H91229
ER

PT P
PN JP2006011591-A
TI Personal identification system for building, authenticates user, only when combination of fingerprint data, face image, air-conduction sound or bone-conduction sound of user are acquired simultaneously as characteristic information.
AB    NOVELTY - The system authenticates the user holding a mobile telephone (1), only when combination of any two of fingerprint information from a fingerprint sensor (342), face image from a camera (341), air-conduction sound from a transmitter (304) or bone-conduction sound from a microphone (340), of the user are acquired simultaneously as characteristic information.
   USE - For authenticating user for entry in building or residence, unlocking of motor vehicle, and for banking and shopping applications, using mobile or video telephone.
   ADVANTAGE - Authenticates the user reliably, thereby improving the security level.
   DESCRIPTION OF DRAWING(S) - The figure shows a perspective view of the mobile telephone.
   mobile telephone (1)
   transmitter (304)
   microphone (340)
   camera (341)
   fingerprint sensor (342)
PD JP2006011591-A   12 Jan 2006   G06T-007/00   200611   Pages: 29   Japanese
UT DIIDW:2006104105
ER

PT P
PN WO2005059831-A1; EP1695287-A1; CN1890689-A; JP2007516744-W; IN200602522-P4; US2007179377-A1; CN100538739-C; EP1695287-B1; DE602004031551-E; JP4787171-B2; IN253824-B; US8326086-B2
TI Image registration method in medical application, involves determining similarity value to determine similarity between respective regions in images defined by corresponding landmarks in floating and reference images.
AB    NOVELTY - The method involves selecting corresponding landmarks in floating and reference images and determining a similarity value to describe a similarity between respective regions in the images defined by the landmarks.
   USE - For storing images e.g. floating and reference images of object e.g. brain, face, abdomen and heart for anatomical and physiological information analysis in medical application such as tumor diagnostics and surgery. Also applicable for non-medical application such as material testing or quality control and image recognition systems.
   ADVANTAGE - Improves speed and robustness of image registration. Provides very accurate and reliable registration of image data set.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) image processing device; and
   (2) computer program for registering images.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart illustrating image processing method.
PD WO2005059831-A1   30 Jun 2005   G06T-003/00   200551   Pages: 22   English
   EP1695287-A1   30 Aug 2006   G06T-003/00   200657      English
   CN1890689-A   03 Jan 2007   G06T-003/00   200740      Chinese
   JP2007516744-W   28 Jun 2007   A61B-006/03   200744   Pages: 14   Japanese
   IN200602522-P4   08 Jun 2007   G06T-003/00   200748      English
   US2007179377-A1   02 Aug 2007   A61B-005/05   200753      English
   CN100538739-C   09 Sep 2009   G06T-003/00   200965      Chinese
   EP1695287-B1   23 Feb 2011   G06T-003/00   201115      English
   DE602004031551-E   07 Apr 2011   G06T-003/00   201125      German
   JP4787171-B2   05 Oct 2011   A61B-006/03   201165   Pages: 12   Japanese
   IN253824-B   31 Aug 2012   G06T-003/00   201260      English
   US8326086-B2   04 Dec 2012   G06K-009/32   201279      English
UT DIIDW:2005506352
ER

PT P
PN US2001019620-A1; JP2001243466-A; US7155037-B2; JP4526639-B2
TI Human face recognition apparatus for human type robot, controls driver to change position and angle of input unit, based on the detected orientation of face in the receiving image.
AB    NOVELTY - A driver (20) changes the position and angle of input unit (11) which receives image data of human face. A detector (15) detects the orientation of the face in image data, based on which controller (16) controls the driver.
   USE - For recognizing human face in control of human-type robot, pet-type robot or self controlled robot. Also for vehicle driver recognition.
   ADVANTAGE - As the images of human face at different angles are obtained without need for movement of person, accurate face recognition is ensured.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) Human face recognition method;
   (b) Robot, driver recognition apparatus
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of human face recognition apparatus.
   Input unit (11)
   Detector (15)
   Controller (16)
   Driver (20)
PD US2001019620-A1   06 Sep 2001   G06K-009/00   200227   Pages: 18   English
   JP2001243466-A   07 Sep 2001   G06T-007/00   200227   Pages: 11   Japanese
   US7155037-B2   26 Dec 2006   G06K-009/00   200702      English
   JP4526639-B2   18 Aug 2010   G06T-007/00   201054   Pages: 15   Japanese
UT DIIDW:2002215357
ER

PT P
PN WO200148438-A1; DE19963333-A1; EP1242786-A1; US2003002052-A1; JP2003518614-W; JP3571325-B2; US6813035-B2; EP1242786-B1; DE50011040-G
TI Procedure to determine three dimensional surface co-ordinates - forms two dimensional colour pattern of pattern elements and performs triangulation to determine object point.
AB       Object (3) is illuminated with colour pattern (9) of known structure by a projector (1) and object image is taken with camera (2). Colour pattern consists of pattern element of different colours or different spectral composition of projected light. Colour pattern (9) is projected onto object once to take image. Colour image (9) has at least two neighbouring lines (4) with number of pattern elements (10). Each pattern element is different in colour or in spectral composition of projected light, from a neighbouring pattern element (11) inside the lines and is different from a neighbouring pattern element in a neighbouring line. The position of the pattern element in the line (4) in the colour pattern (9) of the image can be exactly determined.
   A projection angle is assigned to the pattern element, used to determine three-dimensional co-ordinates of object points using known relative position between camera and projector.
   USE -   For detection of face of person, face recognition, recognition of gestures.
   ADVANTAGE -   Compact and interference free colour pattern for coding, only needs projection of single pattern, coding interference free as restricted to immediate neighbouring relations by comparing in horizontal and vertical direction.
PD WO200148438-A1   05 Jul 2001   G01B-011/25   200144   Pages: 26   German
   DE19963333-A1   12 Jul 2001   G01B-011/25   200147      German
   EP1242786-A1   25 Sep 2002   G01B-011/25   200271      German
   US2003002052-A1   02 Jan 2003   G01B-011/25   200305      English
   JP2003518614-W   10 Jun 2003   G01B-011/00   200339   Pages: 19   Japanese
   JP3571325-B2   29 Sep 2004   G01B-011/25   200465   Pages: 9   Japanese
   US6813035-B2   02 Nov 2004   G01B-011/24   200472      English
   EP1242786-B1   24 Aug 2005   G01B-011/25   200556      German
   DE50011040-G   29 Sep 2005   G01B-011/25   200564      German
UT DIIDW:2001418324
ER

PT P
PN JP2001155137-A; JP3738629-B2
TI Portable electronic device such as portable telephone, detects fingerprint by moving finger along sequence direction of sensors provided below detection faces for generating fingerprint image.
AB    NOVELTY - Sensors (2) with sensing elements are provided below respective detection faces on which a finger (1) is placed. By moving a finger along sequence direction of sensor and combining the information from the sensors, the fingerprint is detected. A processing unit generates fingerprint image from detected fingerprint. A comparison unit compares fingerprint image generated, with a preset fingerprint image.
   USE - Portable electronic device such as portable telephone with individual identification function.
   ADVANTAGE - The sensor unit is mounted to the backside of the portable electronic device and it occupies less space. Hence simplified structure of the portable device, is secured.
   DESCRIPTION OF DRAWING(S) - The figure shows the fingerprint detection process using sensors in portable electronic device.
   Finger (1)
   Sensors (2)
PD JP2001155137-A   08 Jun 2001   G06T-001/00   200169   Pages: 14   Japanese
   JP3738629-B2   25 Jan 2006   G06T-001/00   200608   Pages: 19   Japanese
UT DIIDW:2001605028
ER

PT P
PN DE19847261-A1; WO200021021-A1; EP1119822-A1; EP1119822-B1; DE59904236-G; US6999606-B1
TI Face finding method for biometric recognition of person.
AB    NOVELTY - The method involves comparing the point group of a binarized image with the point group of a face model. The point groups of the binarized image and the face model are compared using the Hausdorff separation between the points. A face is detected in the image if one quantity derived from the Hausdorff separation falls below a threshold. The binarized image may be produced from an original image using edge extraction. Different face models and different resolutions may be used, depending on the size of the binarized image.
   USE - For locating the face of a person in picture for person recognition.
   ADVANTAGE - Robust method which operates in real-time.
   DESCRIPTION OF DRAWING(S) - The drawing shows a binary face model in the form of an outline drawing.
PD DE19847261-A1   06 Apr 2000   G06K-009/62   200027   Pages: 5   German
   WO200021021-A1   13 Apr 2000   G06K-009/00   200027      German
   EP1119822-A1   01 Aug 2001   G06K-009/00   200144      German
   EP1119822-B1   05 Feb 2003   G06K-009/00   200318      German
   DE59904236-G   13 Mar 2003   G06K-009/00   200320      German
   US6999606-B1   14 Feb 2006   G06K-009/00   200613      English
UT DIIDW:2000304699
ER

PT P
PN US2019050866-A1; US10242364-B2
TI Mobile device for computer-implemented method of authenticating user of mobile device, comprises processor, and memory storing instructions that cause mobile device to e.g. identify the user based in portion on infrared image.
AB    NOVELTY - Mobile device comprises a processor; and memory storing instructions that, as a result of being executed by the processor, cause the mobile device to: obtain a request to authorize a transaction; illuminate, with an infrared light emitting diode, a user (102) that is interacting with the mobile device; use a camera (106) on the mobile device to capture an infrared image (120) of a portion of a face (122) of the user; analyze the infrared image to identify eyes of the user; determine that the infrared image corresponds to a living being by analyzing a thermal characteristic of the eyes of the user; identify the user based in portion on the infrared image; authenticate the user as a result of the identity of the user being recognized as an authorized user and as a result of the infrared image corresponding to a living being; and allow the transaction to be performed as a result of the user being authenticated.
   USE - The mobile device is useful for computer-implemented method of authenticating user of mobile device (claimed). Generally, it can be used for image analysis for user authentication.
   ADVANTAGE - The device can capture various other types of image information as well, such as infrared image information, thermal imaging data, or other such information which can assist in determining the presence of a human user. Two or more of these approaches can be combined to improve an accuracy of the determinations.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for:
   (1) a computer-implemented method of authenticating a user of a mobile device, which involves projecting an infrared light onto a user that is interacting with the mobile device; capturing an infrared image of a portion of the user with a camera on the mobile device; analyzing the infrared image to identify an eye of the user; determining that the infrared image corresponds to a living being by analyzing a variance in a thermal characteristic of the eye; identifying the user based in portion on the infrared image; and authenticating the user as a result of the identity of the user being recognized as an authorized user and as a result of the infrared image corresponding to a living being; and
   (2) a non-transitory computer-readable storage medium storing instructions that, as a result of being executed by a processor on a mobile computing device (104), cause the mobile computing device to: project an infrared light onto a user that is interacting with the mobile computing device; capture an infrared image of a portion of the user with a camera on the mobile computing device; analyze the infrared image to identify an eye of the user; determine that the infrared image corresponds to a living being by analyzing a variance in a thermal characteristic of the eye; identify the user based in portion on the infrared image; and authorize a request initiated by the user as a result of the user being recognized as an authorized user stored in memory on the mobile computing device and as a result of the image corresponding to a physical being.
   DESCRIPTION OF DRAWING(S) - The drawings show schematic views of situations where a computing device is configured to capture image information.
   User (102)
   Computing device (104)
   Camera (106)
   Image (120)
   Face (122)
TF TECHNOLOGY FOCUS - IMAGING AND COMMUNICATION - Preferred Device: The infrared light emitting diode is on the mobile device. The camera is capable of capturing thermal information; and the infrared image includes thermal imaging data. Preferred Parameters: The infrared image captures a reflection of infrared light emitted from the infrared light emitting diode. The instructions further cause the mobile device to: obtain identifying information for an authorized user by performing a visual authentication enrollment process; and authenticate the user as a result of the identity of the user being recognized as the authorized user associated with the identifying information. The instructions executable to further cause the mobile computing device to: as portion of a visual authentication enrolment process, project an infrared light on to a face of an authorized user; capture a reflection of the infrared light from the eye of the user using the camera; and store a characteristic of the eye. The instructions executable to further cause the mobile computing device to determine that the gaze of the user is directed at the device. The instructions executable to further cause the mobile computing device to: obtain a request to authorize a transaction; and deny the transaction as a result of determining that the image does not correspond to a physical being. Preferred Process: Facial data of the user is collected as portion of a visual authentication enrollment process; facial data of the user is obtained from the infrared image using a facial recognition algorithm; and the identity of the user is determined based in portion on the facial data. The computer-implemented method further involves directing the user to look at the camera on the mobile device. The infrared light is projected using an infrared light emitting diode under the control of the mobile device. The method also involves: (a) obtaining the infrared image from a sequence of images in a video stream; (b) obtaining a 2nd image of the user, the 2nd image representing light captured from a visible spectrum; and identifying the user based in portion on the 2nd image; (c) detecting that the user is looking at the camera; and initiating an authentication process as a result of detecting that the user is looking at the camera; (d) capturing a sequence of images of the eye of the user; detecting a blink of the eye based on the sequence of images; and determining that the image represents a living being as a result of detecting the blink; (e) presenting a signal to the user indicating that the user should look at the mobile device to perform an authentication process; (f) obtaining a request to authorize a transaction; and determining that the identified user is authorized to perform the transaction; and (g) denying the transaction as a result of determining that the image does not correspond to a physical being.
PD US2019050866-A1   14 Feb 2019   G06Q-020/40   201915   Pages: 18   English
   US10242364-B2   26 Mar 2019   G06Q-020/40   201922      English
UT DIIDW:2019149501
ER

PT P
PN US10198625-B1
TI Method for associating physically identifiable feature of person with unique identifier of mobile device, involves clustering input images into face group, and matching particular face group to device using face to mobile matching module.
AB    NOVELTY - The method involves capturing a set of input images of a face of a person (100) at physical location using a camera (104A, 104B) using a candidate face collection module, where the candidate face collection module captures the set of input images during a period of time when a mobile device (102) is detected by mobile signal sensors at the physical location. The unique identifier of the mobile device and the set of input images are stored in a database. The set of input images is clustered into a face group using a face clustering module. A particular face group is matched to a particular mobile device using a face to mobile matching module.
   USE - Method for associating physically identifiable feature of a person with a unique identifier of a mobile device. Uses include but are not limited to a desktop, a workstation, a server, a laptop, a personal computer, a personal digital assistant and a single board computer.
   ADVANTAGE - The method enables learning received signal strength (RSS)-to-distance mapping function in the RSS-to-distance conversion module without prior data if a self-calibration operation is employed. The method enables associating face data with unique mobile identifier information in order to reduce uncertainty when identifying and tracking repeat visitors. The method enables performing tracking of the mobile device and use of location information to improve performance of the data association process by associating the calculated location of the mobile device with known location of a camera.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for associating a physically identifiable feature of a person with a unique identifier of a mobile device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective view of a structure for associating physically identifiable feature of a person with a unique identifier of a mobile device.
   Person (100)
   Mobile device (102)
   Cameras (104A, 104B)
   Access points (106a-106C)
PD US10198625-B1   05 Feb 2019   H04N-007/18   201911   Pages: 27   English
UT DIIDW:201912700M
ER

PT P
PN CN107590452-A
TI Gait and face fusion based face identity recognizing method, involves collecting video stream, weighting gait features and face features according to quality scores, and inputting support vector machine classifier for identification.
AB    NOVELTY - The method involves collecting a video stream. A pedestrian detection and tracking process on a video stream is performed to obtain a sequence of human frame images. Face detection is performed to obtain a face image sequence corresponding to a sequence of human frame images. Quality evaluation parameters are obtained. Facial features of the quality evaluation scores is calculated according to the quality evaluation parameters. Gait features and face features are weighted according to the quality scores. A support vector machine (SVM) classifier is inputted for identification.
   USE - Gait and face fusion based face identity recognizing method.
   ADVANTAGE - The method enables increasing the robustness of the identification system, and improving the accuracy of character identification.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a gait and face fusion based face identity recognizing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a gait and face fusion based face identity recognizing method. '(Drawing includes non-English language text)'
PD CN107590452-A   16 Jan 2018   G06K-009/00   201811   Pages: 12   Chinese
UT DIIDW:201807069T
ER

PT P
PN CN107491726-A; CN107491726-B
TI Real - time expression recognition method, involves extracting facial expression from facial expression data set image.
AB    NOVELTY - The real - time expression recognition method involves extracting facial expression from the facial expression data set image. The pre-processing the depth image and the color image are divided into a training set and a testing set and constructing the multi-channel parallel convolutional neural network. The facial expression profiles characteristic of LBP channel identification model is learned. The final expression recognition mode is obtained.
   USE - Real - time expression recognition method.
   ADVANTAGE - The robustness of the recognition network is increased. The performance of the real-time facial expression recognition system is improved.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of real - time expression recognition. (Drawing includes non-English language text)
PD CN107491726-A   19 Dec 2017   G06K-009/00   201804   Pages: 13   Chinese
   CN107491726-B   04 Aug 2020   G06K-009/00   202065      Chinese
UT DIIDW:2017887621
ER

PT P
PN CN107122705-A; CN107122705-B
TI Three-dimensional face model based human face key point detection method, involves updating three-dimensional face model when parameter residual error reaches threshold value, and extracting face key point on three-dimensional face model.
AB    NOVELTY - The method involves acquiring an initial parameter of a face image and a three-dimensional face model in a human face training sample. State self-adapting features and a normalized coordinate code are generated according to the initial parameter of the face image. A residual real parameter is obtained. The initial parameter is updated according to the residual real parameter. The three-dimensional face model is updated when parameter residual error reaches a preset threshold value. A face key point on the three-dimensional face model is extracted.
   USE - Three-dimensional face model based human face key point detection method.
   ADVANTAGE - The method enables realizing human face key point detection function in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a three-dimensional face model based human face key point detection method. '(Drawing includes non-English language text)'
PD CN107122705-A   01 Sep 2017   G06K-009/00   201769   Pages: 10   Chinese
   CN107122705-B   19 May 2020   G06K-009/00   202049      Chinese
UT DIIDW:201761444Y
ER

PT P
PN CN106372621-A
TI Face identification based fatigue driving detection method, involves obtaining eye position of image, and performing eye and mouth fatigue information extracting process for calculating value according to frequency and eye closing time.
AB    NOVELTY - The method involves collecting a human face video image. A frame image is obtained to extract a left eye according to integral projection algorithm. Eye tracking process is performed according to initial information. Eye position of the frame image is obtained according to a video sequence. Eye and mouth state identifying process is performed to obtain horizontal projection data. Eye and mouth fatigue information extracting process is performed for calculating a PERCLOS value according to blink frequency and eye closing time.
   USE - Face identification based fatigue driving detection method.
   ADVANTAGE - The method enables realizing face identification based fatigue driving detection process in an effective manner, increasing identification precision, adaptability and accuracy and reducing time consumption.
PD CN106372621-A   01 Feb 2017   G06K-009/00   201714   Pages: 9   Chinese
UT DIIDW:201710179N
ER

PT P
PN CN106372576-A
TI Deep learning based intelligent indoor intrusion detecting method, involves performing region image detection and identification to judge whether user image is presented or not, and sending alarm signal to user if user image is presented.
AB    NOVELTY - The method involves establishing a BP neural network model based on user input. Difference image of adjacent frames is obtained from a video picture by using an inter-frame differential algorithm. Binarization process is performed on the difference image for extracting a foreground region. A human face region image is extracted by using the established BP neural network model. The region image detection and identification is performed for judging whether a user image is presented or not. An alarm signal is sent to a user if the user image is presented.
   USE - Deep learning based intelligent indoor intrusion detecting method.
   ADVANTAGE - The method enables reducing an error rate by performing a large number of video data analysis, thus ensuring better intrusion detection and identification in an accurate manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning based intelligent indoor intrusion detecting system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep learning based intelligent indoor intrusion detecting method. '(Drawing includes non-English language text)'
PD CN106372576-A   01 Feb 2017   G06K-009/00   201714   Pages: 11   Chinese
UT DIIDW:201710180L
ER

PT P
PN CN106096535-A; CN106096535-B
TI Dual-linear combination convolutional neural network based human face verification method, involves performing CNN model face authentication process, and determining CNN model face authentication identification accuracy rate.
AB    NOVELTY - The method involves obtaining a human face inputting image of a multi-small rectangular frame by a convolutional neural network (CNN). Human face inputting image training process is performed. Convolution pooling layer initialization parameter is obtained. A convolution pooling layer output matrix value is obtained. A gauss distribution random value of a three-link layer is obtained. CNN adjustment classification training result is obtained. Dual-linear CNN model face authentication process is performed. Dual-linear CNN model face authentication identification accuracy rate is determined.
   USE - Dual-linear combination convolutional neural network based human face verification method.
   ADVANTAGE - The method enables increasing robustness of a human face authentication descriptor and reducing CNN-link layer characteristic parameter amount so as to increase face authentication accuracy rate.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a dual-linear combination convolutional neural network based human face verification method. '(Drawing includes non-English language text)'
PD CN106096535-A   09 Nov 2016   G06K-009/00   201678   Pages: 10   Chinese
   CN106096535-B   23 Oct 2020   G06K-009/00   202090      Chinese
UT DIIDW:201672077V
ER

PT P
PN CN105913025-A; CN105913025-B
TI Multi-characteristic fusion depth study based human face identification method, involves calculating double line insert value, calculating human face matrix, and judging automatic stacked code based on automatic stacked coding algorithm.
AB    NOVELTY - The method involves calculating an initial weight decay parameter, a right sparse punishment parameter, a theta weight parameter. Sparse coefficient values of a first hide layer L1 and a second hide layer are calculated. Original image characteristic value is extracted. Image pixel matrix is calculated. Double line insert value is calculated. Human face matrix is calculated. Automatic stacked code is judged based on automatic stacked coding algorithm. ORL human face database is created. The original image characteristic value is stored into the ORL human face database.
   USE - Multi-characteristic fusion depth study based human face identification method.
   ADVANTAGE - The method enables increasing testing accuracy at specific rate and stability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-characteristic fusion depth study based human face identification method. '(Drawing includes non-English language text)'
PD CN105913025-A   31 Aug 2016   G06K-009/00   201665   Pages: 18   Chinese
   CN105913025-B   26 Feb 2019   G06K-009/00   201917      Chinese
UT DIIDW:2016555207
ER

PT P
PN CN105787878-A; CN105787878-B
TI Human face image beautifying method, involves performing color region image detecting process, performing color processing layer forming process, forming color processing layer on human face key point layer, and generating facial image.
AB    NOVELTY - The method involves performing human face image detection processing process. Human face area idenitifying process is performed. Human face image processing process is performed. Human face image key point checking operation is performed. Human face key point coverage data generating process is performed according to a human face image key point. Skin color region image detecting process is performed. Color processing layer forming process is performed. A color processing layer is formed on a human face key point layer. A facial image is generated.
   USE - Human face image beautifying method.
   ADVANTAGE - The method enables increasing human face image beautifying accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face image beautifying device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face image beautifying method. '(Drawing includes non-English language text)'
PD CN105787878-A   20 Jul 2016   G06T-003/40   201655   Pages: 18   Chinese
   CN105787878-B   28 Dec 2018   G06T-003/40   201904      Chinese
UT DIIDW:201646823F
ER

PT P
PN US2016148425-A1; EP3026636-A1; KR2016062572-A; US9799140-B2; KR1997500-B1; EP3026636-B1
TI Method for generating three-dimensional face model of user using three-dimensional scanner based on two-dimensional images, involves refining personalized three-dimensional face model based on difference in texture patterns.
AB    NOVELTY - The method involves extracting feature points of a face from input images comprising a first face image and a second face image. A generic three-dimensional (3D) face model (410) is deformed to a personalized 3D face model (420, 430) based on the feature points. The personalized 3D face model is projected to the first face image and the second face image. The personalized 3D face model is refined based on a difference in texture patterns between the first face image and the second face image to which the personalized 3D face models are projected.
   USE - Method for generating a 3D face model of a user using a 3D depth camera or a 3D scanner based on an input image i.e. two-dimensional (2D) images (claimed), for various fields e.g. face recognition, games and avatars.
   ADVANTAGE - The method enables utilizing a personalized face model generating apparatus to generate a high-precision personalized face model by quickly generating a coarse personalized face model based on feature points extracted from face images and refining a shape of the personalized face model based on texture information of the face images. The method enables utilizing a personalized face model refiner to compare texture patterns of peripheral areas for respective correspondence points so as to adjust spatial locations of vertices constituting the personalized face model, thus minimizing or reducing difference in texture patterns.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a non-transitory computer-readable medium comprising a set of instructions for generating a 3D face model
   (2) an apparatus for generating a 3D face model.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an apparatus for generating a personalized 3D face model by deforming a generic 3D face model.
   Generic 3D face model (410)
   Personalized 3D face models (420, 430)
PD US2016148425-A1   26 May 2016   G06T-017/20   201637   Pages: 16   English
   EP3026636-A1   01 Jun 2016   G06T-015/04   201637      English
   KR2016062572-A   02 Jun 2016   G06T-013/40   201639      
   US9799140-B2   24 Oct 2017   G06T-017/20   201771      English
   KR1997500-B1   08 Jul 2019   G06T-013/40   201956      
   EP3026636-B1   15 Jan 2020   G06T-015/04   202006      English
UT DIIDW:201631293Q
ER

PT P
PN CN105427421-A
TI Human face identification based access controlling method, involves obtaining human face skin color similarity message by human face image binarization unit, and performing legal user verification operation by indicating voice alarm or call.
AB    NOVELTY - The method involves performing image information collecting operation. A human face video image is captured by using a scene video camera head. The human face video image is received by a video server. Human face video image analyzing operation is performed. Human face photo comparison operation is performed by using a human face image part. Human face skin color similarity rate is determined in a background area. A human face skin color similarity message is obtained by a human face image binarization unit. Legal user verification operation is performed by indicating voice alarm or call.
   USE - Human face identification based access controlling method.
   ADVANTAGE - The method enables reducing human face identification difficulties, improving human face monitoring rate for about 99% and human face recognition rate for about 80%, and increasing utilization rate of a network monitor and a hardware resource module.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a video server. '(Drawing includes non-English language text)'
PD CN105427421-A   23 Mar 2016   G07C-009/00   201626   Pages: 8   English
UT DIIDW:201620278C
ER

PT P
PN CN105205458-A
TI Human face detecting method involves determining face image as human face if matching deviation obtained according to spatial matching process of two images, is more than set threshold value.
AB    NOVELTY - The method involves detecting (102) whether first image contains face image, and obtaining (103) second image on same horizontal position. The fitting plane and the fitting deviation of face image key point are matched according to spatial matching process of two images. The judging process is performed to judge (107) whether matching deviation is more than set threshold value. The face image is determined (108) as human face if matching deviation is more than threshold value. The face image is determined (109) as not a human face if deviation is not more than threshold value.
   USE - Human face detecting method.
   ADVANTAGE - The process has simple operation, strong anti-interference ability and high precision. The model training portion of machine learning method is avoided.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a human face detection device; and
   (2) a human face detection system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart explaining the human face detecting process. (Drawing includes non-English language text)
   Step for detecting whether first image contains face image (102)
   Step for obtaining second image on same horizontal position (103)
   Step for judging whether matching deviation is more than set threshold value (107)
   Step for determining face image as human face (108)
   Step for determining face image not human face (109)
PD CN105205458-A   30 Dec 2015   G06K-009/00   201607   Pages: 16   English
UT DIIDW:201603165C
ER

PT P
PN CN105184277-A; CN105184277-B
TI Living body human face identification method, involves obtaining multiple instant human face image with multi-frame, and checking whether eye or viewing point is determined to generate verification result.
AB    NOVELTY - The method involves obtaining multiple instant human face images with a multi-frame. Judgment is made to check whether an eye or a viewing point is determined to generate a verification result. Processing movement of the eye or viewing point is determined. Target reference object shooting process is performed. A target reference object is displayed by a display screen that displays a human face image. Multiple human face images is placed on a viewing password input panel.
   USE - Living body human face identification method.
   ADVANTAGE - The method enables carrying out facial characteristic verification for extraction of illegal log.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a living body human face identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a living body human face identification method. '(Drawing includes non-English language text)'
PD CN105184277-A   23 Dec 2015   G06K-009/00   201603   Pages: 22   English
   CN105184277-B   21 Feb 2020   G06K-009/00   202018      Chinese
UT DIIDW:201600163G
ER

PT P
PN CN105116994-A; CN105116994-B
TI Human face recognition based artificial intelligent robot tracking method, involves detecting maximum human coordinate position, and locating human face on intelligent robot for realizing human face image capturing process.
AB    NOVELTY - The method involves obtaining an image by an intelligent robot. A human face is detected according to the image. Determination is made to check whether the human face is matched with the image or not. An image location is detected when the human face is matched with the image for realizing human face identity identification process. Maximum human coordinate position is detected according to a coordinate position of the intelligent robot. The human face is located on the intelligent robot for realizing human face image capturing process.
   USE - Human face recognition based artificial intelligent robot tracking method.
   ADVANTAGE - The method enables realizing interaction between the intelligent robot and a user in an accurate manner and increasing moving performance of the intelligent robot.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face recognition based artificial intelligent robot tracking device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a human face recognition based artificial intelligent robot tracking method. '(Drawing includes non-English language text)'
PD CN105116994-A   02 Dec 2015   G06F-003/01   201601   Pages: 21   English
   CN105116994-B   31 May 2019   G06F-003/01   201942      Chinese
UT DIIDW:201580551H
ER

PT P
PN CN105094227-A; TW201709012-A; TW603179-B1
TI Narrow frame or frameless electronic device, has displaying mold group fixed with display face that is formed in display area and non-display area, where display module is fixed in display area.
AB    NOVELTY - The device has a displaying mold group fixed with a display face that is formed in a display area and a non-display area. A display module is fixed in the display area. An ultrasonic wave type fingerprint recognition element and the display module are formed with a shielding layer. An adhesive layer is covered on the shielding layer. Thickness of the shielding layer is about 1 to 10 microns. Acoustic impedance range of the shielding layer is about 1.5 to 4. The shielding layer is made of black resin or printing ink material.
   USE - Narrow frame or frameless electronic device.
   ADVANTAGE - The device reduces frame area size.
   DESCRIPTION OF DRAWING(S) - The drawing shows a sectional view of a narrow frame or frameless electronic device.
PD CN105094227-A   25 Nov 2015   G06F-001/16   201601   Pages: 16   English
   TW201709012-A   01 Mar 2017   G06F-001/16   201737      Chinese
   TW603179-B1   21 Oct 2017   G06F-001/16   201804      Chinese
UT DIIDW:201580610Y
ER

PT P
PN CN104933344-A; CN104933344-B
TI Multiple biological characteristic modal based mobile terminal user identity authentication method, involves obtaining iris image according to characteristic image, and performing identification process according to characteristic image.
AB    NOVELTY - The method involves connecting a mobile terminal with a user identity authentication device. A biological characteristic image is obtained by using an image obtaining module. The biological characteristic image is collected according to a human eye locating result. A human face key point is obtained according to the characteristic image. A determination is made to check whether a human eye distance is less than a threshold value. An iris film image is obtained according to the biological characteristic image. Eye week identification process is performed according to the characteristic image.
   USE - Multiple biological characteristic modal based mobile terminal user identity authentication method.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multi-biological characteristic modal based mobile terminal user identity authentication device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of multiple biological characteristic modal based mobile terminal user identity authentication device. '(Drawing includes non-English language text)'
PD CN104933344-A   23 Sep 2015   G06F-021/32   201573   Pages: 22   Chinese
   CN104933344-B   04 Jan 2019   G06F-021/32   201906      Chinese
UT DIIDW:2015666091
ER

PT P
PN WO2014140834-A1; US2014267642-A1; US9025016-B2
TI Apparatus for aiding visually impaired user, causes audible indicator of identity of associated individual to be announced to visually impaired user based on match between face-identifying information and stored facial information.
AB    NOVELTY - The apparatus determines existence of face-identifying information in real-time image data and accesses database storing facial information associated with selected individuals and audible indicators of identities of individuals. The face-identifying information is compared with stored facial information in database, and a match between face-identifying information and the stored facial information is identified (910). An audible indicator of an identity of an associated individual is caused to be announced (930) to visually impaired user based on the match.
   USE - Apparatus for aiding visually impaired user to identify individuals.
   ADVANTAGE - Since the audible indicator is associated with facial information matched to a face found in the image data, announcement of the audible indicator can indicate identity of nearby person to the user of apparatus. The apparatus can use audible facial recognition function to identify individuals near user of apparatus. In this way, in situations in which a visually-impaired user of apparatus cannot identify or is unsure of the identity of a person nearby, apparatus is configured to recognize the individual and announce an identifier of the nearby person to the user.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a software product for identifying individuals in environment of user.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating method for determining whether individual is recognized.
   Method for determining whether individual is recognized (900)
   Step for identifying match between face-identifying information in real-time image data and stored facial information in database (910)
   Step for determining that the match is not within the predetermined time period (920)
   Step for causing apparatus to announce audible indicator associated with the matched individual (930)
   Step for looking for trigger from the user of apparatus (940)
PD WO2014140834-A1   18 Sep 2014   G06K-009/00   201463   Pages: 66   English
   US2014267642-A1   18 Sep 2014   G09B-021/00   201469      English
   US9025016-B2   05 May 2015   G06K-009/00   201535      English
UT DIIDW:2014R54450
ER

PT P
PN DE102014100352-A1; US2014204193-A1; CN104200192-A; US9405982-B2; CN104200192-B; DE102014100352-B4
TI Method for detecting condition of viewing direction of rider of vehicle, involves estimating driver's line of sight on basis of detected location for each of eye characteristic of eyeball of rider and estimated position of head.
AB    NOVELTY - The method involves monitoring facial feature points of the driver within the input image data, which are received by an in-vehicle camera device (10). The location for each of eye characteristics of an eyeball of the driver is detected on the basis of the monitored facial features. The head position of the driver is estimated on the basis of the monitored facial feature points. The driver's line of sight is estimated on the basis of the detected location for each of eye features and the estimated position of the head.
   USE - Method for detecting condition of viewing direction of rider of vehicle.
   ADVANTAGE - Since the driver's line of sight is estimated on the basis of the detected location for each of eye characteristic of eyeball and the estimated position of the head, the viewing direction of the rider of the vehicle is detected accurately with high precision in real time.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of the rider view tracking system.
   In-vehicle camera device (10)
   Processing device (20)
   Head position measuring module (24)
   Viewing direction measuring module (26)
   Rider view tracking system (200)
PD DE102014100352-A1   24 Jul 2014   B60W-040/08   201452   Pages: 21   German
   US2014204193-A1   24 Jul 2014   G06K-009/00   201452      English
   CN104200192-A   10 Dec 2014   G06K-009/00   201507      Chinese
   US9405982-B2   02 Aug 2016   G06K-009/00   201651      English
   CN104200192-B   17 Nov 2017   G06K-009/00   201777      Chinese
   DE102014100352-B4   23 Jul 2020   B60W-040/08   202060      German
UT DIIDW:2014N59680
ER

PT P
PN CN103778414-A
TI Neural network depth based real-time human face identifying method, involves detecting human face base image by utilizing Viola-Jones human face detector, and determining depth of neural network by human face base image.
AB    NOVELTY - The method involves determining diversity of an ultra-scale human face database. Strong image illumination normalization operation is performed to reduce illumination influence. A human face base image is detected by utilizing a Viola-Jones human face detector according to the ultra-scale human face database. Depth of a neural network is determined by the human face base image according to the ultra-scale human face database. Normal data ability of the neural network is enhanced by an auto-encoder.
   USE - Neural network depth based real-time human face identifying method.
   ADVANTAGE - The method enables reducing human face image distance to reduce identification time consumption, increasing human face identifying rate and speed and effectively realizing real-time time human face identifying process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a neural network depth based real-time human face identifying method. '(Drawing includes non-English language text)'
PD CN103778414-A   07 May 2014   G06K-009/00   201444   Pages: 12   Chinese
UT DIIDW:2014M62131
ER

PT P
PN US2014075548-A1; EP2709031-A1; JP2014056576-A; KR2014034088-A; CN103678968-A; US9032510-B2; EP2709031-B1
TI Computer e.g. laptop computer, for e.g. operating door for medical hospital applications, has processor executing computer behavior when subsequent input of input parameter is received as authentication and allowing user to select behavior.
AB    NOVELTY - The computer has a processor for permitting a user to select an input mode with associated input parameter, where the first input parameter is face recognition in combination with infrared (IR) sensing, and the face recognition is in combination with a user-defined facial expression and image of a physical gesture established by the user. The processor executes a behavior of the computer when a subsequent input of the input parameter is received as authentication. The processor allows the user to select the behavior.
   USE - Computer e.g. tablet computer, laptop computer and home automation computer, for operating a door for medical hospital applications or for defense industry security access and for sensing a person presence to establish setting for lighting and music. Can also be used for an ATM computer, transaction kiosk computer and a mobile phone refill station computer.
   ADVANTAGE - The computer allows the user to define a gesture-based input mode with respective input value to establish an authentication protocol to unlock a computer behavior and the user to define the input mode based on face recognition plus IR sensing satisfying a threshold to ensure a live person being imaged for authentication and face recognition plus a particular facial expression e.g. smile and wink. The computer provides subsequent authentication indicated only by a face recognition match and a sensed IR level by the IR sensor that meets a threshold, which is empirically established to indicate the presence of a live human within a few feet of the camera if IR detection is turned on, thus avoiding problem with holding a photograph of the user in front of the camera when the user is otherwise absent.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for operating a computer
   (2) a computing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for operating a computer.
   Step for setting-up screen (36)
   Step for receiving user selection of authentication inputs (38)
   Step for presenting user interface for defining inputs (40)
   Step for subsequently unlocking behavior when correct inputs are received (42)
PD US2014075548-A1   13 Mar 2014   H04L-009/32   201422   Pages: 7   English
   EP2709031-A1   19 Mar 2014   G06F-021/31   201422      English
   JP2014056576-A   27 Mar 2014   G06F-021/32   201422   Pages: 11   Japanese
   KR2014034088-A   19 Mar 2014   G06F-021/30   201423      
   CN103678968-A   26 Mar 2014   G06F-021/31   201433      Chinese
   US9032510-B2   12 May 2015   G06F-012/14   201532      English
   EP2709031-B1   25 Mar 2020   G06F-021/31   202026      English
UT DIIDW:2014E56665
ER

PT P
PN CN103488764-A; CN103488764-B
TI Individualized video content recommendation method, involves taking interested actors with lens of candidate speech segment according to human face identification result to determine whether user of interested actors relating to video clip.
AB    NOVELTY - The method involves analyzing video to acquire video stream, and obtaining a speaking voice print characteristic parameter of a speech segment in audio flow. A talking voice print feature is provided with a sound-print character model parameter of interest of an actor taking voiceprint matching to obtain a candidate voice segment. The interested actors are taken with a lens of a candidate speech segment corresponding to the video stream of a human face recognition according to a human face identification result to determine whether a user of the interested actors relating to a video clip.
   USE - Individualized video content recommendation method.
   ADVANTAGE - The method enables reducing extracting operation amount of the voiceprint and the video human face detecting operation, increasing the speed and easily providing the content of the user interest. The method enables providing fingerprint feature extraction speech sound and interest of the actor taking voiceprint matching so as to accelerate the video lens of the human face interesting actor.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a individualized video content recommendation system comprising a video analyzing unit.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an individualized video content recommendation method. '(Drawing includes non-English language text)'
PD CN103488764-A   01 Jan 2014   G06F-017/30   201417   Pages: 10   Chinese
   CN103488764-B   17 Aug 2016   G06F-017/30   201660      Chinese
UT DIIDW:2014E00538
ER

PT P
PN US2013266193-A1; EP2650822-A1; US8948465-B2; IN201201400-I4; EP2650822-B1; IN362091-B
TI System for performing biometric matching to identify and authenticate sample of e.g. fingerprints of individual, has processor for outputting result based on determination that potential suspect matches person in watch list.
AB    NOVELTY - The system has a processor i.e. special purpose microprocessor, for identifying a person from a watch list. The processor controls a parallel analysis of a determined image of a potential suspect against biometric data and determines whether the potential suspect matches the person in the watch list based on the parallel analysis of the determined image of the potential suspect against the biometric data associated with a set of persons in the watch list. The processor outputs a result based on the determination that the potential suspect matches the person in the watch list.
   USE - System for performing biometric matching to identify and authenticate a sample of biometric information e.g. fingerprints, retina scans and facial images, of an individual utilized for searching a watch list of persons of interest e.g. criminals that government agency is trying to locate, missing persons and persons blacklisted from establishment.
   ADVANTAGE - The potential suspect can be matched against the watch list in a quick manner such that crowd of people can be scanned for the persons of interest in the watch list in a short period of time and resources in cloud are leveraged in scanning the crowd of people for the persons of interest in the watch list, so that processing is flexible, thus balancing cost for scanning the crowd against a desire to locate the persons of interest in the crowd in a quick manner. The system can identify a closest match to a computed similarity score and a group of people having similarity scores within a threshold of the computed similarity score, so that searching based on the similarity score can improve speed of searching the biometric data over traditional techniques, which are used to search based on comparison of more detailed data. The system can select a relatively low number of operations to use in pre-processing such that image quality is determined in an easy manner, so that a sufficient image can determined without a large amount of pre-processing in an effective manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for performing biometric matching to identify and authenticate a sample of biometric information
   (2) a computer-readable storage medium comprising a set of instructions for performing biometric matching to identify and authenticate a sample of biometric information.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a structure for performing biometric matching.
   Step for performing pre-processing on each of captured images in parallel after multiple images of potential suspect are captured (120)
   Step for identifying subset of persons in watch list (130)
   Step for comparing selected image against reference image to obtain index value that reflects how similar selected image is to reference image (140)
   Step for facilitating index value of selected image to be used to search watch list of persons of interest (150)
   Step for performing parallel matching process using detailed face images for persons in subset and detailed face image for potential suspect (160)
PD US2013266193-A1   10 Oct 2013   G06K-009/80   201370   Pages: 46   English
   EP2650822-A1   16 Oct 2013   G06K-009/00   201370      English
   US8948465-B2   03 Feb 2015   G06K-009/00   201510      English
   IN201201400-I4   01 Apr 2016   G06F-000/00   201628      English
   EP2650822-B1   20 Sep 2017   G06K-009/00   201763      English
   IN362091-B   26 Mar 2021   G06F-000/00   202140      English
UT DIIDW:2013R23848
ER

PT P
PN CN103297512-A; CN103297512-B
TI Face recognition based intelligent area network monitoring system, has local server connected with central server, where local server is provided with local people face identifying system and local people face information database.
AB    NOVELTY - The system has a local server connected with a central server though a network. The local server is provided with a local people face identifying system, a local staff basic information data base, a local people face information database and a local operator access condition information database. The local people face identifying system is utilized for obtaining human face image data. The local staff basic information data base is utilized for storing local operator basic information. An intact human face information database is utilized for storing a complete area of human face image data.
   USE - Face recognition based intelligent area network monitoring system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face recognition based intelligent area network monitoring method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a face recognition based intelligent area network monitoring method.'(Drawing includes non-English language text)'
PD CN103297512-A   11 Sep 2013   H04L-029/08   201377   Pages: 8   Chinese
   CN103297512-B   28 Dec 2016   H04L-029/08   201704      Chinese
UT DIIDW:2013V52689
ER

PT P
PN AU2013201778-B1; DE102013102399-A1; GB2500321-A; US2013247175-A1; KR2013105527-A; CN103324909-A; KR1326221-B1; GB2500321-B; CN103324909-B; US9177130-B2; DE102013102399-B4
TI Method for detecting facial features of images captured for use in facial recognition, involves capturing image, calculating face template, analyzing face template, and outputting notification for user to remove removable facial feature.
AB    NOVELTY - The method involves capturing (502) an image including a user face. A face template is calculated (504) in the image. The face template is analyzed (506) to determine whether the face includes a removable facial feature that decreases a level of distinctiveness between two faces and a non-removable facial feature that decreases a level of distinctiveness between faces. A notification is outputted (510) for the user to remove the removable facial feature. A similarity score threshold is adjusted (514) to another similarity score threshold when the face includes the non-removable facial feature.
   USE - Method for detecting facial features of images captured for use in facial recognition by a computing device (Claimed).
   ADVANTAGE - The face template is analyzed to determine whether the face includes a removable facial feature that decreases a level of distinctiveness between two faces and a non-removable facial feature that decreases a level of distinctiveness between faces, and thus reduces the chances of an unauthorized user causing erroneous authentication by attempting to appear less distinct, and enhances the chances of authorized users to grant the access. The notification is outputted for the user to remove the removable facial feature, and provides improved results by preventing potential enrollment templates including removable facial features or non-removable facial features from being stored as enrollment templates.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a computer-readable storage device comprises instructions; and
   (2) a computing device comprises a camera.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a facial feature detection method.
   Capturing (502)
   Calculating (504)
   Analyzing (506)
   Outputting (510)
   Adjusting (514)
PD AU2013201778-B1   05 Sep 2013   G06K-009/00   201362   Pages: 49   English
   DE102013102399-A1   19 Sep 2013   G06K-009/62   201362      German
   GB2500321-A   18 Sep 2013   G06K-009/00   201362      English
   US2013247175-A1   19 Sep 2013   G06F-021/32   201362      English
   KR2013105527-A   25 Sep 2013   G06T-007/00   201365      
   CN103324909-A   25 Sep 2013   G06K-009/00   201379      Chinese
   KR1326221-B1   11 Nov 2013   G06T-007/00   201379      
   CN103324909-B   29 Apr 2015   G06K-009/00   201544      Chinese
   US9177130-B2   03 Nov 2015   G06F-021/32   201573      English
   DE102013102399-B4   07 Feb 2019   G06K-009/62   201911      German
UT DIIDW:2013N35971
ER

PT P
PN CN103235825-A; CN103235825-B
TI Hadoop cloud computing framework based face recognition massive search engine design method, involves matching human face characteristic vector with human face feature vector cluster index table and multiple cluster list tables.
AB    NOVELTY - The method involves arranging an inner layer, a middle layer and a cloud outer layer with a three-layer structure. The inner layer is provided with face identity information data. The middle layer is provided with face feature vector cluster index. The outer layer is utilized for receiving a human face characteristic vector. A mass human face image is stored in an HBase database. A face identity information data table is obtained by a non-structure. The human face characteristic vector is matched with a human face feature vector cluster index table and multiple cluster list tables.
   USE - Hadoop cloud computing framework based face recognition massive search engine design method.
   ADVANTAGE - The method enables improving human face image searching speed in the database. The method enables establishing human face character vector clustering index for a cluster list by utilizing a K-average value clustering algorithm. The method enables improving stability and reducing process step.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a hadoop cloud computing framework based face recognition massive search engine design method.'(Drawing includes non-English language text)'
PD CN103235825-A   07 Aug 2013   G06F-017/30   201374   Pages: 12   Chinese
   CN103235825-B   25 May 2016   G06F-017/30   201637      English
UT DIIDW:2013U25073
ER

PT P
PN CN103093215-A; CN103093215-B
TI Human eye locating method, involves obtaining lens information of human face image that is located according to eye location position of human face image, and determining open and close states of human eyes in human face image.
AB    NOVELTY - The method involves obtaining lens information of a human face image. The human face image is located according to an eye location position. Open and close states of human eyes are determined in the human face image. A position of the human eyes is detected in the human face image by adopting an AdaBoost algorithm. Lens information is detected according to the position of the human eyes, where lens information is formed as wearing-type glasses information. The position of the human eyes is located by a preset matching module.
   USE - Human eye locating method.
   ADVANTAGE - The method enables improving human eye locating accuracy and increasing face recognition speed.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human eye locating device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human eye locating method.'(Drawing includes non-English language text)'
PD CN103093215-A   08 May 2013   G06K-009/00   201366   Pages: 26   Chinese
   CN103093215-B   28 Dec 2016   G06K-009/00   201704      Chinese
UT DIIDW:2013Q56939
ER

PT P
PN CN103049734-A
TI Method for finding person in public place using mobile phone, involves marking current position of person to be found is marked on electronic map, and displaying position to user.
AB    NOVELTY - The method involves storing a picture of a person to be found by a user. Face identification is performed on the persons in the public place. The identified face images are compared with the stored picture of the person to be found by the user. The current position of the person to be found is marked on the electronic map, and position is displayed to the user when the identified face image is in accordance with the stored picture of the person to be found.
   USE - Method for finding person in public place using mobile phone.
   ADVANTAGE - The system can help people to determine and find the position of friend or relative who wanders away or the persons who make an appointment but never met each other in large-scale public place rapidly.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for system for finding person in public place.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the process for finding person in public place. (Drawing includes non-English language text)
PD CN103049734-A   17 Apr 2013   G06K-009/00   201364   Pages: 13   Chinese
UT DIIDW:2013P47031
ER

PT P
PN CN102902959-A; CN102902959-B
TI Human face identification method using second generation identity card, involves judging human face characteristics, and determining whether specific person is identified.
AB    NOVELTY - The method involves collecting image of human face for detecting the human face image. The human face characteristic point is extracted, and the human face is checked based on the position of the face characteristic point. The human face characteristics are judged, and a determination is made whether the specific person is identified. The probability distribution and judgment threshold value distance are obtained. The serial port connection is established, and serial port data is read.
   USE - Human face identification method using second generation identity card.
   ADVANTAGE - The efficiency is improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a human face identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the human face identification system. (Drawing includes non-English language text)
   Second generation identity card (2)
   Computer (4)
   Human face verification module (43)
   Human face training module (51)
   Record database (52)
PD CN102902959-A   30 Jan 2013   G06K-009/00   201347   Pages: 19   Chinese
   CN102902959-B   06 May 2015   G06K-009/00   201547      Chinese
UT DIIDW:2013H01019
ER

PT P
PN CN102819726-A; CN102819726-B
TI Human face region processing system for mobile terminal, has obtaining module for obtaining linkman head portrait of human face area and related module used for contacting related picture in accordance with matching process.
AB    NOVELTY - The system has an obtaining module for obtaining a linkman head portrait of human face area by using a human face detection technology. A matching module matches the human face region with a selected picture to obtain a matched photo according to a photograph. A related module contacts related picture in accordance with the matching process. A human face identification module is arranged in the matching module. A judging module judges the matching value with respect to a preset threshold picture value.
   USE - Human face region processing system for a mobile terminal.
   ADVANTAGE - The system can quickly search the photo associated linkman head portrait photo.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face region processing method for mobile terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face region processing method for mobile terminal. '(Drawing includes non-English language text)'
PD CN102819726-A   12 Dec 2012   G06K-009/00   201330   Pages: 7   Chinese
   CN102819726-B   24 Aug 2016   G06F-017/30   201661      Chinese
UT DIIDW:2013E44067
ER

PT P
PN CN102760024-A; TW201243823-A
TI Electronic device screen image rotation method, involves comparing current human face with initial vector of T vector to obtain angle difference, and rotating image in screen to present vector direction according to angle difference.
AB    NOVELTY - The method involves setting an electronic equipment with an initial screen of image direction information, and identifying face in an image from a background scanning through a human face identification technique by using an electronic equipment front camera. A human face T vector is established according to human face information. A current human face is compared with an initial vector of the human face T vector to obtain an angle difference. The image in the screen is rotated to present human face T vector direction according to the angle difference.
   USE - Electronic device screen image rotation method.
   ADVANTAGE - The method enables realizing 360 degree rotation of the electronic device screen image.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an electronic device screen image rotation system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an electronic device screen image rotation method. '(Drawing includes non-English language text)'
PD CN102760024-A   31 Oct 2012   G06F-003/048   201315   Pages: 11   Chinese
   TW201243823-A   01 Nov 2012   G09G-005/38   201315      Chinese
UT DIIDW:2013B36263
ER

PT P
PN WO2012139243-A1; TW201303772-A; KR2013136557-A; EP2697742-A1; CN103493068-A; US2014156398-A1; JP2014517371-W; KR2016013266-A; US2016148247-A1; CN103493068-B; EP2697742-A4
TI Method for selecting e.g. advertisement to present to consumer utilized by TV providers, involves identifying advertisements to present to consumer, and presenting selected one of identified advertisements to consumer on media device.
AB    NOVELTY - The method involves detecting a facial region in an image by a face detection module (22). Consumer characteristics (30) of a consumer are identified in the image, where the consumer characteristics comprise facial expression of the consumer in the image. Advertisements are identified to present to the consumer based on comparison of the consumer characteristics with an advertisement database (26) including a set of advertisement profiles by an advertisement selection module (28). A selected one of the identified advertisements is presented to the consumer on a media device (18).
   USE - Method for selecting an advertisement to present to a consumer i.e. child or adult, utilized by media providers e.g. TV providers, radio providers and advertisement providers. Uses include but are not limited to TV advertisement, billboard advertisement, radio advertisement, in-store advertisement, digital sign advertisement, digital menu board advertisement, comedy advertisement, drama advertisement, reality-based advertisement, beer advertisement, jewelry advertisement, holiday advertisement and woman's clothing advertisement.
   ADVANTAGE - The method enables the determination module to reduce/ignore parameters and increase relevancy of the parameters based on the characteristics of the group of consumers watching the advertisements.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a tangible computer-readable medium comprising a set of instructions for selecting an advertisement to present to a consumer
   (2) an apparatus for selecting an advertisement to present to a consumer.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a system for selecting and displaying advertisements to a consumer based on facial analysis of the consumer.
   Media device (18)
   Face detection module (22)
   Advertisement database (26)
   Advertisement selection module (28)
   Consumer characteristics (30)
PD WO2012139243-A1   18 Oct 2012   G06K-009/62   201270   Pages: 31   English
   TW201303772-A   16 Jan 2013   G06Q-030/02   201328      Chinese
   KR2013136557-A   12 Dec 2013   G06K-009/46   201401      
   EP2697742-A1   19 Feb 2014   G06K-009/62   201414      English
   CN103493068-A   01 Jan 2014   G06K-009/62   201417      Chinese
   US2014156398-A1   05 Jun 2014   G06Q-030/02   201437      English
   JP2014517371-W   17 Jul 2014   G06Q-030/02   201446   Pages: 21   Japanese
   KR2016013266-A   03 Feb 2016   G06Q-030/02   201615      English
   US2016148247-A1   26 May 2016   G06Q-030/02   201636      English
   CN103493068-B   13 Jun 2017   G06K-009/62   201742      Chinese
   EP2697742-A4   05 Nov 2014   G06K-009/62   201772      English
UT DIIDW:2012N18628
ER

PT P
PN CN102708606-A; CN102708606-B
TI Human face identification based prison intuitive area inlet and outlet monitoring system, has human face identifying device connected with gate and server connected with information collecting registration device.
AB    NOVELTY - The system has a human face identifying device controlling an inlet and an outlet of a prison intuitive area via a relay. A personnel information collecting registration device is connected with the human face identifying device and a server. A card reader is provided with a camera for collecting intuitive human face information and identity information that are registered on the human face identifying device. The human face identifying device is connected with a gate and the server. The server is connected with the information collecting registration device.
   USE - Human face identification based prison intuitive area inlet and outlet monitoring system.
   ADVANTAGE - The system effectively realizes prison intuitive area monitoring with high precision in real-time and ensures safety of an operator.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face identification based prison intuitive area inlet and outlet monitoring system.'(Drawing includes non-English language text)'
PD CN102708606-A   03 Oct 2012   G07C-009/00   201304   Pages: 9   Chinese
   CN102708606-B   04 Sep 2013   G07C-009/00   201377      Chinese
UT DIIDW:2013A48793
ER

PT P
PN US2012086727-A1; WO2012045914-A1; EP2625647-A1; ZA201303256-A; US9317133-B2; IN201303083-P4; EP2625647-A4
TI Method for providing augmented reality services based on tracking of e.g. features in e.g. mobile device, involves determining to generate display information including items overlaid on representation based on points.
AB    NOVELTY - The method involves determining information including location information and orientation information of a mobile device e.g. handset. An image representation of a location indicated based on the information is determined. Items e.g. images, are selected to associate with points within the representation. Determination is made to generate display information including the items overlaid on the representation based on the points. Determination is made to cause a tracking of the location and the orientation in relation to a spatial layout.
   USE - Method for providing augmented reality services based on tracking of features in a mobile device. Uses include but are not limited to moving objects, cars, people, faces, buildings and animals, in a mobile phone, mobile handset, multimedia computer, multimedia tablet, Internet node, communicator, desktop computer, laptop computer, notebook computer, tablet computer, personal communication system (PCS) device, personal navigation device, personal digital assistant (PDA), audio/video player, digital camera/camcorder, positioning device, TV receiver and radio broadcast receiver.
   ADVANTAGE - The method enables presenting augmented content to a user by generating a scene to include both virtual content and real-time properties in an efficient manner. The method enables restricting intensive processing to areas in an image, so that a tracking engine running on a runtime module is not attempt to detect/track feature points, thus enhancing efficient tracking of the location of the device when the device is moving to provide such augmented reality content.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer-readable storage medium comprising a set of instructions for providing augmented reality based on tracking
   (2) an apparatus for providing augmented reality based on tracking comprising a processor.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of components of a user equipment.
   User equipment (101)
   Data collection module (111)
   Accelerometer module (205)
   Image capture module (207)
   Runtime module (209)
   User interface (211)
   Image processing module (215)
PD US2012086727-A1   12 Apr 2012   G09G-005/00   201227   Pages: 23   English
   WO2012045914-A1   12 Apr 2012   G06K-009/00   201227      English
   EP2625647-A1   14 Aug 2013   G06K-009/00   201355      English
   ZA201303256-A   23 Dec 2014   G06K-000/00   201505      English
   US9317133-B2   19 Apr 2016   G09G-005/00   201628      English
   IN201303083-P4   03 Jun 2016   G06K-009/00   201668      English
   EP2625647-A4   15 Feb 2017   G06T-007/20   201714      English
UT DIIDW:2012E20153
ER

PT P
PN CN102034079-A; CN102034079-B
TI Method for identifying human face shielded by eyes, involves projecting vivid human face mode after processing to generate multiple virtual samples, and memorizing virtual samples as images to be matched to identify human face.
AB    NOVELTY - The method involves adding glass grid mode to a delicate human face grid mode and adhering veins of a human face and glass to a human face grid mode wearing glass to obtain a vivid human face mode. A fuzzy treatment and reflection treatment is performed for a part of the glass mode in the vivid human face mode. The vivid human face mode is projected after processing to generate multiple virtual samples in presence of the glass under different conditions. The virtual samples are memorized as images to be matched to identify the human face.
   USE - Method for identifying human face shielded by eyes.
   ADVANTAGE - The method enables a system for identifying human face correspondingly. The method ensures wide simulation range, capability of covering more changes of the glass, better universality and improved identification effect, better real-time effect, quick speed, simple operation and high identification ratio.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for identifying human face shielded by eyes. '(Drawing includes non-English language text)'
PD CN102034079-A   27 Apr 2011   G06K-009/00   201164   Pages: 13   Chinese
   CN102034079-B   28 Nov 2012   G06K-009/00   201318      Chinese
UT DIIDW:2011M35297
ER

PT P
PN CN102004906-A
TI Human face identification system, has human face identification module for performing human face identification in human face region when temperature is within predetermined range, and human face detecting module detecting human face region.
AB    NOVELTY - The system has an image collecting module for collecting an image. A human face detecting module is utilized for detecting a human face region in the image. A temperature sensing module is utilized for detecting temperature of a corresponding region of a collecting region of the image collecting module. A human face identification module is utilized for performing human face identification in the human face region when the temperature is within a predetermined range. An infrared sensing module is utilized for sending a signal to the image collecting module.
   USE - Human face identification system.
   ADVANTAGE - The system is provided with a temperature detecting system in the human face identification process to judge whether the identified human face image is collected from the real human face through the temperature in the human face identification process. The system is inexpensive, and has better practical effect.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating operations of a human face identification system.'(Drawing includes non-English language text)'
PD CN102004906-A   06 Apr 2011   G06K-009/00   201134   Pages: 8   Chinese
UT DIIDW:2011F15861
ER

PT P
PN CN101902602-A
TI Automatic digital TV program adult rank adjusting method, involves watching TV program according to minimum age and relative relationship between stored age and adult rank for watching TV program.
AB    NOVELTY - The method involves monitoring image collecting time for a watcher, and judging whether the image collecting time for the watcher is achieved. Face recognition is performed on a collected image of the watcher. Age recognition is provided on the watcher according to a face age characteristic to obtain a minimum age of the watcher according to an age recognition result. An adult rank is provided for watching a TV program according to the minimum age and relative relationship between a stored age and the adult rank for watching the TV program.
   USE - Automatic digital TV program adult rank adjusting method.
   ADVANTAGE - The method enables automatic collection of the image for the watcher according to the age characteristic and setting the adult rank for watching the TV program according to the age for automatic and dynamic adjusting of the adult rank of a digital TV and effectively reducing impact to juveniles of the bad TV program.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a digital TV terminal comprising a TV program adult rank automatic adjusting device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an automatic digital TV program adult rank adjusting method.'(Drawing includes non-English language text)'
PD CN101902602-A   01 Dec 2010   H04N-005/445   201115   Pages: 10   Chinese
UT DIIDW:2011A40123
ER

PT P
PN CN101819628-A; CN101819628-B
TI Method for sparsely showing human face, involves calculating texture residual error according to coefficient expressed by linear expression, and calculating similarity of human face image to-be-recognized according to shape residual error.
AB    NOVELTY - The method involves calculating texture residual error according to coefficient expressed by a linear expression, and utilizing shape feature vector of a training set to express shape feature vector of an image to-be-recognized. Shape residual error is calculated according to a coefficient expressed by the linear expression. Similarity of a human face image to-be-recognized is calculated according to the texture residual error. Another similarity of the human face image to-be-recognized is calculated according to the shape residual error.
   USE - Method for sparsely showing a human face.
   ADVANTAGE - The method has higher human face recognition rate.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic illustration of a method for sparsely showing a human face. '(Drawing includes non-English language text)'
PD CN101819628-A   01 Sep 2010   G06K-009/00   201066   Pages: 10   Chinese
   CN101819628-B   28 Dec 2011   G06K-009/00   201208      Chinese
UT DIIDW:2010M02190
ER

PT P
PN US2010060727-A1; US8509496-B2
TI Method of tracking face in reference image stream, involves applying concentrated face detection to portion of full resolution main image after determining size and location of face regions to determine candidate face regions.
AB    NOVELTY - The method involves acquiring a full resolution main image and an image stream of low resolution reference images. The face regions are identified from reference images and size and location of face regions within reference images are determined. Concentrated face detection is applied to the portion of full resolution main image after determining size and location of face regions within reference images to determine candidate face regions.
   USE - Method of tracking face in reference image stream acquired by digital image acquisition device (claimed) such as digital camera, hand held computer and cellular phone.
   ADVANTAGE - Concentrated face detection process can be applied effectively to portion of full resolution main image after determining size and location of face regions. Hence, face detection process can be performed effectively and face detection overhead is significantly reduced. Complex calculation involved in face tracking process can be minimized and quality of the image can be improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) digital image acquisition device for detecting faces; and
   (2) precursor-readable media storing program for tracking face.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of image processing apparatus.
   Raw image acquisition unit (105)
   Camera memory (110)
   Candidate region extractor (125)
   Face recognition unit (160)
   Candidate region tracker (290)
PD US2010060727-A1   11 Mar 2010   H04N-007/18   201020   Pages: 18   English
   US8509496-B2   13 Aug 2013   G06K-009/00   201354      English
UT DIIDW:2010C87891
ER

PT P
PN US2010021021-A1; US7831069-B2
TI Digital unknown image matching method for classification of digital facial images received over network, involves matching processed unknown facial image with database processed images and sorting matches based on human perception.
AB    NOVELTY - The unknown facial image such as a JPEG image is analyzed at a server to determine whether the unknown facial image is acceptable and processed to obtain a processed unknown facial image. The matching is performed for the processed unknown facial image of an individual and the database processed image of another individual that are twin in appearance. The matches are sorted based on predicted human perception to generate best matched images after determining the perception value of matched images.
   USE - Digital unknown image matching method for classification of digital facial images received over network e.g. wireless digital network or internet.
   ADVANTAGE - An expedient, inexpensive and technologically easy way of determining whether an individual looks like another individual is provided for the individual by reducing the complexity of the digital image data into a small set of variables by processing the image into a feature vector. The internet hosted system is provided to find, compare, contrast and identify similar characteristics of individuals using a digital camera, cellular telephone camera or wireless device and classify unknown facial images from a variety of internet accessible sources including mobile phones, wireless camera enabled devices, images obtained from digital cameras or scanners and uploaded from personal computers (PCs), third party applications and databases.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flowchart explaining the digital unknown image matching method.
PD US2010021021-A1   28 Jan 2010   G06K-009/00   201010   Pages: 15   English
   US7831069-B2   09 Nov 2010   G06K-009/00   201074      English
UT DIIDW:2010B17910
ER

PT P
PN US2009190803-A1; WO2009095168-A1; KR2010116178-A; JP2011511977-W; CN102007499-A; US8750578-B2; JP5639478-B2; CN102007499-B; KR1615254-B1
TI Method of in-camera processing of still image, involves classifying statistical smile state of face based on accumulated smile state information and initiating smile state dependent operations.
AB    NOVELTY - The method involves identifying (104) a group of pixels that correspond to a face within a digitally-acquired image on a portable camera. A face is tracked (108) in a collection of lower resolution images. The cropped versions of multiple images are acquired (110) from collection of relatively lower resolution images including face. The statistical smile state information of face is classified (114) based on accumulated smile state information. The smile state-dependent operations are initiated (116) based upon analysis of smile state information of face.
   USE - Method of in-camera processing of still image obtained using portable digital image acquisition device (claimed) such as portable camera.
   ADVANTAGE - The special facial expressions such as smiles and frowns can be detected accurately.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) portable digital image acquisition device; and
   (2) processor-readable media storing method of in-camera processing of still image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the technique of still image including the face.
   Step for identifying group of pixel (104)
   Step for tracking face (108)
   Step for acquiring cropped version of image (110)
   Step for classifying statistical smile state (114)
   Step for initiating smile state-dependent operations (116)
PD US2009190803-A1   30 Jul 2009   G06K-009/00   200952   Pages: 14   English
   WO2009095168-A1   06 Aug 2009   G06K-009/00   200952      English
   KR2010116178-A   29 Oct 2010   G06K-009/00   201074      
   JP2011511977-W   14 Apr 2011   G06T-007/20   201127   Pages: 24   Japanese
   CN102007499-A   06 Apr 2011   G06K-009/00   201132      Chinese
   US8750578-B2   10 Jun 2014   G06K-009/00   201438      English
   JP5639478-B2   10 Dec 2014   G06T-007/20   201482   Pages: 19   Japanese
   CN102007499-B   11 Mar 2015   G06K-009/00   201535      Chinese
   KR1615254-B1   25 Apr 2016   G06K-009/00   201631      English
UT DIIDW:2009M13047
ER

PT P
PN FR2922400-A1; US2009097716-A1; CN101409784-A; CA2639781-A1; CA2639781-C; US8842885-B2; FR2922400-B1
TI Camera device e.g. camera, for use on computer monitor to capture human image, has prompt output module outputting prompt information based on distance information, where prompt information is configured to provide prompt to user.
AB    NOVELTY - The device has an image capturing module (1) e.g. image sensor, to acquire image data. An information processing module (2) obtains human face image data from the acquired data, and obtains information about distance between a human face and the device using the face image data. A decision module generates prompt information e.g. voice prompt information, when the distance is less than a preset value. A prompt output module (3) e.g. voice prompt module, outputs the prompt information based on the distance information, where the prompt information is configured to provide a prompt to a user.
   USE - Camera device e.g. camera, for use on a monitor of a computer for capturing a human image.
   ADVANTAGE - The prompt output module outputs the prompt information based on the distance information, so that the user is timely prompted to prevent his eyes from being too close to the computer screen, thus protecting the eyesight of the user. The information processing module obtains the distance information between the human face and the camera device using the face image data, thus eliminating the need for a human face detection device that causes radiation to the human body. The device is flexible and convenient to use. The device is easy to manufacture and is extendable.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an information prompt method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a camera device.'(Drawing includes non-English language text)'
   Image capturing module (1)
   Information processing module (2)
   Prompt output module (3)
   Human face identification module (21)
   Area identification module (22)
   Pixel calculation module (23)
PD FR2922400-A1   17 Apr 2009   H04N-005/225   200927   Pages: 26   French
   US2009097716-A1   16 Apr 2009   G06K-009/00   200927      English
   CN101409784-A   15 Apr 2009   H04N-005/225   200929      Chinese
   CA2639781-A1   10 Apr 2009   G01B-011/14   200931      English
   CA2639781-C   28 May 2013   G01B-011/14   201338      English
   US8842885-B2   23 Sep 2014   G03B-003/00   201470      English
   FR2922400-B1   29 Dec 2017   H04N-005/225   201804      French
UT DIIDW:2009H16277
ER

PT P
PN US2009016572-A1; US7844079-B2
TI Anatomical feature e.g. face, recognition performing method, involves recovering pattern information from image reflected from surface for each of set of modulated structured light patterns.
AB    NOVELTY - The method involves modulating structured light patterns by using a respective carrier frequency along an orthogonal dimension. A composite image comprising a set of modulated structured light patterns is projected at an anatomical feature. Pattern information is recovered from an image reflected from a surface for each of the set of modulated structured light patterns. Each modulated structured light pattern is obtained by modulating a signal waveform associated with respective carrier frequency.
   USE - Method for performing recognition of an anatomical feature e.g. face, head and hand, of a human being and an animal by using depth information about a surface of the anatomical feature.
   ADVANTAGE - The method facilitates retrieval of depth information about surface of an object employing traditional, new and structured light pattern projections in effective manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for obtaining orientation data about an anatomical feature by using information retrieved from a surface of an anatomical feature
   (2) a system for performing recognition of an anatomical feature by using depth information about a surface of an anatomical feature observed by a camera
   (3) a system for obtaining orientation data about an anatomical feature by using information retrieved from a surface of an anatomical feature observed by a camera
   (4) a computer readable storage medium with a program code for performing recognition of an anatomical feature.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a method for retrieving depth information about surface of an object.
PD US2009016572-A1   15 Jan 2009   G06K-009/46   200907   Pages: 19   English
   US7844079-B2   30 Nov 2010   G06K-009/00   201079      English
UT DIIDW:2009E02416
ER

PT P
PN CN101226591-A
TI Identity identification method, involves collecting picture of human face image of person to be identified using mobile phone camera, and sending identifiable information and human face image to server terminal through mobile internet.
AB    NOVELTY - The method involves obtaining identifiable information of a person to be identified by a user, and inputting the identifiable information into a mobile phone terminal. A picture of human face image of the person to be identified is collected by a mobile phone camera, and the identifiable information and the human face image obtained are sent to a server terminal through mobile internet. An identity identification request is sent to the server terminal, and the identifiable information of the person to be identified is used to select data in an identity database of the server terminal.
   USE - Method for identification of identity based on mobile phone camera combining human face identification technique.
   ADVANTAGE - The realizing cost and degree of difficulty of the method are very less, and the speed and accuracy of the identity identification are high.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of an identity identification method.'(Drawing includes non-English language text)'
PD CN101226591-A   23 Jul 2008   G06K-009/00   200858   Pages: 19   Chinese
UT DIIDW:2008J84980
ER

PT P
PN EP1830304-A1; JP2007235872-A; US2007216797-A1; CN101030013-A; KR2007090726-A; KR859764-B1; EP1830304-B1; CN100533251-C; DE602006008736-E; US7646423-B2; JP4566930-B2
TI Image capturing apparatus for capturing objects`s image e.g. image of living body, has distance-measuring light-emitting device irradiating object with light, and control circuit detecting spot light position of device.
AB    NOVELTY - The apparatus has a ring shaped light guide (10) guiding light of light emitting devices (22, 24) to an image capturing range, and illuminating the range. An optical unit e.g. converging lens, guides the reflected light of an object e.g. living body, in the range to an image sensor e.g. complementary metal-oxide semiconductor image sensor. A distance-measuring light-emitting device (52) irradiates the object with light. A control circuit such as microcontroller, detects a spot light position of the device (52) from a photographed image of the sensor, and obtains the distance to the object.
   USE - Used for capturing an object`s image e.g. image of living body, image of a portion of a human body, and image of biometric such as fingerprint pattern, or patterns of limb, eye retina, face, blood vessel, skin pattern of a palm and back of the hand and finger, to provide personal identification.
   ADVANTAGE - The arrangement of the distance-measuring light emitting device easily facilitates the detection of positions in the image far from the center without providing photodetector devices, and accurately facilitates the detection of inclination of the image, thus providing reduction in cost, and miniaturization of the image capturing apparatus. The configuration of the apparatus obtains sufficient irradiation amount even if a low-power light-emitting device is utilized.
   DESCRIPTION OF DRAWING(S) - The drawing shows an exploded structural view of an image capturing apparatus.
   Ring shaped light guide (10)
   Camera substrate (20)
   Light-emitting devices (22, 24)
   Photodetector devices (26)
   Image sensor (30)
   Polarizing plate (32)
   Optical unit (34)
   Diffusion plates (42, 44)
   Diffusion/polarization plate mount tables (46)
   Aperture (50)
   Distance-measuring light-emitting device (52)
   Control substrate (60)
PD EP1830304-A1   05 Sep 2007   G06K-009/00   200770   Pages: 39   English
   JP2007235872-A   13 Sep 2007   H04N-005/225   200770   Pages: 26   Japanese
   US2007216797-A1   20 Sep 2007   H04N-005/222   200770      English
   CN101030013-A   05 Sep 2007   G03B-015/02   200810      Chinese
   KR2007090726-A   06 Sep 2007   H04N-005/30   200810      
   KR859764-B1   24 Sep 2008   H04N-005/30   200910      
   EP1830304-B1   26 Aug 2009   G06K-009/00   200956      English
   CN100533251-C   26 Aug 2009   G03B-015/02   200965      Chinese
   DE602006008736-E   08 Oct 2009   G06K-009/00   200966      German
   US7646423-B2   12 Jan 2010   H04N-005/222   201005      English
   JP4566930-B2   20 Oct 2010   H04N-005/225   201069   Pages: 24   Japanese
UT DIIDW:2007742957
ER

PT P
PN DE102006005617-A1; WO2007090727-A1; EP1984851-A1; CN101379488-A; CA2641355-A1; IN200804416-P4; RU2008135986-A; RU2437154-C2; CN104217201-A; EP1984851-B1; ES2538780-T3; IN289503-B; CA2641355-C
TI Image e.g. iris image, quality evaluating method for producing e.g. driving license, involves accessing evaluation file, which contains evaluation pattern or schema, and evaluating image recognition results based on pattern.
AB    NOVELTY - The method involves accessing an evaluation file, which contains an evaluation pattern or schema. Image recognition results are evaluated based on the evaluation pattern. Evaluation results are output. Image characteristics corresponding to a threshold value are defined by the schema. The image recognition is carried out with respect to the characteristics in order to determine a measured value of the characteristics. The measured value of the characteristics is included in the image recognition results.
   USE - Used for evaluating quality of an image e.g. face image, fingerprint image and iris image, of a person to produce a document (claimed) e.g. valuable document, security document and identification document such as passport, identity card and driving license, and visa, in passport and identification offices by an electronic device e.g. computer such as personal computer, database system e.g. face database system, camera system and passport photograph automation system.
   ADVANTAGE - The image recognition results are evaluated based on the evaluation pattern, thus making the evaluation of the image quality independent of subjective characteristics of a person in the passport and identification offices, such that the examination of the image is performed for verification of a biometric efficiency of the image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer program product comprising instructions for performing in image quality evaluation method
   (2) an user-interface for an electronic device to evaluate the quality of the image
   (3) an electronic device with a unit for inputting image data
   (4) an evaluation file comprising an evaluation schema for defining image characteristics.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a method for evaluating quality of an image. `(Drawing includes non-English language text)`.
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The data file contains valuation criteria data type with an assigned criterion e.g. JPEG or JPEG2000.
PD DE102006005617-A1   16 Aug 2007   G06K-009/62   200755   Pages: 22   German
   WO2007090727-A1   16 Aug 2007   G06F-017/24   200755      German
   EP1984851-A1   29 Oct 2008   G06F-017/24   200876      German
   CN101379488-A   04 Mar 2009   G06F-017/24   200921      Chinese
   CA2641355-A1   16 Aug 2007   G06F-017/24   200936      English
   IN200804416-P4   13 Mar 2009   G06F-017/24   200960      English
   RU2008135986-A   20 Mar 2010      201104      Russian
   RU2437154-C2   20 Dec 2011      201204      Russian
   CN104217201-A   17 Dec 2014   G06K-009/03   201509      Chinese
   EP1984851-B1   11 Mar 2015   G06K-009/03   201519      German
   ES2538780-T3   24 Jun 2015   G06K-009/03   201545      Spanish
   IN289503-B   17 Nov 2017   G06F-017/24   201779      English
   CA2641355-C   29 May 2018   G06T-007/00   201838      English
UT DIIDW:2007562427
ER

PT P
PN CN1885310-A; CN100458831-C
TI Human face model training module and method, human face real-time certification system and method.
AB    NOVELTY - The invention relates to a face model training module, relative method, and a face real-time identify system, and method, wherein in the identification, first using face sample image, to supply one face model supporting vector machine to each user; via collecting the video image input by camera, searching and checking the face of image, and tracking and identifying the image; then automatically marking the organ character point of face, to pre-treat checked face; calculating the Gabor character of face image after pretreatment; selecting low-dimension character vector from high-dimension Gaborcharacter; inputting the selected low-dimension character vector into face model, to process face recognition, to feedback similarity data of each face model; based on said similarity data, outputting final face identifying result. The invention can improve the right rate of face recognition identification.
PD CN1885310-A   27 Dec 2006   G06K-009/00   200730      Chinese
   CN100458831-C   04 Feb 2009   G06K-009/00   200968      Chinese
UT DIIDW:2007303659
ER

PT P
PN CN1811793-A; CN100375108-C
TI Automatic positioning method for characteristic point of human faces comprises human facial figure and texture information, establishing changeable shape and texture pattern, initializing model parameter.
AB    NOVELTY - Present invention is a fast human face characteristic point positioning method. Said algorithm can fast locate out a lot of facial prominent feature point position to an arbitrary inputted human facefront or side (deflection angle deflection angle within 45 degree) digital image. Said algorithmic can expand to use on other objective characteristic point positioning. Said method comprehensive utilizes human facial figure and texture information, establishing changeable shape and texture pattern, initializing model parameter, adopting real time AAM and genetic algorithm genetic algorithm to optimize shape factor, finally fine adjusting part characteristic point through edge detection and complexion interval detective method, to obtain precise characteristic point positioning.
PD CN1811793-A   02 Aug 2006   G06K-009/00   200732      Chinese
   CN100375108-C   12 Mar 2008   G06K-009/00   200840      Chinese
UT DIIDW:2007328487
ER

PT P
PN WO2006073076-A1; JP2006190201-A; EP1835460-A1; CN101138007-A; US2009041340-A1; JP4613617-B2; CN101138007-B; EP1835460-B1; US8582887-B2; EP1835460-A4
TI Image processing system for recognition of person's image, compares characteristic point extracted from learning input image with that of learning model image.
AB    NOVELTY - The system extracts characteristic point from a learning model image, from which characteristic amount is obtained and registered in a learning model dictionary registration unit. A characteristic point is extracted from learning input image containing a model object contained in the learning model image. A characteristic amount is extracted based on the characteristic point, and compared to registered characteristic amount. The characteristic amount having frequent usage is registered for recognition processing, based on the comparison result.
   USE - For processing image of person's face, for recognition of person's face using robot.
   ADVANTAGE - The characteristic amount of image is extracted easily.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for learning apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the image processing system. (Drawing includes non-English language text).
PD WO2006073076-A1   13 Jul 2006   G06T-007/00   200655   Pages: 44   Japanese
   JP2006190201-A   20 Jul 2006   G06T-007/00   200655   Pages: 22   Japanese
   EP1835460-A1   19 Sep 2007   G06K-009/62   200763      English
   CN101138007-A   05 Mar 2008   G06T-007/00   200840      Chinese
   US2009041340-A1   12 Feb 2009   G06K-009/46   200912      English
   JP4613617-B2   19 Jan 2011   G06T-007/00   201106   Pages: 21   Japanese
   CN101138007-B   08 Dec 2010   G06T-007/00   201113      Chinese
   EP1835460-B1   01 Aug 2012   G06K-009/62   201250      English
   US8582887-B2   12 Nov 2013   G06K-009/00   201374      English
   EP1835460-A4   18 Mar 2009   G06K-009/62   201746      English
UT DIIDW:2006538748
ER

PT P
PN CN1801181-A
TI Robot capable of automatically recognizing face and vehicle license plate.
AB    NOVELTY - The invention discloses a robot to identify the moving face and license plate number in the real environment, which comprises the following parts: two CMOS colorful cameras, one image disposal board,one individual computer, 5-freedom degree binocular stereo vision device, 5-direct current servo motor, one direct current servo driver, one data gathering card, face identification system, license plate number identification system, fingerprint identification device and voice identification device. The invention is fit for monitoring real-time personnel and vehicle of penal scout, entry and exit customs pass and military place.
PD CN1801181-A   12 Jul 2006   G06K-009/00   200726      Chinese
UT DIIDW:2007255874
ER

PT P
PN US2006092455-A1; JP2006133847-A; US7586635-B2
TI Print data output system has management server which extracts print data corresponding to print job associated with user identification of authenticated user from database.
AB    NOVELTY - A user authentication unit executes user authentication based on comparison of face image data stored in database and face image data received from information output unit (200). A management server extracts print data corresponding to print job associated with user identification (ID) of authenticated user from database (306) and transmits print data to output unit (200).
   USE - Print data output system.
   ADVANTAGE - The print data is reliably provided to the user.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) print data output unit;
   (2) information processor;
   (3) information processing method; and
   (4) storage medium storing information processing program.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic diagram of the print data output system.
   print job issuing user (171)
   print reception user (181)
   print data output units (200,251,252)
PD US2006092455-A1   04 May 2006   G06F-015/00   200633   Pages: 26   English
   JP2006133847-A   25 May 2006   G06F-003/12   200635   Pages: 28   Japanese
   US7586635-B2   08 Sep 2009   G06F-015/00   200959      English
UT DIIDW:2006315905
ER

PT P
PN CN1704966-A; CN1323370-C
TI Method for detecting pornographic images for sexy images characterizing and finally sorting the images based on the extracted image visual characters to judge sexy images.
AB    NOVELTY - This invention discloses a test method for sexy images characterizing in removing images with few skin color pixels, carrying out man-face detect to judge if the image contains man-faces, setting up abody skin color model by the detected color distribution information of the man-face zone, detecting the body skin zone in the image, extracting the visual characters with high level semantic contents related to man-faces, finally sorting the images based on the extracted image visual characters to judge if the image is sexy.
PD CN1704966-A   07 Dec 2005   G06K-009/64   200629      Chinese
   CN1323370-C   27 Jun 2007   G06K-009/64   200803      Chinese
UT DIIDW:2006274495
ER

PT P
PN US2005068165-A1; WO2005031658-A2; US6911907-B2; WO2005031658-A3
TI Site e.g. office, security providing method, involves re-establishing identity of individual based on comparison between individual scanned at one checkpoint and new scan at another checkpoint.
AB    NOVELTY - The method involves capturing a facial profile of an individual from two sensors (106, 108) at a checkpoint (104). An identity of the individual is established and predicted information is generated based on the profile. A set of three dimensional structures is generated by scanning the individual at another checkpoint and is compared with a new scan of the individual to re-establish the identity of the individual.
   USE - Used for providing security to a site e.g. office, shopping mall, airport, warehouse, home, building, sports arena, country border.
   ADVANTAGE - The method re-establishes the identity of the individual at latter checkpoints even without universal coverage of the site, thus performing identification and tracking confidently.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a system for providing security
   (B) a computer readable medium having instructions for performing a method for providing security.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a system for providing security for a site.
   Individual (102)
   Checkpoints (104, 110)
   Sensors (106, 108, 112)
   Network (114)
   Computing device (116)
PD US2005068165-A1   31 Mar 2005   G08B-019/00   200532   Pages: 7   English
   WO2005031658-A2   07 Apr 2005   G07C-009/00   200532      English
   US6911907-B2   28 Jun 2005   G08B-019/00   200542      English
   WO2005031658-A3   09 Jun 2005   G07C-009/00   201217      English
UT DIIDW:2005312841
ER

PT P
PN WO2004029885-A1; AU2003256021-A1; EP1547024-A1; JP2006500675-W; US2005280502-A1; CN1685371-A; KR2005067396-A; EP1547024-B1; DE60318226-E; DE60318226-T2; JP4515910-B2; KR971451-B1; US7930555-B2; CN1685371-B
TI Personal data retrieval apparatus e.g. for contact details, matches captured image of target person with candidate image data and retrieves personal data of target person.
AB    NOVELTY - A search engine matches an image of a target person captured by an image acquisition devices (12,22) with the candidate image data, and retrieves the personal data (16,26) of target person. The displays (14,24) display the retrieved personal data. The controllers (17,27) control third party access to the stored personal data relating to the candidate.
   USE - For retrieving personal data such as contact details including name, address, facsimile number, electronic mail address and telephone number, professional information including job title, employer details, membership of professionals and/or specialist interest groups, social information such as hobby, membership or affiliation with club and society. of target person using image recognition system, in social or business meeting environment.
   ADVANTAGE - Since the captured image data of target person is compared with candidate image data for data retrieval, contact details are obtained without face-to-face, verbal or other close contact.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) portable electronic device;
   (2) system for providing personal information related to target person;
   (3) method of obtaining information related to target person;
   (4) computer program product for personal data retrieval; and
   (5) personal data retrieval program.
   DESCRIPTION OF DRAWING(S) - The figure shows the schematic block diagram of personal data retrieving device.
   image acquisition device (12)
   display device (14)
   image data items (15,25,35)
   controllers (17,27,37)
   personal data (26,36)
PD WO2004029885-A1   08 Apr 2004   G07C-009/00   200427   Pages: 29   English
   AU2003256021-A1   19 Apr 2004   G07C-009/00   200462      English
   EP1547024-A1   29 Jun 2005   G07C-009/00   200543      English
   JP2006500675-W   05 Jan 2006   G06F-017/30   200603   Pages: 18   Japanese
   US2005280502-A1   22 Dec 2005   G07C-009/00   200603      English
   CN1685371-A   19 Oct 2005   G07C-009/00   200612      Chinese
   KR2005067396-A   01 Jul 2005   G06K-009/80   200643      
   EP1547024-B1   19 Dec 2007   G07C-009/00   200802      English
   DE60318226-E   31 Jan 2008   G07C-009/00   200810      German
   DE60318226-T2   04 Dec 2008   G07C-009/00   200882      German
   JP4515910-B2   04 Aug 2010   G06F-017/30   201051   Pages: 14   Japanese
   KR971451-B1   22 Jul 2010   G06K-009/80   201056      
   US7930555-B2   19 Apr 2011   G06F-021/24   201128      English
   CN1685371-B   06 Apr 2011   G06K-009/00   201168      Chinese
UT DIIDW:2004295787
ER

PT P
PN US2003161500-A1; US7035431-B2
TI Automatic probabilistic pattern tracking system for sequencing moving images, predicts target state by tracking pattern in target data using acquired observation likelihood function and target data.
AB    NOVELTY - A set of exemplars learned from the set of training data stored in a database (220) is clustered. An observation likelihood function is generated for each exemplar cluster, based on computed distance between representative exemplar in clusters. The pattern in the target data stored in a database (270) is tracked, by using the acquired observation likelihood function and target data to predict the target state.
   USE - For tracking of patterns in sequence of moving person or running people, facial, mouth and tongue emotion images using exemplars generated from training data.
   ADVANTAGE - Enables tracking many types of patterns in walking person image using exemplars, efficiently.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) set of observation likelihood functions generation method;
   (2) computer implemented process for tracking patterns; and
   (3) computer readable medium storing executable instructions program for automatically tracking patterns
   DESCRIPTION OF DRAWING(S) - The figure shows an exemplary architectural view of the automatic probabilistic pattern tracking system.
   data analysis module (210)
   training data database (220)
   database for target data (270)
   tracking output module (280)
   output device (285)
PD US2003161500-A1   28 Aug 2003   G06K-009/00   200377   Pages: 25   English
   US7035431-B2   25 Apr 2006   G06K-009/00   200628      English
UT DIIDW:2003830275
ER

PT P
PN WO2003019475-A1; EP1343115-A1; CN1476589-A; KR2004028660-A; JP2003523462-X; US2005036649-A1; EP1343115-B1; CN1273912-C; DE60216411-E; DE60216411-T2; US7369686-B2; JP2009157948-A; JP4333364-B2; KR941209-B1; JP4609584-B2; EP1343115-A4
TI Robot for face recognition application, includes face recognition unit with support vector machine, to map facial feature extraction result onto non-linear space, for user face identification.
AB    NOVELTY - The robot has a face extraction unit comprising filters with different selectivity and frequency components, extract the facial features from an image captured by a CCD camera. A face recognition unit including a support vector machine, maps the extraction result onto non-linear space, thereby distinguishing face distinctly.
   USE - Robot for recognizing user face.
   ADVANTAGE - Enables robot to recognize user face rapidly in dynamically changing environment within specific time.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) face recognition method; and
   (2) face recognition apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the robot. (Drawing includes non-English language text).
PD WO2003019475-A1   06 Mar 2003   G06T-007/00   200330   Pages: 70   Japanese
   EP1343115-A1   10 Sep 2003   G06K-009/00   200367      English
   CN1476589-A   18 Feb 2004   G06T-007/00   200430      Chinese
   KR2004028660-A   03 Apr 2004   G06K-009/46   200451      
   JP2003523462-X   16 Dec 2004   G06T-007/00   200482      Japanese
   US2005036649-A1   17 Feb 2005   G06K-009/00   200514      English
   EP1343115-B1   29 Nov 2006   G06K-009/00   200680      English
   CN1273912-C   06 Sep 2006   G06F-015/18   200706      Chinese
   DE60216411-E   11 Jan 2007   G06K-009/00   200706      German
   DE60216411-T2   04 Oct 2007   G06K-009/00   200766      German
   US7369686-B2   06 May 2008   G06K-009/00   200834      English
   JP2009157948-A   16 Jul 2009   G06T-007/00   200947   Pages: 42   Japanese
   JP4333364-B2   16 Sep 2009   G06T-007/00   200961   Pages: 31   Japanese
   KR941209-B1   10 Feb 2010   G06K-009/46   201034      
   JP4609584-B2   12 Jan 2011   G06T-007/00   201104   Pages: 41   Japanese
   EP1343115-A4   09 Mar 2005   G06K-009/00   201747      English
UT DIIDW:2003312942
ER

PT P
PN WO9843216-A2; DE19712844-A1; EP970435-A2; KR2001005577-A; EP970435-B1; DE59810342-G; ES2212294-T3; WO9843216-A3; KR320626-B1
TI Three=dimensional face identification method e.g. for cash dispenser and building access - illuminating face by sequence of light patterns with simultaneous photographing from other direction.
AB       A digital micro-mirror (3) is illuminated by a light source (1) and is controlled in such a way that one or more coded lighting patterns are projected one after the other onto the object surface (7).
   The object (7) illuminated by the sequence of light patterns is photographed by a camera (6) from a direction different from the illumination direction and from this the topography of the object (7) is computed with great accuracy in a control-and evaluation unit (11).
   USE -   For quick identification of objects, in particular three-dimensional identification of users of cash dispensing points.
   ADVANTAGE -   Speedier ascertaining of topography of three- dimensional objects by coded illumination and TV processing.
PD WO9843216-A2   01 Oct 1998   G08B-000/00   199845   Pages: 11   German
   DE19712844-A1   08 Oct 1998   G07C-011/00   199846      German
   EP970435-A2   12 Jan 2000   G06K-009/00   200008      German
   KR2001005577-A   15 Jan 2001   C21C-005/46   200151      
   EP970435-B1   03 Dec 2003   G06K-009/00   200403      German
   DE59810342-G   15 Jan 2004   G06K-009/00   200406      German
   ES2212294-T3   16 Jul 2004   G06K-009/00   200447      Spanish
   WO9843216-A3   14 Jan 1999   G08B-000/00   201729      German
   KR320626-B1   19 Jan 2002   G06K-009/00   200254      
UT DIIDW:1998532247
ER

PT P
PN JP10040386-A
TI Iris recognition system for identifying individuals - has camera control unit which obtains positional data of eye from image analysis unit and accordingly moving camera, takes only photograph of eye such that iris data of eye is acquired.
AB       The system includes a main camera (1) which takes the photograph of the individual's face. An image analysis unit (3) analyses the photographed image of the face, and detects only the position of the eye. A camera control unit (4) obtains the positional data of the eye from the image analysis part and accordingly a moving camera (2), takes only the photograph of the eye such that iris data of the eye is acquired.
   ADVANTAGE -   Unnecessiates need for optical process system along with camera for zooming purpose.
PD JP10040386-A   13 Feb 1998   G06T-007/00   199817   Pages: 3   Japanese
UT DIIDW:1998184697
ER

PT P
PN JP9134194-A; US5806036-A
TI Image recognition system for telephone set - has recognition processing logic circuit connected to data communication path, that identifies telephone user based on image data containing characteristics of face of telephone user.
AB       The system has a housing (105) that accommodates and attaches a transmitter (104). A camera (101) gathers image data containing the characteristics of the face of a telephone user. A data communication path (103) connected to the camera, transmits the image data output by the camera.
   A recognition processing logic circuit connected to the data communication path, identifies the user based on the image data transmitted through the data communication path.
   ADVANTAGE -   Improves speech recognition that is based on sound data by using user recognition based on image data. Maintains recognition precision even when applied to different users with different types of languages.
PD JP9134194-A   20 May 1997   G10L-003/00   199730   Pages: 6   Japanese
   US5806036-A   08 Sep 1998   G10L-005/06   199843      English
UT DIIDW:1997329875
ER

PT P
PN DE4015048-A; GB2233866-A; JP2304595-A; US5036251-A; DE4015048-C; KR9203648-B1
TI TV screen display automatic tuning appts. - has video camera providing feedback for compute controlled tuning of image.
AB       The computer controlled television system (12) has the displayed image monitored by a video camera (16) that provides an input to a matrix (18) to generate inputs to 3 A/D converters (20, 22,24). Each converter is coupled to a synchronizing circuit (26) that is coupled to a pattern generator (14) supplying the TV controller.
   The A/D converter outputs are received by a pattern recognition stage (28) that identifies differences in relation to programmed values entered as references. The difference signals are processed by a (30) to provide deflection control signals to adjust the display.
   ADVANTAGE -   Provides automatic adjustment of scree image to tune set. @(5pp Dwg.No.1/2)@
PD DE4015048-A   15 Nov 1990      199047      German
   GB2233866-A   16 Jan 1991      199103      English
   JP2304595-A   18 Dec 1990      199105      Japanese
   US5036251-A   30 Jul 1991      199133      English
   DE4015048-C   26 Mar 1992      199213   Pages: 6   German
   KR9203648-B1   06 May 1992   H04N-005/222   201561      
UT DIIDW:1990349730
ER

PT P
PN CN108009528-A; WO2019128367-A1; CN108009528-B
TI Triplet loss based human face recognition method, involves calculating cosine distance between characteristic vectors, comparing cosine distance with predetermined threshold, and outputting face recognition result based on comparison result.
AB    NOVELTY - The method involves obtaining a certificate photo and character of a scene picture based on face recognition request. Face detection process is performed on the certificate photo and the scene picture. Key point location in the scene picture is determined. Convolutional neural network model is established for obtaining first characteristic vector of the scene picture. Cosine distance between the first vector and a second characteristic vector is calculated. The cosine distance is compared with a predetermined threshold. Face recognition result is output based on a comparison result.
   USE - Triplet loss based human face recognition method.
   ADVANTAGE - The method enables improving reliability of face recognition.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a triplet loss based human face recognition device
   (2) a computer device
   (3) a storage medium for storing a set of instructions for executing a triplet loss based human face recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a triplet loss based human face recognition method. '(Drawing includes non-English language text)'
PD CN108009528-A   08 May 2018   G06K-009/00   201835   Pages: 21   Chinese
   WO2019128367-A1   04 Jul 2019   G06K-009/00   201950      Chinese
   CN108009528-B   07 Apr 2020   G06K-009/00   202032      Chinese
UT DIIDW:201837361P
ER

PT P
PN CN107871134-A
TI Human face image detecting method, involves screening multiple selected areas according to face confidence level, and eliminating and polymerizing detection frame according to selected areas for obtaining face detection result.
AB    NOVELTY - The method involves classifying a to-be-detected image using a first pre-trained convolutional neural network model to determine a first face confidence level of input area in the to-be-detected image. A candidate area is sorted and processed using a second pre-trained convolution neural network model to determine a second face confidence level of the candidate area. Multiple selected areas are screened according to the second face confidence level. A detection frame is eliminated and polymerized according to the selected areas for obtaining a face detection result.
   USE - Human face image detecting method.
   ADVANTAGE - The method enables realizing small sized input area of the first pre-trained convolutional neural network so as to increase face image detecting speed. The method enables utilizing two pre-trained convolutional neural network models for different depths and performing of secondary classification on the candidate area so as to ensure accurate class predictor, realize effectively filtering of false sample and improve face image detecting performance.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face image detection device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face image detecting method. '(Drawing includes non-English language text)'
PD CN107871134-A   03 Apr 2018   G06K-009/62   201826   Pages: 27   Chinese
UT DIIDW:201827466S
ER

PT P
PN CN107506722-A
TI Depth-sparse convolutional neural network based human face emotion recognition method, involves identifying input value in convolutional neural network and outputting recognition result based on emotion category corresponding to label value.
AB    NOVELTY - The method involves constructing a convolution layer and a sub-sampling layer based on emotion recognition classification. A principal component analysis (PCA) feature map is obtained based on a training set and a label value of a corresponding emotion input depth sparse convolutional neural network. Depth value is optimized based on Nesterov accelerated gradient descent (NAGD) algorithm in the sparse convolutional neural network. Input value is identified in the convolutional neural network and recognition result is outputted based on an emotion category corresponding to a label value.
   USE - Depth-sparse convolutional neural network based human face emotion recognition method.
   ADVANTAGE - The method enables optimizing the NAGD algorithm and preventing the NAGD algorithm from advancing too fast or too slow to enhance response capability, predicting ability and foreseeability of the algorithm to obtain better local optimum result effectively, thus improving generalized face emotion recognition efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a process flow diagram illustrating a depth-sparse convolutional neural network based human face emotion recognition method. '(Drawing includes non-English language text)'
PD CN107506722-A   22 Dec 2017   G06K-009/00   201806   Pages: 22   Chinese
UT DIIDW:201789334X
ER

PT P
PN CN107423701-A; CN107423701-B
TI Generated-type human face based network unsupervised feature learning method, involves performing generated random vectors input collecting process in trained network, and extracting human face characteristic vector to generate image set.
AB    NOVELTY - The method involves collecting an original face image for performing an original face image pre-processing operation. An original face image conversion process is performed to set size of a human face training image. The human face training image is generated by using a converted target network in depth construction of convolution. Training data is generated in network training. A generated random vectors input collecting process is performed in a trained network. Human face characteristic vector is extracted to generate image set.
   USE - Generated-type human face based network unsupervised feature learning method.
   ADVANTAGE - The method enables realizing unsupervised feature learning, process in effective manner by using advanced regression network to learn a reverse target-generation network and improving learning effect and identification precision.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a generated-type human face based network unsupervised feature learning device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a generated-type human face based network unsupervised feature learning method. '(Drawing includes non-English language text)'
PD CN107423701-A   01 Dec 2017   G06K-009/00   201801   Pages: 17   Chinese
   CN107423701-B   01 Sep 2020   G06K-009/00   202076      Chinese
UT DIIDW:2017837741
ER

PT P
PN CN106529447-A; CN106529447-B
TI Small sample face recognition method involves processing face image to be recognized, extracting discrete wavelet transform (DWT) low-frequency subband image, and predicting classification results using softmax classifier.
AB    NOVELTY - The method involves transforming human face image collected from a computer USB interface from red, green, blue (RGB) space to grey space to obtain grey image. A gradient value of each pixel size of normalization grey image is calculated. The minimum error is calculated using gradient descent process so as to obtain the optimal network weights and bias. The face image to be recognized is processed, and the DWT low-frequency subband image is extracted. Softmax classifier is used to predict the classification results so as to realize face recognition.
   USE - Small sample face recognition method.
   ADVANTAGE - The small sample face recognition problem is solved effectively. The face recognition rate is increased. The characters classification fusion is more accurate.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating the small sample face recognition process. (Drawing includes non-English language text)
PD CN106529447-A   22 Mar 2017   G06K-009/00   201725   Pages: 15   Chinese
   CN106529447-B   21 Jan 2020   G06K-009/00   202012      Chinese
UT DIIDW:201721501T
ER

PT P
PN CN105844206-A
TI Identity authentication method, involves comparing record information with identity certificate information by certificate holder, and obtaining identity document image for detecting face region to carry out point identification.
AB    NOVELTY - The method involves collecting identity certificate of an image. A human face image is collected from the identity certificate. Record information is obtained from the collected identity face image. Relative certificate information is collected from a collected human face image. Record information is compared with identity certificate information by a certificate holder. An identity document image is obtained for detecting a face region to carry out human face key point identification. A human face key point is pointed for identifying a human face.
   USE - Identity authentication method.
   ADVANTAGE - The method enables satisfying user identity authentication in an accurate manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an identity authentication device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an identity authentication method. '(Drawing includes non-English language text)'
PD CN105844206-A   10 Aug 2016   G06K-009/00   201658   Pages: 20   Chinese
UT DIIDW:2016520149
ER

PT P
PN CN105678250-A; US2017193286-A1; US10102421-B2; CN105678250-B
TI Video based human face identifying method, involves obtaining characteristic extracting order, and detecting multi-characteristic vector quantity according to pre-determined dimension characteristic vector to identify target human face.
AB    NOVELTY - The method involves performing target human face characteristic extraction process based on a multi-frame image. A multi-characteristic vector is detected corresponding to a target human face multi-frame image vector. A characteristic extracting order is obtained. Multi-characteristic vector quantity is detected according to pre-determined dimension characteristic vector to identify a target human face. A continuous multi-frame image video is displayed. Multi-characteristic vector generation process is performed. The target human face is determined corresponding to a human face image area.
   USE - Video based human face identifying method.
   ADVANTAGE - The method enables realizing human face identifying operation based on video image associated with time dimension frame characteristics for image information processing operation so as to increase accuracy of human face identification.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a video based human face identifying device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a video based human face identifying method. '(Drawing includes non-English language text)'
PD CN105678250-A   15 Jun 2016   G06K-009/00   201645   Pages: 12   Chinese
   US2017193286-A1   06 Jul 2017   G06K-009/00   201747      English
   US10102421-B2   16 Oct 2018   G06K-009/00   201871      English
   CN105678250-B   11 Oct 2019   G06K-009/00   201981      Chinese
UT DIIDW:2016409299
ER

PT P
PN CN105426723-A
TI Identity-based authentication method for detection of living body, involves passing authentication for access to living body only when face recognition and voiceprint identification verification are successful.
AB    NOVELTY - The method involves obtaining (101) a user's voice and video signals. The user's voice and video signals are detected (102) in vivo testing. The voiceprint identification and face recognition are carried out (104) if the test detection result is greater than the threshold. The authentication for an access to the living body is passed only when the face recognition and voiceprint identification verification are successful. The authentication for the access to the living body is stopped (105) when the face recognition and voiceprint identification verification fails.
   USE - Identity-based authentication method for detection of living body.
   ADVANTAGE - The accuracy of authentication, security and reliability for detection of living body is increased.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an identity-based authentication system for detection of living body.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an identity-based authentication method for detection of living body. (Drawing includes non-English language text)
   Step for obtaining a user's voice and video signals (101)
   Step for detecting user's voice and video signals in vivo testing (102)
   Step for comparing vivo detection results with a predetermined threshold (103)
   Step for carrying out voiceprint identification and face recognition (104)
   Step for stopping authentication for access to the living body (105)
PD CN105426723-A   23 Mar 2016   G06F-021/32   201625   Pages: 17   English
UT DIIDW:2016202855
ER

PT P
PN WO2016018488-A2; WO2016018488-A9; WO2016018488-A3; AU2015297036-A1; EP3140780-A2; US2017123492-A1; IN201647041742-A; KR2017047195-A; CN107087431-A; JP2017526079-W; AU2015297036-B2; CN107087431-A8; US2018011533-A9; EP3140780-A4; JP6550460-B2; US10564714-B2; EP3140780-B1; KR2020127267-A; KR2173699-B1; CN107087431-B
TI System for providing continuous biometric identification of user, has processing unit that is coupled to electronic display identifies user based on iris and authorizes user to perform actions upon confirming user's identity.
AB    NOVELTY - The system has a headgear that is configured to be worn on a user's head. Multiple cameras (125) are mounted on the headgear such that the cameras are oriented towards the user's eye from different angles. A processing unit (165) is coupled to the detector and electronic display to analyze images from the cameras to identify an iris (115) of the eye. The image is analyzed from the first camera to identify features of the iris. The user is identified based on the features of the iris. The user is authorized to perform actions upon confirming the user's identity.
   USE - System for providing continuous biometric identification of user.
   ADVANTAGE - The user's decision making process providing information interactively through eye signals is facilitated. The multiple cameras enhance the ability to image an iris on-axis over a wider range of viewing angles. The construction and viewing synthetic faces can greatly reduce the bandwidth required among conferencing locations.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for providing continuous biometric identification of a user wearing a headgear.
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective view of the locations of multiple illumination sources and multiple cameras pointing at a single iris.
   Iris (115)
   Camera (125)
   Central pupil (145)
   Surrounding sclera (150)
   Processing unit (165)
PD WO2016018488-A2   04 Feb 2016   G06K-009/00   201612   Pages: 81   English
   WO2016018488-A9   24 Mar 2016      201622      English
   WO2016018488-A3   12 May 2016   G06K-009/00   201633      English
   AU2015297036-A1   24 Nov 2016   G06F-021/32   201679      English
   EP3140780-A2   15 Mar 2017   G06K-009/00   201720      English
   US2017123492-A1   04 May 2017   G06F-003/01   201731      English
   IN201647041742-A   28 Apr 2017   G06K-009/00   201733      English
   KR2017047195-A   04 May 2017   G06F-021/32   201733      
   CN107087431-A   22 Aug 2017   G06F-003/01   201759      Chinese
   JP2017526079-W   07 Sep 2017   G06T-001/00   201760   Pages: 63   Japanese
   AU2015297036-B2   28 Sep 2017   G06F-021/32   201767      English
   CN107087431-A8   22 Dec 2017   G06F-003/01   201803      Chinese
   US2018011533-A9   11 Jan 2018   G06F-003/01   201805      English
   EP3140780-A4   11 Apr 2018   G06K-009/00   201829      English
   JP6550460-B2   24 Jul 2019   G06T-001/00   201957   Pages: 54   Japanese
   US10564714-B2   18 Feb 2020   G06F-003/01   202016      English
   EP3140780-B1   04 Nov 2020   G06F-003/01   202090      English
   KR2020127267-A   10 Nov 2020   G06F-021/32   202093      
   KR2173699-B1   03 Nov 2020   G06F-021/32   202097      
   CN107087431-B   05 Feb 2021   G06F-003/01   202115      Chinese
UT DIIDW:201608948Q
ER

PT P
PN WO2015123647-A1; US2015302027-A1; US9501498-B2; US2017039442-A1; CN106462774-A; US10095945-B2; US2019012557-A1; CN106462774-B; US10832075-B2; US2021027084-A1
TI Object recognition ingestion system for use during recognition of e.g. car in automobile lot, has object ingestion engine for creating key frames bundles from descriptor model and storing key frame bundles in object recognition database.
AB    NOVELTY - The system (100) has an object ingestion engine (120) for instantiating a descriptor object model (127) from an object model (125), where the descriptor model comprises recognition algorithm descriptors for defining locations on the object model relative to model key frame points-of-views. The object ingestion engine creates a set of key frames bundles from the descriptor object model as a function of the set of model key frame points-of-views and stores the set of key frame bundles in an object recognition database (140) of the system.
   USE - Object recognition ingestion system for use during recognition or analysis of a shaped object e.g. three-dimensional object such as vehicle e.g. automobile or car, building, appliance, plant, toy, face, person and internal organ (all claimed), in an uncontrolled in-the-field setting e.g. grocery store and automobile lot. Can also be used for recognition of a food item, purchasable product, document, clothing, animal, book, weapon, laboratory equipment, bomb, stationery item and a file, in a school, military training field, garden, shopping mall, tourist attraction location, highway location, office, home and a hospital.
   ADVANTAGE - The system is designed such that real- world objects can be ingested into the object recognition database using canonical shapes. The system enables the object ingestion to be performed autonomously and quickly without a need of human interferences, and in an uncontrolled-in-the-field setting with use of existing image data of objects to build the objects database. The system allows the objects database to be updated with new-information, thus continuously improving accuracy of object recognition over time.
   DETAILED DESCRIPTION - The recognition algorithm descriptors include SIFT descriptors, FREAK descriptors, FAST descriptors, DAISY descriptors and BRISK descriptors.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of an object recognition ingestion ecosystem.
   Object recognition ingestion system (100)
   Object ingestion engine (120)
   Object model (125)
   Descriptor object model (127)
   Object recognition database (140)
PD WO2015123647-A1   20 Aug 2015   G06K-009/68   201556   Pages: 43   English
   US2015302027-A1   22 Oct 2015   G06F-017/30   201570      English
   US9501498-B2   22 Nov 2016   G06K-009/00   201677      English
   US2017039442-A1   09 Feb 2017   G06K-009/32   201712      English
   CN106462774-A   22 Feb 2017   G06K-009/68   201719      Chinese
   US10095945-B2   09 Oct 2018   G06K-009/00   201867      English
   US2019012557-A1   10 Jan 2019   G06K-009/32   201904      English
   CN106462774-B   24 Jan 2020   G06K-009/68   202012      Chinese
   US10832075-B2   10 Nov 2020   G06K-009/32   202091      English
   US2021027084-A1   28 Jan 2021   G06K-009/32   202111      English
UT DIIDW:201548201L
ER

PT P
PN CN104680131-A; CN104680131-B
TI Identity verification method for verifying e.g. second-generation resident identification card, involves obtaining identity verification result based on human face, gender and age matching degrees.
AB    NOVELTY - The method involves obtaining the identity certificate information, human face image and identity information containing gender and age information. The high definition human face image is obtained and gender matching degree is calculated. The age of human face image is determined. The high-definition human face image and identity certificate relevant human face image are compared to obtain human face matching degree. The identity verification result is obtained based on human face matching degree, gender matching degree and age matching degree.
   USE - Identity verification method for verifying second-generation resident identification card, passport and driving license.
   ADVANTAGE - The identity can be verified accurately based on the identity certificate information and human face characteristic recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for identity verification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the identity verification system. (Drawing includes non-English language text)
PD CN104680131-A   03 Jun 2015   G06K-009/00   201562   Pages: 12   Chinese
   CN104680131-B   11 Jan 2019   G06K-009/00   201908      Chinese
UT DIIDW:201547782Y
ER

PT P
PN WO2015054562-A1; KR2016068916-A; CN105814516-A; EP3055754-A1; US2016259977-A1; US9922253-B2; KR2018066276-A; US2018211112-A1; CN105814516-B; KR2154912-B1; US11250263-B2; US2022092308-A1
TI Method for providing gaze-driven augmented reality to user for sightseeing application for wireless transmit/receive unit, involves receiving information regarding objects in region of interest from server, and displaying information.
AB    NOVELTY - The method involves estimating a gaze point of a user to define a region of interest (ROI) (1508). An emotional state of the user is estimated using voice recognition and facial recognition of the user. Information associated with the ROI and the emotional state of the user is sent to a server (1510). Information regarding multiple objects in the ROI is received from the server and displayed. The information about the objects in the ROI is refined or filtered based on the emotional state of the user. Size of the ROI is adjusted based upon a number of objects detected and a user activity.
   USE - Method for providing gaze-driven AR to a user for a sightseeing application, a gaming application and a navigation application for a wireless transmit/receive unit (WTRU) and a gyroscope. Uses include but are not limited to a mobile station, a fixed or mobile subscriber unit, a pager, a cellular telephone, a personal digital assistant (PDA), a smartphone, a laptop, a netbook, a personal computer, a wireless sensor and consumer electronics.
   ADVANTAGE - The method enables enhancing user experience in an AR system by presenting information relevant to the user and characterizing a search space by estimating the user's direction of view, so that quality of results can be improved and the usage of processing and network resources can be reduced. The method enables estimating a user's gaze point that is used to search for and present information relating to areas to which the user is focusing a direction of view so as to facilitate or enable modes of interactivity and/or user interfaces that can be controlled by the direction of view of the user. The method ensures that results can be superimposed on preexisting images or video or overlaid on images or video captured using a camera, so that little information can be sent to and received from the server so as to reduce communication overhead and improve response time. The method enables a client e.g. client-server model, to extract relevant features from the captured images to obtain a set of descriptors that are used by a server to perform the search, so that the amount of information sent over a network can be significantly reduced in order to improve system response time and a subset of a database can be cached at the client to improve system performance if client capability is high.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a device for providing gaze-driven augmented reality (AR) to a user.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a gaze-driven AR system.
   Gaze-point detection/estimation subsystem (1502)
   Emotional state detection/estimation subsystem (1504)
   Eye-facing cameras (1506)
   ROI (1508)
   Server (1510)
PD WO2015054562-A1   16 Apr 2015   G06F-003/01   201527   Pages: 54   English
   KR2016068916-A   15 Jun 2016   G06F-003/01   201642      
   CN105814516-A   27 Jul 2016   G06F-003/01   201652      Chinese
   EP3055754-A1   17 Aug 2016   G06F-003/01   201655      English
   US2016259977-A1   08 Sep 2016   G06K-009/00   201660      English
   US9922253-B2   20 Mar 2018   G09G-005/00   201820      English
   KR2018066276-A   18 Jun 2018   G06F-003/01   201843      
   US2018211112-A1   26 Jul 2018   G06K-009/00   201850      English
   CN105814516-B   30 Jul 2019   G06F-003/01   201959      Chinese
   KR2154912-B1   11 Sep 2020   G06F-003/01   202076      
   US11250263-B2   15 Feb 2022   A61B-005/00   202218      English
   US2022092308-A1   24 Mar 2022   G06K-009/00   202225      English
UT DIIDW:201524723N
ER

PT P
PN KR2014139730-A; KR1538935-B1
TI Method for classifying elements of face using depth difference, involves scanning pattern light or laser line on face of object through pattern generator and extracting interest area of elements of face from face image.
AB    NOVELTY - The method involves scanning (S10) a pattern light or a laser line on the face of the object through the pattern generator and extracting (S20) the interest area of the elements of face from the face image by removing the condition domain based on the neck domain and extracting the face image from the image of point group data. The information of the depth of each face element is acquired (S31) and the symmetric distance information of the face element is acquired (S33). A database of the category information of the extracted face components and the whole face is prepared (S40) and stored.
   USE - Method for classifying elements of face using depth difference.
   ADVANTAGE - The method involves scanning the pattern light or the laser line on the face of the object through the pattern generator and extracting the interest area of the elements of face from the face image by removing the condition domain based on the neck domain, and hence ensures simple and automatic classification of elements of face using depth difference.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a method for classifying elements of face using depth difference. (Drawing includes non-English language text).
   Scanning the pattern light or the laser line on the face of the object through the pattern generator (S10)
   Extracting the interest area of the elements of face from the face image (S20)
   Acquiring the information of the depth of each face element (S31)
   Acquiring the symmetric distance information of the face element (S33)
   Preparing a database of the category information of the extracted face components and the whole face (S40)
PD KR2014139730-A   08 Dec 2014   G06T-017/00   201501   Pages: 8   
   KR1538935-B1   24 Jul 2015   G06T-017/00   201552      
UT DIIDW:2014W35064
ER

PT P
PN CN103632138-A; CN103632138-B
TI Low rank chunk sparse representation based human face identifying method, involves receiving human face image from database, and obtaining integrated training data matrix and test matrix from training and test images.
AB    NOVELTY - The method involves receiving a human face image from a database. A training image and a test image are randomly selected. An integrated training data matrix and a test matrix are obtained from the training image and the test image. Decomposed sparse error is represented by minimizing decomposition rank of a low rank matrix. Simplifying process is performed to obtain target function. Spares coefficients are represented by a human face identify algorithm. Chunk processing process is performed. Spares coefficients are combined together.
   USE - Low rank chunk sparse representation based human face identifying method.
   ADVANTAGE - The method enables improving identifying speed, and ensuring identifying accuracy and efficiency and lower occurrence of camouflage and illumination change so as to improve precision and robustness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a low rank chunk sparse representation based human face identifying method.'(Drawing includes non-English language text)'
PD CN103632138-A   12 Mar 2014   G06K-009/00   201429   Pages: 17   Chinese
   CN103632138-B   28 Sep 2016   G06K-009/00   201673      Chinese
UT DIIDW:2014H64039
ER

PT P
PN CN103605958-A
TI Gray level co-occurrence matrix based wavelet analysis living body human face detect method, involves estimating average value and variance value, utilizing SVM classifier for obtaining picture of human face.
AB    NOVELTY - The method involves utilizing a camera for capturing a RGB color space of a human face input image. A human face area is determined from the RGB color space. The human face input image is utilized for converting a gray scale input image. Grey co-occurrence matrix value is calculated. The gray scale input image is fixed on a co-occurrence matrix. The grey co-occurrence matrix value is provided with an average value and a variance value. The average value and the variance value are estimated. A SVM classifier is utilized for obtaining a picture of a human face.
   USE - Gray level co-occurrence matrix based wavelet analysis living body human face detect method.
   ADVANTAGE - The method enables reducing calculation complexity and improving detecting accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a gray level co-occurrence matrix based wavelet analysis living body human face detect method.'(Drawing includes non-English language text)'
PD CN103605958-A   26 Feb 2014   G06K-009/00   201429   Pages: 13   Chinese
UT DIIDW:2014H18022
ER

PT P
PN CN103605972-A; CN103605972-B
TI Method for verifying human face in non-limiting environment using block depth neural network, involves comparing test images for different pictures of human face image, if distance is greater than classification threshold value.
AB    NOVELTY - The method involves detecting the human face image of human face area by normalization processing. The depth neural network is formed based on the characteristic of extraction of sub-image. The optimal depth neural network structure parameter is obtained. The new characteristic distance of human face is calculated. The test images for different pictures of human face image are compared, if the distance is greater than classification threshold value, and the test images for the human face image of same human body are compared, if the distance is less than the threshold value.
   USE - Method for verifying human face in non-limiting environment using block depth neural network.
   ADVANTAGE - The human face verification can be performed effectively.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the process for verifying human face in non-limiting environment using block depth neural network. (Drawing includes non-English language text)
PD CN103605972-A   26 Feb 2014   G06K-009/00   201430      Chinese
   CN103605972-B   15 Feb 2017   G06K-009/00   201716      Chinese
UT DIIDW:2014H32053
ER

PT P
PN CN103413113-A
TI Intelligent service robot emotional interactive method, involves inputting voice emotion input from emotion input module to binocular vision and microphone devices, and emotion expression module provided with sound system and motion module.
AB    NOVELTY - The method involves inputting voice emotion input from an emotion input module to a binocular vision device and a microphone device. A visual and voice input emotion fusion process is performed. Facial emotion expression is identified by an emotion recognition module. An emotion expression module is provided with a sound system and a motion module. The binocular vision device is mounted on a head of a robot. The microphone device is connected with a host through an USB interface. A robot head is moved along up, down, left and right directions.
   USE - Intelligent service robot emotional interactive method.
   ADVANTAGE - The method enables utilizing life science technology and a computer science technology so as to realize an intelligent service robot emotion interactive process. The method enables improving robot emotional interaction capability.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an intelligent service robot emotional interactive method. '(Drawing includes non-English language text)'
PD CN103413113-A   27 Nov 2013   G06K-009/00   201406   Pages: 15   Chinese
UT DIIDW:2014B73515
ER

PT P
PN US2012098968-A1; US8325986-B2
TI Imaging system for displaying information to driver of vehicle, has cruise control unit controlling lane departure warning, traffic sign recognition and object detection responsive to image processing.
AB    NOVELTY - The system has a camera comprising a field of view exterior of a vehicle, where the field of view is in forward direction of travel of the vehicle. A cruise control unit adjusts a light beam emanating from forward facing light of the vehicle responsive to image processing of frames of image data. The control unit controls adjustment of beam direction, visible intensity or range of the light beam emanating from a forward facing light responsive to image processing. The control unit controls lane departure warning, traffic sign recognition and object detection responsive to image processing.
   USE - Imaging system for displaying information to a driver of a vehicle.
   ADVANTAGE - The system detects, recognizes and reads traffic control signage along the side of the road along which the vehicle travels. The system can display information to the driver of the vehicle regarding the detected and recognized signage and provide a warning or alert signal to the driver if an unsafe or unwanted driving condition is encountered and the vehicle travels at speed that is over the speed limit or approaches a turn at too high.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an imaging system.
   Imaging device (14)
   Image processor (16)
   Display device (18)
   Vehicle speed sensor (24)
   Cruise control unit (28)
PD US2012098968-A1   26 Apr 2012   H04N-007/18   201230   Pages: 26   English
   US8325986-B2   04 Dec 2012   G06K-009/00   201280      English
UT DIIDW:2012E85810
ER

PT P
PN CN102375970-A; CN102375970-B
TI Human face based identity authentication method, involves analyzing gesture analyzing result and multi-frames of human face image authentication result when statistical analysis result is accordant with preset condition.
AB    NOVELTY - The method involves collecting an image pickup device of multiple frames to a human face image. Gesture information is obtained by the human face image. The human face image is compared with an appointed base image, where the comparison result is greater than or equal to a designated threshold value. An authentication result is passed to the human face image. A gesture analyzing result and multi-frames of a human face image authentication result are analyzed when a statistical analysis result is accordant with a preset condition. The human face image is formed between two key points.
   USE - Human face based identity authentication method.
   ADVANTAGE - The method enables utilizing a two-dimensional (2D) plane image to distinguish photo and real person, which avoids three-dimensional (3D) information. The method enables reducing picture identifying system identifying speed.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face based identity authentication device comprising a human face detection tracking module.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram of a human face based identity authentication method. '(Drawing includes non-English language text)'
PD CN102375970-A   14 Mar 2012   G06K-009/00   201224   Pages: 14   Chinese
   CN102375970-B   30 Mar 2016   G06K-009/00   201625      English
UT DIIDW:2012D68781
ER

PT P
PN JP4893862-B1; WO2012124142-A1; JP2012190350-A; EP2657903-A1; US2014072230-A1; US8750623-B2; EP2657903-B1; EP2657903-A4
TI Image processing apparatus for digital camera has identification part, which pinpoints area of pupil in face image based on ellipse showing outline of pupil, curve showing outline of upper lid, and curve showing outline of lower lid.
AB    NOVELTY - The image processing apparatus (6) has an identification part (34), which pinpoints the area of the pupil in the face image based on the ellipse showing the outline of pupil, the curve showing the outline of upper lid, and the curve showing the outline of lower lid. In a B spline curve or a Bezier curve, the curves that adapt the edge point located above the start-of-the-eye point or corner-of-the-eye point and the edge point located below the start-of-the-eye point or corner-of-the-eye point are identified as the outline of an upper lid and a lower lid, respectively.
   USE - Image processing apparatus for digital camera, used for pinpointing the region of a pupil.
   ADVANTAGE - The region of pupil is pinpointed correctly using the ellipse showing the outline of pupil and by excluding the edges, such as outline of eyelid mainly extended transversely, which become noise in the elliptical identification by differentiating to the horizontal direction in the eye. Misdetecting the outline of red-eye region as outline of pupil is prevented.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an image-processing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of the schematic structure of the digital camera. (Drawing includes non-English language text).
   Digital camera (1)
   Image processing apparatus (6)
   Identification part (34)
PD JP4893862-B1   07 Mar 2012   G06T-001/00   201218   Pages: 20   Japanese
   WO2012124142-A1   20 Sep 2012   G06T-001/00   201262      Japanese
   JP2012190350-A   04 Oct 2012   G06T-001/00   201265   Pages: 25   Japanese
   EP2657903-A1   30 Oct 2013   G06T-001/00   201371      English
   US2014072230-A1   13 Mar 2014   G06K-009/00   201420      English
   US8750623-B2   10 Jun 2014   G06K-009/48   201438      English
   EP2657903-B1   26 Jul 2017   G06T-001/00   201750      English
   EP2657903-A4   26 Mar 2014   G06T-001/00   201770      English
UT DIIDW:2012C88104
ER

PT P
PN CN102254169-A; CN102254169-B
TI Multi-camera based human face recognition method, involves tracking object appearing in preset monitoring sub-area, and utilizing multi-view face recognition for recognizing target when target enters preset monitoring sub-area.
AB    NOVELTY - The method involves positioning and tracking an object appearing in a preset monitoring sub-area. Target human face pose parameters and target position information are determined. A proper camera building working group is selected according to the target position information and the face pose parameters. Multiple cameras are rotated for collecting multi-view human face images. The multi-view human face images are transmitted to a human face database. A multi-view face recognition technique is utilized for recognizing a target when the target enters the preset monitoring sub-area.
   USE - Multi-camera based human face recognition method.
   ADVANTAGE - The method enables positioning and tracking the target by utilizing the cameras and improving availability of the collected human face images, efficiency, expandability and robustness.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multi-camera based human face recognition system comprising a central processor.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-camera based human face recognition method.'(Drawing includes non-English language text)'
PD CN102254169-A   23 Nov 2011   G06K-009/00   201218   Pages: 15   Chinese
   CN102254169-B   22 Aug 2012   G06K-009/00   201302      Chinese
UT DIIDW:2011Q29571
ER

PT P
PN CN102129572-A; CN102129572-B
TI Face detecting method, involves stopping classification for rectangular window by next level weak classifier, if rectangular window is classified into non face by one-level classifier.
AB    NOVELTY - The method involves stopping classification for a rectangular window by next level weak classifier, if a rectangular window is classified into a non face by a one-level classifier. A detecting result that the window is a non face, is output, and the detecting result that the window is a face, is output if the window is classified into the face by each level of a weak classifier in a light cascade structure classifier. Each level of the weak classifier in the light cascade classifier is obtained by training on the basis of all the weak classifiers before the level weak classifier.
   USE - Face detecting method.
   ADVANTAGE - The weak classifier of the light cascade structure classifier can remove part of the non face so as to ensure high detection speed and realize the real-time detection for the face in the million high-definition video.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face detecting device using cascade classifier, comprising a light cascade structure classifier training module for training and obtaining a light cascade structure classifier.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a face detecting method. '(Drawing includes non-English language text)'
PD CN102129572-A   20 Jul 2011   G06K-009/66   201160   Pages: 22   Chinese
   CN102129572-B   15 May 2013   G06K-009/66   201366      Chinese
UT DIIDW:2011K34927
ER

PT P
PN WO2010133661-A1; US2013300891-A1; US2015269424-A1; US9818024-B2; US10810409-B2
TI Method for recognizing facial expression or identity of face, involves applying model with multiple shape parameters to one of subsets of features of each of two eyes, and identifying probable facial expression based on determination.
AB    NOVELTY - The method involves detecting and identifying a face within a digital image. Multiple features of a face within the image are separately extracted, which comprises two independent eyes or subsets of features of each of the two eyes, or lips or partial lips or multiple other mouth features and one or both eyes. A model i.e. active appearance model, with multiple shape parameters is applied to the features of the face. Multiple similarities are determined between the features of the face and library of reference feature sets. A probable facial expression is identified based on the determination.
   USE - Method for recognizing a facial expression or an identity of a face, or both facial expression and face identity, e.g. in a gaming console or similar.
   ADVANTAGE - The facial expression or the identity of the face, or both facial expression and face identity is easily recognized for a user playing in-game avatar in real-time by identifying the probable facial expression based on determination of multiple similarities between features of the face and the library of the reference feature sets. The method enables applying the model with multiple shape parameters to two independent eyes or subsets of features of each of the two eyes, or to lips or partial lips or multiple other mouth features and one or both eyes, or both features and eyes. The high accuracy is obtained for eye tracking, blink, or gaze detection. The memory space requirements are reduced, so that one set of eye-model characteristics is stored, thus leading in low-cost consumer electronic devices such as gaming peripherals.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer-readable media comprising a set of instructions for recognizing facial expression or identity of face or both expression and identity
   (2) a digital image acquisition device for recognizing a facial expression or an identity of a face, or both expression and identity, comprising a lens and an image sensor.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematically block diagram of a facial analysis and classification system.
   Image acquisition (2)
   Raw data (4)
   Face detection (6)
   Feature extraction (10)
   Face recognition (14)
PD WO2010133661-A1   25 Nov 2010   G06K-009/00   201101   Pages: 54   English
   US2013300891-A1   14 Nov 2013   G06K-009/00   201378      English
   US2015269424-A1   24 Sep 2015   G06K-009/00   201563      English
   US9818024-B2   14 Nov 2017   G06K-009/00   201776      English
   US10810409-B2   20 Oct 2020   G06K-009/00   202095      English
UT DIIDW:2010P62276
ER

PT P
PN CN101877056-A
TI Face expression identification method, involves acquiring face expression classifier, and acquiring hidden Markov model by training neutral expression sequence and weak expression sequence in face expression sample.
AB    NOVELTY - The method involves carrying out characteristic extraction of fused binary image and optical flow field of grey image sequence on input video image to acquire characteristic sequence. Input characteristic sequence is used as observation sequence. Expression classification of the observation sequence is determined according to a face expression classifier. The face expression classifier is acquired by a Hidden Markov Model combining various expressions. The Hidden Markov Model is acquired by training neutral expression sequence and weak expression sequence in a face expression sample.
   USE - Face expression identification method.
   ADVANTAGE - The method enables overcoming defects of discrete, jumping and unnatural expression identification result.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face expression identification system comprising a characteristic extraction module.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a face expression identification method.'(Drawing includes non-English language text)'
PD CN101877056-A   03 Nov 2010   G06K-009/00   201106   Pages: 17   Chinese
UT DIIDW:2010P99170
ER

PT P
PN US2010254577-A1; US8311289-B2
TI Method for determining face similarity of persons on e.g. online dating site, involves performing similarity search using some of tent and meta data associated with image of person to identify other persons of similar appearance.
AB    NOVELTY - The method involves identifying a person form an image. The set of tent or metadata associated with the image is identified and biographical information (104) about the person is determined using the tent and metadata. The similarity search using some of the tent and metadata of the person is performed to determine one or more classification for the person in order to identify images (114) of other persons of similar appearance to the person in the image. The ethnicity, gender or age classification of the person in the image are included in the classifications.
   USE - Method for determining face similarity of persons in social network site, online dating site, and for amusement purpose.
   ADVANTAGE - Enhanced face similarity search is performed effectively by enhanced determinations made about the image using the different factors such as biographical information, ethnicity gender, age classification as text data and meta data associated with image of the person.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) system for determining face similarity; and
   (2) computer readable recording medium storing program for determining face similarity.
   DESCRIPTION OF DRAWING(S) - The drawing shows an explanatory view of the system for performing face similarity search.
   Search query (102)
   Biographic information (104)
   Search engine (110)
   Result image (114)
   Collection (120)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The images involved in the face similarity search is in JPEG format.
PD US2010254577-A1   07 Oct 2010   G06K-009/68   201071   Pages: 11   English
   US8311289-B2   13 Nov 2012   G06K-009/00   201275      English
UT DIIDW:2010M78178
ER

PT P
PN CN101706872-A
TI Common open type face identification system, has testing report generating module connected with database to read information correlative with identifying result from database and generate testing report to output.
AB    NOVELTY - The system has a dynamic real-time face identification testing module connected with a database. A face identifying module of a static picture is connected with the database to match and identify read static face picture and trained classification feature in the database to store identifying result to the database. A database managing module is connected with the database. A testing report generating module is connected with the database to read the information correlative with the identifying result from the database and generate a testing report to output.
   USE - Common open type face identification system.
   ADVANTAGE - The modules in the system can independently work to realize corresponding functions and work together to achieve test and real-time face identification task by using different face identification algorithms under different databases and situations in an open and common frame system and offer a reference model to a design of another face identification practical system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a common open type face identification system.'(Drawing includes non-English language text)'
PD CN101706872-A   12 May 2010   G06K-009/00   201036   Pages: 9   Chinese
UT DIIDW:2010F53704
ER

PT P
PN JP2010028404-A
TI Recording-reproducing apparatus for video camera used in user face authentication application, identifies whether user is provided with operation authority to perform reproduction or editing operation with respect to image information.
AB    NOVELTY - An association unit links moving-image information with face image to identify user having operation authority with respect to moving-image information. Another association unit links the still-image information with face image to identify the user having operation authority with respect to still-image information. An identification unit identifies whether the user is provided with operation authority to perform reproduction or editing operation with respect to image information, based on which image information linked to face image is encrypted and then decoded.
   USE - Recording-reproducing apparatus for video camera used in user face authentication application, for recording and reproducing data on recording medium e.g. hard disk drive (HDD), DVD-RAM, etc. Can also be used in DVD recorder, digital camera, mobile telephone with imaging function, etc., used in recording and reproducing of data on Blu-ray disk (BD) like recordable BD (BD-R), etc., DVD-R, rewritable DVD (DVD-RW), DVD-RAM, DVD+R, DVD+RW, etc., semiconductor memory, high definition DVD (HDDVD), minidisk (MD), magneto-optical (MO) disk, secure digital (SD) (RTM: non-volatile memory card format developed by Panasonic, SanDisk, and Toshiba) card, compact disk (CD) such as recordable CD (CD-R), etc.
   ADVANTAGE - Secrecy of the recorded image information using recording-reproducing apparatus can be easily and appropriately maintained by user, as reproduction and editing operation to be performed with respect to image are restricted to other person except the specific person, even when the image is saved at the video camera. Face lock function can be easily achieved by embedding the information for face lock on the header of a scene, without using the encryption of the data of the scene.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flowchart illustrating the face lock processing of recording-reproducing apparatus. (Drawing includes non-English language text)
PD JP2010028404-A   04 Feb 2010   H04N-005/225   201011   Pages: 19   Japanese
UT DIIDW:2010B30030
ER

PT P
PN US2009141940-A1; US8170280-B2
TI Object images e.g. face, tracking method for e.g. production company, involves collecting extracted object images from video file into object group when specific similarity value exceeds predetermined similarity threshold.
AB    NOVELTY - The method involves receiving a video file that comprises multiple frames, extracting an image of an object from a particular frame in the video file, and extracting a subsequent image of the object from a subsequent frame in the video file. A similarity value between the extracted object image from the particular frame and the subsequent extracted object image in the subsequent frame is calculated. The extracted object images from the video file are collected into an object group when the calculated similarity value exceeds a predetermined similarity threshold.
   USE - Method for tracking object images of face, automobile and article of clothing, in a still photograph and video files e.g. old/previously created movies, TV programs such as Gilmore Girls, and video clips of new advertisements, promotions and trailers, that are utilized by a production company and an advertisement agency and in a facial recognition field.
   ADVANTAGE - The method enables generation of effective object models for object recognition based on video data, and tracks temporal coherence of videos, so as to dynamically update and optimize the generated models. The method facilitates correct recognition of a statistically significant part of facial images in videos. The tests for merged models as compared to pure models confirm that the generated models are more accurate and reliable. The method utilizes only five episodes of the TV program i.e. Gilmore Girls, to generate facial models, or utilizes multiple videos within TV or movie series for modeling, so the generated models are highly robust and accurate.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for tracking a specific object through multiple frames of video
   (2) a method for generating an object model from one or more images in a video
   (3) a system for identifying object images from videos.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a sample frame of a video scene indicating recognized images within the frame.
PD US2009141940-A1   04 Jun 2009   G06K-009/00   200940   Pages: 29   English
   US8170280-B2   01 May 2012   G06K-009/00   201230      English
UT DIIDW:2009J98942
ER

PT P
PN CN101440676-A; CN101440676-B
TI Intelligent safe-guard door lock, has human face identification module obtaining video images from camera device, and alarm processing module providing alarm after receiving alarm control command.
AB    NOVELTY - The device has a camera device shooting video images in real time, and a human face identification module obtaining video images from the camera device for detecting and tracking a human face. A human face feature is extracted for shield analysis while detecting the human face. An alarm control command is transmitted to an alarm processing module when shield area in human face area is greater than a shield threshold value. An alarm processing module sounds an alarm after receiving the alarm control command.
   USE - Intelligent safe-guard door lock.
   ADVANTAGE - The lock performs movement detection analysis and sounds an alarm when dangerous action is judged. The lock collects images by the camera in a natural obtaining manner, and carries out non-contact identification of the images for human face information and action information by utilizing image identification technique for realizing intelligent alarming.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for giving an alarm of an intelligent safe-guard door lock.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an intelligent safe-guard door lock.'(Drawing includes non-English language text)'
PD CN101440676-A   27 May 2009   E05B-049/00   200943   Pages: 18   Chinese
   CN101440676-B   16 Oct 2013   E05B-049/00   201402      Chinese
UT DIIDW:2009K10384
ER

PT P
PN US2009067730-A1; US8064688-B2
TI Three-dimensional object e.g. airplane, recognizing and detecting method, involves classifying two-dimensional image based on ratio of set of graphical probability models using visual information.
AB    NOVELTY - The method involves receiving a digitized version of a two-dimensional (2D) image containing a 2D representation of a three-dimensional (3D) object. Visual information is obtained from the digitized version of the 2D image. The 2D image is classified based on a ratio of a set of graphical probability models using the information. A wavelet transform of the 2D image is computed, where the wavelet transform generates a set of transform coefficients. Each transform coefficient represents a portion of the information from the 2D image that is localized in space, frequency, and orientation.
   USE - Method for recognizing and detecting a three-dimensional object in a two-dimensional image using Bayesian network based classifier. Uses include but are not limited to human face, cat face, automobile, telephone and airplane.
   ADVANTAGE - The method enables utilization of extensive scanning for each image size and image window location, thus improving the accuracy of detection of the object.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for providing assistance in detecting the presence of a three-dimensional object in a two-dimensional image containing a two dimensional representation of three-dimensional object
   (2) a system for providing assistance in detecting the presence of a three-dimensional object in a two-dimensional image containing a two-dimensional representation of three-dimensional object
   (3) a method for detecting instances of an object in a two-dimensional image
   (4) a computer system configured to perform a method for detecting instances of an object in a two-dimensional image
   (5) a computer readable medium comprising a set of instructions to perform a method for detecting instances of an object in a two-dimensional image
   (6) a computer readable data storage medium comprising a set of instructions to perform a method for recognizing and detecting three-dimensional object in two-dimensional image
   (7) a system for detecting instances of an object in a two-dimensional image
   (8) a system for detecting instances of an object in a two-dimensional image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a screen shot illustrating a classification decision process involving a fixed object size, orientation, and alignment.
   Digital image (16)
   Classification window (32)
   Classifier (34)
PD US2009067730-A1   12 Mar 2009   G06K-009/62   200929   Pages: 55   English
   US8064688-B2   22 Nov 2011   G06K-009/62   201177      English
UT DIIDW:2009H71889
ER

PT P
PN CN101187975-A; CN100543764-C
TI Face character extracting method, involves counting line logarithmic energy entropy formula and row logarithmic energy entropy formula of sub-block, and counting distance value of unknown face image.
AB    NOVELTY - The method involves extracting a figure character of a face with an edge detecting arithmetic to an original planar face image, and processing a binaryzation processing to a face figure image. A face pre-processing image is partitioned equably without overlapping. A variance projection entropy formula is provided at a line direction and a row direction of a sub-block. A line logarithmic energy entropy formula and a row logarithmic energy entropy formula of the sub-block are counted. Distance value of an unknown face image is counted.
   USE - Method for extracting a face character.
   ADVANTAGE - The method avoids the need to use the complex illumination mode or imaging device, and has good robustness for the illumination. The character to which the face identification needs could be extracted effectively for the face image that is formed under the condition of bad illumination, so that the identifying rate is high. The method has real-time processing ability and good processing speed.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a process of extracting a face character.'(Drawing includes non-English language text)'
PD CN101187975-A   28 May 2008   G06K-009/00   200843   Pages: 16   Chinese
   CN100543764-C   23 Sep 2009   G06K-009/00   201001      Chinese
UT DIIDW:2008G73957
ER

PT P
PN US2008049969-A1; WO2008025019-A1; EP2062161-A1
TI Symbol e.g. star, representation generating method for e.g. personal computer, involves defining version of symbol by adding noise component to another version of symbol, where one of views is similar to another version of symbol.
AB    NOVELTY - The method involves providing a symbol at a system e.g. personal computer, and defining a version of the symbol at the system. Another version of the symbol is defined at the system by adding a noise component to the version of the symbol where two views of another version of the symbol in a viewing plane are associated with two selectable view options, respectively. One of the views is substantially similar to the former version of the symbol and different from other view.
   USE - Method for generating a representation of a symbol e.g. non-alphanumeric images, such as icon e.g. happy face, star, stop sign, traffic light, speaker, eye, magnifying glass, lightning bolt, heart and check- mark, directional arrow e.g. left, right, up, down, circular, back and forth, button clicks, and compass point directions, descriptive direction e.g. lick on plaid square, descriptive problem, and company logo, that presents an identification challenge for an automated agent i.e. malicious autonomous software applications such as virus, Trojan, worm, bot, spider, and crawler, in a personal computer, personal device assistant (PDA), and cellular telephone.
   ADVANTAGE - The method enables a human user to distinguish one of original and false symbol from other of the original and false symbol in an effective manner. The method generates the representation of the symbol for the automated agent in an efficient manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a machine readable medium having a set of instructions for performing a symbol representation generating method
   (2) a system for generating a representation of a symbol that presents an identification challenge for an automated agent.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method of generating a representation of a symbol that presents a challenge for an automated agent.
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The symbol is represented using symbol data representations such as MPEG, JPEG, and American standard code for information interchange (ASCII) representation.
PD US2008049969-A1   28 Feb 2008   G06K-009/00   200818   Pages: 26   English
   WO2008025019-A1   28 Feb 2008   G06F-017/00   200819      English
   EP2062161-A1   27 May 2009   G06F-017/00   200935      English
UT DIIDW:2008C46752
ER

PT P
PN US2007282506-A1; US7769513-B2
TI Arrangement for obtaining information about objects in or around vehicle e.g. truck, processes taken images of environment to determine edges of objects in images, and inputs data about edges into neural network.
AB    NOVELTY - The arrangement uses charge-coupled device (CCD) or complementary metal oxide semiconductor (CMOS) arrays to take pictures of the passenger compartment or environment outside of the vehicle. The images are then processed to determine the edges of the objects e.g. child seats, in the images. The data about the images are input to a neural network or a trained pattern recognition algorithm, which in turn provides information about the objects.
   USE - Used for providing information about objects e.g. rear-facing child seats, boxes, child restraints, even adult passengers, tail lights, within passenger compartment or boot compartment or outside environment around vehicles e.g. trucks, truck trailers, shipping containers, automobiles, such information including classification, location, and or identification of object. Object information is used to control vehicle components e.g. airbags, light filters, or other vehicular systems e.g. headrest adjustment systems, occupant restraint control systems, automatically adjustable steering column and pedal systems, outside monitoring systems. Also used for locating other objects, including electronic objects e.g. cell phones, personal digital assistants, compact disc players, radios, or even non-electronic devices e.g. car keys.
   ADVANTAGE - Achieves control of vehicular components or systems based on image analysis of environments within or around vehicle.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a vehicle;
   (2) a method for obtaining information about objects in an environment in or around a vehicle; and
   (3) a method for controlling at least one component having a variable condition, state, or position depending on the presence of objects in the environment in or around the vehicle.
   DESCRIPTION OF DRAWING(S) - The figure is a diagram of an occupant sensing strategy using a single camera optical system.
PD US2007282506-A1   06 Dec 2007   B60N-002/02   200807   Pages: 123   English
   US7769513-B2   03 Aug 2010   G06K-009/00   201054      English
UT DIIDW:2008B12348
ER

PT P
PN WO2007097144-A1; JP2007226327-A; EP1990769-A1; KR2008106426-A; US2009060293-A1; TW200745971-A; JP4367424-B2; EP1990769-A4
TI Personal identification apparatus installed in mobile phone with camera, compares extracted characteristic data of user's face, with registration data and recognition parameter stored in memories, to recognize registered user.
AB    NOVELTY - A memory stores registration data containing characteristic data of registered user's face. Another memory stores recognition parameter indicating facial recognition level. An extraction unit extracts characteristics data of user's face from face image. A comparison unit compares extracted characteristic data with registration data and recognition parameter which are stored in respective memories. A recognition unit recognizes whether a user is registered user, based on comparison result.
   USE - For personal identification apparatus installed in mobile phone with camera.
   ADVANTAGE - High security level is maintained, when conditions of personal identification are good. The person can be recognized suitably and setting can be switched automatically, when conditions of personal identification are not good.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for personal identification method.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the personal identification apparatus. (Drawing includes non-English language text)
   Personal identification apparatus (1)
PD WO2007097144-A1   30 Aug 2007   G06T-007/00   200759   Pages: 45   Japanese
   JP2007226327-A   06 Sep 2007   G06T-007/00   200760   Pages: 23   Japanese
   EP1990769-A1   12 Nov 2008   G06K-009/00   200877      English
   KR2008106426-A   05 Dec 2008   G06K-009/62   200914      
   US2009060293-A1   05 Mar 2009   G06K-009/00   200918      English
   TW200745971-A   16 Dec 2007   G06K-009/00   200932      Chinese
   JP4367424-B2   18 Nov 2009   G06T-007/00   200976   Pages: 22   Japanese
   EP1990769-A4   03 Feb 2010   G06K-009/00   201743      English
UT DIIDW:2007624601
ER

PT P
PN US7227977-B1
TI Recognition method for recognizing object e.g. human face, involves obtaining albedo using calculated parameters and using obtained albedo to recognize object.
AB    NOVELTY - The method involves acquiring an image of an object exposed to direct light and dispersing light. A parameter indicative of the direct light and the parameter indicative of the disperse light are calculated. An albedo is obtained using the calculated parameters. The obtained albedo is used for identifying the object.
   USE - For recognizing an object, e.g. human face, using the albedo of object, such that albedo of the object is a measure of the amount of light reflected by the object, or radiance, relative to the amount of incidence light shone on the object.
   ADVANTAGE - Enables recognizing an object in real time with minimal intrusion on the person. Avoids time and processor intensive applications since computational efficiency is increased. Improves recognition accuracy since the computation of the albedo takes into account the types of light that can illuminate the object.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for identifying an object;
   (2) a method for identifying the individual from an acquired image of the individual; and
   (3) a computer readable medium for holding device executable instructions.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart for identifying an object.
PD US7227977-B1   05 Jun 2007   G06K-009/00   200740   Pages: 36   English
UT DIIDW:2007417931
ER

PT P
PN US2007064989-A1; WO2007035829-A2; WO2007035829-A3; EP1938245-A2; US7477767-B2; JP2009508648-W
TI Skin condition e.g. acne, analyzing method, involves acquiring white-light image and ultra violet image of portion of body surface, by portable image acquisition device, and sending images to computing device for analysis.
AB    NOVELTY - The method involves acquiring a white-light image and an ultra violet (UV) image of a portion of a body surface e.g. person`s face, by a portable image acquisition device. The white-light and UV images are provided with pixels, where the pixels in the white-light image correspond to respective pixels in the UV image. The images are sent to a computing device for analysis. The properties of pixels to identify skin pixels in the images are examined by the device. The results associated with a skin condition such as acne, using information in the skin pixels, are obtained by the device.
   USE - Used for analyzing skin condition e.g. acne, wrinkle, ultra violet (UV) damage, skin tone, pores, hydration level, collagen content, skin type, topical inflammation or recent ablation, keratosis, deeper inflammation, sun spots, freckle, mole, scar, fungi and erythema, associated with a subject e.g. face, nose, eyes and mouth, in a location e.g. medical spa, park, beach and inside transportation vehicle such as car and airplane.
   ADVANTAGE - The method enables doctor and skin care specialist to obtain quantitative measures of a variety of skin conditions using digital images, where the quantitative measures is acquired before or after a skin care treatment to evaluate the suitability of the treatment for a given condition. The method enables patients to obtain rapid assessments of the skin at any location e.g. beach, thus assisting the patients in a proper care and maintenance of the skin on a daily basis.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a computer readable medium has program with instructions to perform a method for analyzing skin conditions associated with a subject
   (2) a system comprising a portable image acquisition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for analyzing skin conditions using digital images.
PD US2007064989-A1   22 Mar 2007   G06K-009/00   200744   Pages: 36   English
   WO2007035829-A2   29 Mar 2007   G06K-009/00   200744      English
   WO2007035829-A3   17 Jan 2008   G06K-009/00   200807      English
   EP1938245-A2   02 Jul 2008   G06K-009/00   200845      English
   US7477767-B2   13 Jan 2009   G06K-009/00   200907      English
   JP2009508648-W   05 Mar 2009   A61B-005/107   200919   Pages: 32   Japanese
UT DIIDW:2007455263
ER

PT P
PN JP2007027990-A; JP4599244-B2
TI Moving image data editing apparatus e.g. personal computer, has blowing preparation unit that produces blowing data for displaying text data into image, based on speaker's position and text data.
AB    NOVELTY - A speaker specifying unit compares face and audio feature values respectively detected by a face detection unit (103) and voice authentication unit (104) with registered speaker's feature value to pinpoint specific speaker's position. A speech recognition unit (105) produces text data of speaker's audio. A blowing preparation unit (112) produces blowing data for displaying the text data into an image, based on speaker's position and text data. A moving image preparation unit (114) gathers blowing data in image data and audio data to produce new dynamic image data.
   USE - For editing moving image data acquired by digital camcorder and digital camera, using personal computer.
   ADVANTAGE - The blowing, preparation of subtitle and edit are carried out easily.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) moving image data editing method;
   (2) moving image data editing program; and
   (3) storage medium storing moving image data editing program.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the automatic moving image data editing apparatus. (Drawing includes non-English language text)
   Face detection unit (103)
   Voice authentication unit (104)
   Speech recognition unit (105)
   Blowing preparation unit (112)
   Moving image preparation unit (114)
PD JP2007027990-A   01 Feb 2007   H04N-005/262   200731   Pages: 26   Japanese
   JP4599244-B2   15 Dec 2010   H04N-005/262   201082   Pages: 26   Japanese
UT DIIDW:2007314607
ER

PT P
PN US2005123202-A1; KR2005054394-A; KR543707-B1; US7734087-B2
TI Face image feature vector extracting apparatus for e.g. financial security application, has extraction unit generating feature vector sets by projecting image to linear discriminate analysis basis vector set.
AB    NOVELTY - The apparatus has a learning unit to generate a principal component analysis (PCA)-based linear discriminant analysis (LDA) (PCLDA) basis vector set for each subgroup of training data set. An extraction unit projects face image to be registered to the vector set, to generate feature vector sets. Similarity between an input face image and the registered image is calculated using weights found based on the feature vector sets.
   USE - Used for extracting a feature vector of a face image, for application e.g. financial security, public security, harbor security, entrance departure management at border, identification in electronic vote, preventing falsification of identity, search and inquiry for criminal.
   ADVANTAGE - The feature vector set is extracted using the PCLDA basis vector set of each subgroup, so that eigen characteristics of each subgroup can be effectively reflected regardless of a data size of each subgroup, thus preventing over fitting, and providing reliable face recognition performance even in small scale installation places.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a method for extracting a feature vector
   (B) a computer readable recording medium storing a program for extracting a feature vector
   (C) a face recognition apparatus comprising a learning unit and a feature vector extraction unit
   (D) a face recognition method
   (E) a computer-readable medium storing a program for performing face recognition method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart explaining a face recognition process.
PD US2005123202-A1   09 Jun 2005   G06K-009/62   200544   Pages: 16   English
   KR2005054394-A   10 Jun 2005   G06K-009/38   200641      
   KR543707-B1   20 Jan 2006   G06K-009/38   200682      
   US7734087-B2   08 Jun 2010   G06K-009/38   201038      English
UT DIIDW:2005433912
ER

PT P
PN US2005105778-A1; EP1533744-A2; JP2005149515-A; KR2005048413-A; KR580630-B1; US7536037-B2; JP4679121-B2; EP1533744-A3
TI Human face distinction apparatus for use in e.g. airport, has image processing unit for comparing infrared image and non-infrared image of preset areas detected in each output image to determine whether subject is real human face.
AB    NOVELTY - The apparatus has a photographing unit (10) to photograph a subject without radiating infrared light and photograph the subject by radiating the light onto the subject. An image processing unit (11) receives the images output from the unit (10) to detect a preset area in each output image. The processing unit compares infrared and non-infrared images of the detected areas to determine whether subject is a real human face.
   USE - Used for discriminating a human face in an airport, seaport, off-limits building, electronic payment or commerce and financial company e.g. bank.
   ADVANTAGE - The processing unit compares the infrared and non-infrared images of the detected areas to determine whether subject is the real human face, thus preventing an unauthorized person from entering or exiting an entrance system which is operated based on face recognition.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a method of human distinction
   (B) an apparatus for detecting an object
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an apparatus for human distinction.
   Photographing unit (10)
   Image processing unit (11)
   Camera (101)
   Infrared generator (102)
   Photographing controller (111)
PD US2005105778-A1   19 May 2005   G06K-009/00   200543   Pages: 10   English
   EP1533744-A2   25 May 2005   G06K-009/00   200543      English
   JP2005149515-A   09 Jun 2005   G06T-001/00   200543   Pages: 12   Japanese
   KR2005048413-A   24 May 2005   G06K-009/36   200641      
   KR580630-B1   16 May 2006   G06K-009/36   200913      
   US7536037-B2   19 May 2009   G06K-009/00   200933      English
   JP4679121-B2   27 Apr 2011   G06T-001/00   201129   Pages: 12   Japanese
   EP1533744-A3   13 Sep 2006   G06K-009/00   201731      English
UT DIIDW:2005423785
ER

PT P
PN US2005100243-A1; US7039216-B2
TI Person`s face sketch generating method for use with e.g. personal computer, involves employing active shape module to locate prescribed set of facial feature points in facial training image.
AB    NOVELTY - The method involves employing an active shape module (808) to locate a prescribed set of facial feature points in a facial training image (800). A geometric transform function is employed for the input image for warping the input image to create a transformed input image. A non-parametric sampling module is employed to create an expected sketch image from the transformed input image, transformed training images and sketches.
   USE - Used with a personal computer, server computer, handheld or laptop device, multiprocessor system, microprocessor based system, set top box, programmable consumer electronic, network PCs, minicomputer and mainframe computer for generating a sketch of a person`s face.
   ADVANTAGE - The method can automatically generate the sketch images from images of the person, where the sketch exhibits the stylistic characteristics of the sketch artist.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a computer implemented process for training a computer using a set of training images.
   DESCRIPTION OF DRAWING(S) - The drawing shows an image pictorially summarizing a training and runtime phases.
   Facial training image (800)
   Sketches (802)
   Broken line box (804)
   Labeled images (806)
   Active shape module (808)
PD US2005100243-A1   12 May 2005   G06K-009/36   200537   Pages: 34   English
   US7039216-B2   02 May 2006   G06K-009/00   200629      English
UT DIIDW:2005365571
ER

PT P
PN US2004181552-A1; US7187787-B2
TI Facial identification method, involves reconstructing image database of persons into one in which persons have multiple pictures and comparing photograph of persons to database to generate confidence percentage for matching image.
AB    NOVELTY - The method involves reconstructing an image database (101) of persons into one in which each person has multiple pictures representing different angles and lighting conditions. Photograph of a person is compared to the database, and a confidence percentage is generated for each matching image based on similarities in matching images. An extra confidence percentage corresponding to the matching images is then generated.
   USE - Used for facial identification of a person.
   ADVANTAGE - The image database of persons is reconstructed into one in which each person has multiple pictures representing different angles and lighting conditions, thereby enhancing the facial recognition process to increase the judgment confidence on identifying the person from the large image database.
   DESCRIPTION OF DRAWING(S) - DESCRIPTION OF DRAWING - The drawing shows a conceptual dataflow diagram of enhanced facial recognition architecture.
   Image database (101, 102)
   Voting group (104)
   Output (107, 108)
PD US2004181552-A1   16 Sep 2004   G06F-007/00   200466   Pages: 6   English
   US7187787-B2   06 Mar 2007   G06K-009/00   200718      English
UT DIIDW:2004675793
ER

PT P
PN WO2004072899-A1; EP1594085-A1; US2006093183-A1; JP2005504985-X; KR2005096187-A; CN1748227-A; CN101064004-A; CN101064005-A; KR816602-B1; CN100576245-C; US7680299-B2; JP4465719-B2; CN101064004-B; EP1594085-A4
TI Unauthorized person detector determines whether stripe pattern formed on target object are straight lines, based on which target object is detected to be plane-shaped/3D object and personal authentication is performed.
AB    NOVELTY - A light source projects light on target object through a filter, to form a stripe pattern on the target object. A determination unit determines whether the strips formed on the target object are straight lines, based on which it is detected that the target object is a plane-shaped object e.g. photograph/image display device or 3D object e.g. human face and authentication of a person is performed.
   USE - For personal-identification system.
   ADVANTAGE - Enables to perform personal authentication efficiently.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for unauthorized person detecting method.
   DESCRIPTION OF DRAWING(S) - The figure shows a flow chart of the unauthorized person detecting process.(Drawing includes non-English language text).
PD WO2004072899-A1   26 Aug 2004   G06T-001/00   200460   Pages: 104   Japanese
   EP1594085-A1   09 Nov 2005   G06T-001/00   200573      English
   US2006093183-A1   04 May 2006   G06K-009/00   200631      English
   JP2005504985-X   01 Jun 2006   G06T-001/00   200637   Pages: 50   Japanese
   KR2005096187-A   05 Oct 2005   G06K-009/00   200648      
   CN1748227-A   15 Mar 2006   G06T-001/00   200649      Chinese
   CN101064004-A   31 Oct 2007   G06K-009/00   200820      Chinese
   CN101064005-A   31 Oct 2007   G06K-009/00   200820      Chinese
   KR816602-B1   24 Mar 2008   G06K-009/00   200864      
   CN100576245-C   30 Dec 2009      201009      Chinese
   US7680299-B2   16 Mar 2010   G06K-009/00   201020      English
   JP4465719-B2   19 May 2010   G06T-001/00   201033   Pages: 42   Japanese
   CN101064004-B   26 May 2010   G06K-009/00   201043      Chinese
   EP1594085-A4   27 Feb 2008   G06K-009/00   201747      English
UT DIIDW:2004625967
ER

PT P
PN JP2004054788-A; CN1480902-A; US2004165754-A1; JP4013684-B2; US7336807-B2; CN1480902-B
TI Irregular registration prevention apparatus for use in personal identification system, registers or abandons photographed image of face, when variation is less than or greater than threshold value.
AB    NOVELTY - A selecting unit (7) detects position of face that is continuously photographed by camera. An arithmetic unit (8) calculates the variation between detected position and currently photographed position of face. A storage unit registers or abandons the photographed image of face, when the variation is less than or greater than threshold value, respectively.
   USE - For preventing irregular registration in personal identification system.
   ADVANTAGE - The irregular registration of the photographed image is prevented by calculating the variation between detected and currently photographed face position.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the personal identification system. (Drawing includes non-English language text).
   torage unit (5)
   display section (6)
   detecting unit (7)
   arithmetic unit (8)
   determination unit (11)
PD JP2004054788-A   19 Feb 2004   G06T-007/00   200417   Pages: 15   Japanese
   CN1480902-A   10 Mar 2004   G06T-007/00   200437      Chinese
   US2004165754-A1   26 Aug 2004   G06K-009/00   200457      English
   JP4013684-B2   28 Nov 2007   G06T-007/00   200780   Pages: 13   Japanese
   US7336807-B2   26 Feb 2008   G06K-009/00   200816      English
   CN1480902-B   28 Apr 2010   G06T-007/00   201042      Chinese
UT DIIDW:2004175651
ER

PT P
PN US6567765-B1
TI Biometric detection system evaluation e.g. for fingerprint recognition system in bank, involves editing small feature database and building statistical perturbation model describing degradation characteristics of extracted features.
AB    NOVELTY - A small feature database (108) containing biometric images, is edited to correct the errors in the extracted features. A statistical perturbation model (136) is built, to describe the degradation characteristics of the extracted features. The perturbation model is applied to the edited database to construct a perturbed database. A fingerprint recognition system is evaluated based on the edited and perturbed databases.
   USE - For evaluating biometric detection system such as fingerprint recognition system, face recognition system, retina recognition system used in secured areas and banks.
   ADVANTAGE - The performance of biometric detection system is evaluated quickly, accurately and inexpensively without the need to acquire a large number of testing images.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for biometric detection system evaluation program storage device.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the biometric detection system evaluation system.
   mall feature database (108)
   tatistical perturbation model (136)
PD US6567765-B1   20 May 2003   G06K-009/00   200362   Pages: 9   English
UT DIIDW:2003655821
ER

PT P
PN US2002102010-A1; US7020305-B2
TI Head motion estimation method for animation modeling, involves estimating head motion from points identified from two images, based on set of physical face parameters.
AB    NOVELTY - The locations of a number of distinct facial features in two images, are identified and are converted into set of physical parameters based on the symmetry of distinct facial features. The head motion is estimated from points identified from the two images, based on the set of physical face parameters.
   USE - For estimating head motion between two images of a face during animation modeling used for computer games, film making, online chat, virtual presence and video conferencing.
   ADVANTAGE - Increases the redundancy and the robustness of the head motion estimation.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for computer readable medium storing head motion estimation program.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart of a 3D face model creating method.
PD US2002102010-A1   01 Aug 2002   G06K-009/00   200275   Pages: 17   English
   US7020305-B2   28 Mar 2006   G06K-009/00   200623      English
UT DIIDW:2002697813
ER

PT P
PN US5479529-A
TI Computer-implemented fortune telling and character analysis - analyses two-dimensional facial image elements and uses these to extract stored messages.
AB       Computer-implemented fortune-telling and character analysis comprises storing appropriate messages corresponding to a respective facial element value included in a facial image. The image is converted to two dimensions. Image features corresponding to elements are extracted. Normalised features are calculated from the extracted features, and elements are calculated as the diff. between normalised and standard features.
   The memory is searched for messages corresponding to the values obtained. The messages are retrieved and output concurrently. For feature extraction may also extract skin colour and texture information.
   ADVANTAGE -   Provides great versatility and amusement. No need for keyboard input from user.
PD US5479529-A   26 Dec 1995   G06K-009/00   199606   Pages: 8   English
UT DIIDW:1996058050
ER

PT P
PN US10108850-B1; US2018307899-A1; EP3396591-A2; CN108734300-A; EP3396591-A3
TI Apparatus for facilitating recognition, reidentification, and security in machine learning at autonomous machines, has reidentification and model logic that is configured to build classification model based on extracted features.
AB    NOVELTY - The apparatus has a detection/observation logic (701) is configured to facilitate a camera to detect one or more objects within a physical vicinity. An extraction and comparison logic (713) is configured to extract features based on the one or more portions of the structure. The extraction and comparison logic is configured to compare the extracted features with feature vectors stored at a database. A reidentification and model logic (715) is configured to build a classification model based on the extracted features over a period of time to facilitate recognition or reidentification of the person independent of facial recognition of the person.
   USE - Apparatus for facilitating recognition, reidentification, and security in machine learning at autonomous machines.
   ADVANTAGE - The ability to graphic processing unit (GPU) attached memory to be accessed as system memory without onerous cache coherence overhead provides a beneficial operating environment for GPU offload and GPU-biased pages are ensured.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a method for facilitating recognition, reidentification, and security in machine learning at autonomous machines; and
   (2) a machine-readable medium includes instruction for facilitating recognition, reidentification, and security in machine learning at autonomous machines.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of recognition and security mechanism.
   Security Mechanism (610)
   Detection/observation logic (701)
   Extraction and comparison logic (713)
   Reidentification and model logic (715)
   Training Logic (717)
PD US10108850-B1   23 Oct 2018   G06K-009/00   201872   Pages: 80   English
   US2018307899-A1   25 Oct 2018   G06K-009/00   201872      English
   EP3396591-A2   31 Oct 2018   G06K-009/00   201873      English
   CN108734300-A   02 Nov 2018   G06N-099/00   201875      Chinese
UT DIIDW:201882947V
ER

PT P
PN CN108446667-A
TI Strengthen against network data generating based human face expression identification method, involves obtaining enhanced data of Cycle GAN model, and performing data classification operation by convolutional neural network model.
AB    NOVELTY - The method involves obtaining face image data and training data set of a resistance network. Training data pre-processing operation is performed according to training target. A Cycle GAN model and a convolutional neural network model are established based on loss function of the Cycle GAN model. Data enhancement and completion operations are performed based on the Cycle GAN model and the convolutional neural network model. Enhanced data of the Cycle GAN model is obtained. Data classification operation is performed by the convolutional neural network model.
   USE - Strengthen against network data generating based human face expression identification method.
   ADVANTAGE - The method enables realizing human face expression identifying process in a simple manner and improving model accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a strengthen against network data generating based human face expression identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a strengthen against network data generating based human face expression identification method. '(Drawing includes non-English language text)'
PD CN108446667-A   24 Aug 2018   G06K-009/00   201862   Pages: 19   Chinese
UT DIIDW:201868060G
ER

PT P
PN CN108446680-A; CN108446680-B
TI Edge computing node based face authentication system privacy protecting method, involves storing sub secret in distributed storage of connection edge computing node in computing device according to characteristic vector.
AB    NOVELTY - The method involves collecting a to-be registered human face image of a user by a camera. The obtained to-be registered human face image is uploaded to a connection edge computing node. Authority vector is distributed to a computing node by a permission distribution mechanism after collecting user face information based on a deep convolutional neural network. Requester user registration face image characteristic extraction process is performed for obtaining feature vector and safety nearest neighbor algorithm feature vector of a registered user. Sub secret is stored in a distributed storage of the connection edge computing node in a computing device according to characteristic vector.
   USE - Edge computing node based face authentication system privacy protecting method.
   ADVANTAGE - The method enables avoiding requirement to directly upload sensitive data to a cloud server so as to protect privacy of face data and improve fault tolerance of a face authentication system, and improving face recognition and plaintext lower completely equal face recognition accuracy.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an edge computing node based face authentication system privacy protecting method. '(Drawing includes non-English language text)'
PD CN108446680-A   24 Aug 2018   G06K-009/00   201861   Pages: 20   Chinese
   CN108446680-B   21 Dec 2021   G06K-009/00   202205      Chinese
UT DIIDW:2018680603
ER

PT P
PN CN107818313-A; CN107818313-B; WO2019096029-A1; US2020257914-A1; US11176393-B2
TI Living human face image identification method, involves performing living body identification according to background feature data, and obtaining identification result of target image as human face image according to confidence levels.
AB    NOVELTY - The method involves obtaining a target image. Face feature data of a face image is extracted from the target image. First living body identification is performed according to the face feature data for to obtain a first confidence level. Background feature data is extracted from a face extension image. The living body identification is performed according to the background feature data to obtain a second confidence level. An identification result of the target image as a living human face image is obtained according to first and second confidence levels.
   USE - Living human face image identification method.
   ADVANTAGE - The method enables improving living body detection efficiency.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a living human face image identification device
   (2) a computer readable storage medium for storing a set of instructions for performing living human face image identification process.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a living human face image identification method. '(Drawing includes non-English language text)'
PD CN107818313-A   20 Mar 2018   G06K-009/00   201825   Pages: 29   Chinese
   CN107818313-B   14 May 2019   G06K-009/00   201938      Chinese
   WO2019096029-A1   23 May 2019   G06K-009/00   201939      Chinese
   US2020257914-A1   13 Aug 2020   G06K-009/00   202066      English
   US11176393-B2   16 Nov 2021   G06K-009/62   202193      English
UT DIIDW:201823823F
ER

PT P
PN CN107392109-A
TI Depth neural network based neonatal pain expression identification method, involves acquiring actual expression video corresponding to neonatal expression recognition of deep neural network for obtaining corresponding degree of pain level.
AB    NOVELTY - The method involves collecting neonate corresponding to each preset level of pain of sample pain level expression video. A group of sample expression image frames are obtained corresponding to the pain rating expression. Convolutional neural network and short-term memory network are constructed for building deep neural network by connecting output end of the convolutional neural network with the input end of the memory network. An actual expression video is acquired corresponding to neonatal expression recognition of the deep neural network for obtaining corresponding degree of pain level.
   USE - Depth neural network based neonatal pain expression identification method.
   ADVANTAGE - The method enables breaking bottleneck of traditional manual design and extraction of explicit expression and improving recognition rate and robustness in complicated conditions such as face occlusion, posture tilt and illumination changes.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth neural network based neonatal pain expression identification method. '(Drawing includes non-English language text)'
PD CN107392109-A   24 Nov 2017   G06K-009/00   201783   Pages: 14   Chinese
UT DIIDW:2017815120
ER

PT P
PN CN106845330-A
TI Deep convolutional neural network based two-dimensional face recognition model training method, involves collecting face image samples, and training face recognition convolutional neural network model until reach highest accuracy value.
AB    NOVELTY - The method involves collecting (S1) face image samples. A grey image is obtained (S2) based on gradation processing operation. A face recognition convolutional neural network model is constructed (S3). The grey image is input (S4) to the face recognition convolutional neural network model to train. The grey image is selected (S5) from the face image sample in a random manner. The face recognition convolutional neural network model is trained (S6) to obtain a corresponding characteristic value. The convolutional neural network model is trained (S7) until reach a highest accuracy value.
   USE - Deep convolutional neural network based two-dimensional face recognition model training method.
   ADVANTAGE - The method enables utilizing a human face image in an effective manner, realizing two-dimensional face recognition model training process in an effective manner, reducing number of sample images and number of iterations to obtain an optimal model of a sample set, obtaining a characteristics value in an accurate manner and improving face comparison precision.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a deep convolutional neural network based two-dimensional face recognition model training method. '(Drawing includes non-English language text)'
   Step for collecting face image samples (S1)
   Step for obtaining grey image based on gradation processing operation (S2)
   Step for constructing face recognition convolutional neural network model (S3)
   Step for inputting grey image to face recognition convolutional neural network model to train (S4)
   Step for selecting grey image from face image sample in random manner (S5)
   Step for training face recognition convolutional neural network model to obtain corresponding characteristic value (S6)
   Step for training convolutional neural network model until reach highest accuracy value (S7)
PD CN106845330-A   13 Jun 2017   G06K-009/00   201754   Pages: 7   Chinese
UT DIIDW:2017450456
ER

PT P
PN CN106650699-A; CN106650699-B
TI Face detection method based on convolutional neural network, involves applying multiple sheets to be detected after preprocessing to first-level network to obtain picture include initial face detection frame.
AB    NOVELTY - The method involves dividing (S101) a convolution neural network into a three-level convolutional neural network. The first level network is full convolution neural network. The second level network and third level network is double stream internal cascade convolution neural network. Multiple sheets to be detected after the preprocessing to first-level network is applied (S102) to obtain a picture include initial face detection frame. The picture including initial face detection frame is input (S103) to second-level network and third-level network to obtain a picture includes a face.
   USE - Face detection method based on a convolutional neural network.
   ADVANTAGE - The face detection method can effectively improve the accuracy of face recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a face detection device based on convolutional neural network.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating Face detection method.
   Step for dividing a convolution neural network into a three-level convolutional neural network (S101)
   Step for applying multiple sheets to be detected after the preprocessing to the first-level network (S102)
   Step for inputting picture including the initial face detection frame (S103)
PD CN106650699-A   10 May 2017   G06K-009/00   201739   Pages: 19   Chinese
   CN106650699-B   17 Sep 2019   G06K-009/00   201973      Chinese
UT DIIDW:201732006W
ER

PT P
PN CN106503654-A
TI Sparse depth-based encoding network human face emotion recognition method, involves cutting human face image in training sample, and calculating partial derivative of overall cost function by back propagation algorithm.
AB    NOVELTY - The method involves cutting human face image in a training sample. A gray balancing process is performed. Pixel information and human facial expression information of the human face image are obtained. A depth sparse coding network is constructed according to the pixel information and the human facial expression information of the human face image. Encoding network depth value is classified by a Softmax classifier. A network weight adjusting process is performed based on depth self-coding. Partial derivative of overall cost function is calculated by a back propagation algorithm.
   USE - Sparse depth-based encoding network human face emotion recognition method.
   ADVANTAGE - The method enables ensuring sparsity parameter to reduce neuronal node number, effectively improving training and recognition speed, adjusting network weight value by reverse propagation algorithm, performing a gradient descent process so as to achieve global optimum and avoiding gradient dispersion problem so as to improve human face emotion identification performance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating a sparse depth-based encoding network human face emotion recognition method. '(Drawing includes non-English language text)'
PD CN106503654-A   15 Mar 2017   G06K-009/00   201724   Pages: 16   Chinese
UT DIIDW:201719040D
ER

PT P
PN CN106295566-A; CN106295566-B
TI Face expression identification method, involves detecting face area in local image, obtaining key point in face area, and extracting local image from face area according to key point, where expression identification result is obtained.
AB    NOVELTY - The method involves detecting a face area in a local image. A key point in the face area is obtained. The local image is extracted from the face area according to the key point. An expression identification result is obtained using a face recognition model to identify a partial image. Image blocks are obtained and overlapped with each other according to a preset order while obtaining the local image, where the key point is intercepted into the image blocks with predetermined size, where an expression recognition result is obtained.
   USE - Face expression identification method.
   ADVANTAGE - The method enables extracting expression state features accurately so as to reduce accumulated error and improve the accuracy of facial expression recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face expression identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a face expression identification process. '(Drawing includes non-English language text)'
PD CN106295566-A   04 Jan 2017   G06K-009/00   201711   Pages: 16   Chinese
   CN106295566-B   09 Jul 2019   G06K-009/00   201953      Chinese
UT DIIDW:201706814S
ER

PT P
PN WO2016045593-A1; CN105512689-A; CN105557175-A; CN105988469-A; CN106258166-A; EP3199009-A1; JP2017535279-W; US2018125003-A1; EP3199009-A4; CN105557175-B; EP3199009-B1; US10609862-B2
TI Automatic moving robot e.g. automatic mower robot, for identifying type of floor, has image capture device matched with transmitter, and controller including identification module to extract features from image to identify ground type.
AB    NOVELTY - The robot (1) has an optical transmitter for transmitting light of particular wavelength to a ground. An image capture device is matched with the optical transmitter for receiving the light of wavelength and forming an image based on the light of wavelength. A controller comprises a ground type identification module to extract features from the image to identify a ground type in the image. The ground type identification module includes a disperse degree identification component connected to a frequency domain recognition element and a pattern recognition element.
   USE - Automatic moving robot i.e. gardening robot such as automatic mower robot, for identifying a type of a floor. Uses include but are not limited to outdoor grass land, soil, cement level ground, and brick and carpet faces.
   ADVANTAGE - The robot sets the optical transmitter and the image capture device to be matched with each other to transmit and receive the light of wavelength, so that effect of ambient light on the image can be effectively reduced, thus accurately identifying a ground type at different time and location even at night without a light source.
   DETAILED DESCRIPTION - The optical transmitter is an infrared light emitter and a laser emitter.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an automatic moving robot.
   Automatic moving robot (1)
   Docking station (4)
   Work area of floor (5)
   Edge of floor (6)
   Non-work area of floor (7)
PD WO2016045593-A1   31 Mar 2016   A01D-034/00   201625      Chinese
   CN105512689-A   20 Apr 2016   G06K-009/64   201631      English
   CN105557175-A   11 May 2016   A01D-034/00   201634      English
   CN105988469-A   05 Oct 2016   G05D-001/02   201669      Chinese
   CN106258166-A   04 Jan 2017   A01D-034/00   201706      Chinese
   EP3199009-A1   02 Aug 2017   A01D-034/00   201752      English
   JP2017535279-W   30 Nov 2017   A01D-034/64   201779   Pages: 55   Japanese
   US2018125003-A1   10 May 2018   A01D-034/00   201831      English
   EP3199009-A4   20 Jun 2018   A01D-034/00   201841      English
   CN105557175-B   21 Sep 2018   A01D-034/00   201866      Chinese
   EP3199009-B1   29 Jan 2020   A01D-034/00   202010      English
   US10609862-B2   07 Apr 2020   A01D-034/00   202031      English
UT DIIDW:201619438J
ER

PT P
PN US2016034786-A1; US9646227-B2
TI Method for training models from video data, involves training scoring model based on first set of feature values and second set of feature values such that scoring model determines desirability score associated with video data.
AB    NOVELTY - The method (300) involves receiving (302) video data. The features are extracted (304) from the video data. A first set of feature values are determined that is associated with the features. The first set of feature values for training a classifier and a scoring model. A second set of feature values are determined based on applying the classifier to the video data. The scoring model is trained (306) based on the first set of feature values and the second set of feature values. The scoring model determines a desirability score associated with the video data.
   USE - Method for training models from video data based on extracting low level and high level features.
   ADVANTAGE - The leveraging of techniques enables a user to quickly identify and/or rank desirable video collections, video files, video segments and/or video frames to playback, share and/or edit without a significant investment of time. The combination of strong technical quality and subject importance above a predetermined threshold can result in desirable video data or a desirability score closer to five. The face tracking feature extraction and face recognition feature extraction is useful for identifying a person associated with a detected face and identifying an importance and/or relationship to a user. The scoring module can determine a desirability score for a video segment, video file and/or a video collection by adding together the desirability scores associated with the video frames in the video segment, video file and/or video collection, and finding an average desirability score based on the number of video frames in the video segment, video file and/or video collection.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for training models from video data; and
   (2) a computer-readable storage media storing instructions for training models from video data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram that illustrates a process for training models from video data based on extracting low level and high level features.
   Method for training models from video data (300)
   Step for receiving video data (302)
   Step for extracting features from video data (304)
   Step for training scoring model (306)
PD US2016034786-A1   04 Feb 2016   G06K-009/62   201612   Pages: 24   English
   US9646227-B2   09 May 2017   G06K-009/62   201732      English
UT DIIDW:2016096926
ER

PT P
PN CN105303161-A
TI Multi-person photographing method, involves obtaining multi-piece photo image, selecting sheet photo image as base image, obtaining human face image, and replacing human face image by using another face image.
AB    NOVELTY - The method involves obtaining multi-piece photo image. Sheet photo image is selected as base image. Human face image is obtained. Human face identification process is determined according to face characteristic condition. Base image is obtained according to the human face identification process. Identification serial number is obtained by using a human face number module based on the human face identification process. A characteristic determining module is connected with a number determining unit. Base image is removed. Human face image is replaced by using another face image.
   USE - Multi-person photographing method.
   ADVANTAGE - The method enables performing multi-person photographing process in an efficient manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multi-person photographing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-person photographing method. '(Drawing includes non-English language text)'
PD CN105303161-A   03 Feb 2016   G06K-009/00   201613   Pages: 11   English
UT DIIDW:2016102100
ER

PT P
PN CN104732214-A; CN104732214-B
TI Method for quantitative detection of e.g. type of skin in human face, involves obtaining face image blocks, dividing blocks after calculation of skin properties, and obtaining skin type detection judgment based on results of properties.
AB    NOVELTY - The method involves obtained two clear photographs of a human face by a photographing process. The human face in the photographs is recognized using a human face identification method and a human face identification tool, and eight face image blocks are obtained after facial recognition of the human face. The face image blocks are divided after calculation of skin properties. A skin type detection judgment is obtained based on results of the skin properties. A human face part key point is obtained. An original face image block is calculated using a reflecting identification method.
   USE - Method for quantitative detection of a type and a texture of a skin of a human face in a skin care area.
   ADVANTAGE - The method enables detection of a type and a texture of a skin based on human face image recognition and detection at anytime and anywhere, thus enabling people to choose right skin care methods in a rapid and easy manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a front view of a human face during quantitative detection of a type and a texture of a skin.
PD CN104732214-A   24 Jun 2015   G06K-009/00   201561   Pages: 14   Chinese
   CN104732214-B   12 Oct 2018   G06K-009/00   201871      Chinese
UT DIIDW:201550184M
ER

PT P
PN CN104318262-A
TI Method for changing skin of three-dimensional human face picture, involves fixing human face area in skin treatment area, fixing defect position in skin treatment area, and removing skin defect of human face area.
AB    NOVELTY - The method involves obtaining a process image by performing human face identification operation. A judgment is made to check whether a human face is exist. A human face detection result is obtained. A human face area is fixed in a skin treatment area. A defect position is fixed in the skin treatment area. The skin defect of the human face area is removed. A pre-processing image is processed. Illumination compensation process is performed. Size of a human face image is scaled to preset size. A determination is made to check the human face image is matched with a human face module.
   USE - Method for changing skin of a 3D human face picture.
   ADVANTAGE - The method enables rapidly detecting a human face multi-position and a digital image or video, effectively realizing face area buffing process to remove defect and retaining a detail part of the human face image. The method enables changing skin of a 3D human face picture with aesthetic in appearance.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a system for changing skin of a three-dimensional (3D) human face picture.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a method for changing skin of a 3D human face picture.'(Drawing includes non-English language text)'
PD CN104318262-A   28 Jan 2015   G06K-009/66   201522   Pages: 9   Chinese
UT DIIDW:2015186657
ER

PT P
PN CN103914686-A; CN103914686-B
TI Human face collect illumination certification comparing method for biological characteristic identification field, involves judging whether current certificate is successfully authenticated or not according to obtained matching degree value.
AB    NOVELTY - The method involves collecting a real human face image. Document illumination verification is performed. Human face salient features of a network-shaped structure model are obtained. Local gradient characteristic value is calculated. A real key characteristic point of the human face is detected. An external key characteristic point is selected as a comparison characteristic point. Position information of the comparison characteristic point is obtained. Judgment is made whether current certificate is successfully authenticated or not according to an obtained matching degree value.
   USE - Human face collect illumination certification comparing method for a biological characteristic identification field.
   ADVANTAGE - The method enables avoiding influence of glasses, eyebrows and hair external shields with high precision and ensuring wide range of applications.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face collect illumination certification comparing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face collect illumination certification comparing method. '(Drawing includes non-English language text)'
PD CN103914686-A   09 Jul 2014   G06K-009/00   201465   Pages: 13   Chinese
   CN103914686-B   19 Jan 2018   G06K-009/00   201808      Chinese
UT DIIDW:2014R98637
ER

PT P
PN CN103824059-A; CN103824059-B
TI Video image sequence based human face expression identifying method, involves extracting video user information, video key frame and human face emotional expression characteristic, and identifying human face area.
AB    NOVELTY - The method involves extracting video user information, video key frame and human face emotional expression characteristic. A human face area is identified. Video reflecting texture feature is extracted according to deficit moment characteristic parameter. Large and minimum normalization process are performed according to change curve parameter. RGB model is converted into YCbCr model based on video image. Image identify process is performed by using an Euclidean distance classifier.
   USE - Video image sequence based human face expression identifying method.
   ADVANTAGE - The method enables establishing an expression library for user expression identification, extracting texture feature of the key frame in an effective manner, avoiding interference, reducing calculation complexity and increasing identifying efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram illustrating a video image sequence based human face expression identifying method.'(Drawing includes non-English language text)'
PD CN103824059-A   28 May 2014   G06K-009/00   201449   Pages: 16   Chinese
   CN103824059-B   15 Feb 2017   G06K-009/00   201716      Chinese
UT DIIDW:2014N87548
ER

PT P
PN CN103400108-A; WO2015003522-A1; CN103400108-B
TI Method for identifying human face of mobile terminal, involves capturing visible face image according to preset acquisition rule, and obtaining frame rate and image resolution ratio when sensing camera collects visible face image.
AB    NOVELTY - The method involves capturing a visible face image according to a preset acquisition rule by using a double light sensing camera. The preset collecting rule is included with an image sequence of number of the visible face image. Judgment is made whether the face image is matched with an infrared face image of a human face template image or not by using face identification. The face image is adjusted according to a current environment light condition. Frame rate and image resolution ratio are obtained when the sensing camera collects the visible face image.
   USE - Method for identifying a human face of a mobile terminal (claimed).
   ADVANTAGE - The method enables improving face identification accuracy, saving hardware space and reducing cost. The method enables allowing a user to carry and operate a mobile terminal in a convenient manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a human face identifying device
   (2) a mobile terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a human face identification method.'(Drawing includes non-English language text)'
PD CN103400108-A   20 Nov 2013   G06K-009/00   201406   Pages: 20   Chinese
   WO2015003522-A1   15 Jan 2015   G06K-009/00   201506      Chinese
   CN103400108-B   14 Jul 2017   G06K-009/00   201749      Chinese
UT DIIDW:2014B36800
ER

PT P
PN CN103220583-A
TI Intelligent TV user fatigue detecting system, has human eye location module for detecting retinal part, and data processing module for determining whether closing and prompting function is timely sent to TV or not.
AB    NOVELTY - The system has a human face image capturing module for capturing camera dynamic identification of a human face. A human eye location module for detecting retinal part under illumination range of 850nm and 940nm. An image processing module is utilized for matching a human eye position. A data comparing module is utilized for comparing a time scale value with a preset fatigue threshold value. A human eye closed state analysis module sends comparison result to a data processing module that is utilized used for determining whether closing and prompting function is timely sent to a TV or not.
   USE - Intelligent TV user fatigue detecting system.
   ADVANTAGE - The system utilizes an infrared camera for capturing a human face image to detect retinal and face other position of an operator under condition of same illumination of different 850 nm and 940nm infrared reflecting image grey value so as to accurately locate eyes of the operator. The system can detect a human eye closed state in real time to judge fatigue degree of the operator and shutdown a TV by closing and prompting function according to a set rule.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an intelligent TV user fatigue detecting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an operation of an intelligent TV user fatigue detecting system.'(Drawing includes non-English language text)'
PD CN103220583-A   24 Jul 2013   H04N-021/472   201372   Pages: 8   Chinese
UT DIIDW:2013T54525
ER

PT P
PN US2013156274-A1; WO2013095977-A1; CN103049520-A; TW201337795-A; KR2014105478-A; EP2795570-A1; JP2015510622-W; EP2795570-A4
TI Computer-readable medium for use with instructions executed by computer to identify target users through their ages or any other appropriate information on social networks, includes receiving indication of face in photograph from user.
AB    NOVELTY - The computer-readable medium includes receiving an indication of a face in a photograph from a user (102). The information is sent to represent the face to a server (118) that operates a social network. The server identifies one or more candidates of the face. A list of one or more candidates (132) is received from the server based on the list of candidates. A request is received from the user to add a person associated with the face as a connection in a social network. One or more candidates are added as a connection of the user in the social network.
   USE - Computer-readable medium for use with instructions executed by a computer to identify target users through their ages, photographs, city of residence, workplace, affiliations, interests, or any other appropriate information on social networks such as Facebook or Twitter.
   ADVANTAGE - The computer-readable medium includes receiving an indication of a face in a photograph from a user, where the information is sent to represent the face to a server that operates a social network, and the server identifies one or more candidates of the face, and thus ensures simple and accurate identification of the target users on social networks.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a method for identifying a request target based on a picture; and
   (2) a system for identifying a request target based on a picture.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a computer-readable medium, in which a user uses a picture to perform an action.
   User (102)
   Server (118)
   Social graph (122)
   Selection component (130)
   List of one or more candidates (132)
PD US2013156274-A1   20 Jun 2013   G06K-009/00   201342   Pages: 12   English
   WO2013095977-A1   27 Jun 2013   G06Q-050/30   201342      English
   CN103049520-A   17 Apr 2013   G06F-017/30   201364      Chinese
   TW201337795-A   16 Sep 2013   G06K-009/78   201378      Chinese
   KR2014105478-A   01 Sep 2014   G06Q-050/30   201460      
   EP2795570-A1   29 Oct 2014   G06Q-050/30   201471      English
   JP2015510622-W   09 Apr 2015   G06Q-050/10   201524   Pages: 18   Japanese
   EP2795570-A4   05 Aug 2015   G06Q-010/10   201768      English
UT DIIDW:2013L18522
ER

PT P
PN CN103049084-A; CN103049084-B
TI Display direction adjustable electronic device has display control module that is used for adjusting direction of image displayed by display module according to opposite directions of user face features and display screen.
AB    NOVELTY - The electronic device has a camera module that is used for acquiring picture information of a user. A facial recognition module is used for recognizing direction of the face of user according to picture information of user. A display control module is used for adjusting the direction of an image displayed by display module according to opposite directions of user face features and display screen.
   USE - Display direction adjustable electronic device.
   ADVANTAGE - The direction of the image displayed by the display module can be consistent with the view direction of the user. The display direction of the display screen can be adjusted according to the face direction of the user.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for adjusting display direction of electronic device according to face direction.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the method for adjusting display direction of electronic device according to face direction. (Drawing includes non-English language text)
PD CN103049084-A   17 Apr 2013   G06F-003/01   201364   Pages: 11   Chinese
   CN103049084-B   27 Jan 2016   G06F-003/01   201613      English
UT DIIDW:2013P47616
ER

PT P
PN CN102970411-A; CN102970411-B
TI Intelligent mobile phone screen locking and unlocking control method, involves receiving corresponding unlocking operation key-press when present human face is similar to video/image stored in database.
AB    NOVELTY - The method involves receiving unlocking instruction and starting operation of a mobile phone camera when the phone is unlocked. A video/image is acquired through the camera. Human face identification technology is used for identifying the human face according to the video/image and the human face is identified. The identified present human face is stored in the database of the standard face model to provide authentication. A corresponding unlocking operation key-press is received when the present human face is similar to the video/image stored in the database.
   USE - Intelligent mobile phone screen locking and unlocking control method.
   ADVANTAGE - The method enables allowing the user to prevent error operation conveniently and save energy efficiently when using the intelligent mobile phone.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an intelligent mobile phone screen locking and unlocking control system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating an intelligent mobile phone screen locking and unlocking control method. '(Drawing includes non-English language text)'
PD CN102970411-A   13 Mar 2013   H04M-001/725   201350   Pages: 15   Chinese
   CN102970411-B   26 Jun 2018   H04M-001/725   201844      Chinese
UT DIIDW:2013J81293
ER

PT P
PN US8306267-B1; US2012288142-A1; WO2012154832-A2; WO2012154832-A3; EP2707852-A2; EP2707852-A4
TI Computer-implemented method for object tracking comprises identifying indications of candidate templates normalizing respective regions of image to generate candidate templates, and changing viewpoint of the at least one respective region.
AB    NOVELTY - A computer-implemented method for object tracking comprises accessing an indication of a first template that includes region of a first image; receiving, by the computing system, a second image; identifying indications of candidate templates normalizing respective regions of the second image to generate candidate templates; changing the viewpoint of the at least one respective region; and comparing, by the computing system, at least the first template to each of the multiple candidate templates.
   USE - Computer-implemented method for object tracking.
   ADVANTAGE - The computer system can modify visual display based on the movement of the face.
   DETAILED DESCRIPTION - Computer-implemented method for object tracking comprises:
   (A) accessing, by a computing system, an indication of a first template that includes a region of a first image, the region of the first image including a graphical representation of a human face;
   (B) receiving, by the computing system, a second image;
   (C) identifying, by the computing system, indications of candidate templates, each respective candidate template from multiple candidate templates includes respective candidate region of second image;
   (D) normalizing respective regions of the second image to generate candidate templates including changing viewpoint of at least one of the respective regions of the second image;
   (E) changing the viewpoint of the at least one respective region including changing viewpoint so that the human face in respective candidate template is directed forward; and
   (F) comparing, by the computing system, at least the first template to each of the multiple candidate templates, to identify matching template from among candidate templates that includes candidate region that matches region of first image that includes graphical representation of human face.
   Candidate templates are normalized representations of their respective regions of second image. Human face in the at least one respective region of second image is not directed forward. An INDEPENDENT CLAIM is included for a computerized system which comprises camera adapted to receive frames of video; one or more tangible computer-readable memory devices to store indication of a first region; computerized face candidate generator; computerized match determiner to compare candidate regions and candidate templates.
   DESCRIPTION OF DRAWING(S) - The drawing is a flowchart illustrating the process for performing face identification in an image.
   Performing comprehensive face detection (304)
   Storing template (306)
   Receiving image (310)
   Generating candidate template (312)
   Determining best match (314)
PD US8306267-B1   06 Nov 2012   G06K-009/00   201275   Pages: 25   English
   US2012288142-A1   15 Nov 2012   G06K-009/00   201275      English
   WO2012154832-A2   15 Nov 2012   G06T-007/20   201275      English
   WO2012154832-A3   21 Mar 2013   G06T-007/20   201321      English
   EP2707852-A2   19 Mar 2014   G06T-007/20   201421      English
   EP2707852-A4   23 Sep 2015   G06T-007/20   201770      English
UT DIIDW:2012P32981
ER

PT P
PN CN102750544-A; CN102750544-B
TI Number plate identification based safety belt non-fastening violated vehicle driving checking system, has safety belt detection module detecting whether lower part of human face is fastened with safety belt.
AB    NOVELTY - The system has a human face detecting module detecting a human face image. A safety belt detection module is utilized for detecting whether a lower part of the detected human face is fastened with a safety belt. A number plate is provided with a number plate positioning module, a dividing module and a character identifying module. The number plate positioning and human face detecting modules utilize an adaboost classification algorithm for detecting and locating the human face image and the number plate.
   USE - Number plate identification based safety belt non-fastening violated vehicle driving checking system.
   ADVANTAGE - The system improves detection accuracy.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a number plate identification based safety belt non-fastening violated vehicle driving checking method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a number plate identification based safety belt non-fastening violated vehicle driving checking method.
PD CN102750544-A   24 Oct 2012   G06K-009/62   201313   Pages: 9   Chinese
   CN102750544-B   14 Jan 2015   G06K-009/62   201520      Chinese
UT DIIDW:2013B21244
ER

PT P
PN WO2012128382-A1; CN102682273-A
TI Video-based lip motion detection device for use in speech recognition and video conference systems has detection unit that determines lip motion based on visual feature of mouth region extracted from face searched from input video frame.
AB    NOVELTY - The detection device (10) has a face search unit (110) for searching a face in an input video frame using a Viola-Jones face detector; a mouth region extraction unit (120) for acquiring a mouth region from the searched face using an active shape model (ASM); and a visual feature extraction unit (130) for acquiring a visual feature of the mouth region. A detection unit (140) determines lip motion based on the extracted visual feature of the mouth region. The visual feature comprises a Local Binary Pattern on Three Orthogonal Planes (LBP-TOP)-based visual feature.
   USE - Video-based lip motion detection device for use in speech recognition system and video conference system (both claimed).
   ADVANTAGE - Enables to implement subject-independent lip motion detection for a training set containing a limited number of subjects and enables a higher detection rate for a subject not contained in the training set. Improves usability for a particular user and improves detection accuracy since retraining or adaptation for different users for improving the detection rate is not necessary.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) video-based lip motion detection method;
   (2) a speech recognition system; and
   (3) a video conference system.
   DESCRIPTION OF DRAWING(S) - The drawing shows block diagram of a video-based lip motion detection device.
   Video-based lip motion detection device (10)
   Face search unit (110)
   Mouth region extraction unit (120)
   Visual feature extraction unit (130)
   Detection unit (140)
PD WO2012128382-A1   27 Sep 2012   G06T-007/20   201265   Pages: 46   English
   CN102682273-A   19 Sep 2012   G06K-009/00   201279      Chinese
UT DIIDW:2012M55133
ER

PT P
PN CN101986328-A; CN101986328-B
TI Three-dimensional face identification method, involves connecting points into irregular polygon by straight line, calculating surrounded area, and considering obtained three projection areas as local feature of sampling point.
AB    NOVELTY - The method involves calculating a projection area by respectively projecting points in a neighbor area on an XOY plane, and projecting the points in another neighbor area on an XOZ plane. The points are projected in a third neighbor area on a YOZ plane. An outermost point of a projected scattered two-dimensional point cloud is obtained. The points are connected into an irregular polygon by a straight line. A surrounded area is calculated, and the obtained three projection areas are considered as a local feature of a sampling point.
   USE - Three-dimensional face identification method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a three-dimensional face identification method. '(Drawing includes non-English language text)'
PD CN101986328-A   16 Mar 2011   G06K-009/00   201130   Pages: 16   Chinese
   CN101986328-B   27 Jun 2012   G06K-009/00   201273      Chinese
UT DIIDW:2011E44664
ER

PT P
PN CN101763503-A; CN101763503-B
TI Gesture robust face identification method, involves positioning facial key characteristic point to normalize face region, and inputting linear combination coefficient to classifier to obtain face identification result.
AB    NOVELTY - The method involves detecting a face region in which face images are inputted, and positioning a facial key characteristic point to normalize the face region. A gesture of an input face is estimated to obtain a gesture type of the input face. A ridge regression model is utilized to obtain a linear combination coefficient according to an extracted characteristic of the face region and a coupled face data based on the gesture type. The linear combination coefficient is input to a classifier to obtain a face identification result.
   USE - Gesture robust face identification method.
   ADVANTAGE - The method reduces sensitivity of face identification to gesture difference and enhances adaptability of face identification to various application scenes with better identification capacity.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a gesture robust face identification method. '(Drawing includes non-English language text)'
PD CN101763503-A   30 Jun 2010   G06K-009/00   201055   Pages: 11   Chinese
   CN101763503-B   22 Aug 2012   G06K-009/00   201302      Chinese
UT DIIDW:2010K07569
ER

PT P
PN CN101667246-A; CN101667246-B
TI Face identification method for monitoring and protecting e.g. public security, involves resolving seeking minimum value in residuals and marking as final face identification result that is represented by identity.
AB    NOVELTY - The method involves mapping a training sample matrix and testing sample to a kernel space, and randomly reducing dimensions of the mapped sample to needed dimensionality. A sample construction coefficient vector of normalized training sample matrix and testing sample is resolved. Residual of the reconstructed testing sample and normalized testing sample is counted. Residual of the testing sample is substituted into type judging formula of the testing sample. A seeking minimum value in the residuals is resolved and marked as final face identification result that is represented by an identity.
   USE - Face identification method for monitoring and protecting public security, information security and financial security.
   ADVANTAGE - The method enables resolving the sample reconstruction coefficient vector in reasonable manner. The method enables improving precision in face identification application, and simultaneously popularizing application range to low dimension sample.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating process involved in a face identification method.'(Drawing includes non-English language text)'
PD CN101667246-A   10 Mar 2010   G06K-009/00   201025   Pages: 12   Chinese
   CN101667246-B   20 Jul 2011   G06K-009/00   201173      Chinese
UT DIIDW:2010D15329
ER

PT P
PN CN101561874-A; CN101561874-B
TI Face image recognition method, involves classifying virtual image to obtain classification result as preset classification result, and recognizing two dimensional face image with preset classification result.
AB    NOVELTY - The method involves accurately locating two dimensional (2D) face image in preset database. A three dimensional (3D) rebuilding operation is performed to the 2D face image based on a preset 3D face shape model and accurate located result of the 2D face image to obtain 3D face image. The 3D face image in an illumination model is processed to obtain virtual image with changed posture and illumination. The virtual image is classified to obtain classification result as preset classification result. The 2D face image is recognized with the preset classification result.
   USE - Method for recognizing a face image.
   ADVANTAGE - The method enables producing the virtual image through the three dimensional (3D) rebuilding operation and processing in illumination model to the two dimensional (2D) face image, thus increasing the posture of the image and sample space of changing illumination and increasing the 3D rebuilding speed to implement high efficiency and recognition rate of the face image.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart explaining the process involved in face image recognition.'(Drawing includes non-English language text)'
PD CN101561874-A   21 Oct 2009   G06K-009/00   200974   Pages: 23   Chinese
   CN101561874-B   26 Oct 2011   G06K-009/62   201177      Chinese
UT DIIDW:2009Q63391
ER

PT P
PN CN101515324-A
TI Multiple face postures identification distribution-control system, has comparing unit comparing face characteristics and corresponding face characteristics in database to obtain group of similar values.
AB    NOVELTY - The system has a distribution-control database storing face characteristics of different postures of a distribution-control object. A video acquiring unit acquires video data of each frame, and a face detection locating unit detects a face in the video data and locates positions of important organs of the face. A comparing unit compares the face characteristics which are extracted by face identification classifiers and corresponding face characteristics in the database to obtain group of similar values. The values are sequenced to choose a maximum value to obtain a maximum similarity.
   USE - Multiple face postures identification distribution-control system.
   ADVANTAGE - The system can improve identification rate of the largest posture of the face in a video distribution-control.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face identification method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a multiple face postures identification distribution-control system.'(Drawing includes non-English language text)'
PD CN101515324-A   26 Aug 2009   G06K-009/00   200960   Pages: 12   Chinese
UT DIIDW:2009N35812
ER

PT P
PN EP2075151-A1; CN101468605-A; US2009169068-A1; JP2009160988-A; EP2075151-B1; CN101468605-B; US8306278-B2
TI Detecting device for detecting fake test of blood alcohol concentration, picks up face image of driver shot within predetermined period of time and checks testee and driver based on face images of testee and driver.
AB    NOVELTY - A testee image pick-up unit (152) picks up a face image of testee shot during an alcohol measurement. A driver image pick-up unit (161) picks up a face image of the driver shot within a predetermined period of time before or after the alcohol measurement. A checking unit (132) checks the testee and driver based on the face images of testee and driver.
   USE - Detecting device for detecting fake test of blood alcohol concentration (BAC). Can also be used in other BAC measurements performing blood test.
   ADVANTAGE - The fake test of BAC taken by another testee different from actual driver can be avoided, since the testee and driver are checked based on the face images of testee and driver.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) detecting method for detecting fake test of BAC;
   (2) program for detecting fake test of BAC; and
   (3) detection system for detecting fake test of BAC.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the drunk driving prevention system.
   Drunk driving prevention system (101)
   Drunk driving prevention device (114)
   Checking unit (132)
   Testee image pick-up unit (152)
   Driver image pick-up unit (161)
PD EP2075151-A1   01 Jul 2009   B60K-028/06   200946   Pages: 18   English
   CN101468605-A   01 Jul 2009   B60K-028/06   200946      Chinese
   US2009169068-A1   02 Jul 2009   G06K-009/00   200946      English
   JP2009160988-A   23 Jul 2009   B60R-016/02   200948   Pages: 18   Japanese
   EP2075151-B1   13 Jul 2011   B60K-028/06   201146      English
   CN101468605-B   04 Jul 2012   B60K-028/06   201273      Chinese
   US8306278-B2   06 Nov 2012   G06K-009/00   201274      English
UT DIIDW:2009K99268
ER

PT P
PN CN101447021-A
TI Fast face recognition system, has image match server connected with image application server and transmitting image matching information that comprises image identification, grey chart to be matched, and matched result.
AB    NOVELTY - The system has an image match server connected with an image application server and transmitting image matching information. The image application server transmits the image file to be matched in image recognition information transmitted from a network client to an the image recognition control server, where the image recognition information comprises image identification (ID), image to be matched and recognized result, and the image matching information comprises image ID, grey chart to be matched, and matched result.
   USE - Fast face recognition system.
   ADVANTAGE - The image application server is connected with the image match server and an image database server for respectively transmitting image retrieving requirements and image matching information. The system can process similarity searching in a large range of databases under the condition of recognizing a single image sample.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for recognizing a face utilizing a fast face recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a fast face recognition system.'(Drawing includes non-English language text)'
PD CN101447021-A   03 Jun 2009   G06K-009/00   200941   Pages: 14   Chinese
UT DIIDW:2009K29596
ER

PT P
PN CN101388075-A; CN101388075-B
TI Human face identification method for carrying out fusion of global and partial characteristics of image, involves extracting discrete cosine transform characteristic and Gabor characteristic from training collection images.
AB    NOVELTY - The method involves extracting discrete cosine transform (DCT) characteristic and Gabor characteristic from training collection images, and stacking the DCT characteristic of the training collection image in a vector. An average value removing and whitening process is carried out to the Gabor characteristic of the training collection image. The independent DCT characteristic and the Gabor characteristic of a same image are connected in series, and the independent characteristic of the training image is obtained.
   USE - Human face identification method for carrying out fusion of global and partial characteristics of an image based on independent component analysis (ICA).
   ADVANTAGE - The method effectively reduces the dimension of the characteristic vector for removing the redundant characteristic. The fused independent characteristic is used in a support vector machine (SVM) for realizing the human face classifying identification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face identification method.'(Drawing includes non-English language text)'
PD CN101388075-A   18 Mar 2009   G06K-009/00   200926   Pages: 10   Chinese
   CN101388075-B   16 Nov 2011   G06K-009/00   201182      Chinese
UT DIIDW:2009G75742
ER

PT P
PN US2009052750-A1; US7860274-B2
TI Method for processing digital image using face detection, by automatically providing fill flash to digitally add exposure to brighten group of pixels that correspond to image of face.
AB    NOVELTY - The method involves: determining default values of one or more parameters of at least some portion of the digital image; adjusting values of one or more parameters within the digitally-detected image based upon an analysis of the digital image including the image of the face and the default values; and, automatically providing a fill flash to digitally add exposure to brighten the group of pixels that correspond to the image of the face while not digitally adding exposure to nor otherwise brightening one or more other pixels within the digital image.
   USE - Method for processing digital image using face detection in the image to achieve a desired image processing parameter. Uses include but are not limited to orientation, color, tone, size, luminance, relative exposure, relative spatial location, tone reproduction, sharpness or focus.
   ADVANTAGE - Color correction or enhancement can be automatically suggested or provided for digital images based on color or tonal values of faces in the image. Camera can be automatically focused prior to acquiring an image based on knowledge regarding the faces in an image. Multiple targets such as multiple faces or facial regions can be tracked and/or zoomed, and preferably digitally enhanced, e.g., in view of aesthetic considerations.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a portable digital image acquisition device.
   DESCRIPTION OF DRAWING(S) - The drawing illustrates the main workflow of correcting images based on finding faces in the images.
   Step of determining if faces are in the picture (106)
   Step of marking faces (118)
   Step of panning and zooming into the faces (120)
   Step for automatically orienting the image (130)
   Step of color correction of image (140)
PD US2009052750-A1   26 Feb 2009   G06K-009/00   200918   Pages: 38   English
   US7860274-B2   28 Dec 2010   G06K-009/00   201102      English
UT DIIDW:2009F53753
ER

PT P
PN CN101350063-A; CN101350063-B
TI Face feature points locating device, has feature point search region determination module, and reprocessing module that combines candidate regions to obtain position and size of feature point region to locate feature point.
AB    NOVELTY - The device has a face detection module performing a face detection in a collected image region to obtain a face region, and a feature point search region determination module provided for determining a feature point search range and a possible range in the face region based on feature points on a face. A two-stage classifier classifies a candidate region, and outputs the judged feature point region to a reprocessing module as another candidate region. A reprocessing module combines the candidate regions to obtain a position and size of a feature point region to locate a feature point.
   USE - Face feature points locating device.
   ADVANTAGE - The device ensures the haar-like feature speed, and multiple interferences are avoided by using less operation. The description capability is enhanced by using the Local-Binary-Pattern (LBP) feature. The interferences provided close to the feature point region are differentiated, and the expression capability of the LBP feature is increased.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face feature points locating method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a face feature points locating device.'(Drawing includes non-English language text)'
PD CN101350063-A   21 Jan 2009   G06K-009/00   200913   Pages: 23   Chinese
   CN101350063-B   28 Dec 2011   G06K-009/00   201208      Chinese
UT DIIDW:2009E70327
ER

PT P
PN CN101339606-A; CN101339606-B
TI Method for locating external feature point of facial organ for animation applications, involves calculating average grey value of each pixel point in each candidate group composed according to boundary size of eye profile point.
AB    NOVELTY - Multiple candidate points of the external profile feature points of upper and lower eyelids on the center axis are determined. Multiple candidate point groups are composed according to boundary size of eye profile point. The average grey value of each pixel point is calculated.
   USE - Method for locating external feature point of facial organ, for animation applications.
   ADVANTAGE - Facial action can be grasped reliably in real time so as to generate real and vivid animation.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) method for tracking external feature point of facial organ;
   (2) device for locating external feature point of facial organ; and
   (3) device for tracking external feature point of facial organ.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flowchart explaining the process of locating external feature point of facial organ. (Drawing includes non-English language text)
PD CN101339606-A   07 Jan 2009   G06K-009/00   200914   Pages: 25   Chinese
   CN101339606-B   12 Oct 2011   G06K-009/00   201177      Chinese
UT DIIDW:2009E26708
ER

PT P
PN US2009003709-A1; EP2009577-A1; JP2009015372-A; CN101334839-A; EP2009577-B1; DE602008005958-E; JP4974788-B2; US8538091-B2; CN101334839-B
TI Image-processing apparatus for recognizing human facial expression, has change unit changing evaluation formula and parameter of formula to increase variation, when variation within predetermined time is smaller than predetermined value.
AB    NOVELTY - The apparatus has a face region extraction unit extracting a face region of a person from an input image. A calculation unit calculates a facial expression evaluation value from the extracted face region using an evaluation formula. A determination unit determines a facial expression represented by the face region based on the calculated value. A change unit changes the evaluation formula and a parameter of the evaluation formula to increase a variation, when the variation of the calculated value within a predetermined time is smaller than a predetermined value.
   USE - Image-processing apparatus for recognizing a human facial expression of an emotion e.g. joy and anger.
   ADVANTAGE - The apparatus effectively executes accurate facial expression recognition even for a subject hard to recognize a facial expression of a person whose facial parts such as the eyes and mouth change shapes only in a small amount.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) an image-processing method
   (2) a computer-readable storage medium that stores a program for causing an image-processing apparatus to perform an image-processing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an input image and a normalized image.
   Image (2200)
   Normalized images (2201, 2202)
PD US2009003709-A1   01 Jan 2009   G06K-009/46   200903   Pages: 26   English
   EP2009577-A1   31 Dec 2008   G06K-009/00   200904      English
   JP2009015372-A   22 Jan 2009   G06T-007/20   200907   Pages: 18   Japanese
   CN101334839-A   31 Dec 2008   G06K-009/00   200910      Chinese
   EP2009577-B1   06 Apr 2011   G06K-009/00   201124      English
   DE602008005958-E   19 May 2011   G06K-009/00   201133      German
   JP4974788-B2   11 Jul 2012   G06T-007/20   201245   Pages: 17   Japanese
   US8538091-B2   17 Sep 2013   G06K-009/00   201361      English
   CN101334839-B   14 Aug 2013   G06K-009/00   201375      Chinese
UT DIIDW:2009A76301
ER

PT P
PN CN101227278-A; CN101227278-B
TI Remote network identification authentication system, has.
AB    NOVELTY - The system has a remote network for transmitting the communication data between a central authentication server and a multi-biometric network collecting and controlling terminal. A network connection device e.g. wired card or Bluetooth, for connecting the central authentication server via Ethernet. A central authentication server is provided for comparing the received biometric data with the biometric data stored on the central authentication server and transmitting control instructions to a multi-biometric network collecting and controlling terminal.
   USE - System for authenticating a remote network identification based on multi-biometrics and for connecting a computer to device for collecting a set of biometrics such as fingerprint, human face, iris and retina via a remote network by a central authentication server.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multi-biometric remote network authentication method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a system for authenticating a remote network identification based on multi-biometrics.
PD CN101227278-A   23 Jul 2008   H04L-009/32   200858   Pages: 28   Chinese
   CN101227278-B   27 Oct 2010   H04L-009/32   201108      Chinese
UT DIIDW:2008J85078
ER

PT P
PN WO2008072622-A1; JP2008146539-A; EP2091021-A1; CN101558431-A; US2010316265-A1; CN101558431-B; JP5010905-B2; US8320643-B2; EP2091021-A4
TI Face recognition apparatus in security system, determines whether input and registered face image data are identical based on comparison of set threshold value and similarity between feature value data of input and registered image data.
AB    NOVELTY - A calculation unit (50) calculates a similarity between feature value data of input and registered face image data, and a calculation unit (70) calculates a similarity between stored feature value data and an extracted feature value data similarity. A threshold value is set for judging whether input and registered face image data are identical based on similarity calculated by unit (70), and set threshold value and similarity calculated by unit (50) are compared to determine whether input and registered face image data are identical.
   USE - Face recognition apparatus for use in security system for personal identification.
   ADVANTAGE - Personal identification is carried out with high precision irrespective of the imaging environment of the face image.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) face recognition method; and
   (2) face recognition program.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of the face recognition apparatus. (Drawing includes non-English language text).
   Face image data acquisition unit (10)
   Feature value extraction units (20,40)
   Similarity calculation units (50,70)
   Threshold value setting unit (80)
   Identification judgment unit (90)
PD WO2008072622-A1   19 Jun 2008   G06T-007/00   200845   Pages: 34   Japanese
   JP2008146539-A   26 Jun 2008   G06T-007/00   200845   Pages: 17   Japanese
   EP2091021-A1   19 Aug 2009   G06T-007/00   200955      English
   CN101558431-A   14 Oct 2009   G06T-007/00   200970      Chinese
   US2010316265-A1   16 Dec 2010   G06K-009/00   201082      English
   CN101558431-B   07 Mar 2012   G06T-007/00   201235      Chinese
   JP5010905-B2   29 Aug 2012   G06T-007/00   201256   Pages: 17   Japanese
   US8320643-B2   27 Nov 2012   G06K-009/00   201279      English
   EP2091021-A4   04 Sep 2013   G06T-007/00   201770      English
UT DIIDW:2008H12917
ER

PT P
PN KR787807-B1; US2008149657-A1; US7894656-B2
TI Method and apparatus for inspecting a distribution error in a manual distributing tray of medicine packing machine.
AB    NOVELTY - A method and an apparatus for inspecting a distribution error in a manual distributing tray of medicine packing machine are provided to prevent any defective packing of medicine due to the distribution error made by a pharmacist.
   DETAILED DESCRIPTION - A method for inspecting a distribution error in a manual distributing tray of medicine packing machine includes the steps of: confirming the distribution of medicine in the manual distribution tray(S10); photographing the top face of the tray to obtain an image of the manual distribution state of medicine(S20); making an analysis on whether the number of compartments with medicine of the tray coincides with the number of compartments to be used for distribution of medicine(S21); indicating an error if the numbers are not coincident(S22); obtaining information on medicine(S30); making an analysis on whether information on medicine coincides(S40); displaying a distribution error(S50); and redistributing the medicine if a distribution error shows(S60).Image 1/1
PD KR787807-B1   21 Dec 2007   A61J-003/00   200842      
   US2008149657-A1   26 Jun 2008   G07F-011/00   200844      English
   US7894656-B2   22 Feb 2011   G06K-009/00   201115      English
UT DIIDW:2008G64501
ER

PT P
PN CN1975759-A; CN100423020-C
TI Face recognition method based on structural principal component analysis.
AB    NOVELTY - The invention belongs to the pattern recognition technology field, which is made up of key steps of geometrical normalization, image blocking, 2-dimension principal component analysis (2DPCA), similarity weighting coefficient etc. The invention implements the geometrical normalization taking eyes and mouth as the benchmark. After the geometrical normalization, the size of the face image of all the people is the same and the position of every part of face is fixure in the face image. Through the image block, the local face area contained within every image block is also fixure. The invention computes the principal component and the principal component characteristics with the 2-dimension principal component analysis. The similarity between two image blocks is the distance of their principal component characteristics and the similarity between two face images is the weighting sum of the similarity of their relevant image blocks. Through adjusting the quantity of principal components and the weighting coefficient of the similarity of two image blocks, the function of some image blocks can be protruded or restrained in the face recognition so as to realize the recognition task of many fragmentary face images.
PD CN1975759-A   06 Jun 2007   G06K-009/00   200775      Chinese
   CN100423020-C   01 Oct 2008   G06K-009/00   200906      Chinese
UT DIIDW:2007798864
ER

PT P
PN JP2006301847-A; US2007019863-A1; US7366330-B2; JP4744918-B2
TI Face detection method for use in e.g. recognition of photographed person, involves detecting small face in areas other than large face detection area of detection target images.
AB    NOVELTY - A large face is detected in an area of the detection target images when the face to be detected from the detection target images is set as a large face by a resolution image selector of a face detection system. A small face is detected in areas other than the large face detection area of the detection target images when the face to be detected from the detection target images is set as a small face by the resolution image selector.
   USE - For recognition of photographed person and for correction of skin color of face of photographed person.
   ADVANTAGE - The face detection operation is performed at high speed within a short time by eliminating useless face detection operation in the large face detection area.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) face detector; and
   (2) face detection program.
   DESCRIPTION OF DRAWING(S) - The figure shows a flowchart explaining the process performed by the face detection system. (Drawing includes non-English language text).
PD JP2006301847-A   02 Nov 2006   G06T-001/00   200708   Pages: 19   Japanese
   US2007019863-A1   25 Jan 2007   G06K-009/46   200710      English
   US7366330-B2   29 Apr 2008   G06K-009/00   200831      English
   JP4744918-B2   10 Aug 2011   G06T-001/00   201153   Pages: 19   Japanese
UT DIIDW:2007076548
ER

PT P
PN US2006082439-A1; WO2008054410-A2; WO2008054410-A3; EP1955290-A2; JP2009527804-W; EP1955290-B1; US7817013-B2; DE602006017578-E; IL191866-A
TI Biometric information verification method of person passing through control gate of large facilities, involves comparing face prints of person, retrieved from radio frequency identification tag with face image of person.
AB    NOVELTY - The information about the person and several face prints of the person are stored in a radio frequency identification (RFID) tag with an ID number. The tag is read for relating the read ID number with information stored in database. A face print retrieved from RFID tag is compared with the face image of the person.
   USE - For verifying biometric information of persons such as driver of motor vehicle passing through control gate of large facilities.
   ADVANTAGE - Fast and secure verification method is achieved. The system dynamically identifies vehicles and/or person by combining RFID and advanced facial detection and recognition techniques.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for dynamic stand-off verification system.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the security system.
PD US2006082439-A1   20 Apr 2006   G05B-019/00   200633   Pages: 19   English
   WO2008054410-A2   08 May 2008   G07C-009/00   200835      English
   EP1955290-A2   13 Aug 2008   G07C-009/00   200856      English
   JP2009527804-W   30 Jul 2009   G06K-017/00   200951   Pages: 22   Japanese
   EP1955290-B1   13 Oct 2010   G07C-009/00   201067      English
   US7817013-B2   19 Oct 2010   B60R-025/00   201068      English
   DE602006017578-E   25 Nov 2010   G07C-009/00   201077      German
   IL191866-A   31 Jul 2013   G07B-001/00   201360      English
UT DIIDW:2006314627
ER

PT P
PN JP2006099493-A; CN1760890-A; KR2006051655-A; CN100351850-C; KR746194-B1
TI Image pick-up device used for personal identification authentication has imaging section which sends light to interference-filter unit via mirror, so that image processing of digital veins was performed.
AB    NOVELTY - The imaging section sends the light to the interference-filter unit (5) for images via mirror, so that the image processing unit produces digital veins image data. The interference filter section has a light transmittance with respect to the wavelength. The light source generates the transmitted light from the side face with respect to the finger.
   USE - Used for personal identification authentication.
   ADVANTAGE - Provides easy authentication with respect to the finger, without reducing the authentication precision.
   DESCRIPTION OF DRAWING(S) - The figure shows the external-appearance perspective diagram of the personal-identification apparatus. (Drawing includes non-English language text).
   Top case (1)
   Bottom case (2)
   Light-source mounting section (3)
   Light-source window (4)
   Interference-filter unit (5)
PD JP2006099493-A   13 Apr 2006   G06T-001/00   200631   Pages: 8   Japanese
   CN1760890-A   19 Apr 2006   G06K-009/00   200652      Chinese
   KR2006051655-A   19 May 2006   G06K-009/00   200673      
   CN100351850-C   28 Nov 2007   G06K-009/00   200831      Chinese
   KR746194-B1   06 Aug 2007   G06K-009/00   200838      
UT DIIDW:2006296958
ER

PT P
PN US2006039600-A1; WO2006019350-A1; SE200402048-A; SE528068-C2; EP1810216-A1; US8064685-B2; EP1810216-B1; ES2385041-T3
TI Three dimensional object recognizing method for e.g. aircraft, involves detecting image features in obtained two dimensional representation, and comparing recovered three dimensional shape with reference representation of object.
AB    NOVELTY - The method involves obtaining a two dimensional (2D) representation of an object, and detecting image features in the obtained 2D representation. A highly probable three dimensional (3D) shape of a certain object class consistent with 2D images is recovered using the obtained image. The recovered 3D shape is compared with a reference representation of the object of the object class.
   USE - Used for recognizing a three dimensional (3D) object in an industrial process and a military purpose e.g. automatic determination of military vehicle, military ship, aircraft, and a face recognition system e.g. biometrics, information security, law enforcement, smart card, and access control.
   ADVANTAGE - The method effectively recognizes the three dimensional (3D) object in the industrial process in a simple and cost effective manner.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a computer program stored in a computer readable storage medium and executed in a computational unit for object recognition of a three dimensional (3D) object
   (B) a system for object recognition of a three dimensional (3D) object.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic illustration of a system based on the three dimensional object recognizing method.
   Image acquisition device (601)
   Computational device (603)
   Display system (604)
   Reference image (605)
   Obtained image (606)
PD US2006039600-A1   23 Feb 2006   G06K-009/00   200620   Pages: 13   English
   WO2006019350-A1   23 Feb 2006   G06K-009/00   200620      English
   SE200402048-A   20 Feb 2006   G06K-009/00   200623      Swedish
   EP1810216-A1   25 Jul 2007   G06K-009/00   200750      English
   US8064685-B2   22 Nov 2011   G06K-009/00   201177      English
   EP1810216-B1   28 Mar 2012   G06K-009/00   201222      English
   ES2385041-T3   17 Jul 2012   G06K-009/00   201323      Spanish
UT DIIDW:2006191667
ER

PT P
PN GB2414614-A; EP1600882-A1; US2005265603-A1; JP2006031678-A; US7630561-B2; JP4616702-B2
TI Image processing method in e.g. surveillance apparatus, involves detecting similarity between images in test set of images, so that selected subset of images comprises images having lowest similarity compared to other images in test set.
AB    NOVELTY - The method involves detecting similarity between images in a test set of images, so that the selected subset of images comprises the images having lowest similarity compared to other images in test set of images.
   USE - Used in image processing apparatus (claimed) such as video conferencing apparatus (claimed), closed circuit television (CCTV) and surveillance apparatus (claimed) for face detection and object tracking in harsh lighting environment such as shop.
   ADVANTAGE - The target objects can be tracked efficiently and easily.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) image processing software;
   (2) medium storing image processing software;
   (3) image processing apparatus;
   (4) video conferencing apparatus; and
   (5) surveillance apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic view of similarity detection process.
   face stamps (2000,2010,2020,2030,2040)
   earch window (2100)
PD GB2414614-A   30 Nov 2005   G06T-007/00   200601   Pages: 39   English
   EP1600882-A1   30 Nov 2005   G06K-009/62   200601      English
   US2005265603-A1   01 Dec 2005   G06K-009/00   200601      English
   JP2006031678-A   02 Feb 2006   G06T-007/20   200610   Pages: 24   Japanese
   US7630561-B2   08 Dec 2009   G06K-009/68   200980      English
   JP4616702-B2   19 Jan 2011   G06T-007/20   201106   Pages: 23   Japanese
UT DIIDW:2006001928
ER

PT P
PN US2004234109-A1; US7116803-B2
TI Motor vehicle e.g. car, security system, has facial-recognition system activating ignition switch if signals representative of characteristics of scanned face match one unit of set of signals stored in storage device.
AB    NOVELTY - The system has an electronic camera (12) to scan a face of a person (20) at a driver`s station of a motor vehicle and produce image signals. An electronic storage device stores a set of image signals. A facial-recognition system provides an authorization control signal and activates an ignition switch if signals representative of characteristics of the scanned face match a unit of the signals stored in the storage device.
   USE - Used for securing a motor vehicle e.g. car or a generic passenger vehicle such as a sedan-type automobile.
   ADVANTAGE - The use of the facial recognition system efficiently secures the motor vehicle against unauthorized operation and theft.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a method of securing a movable vehicle
   (B) an automatically starting movable vehicle.
   DESCRIPTION OF DRAWING(S) - The drawing shows a side view of a system for securing a motor vehicle.
   Electronic camera (12)
   Person (20)
   Driver`s seat (21)
   Ignition switch (22)
   Steering wheel (39)
   Rear window (43)
PD US2004234109-A1   25 Nov 2004   G06K-009/00   200505   Pages: 19   English
   US7116803-B2   03 Oct 2006   G06K-009/00   200665      English
UT DIIDW:2005046142
ER

PT P
PN EP1471722-A2; US2004215965-A1; CA2465088-A1; JP2004343712-A; KR2004092456-A; US6883982-B2; TW200426606-A; KR605432-B1; TW280489-B1; JP4227048-B2; JP2009050023-A; EP1471722-B1; DE602004020416-E; JP4746663-B2; EP1471722-A3
TI Facial image processing system for personal authentication, extracts spatial frequency component unique to key information from recorded composite image data, and reconstructs security information from extracted component.
AB    NOVELTY - A recording unit (106) records a composite image information generated by embedding invisible security information in visible human facial image on a recording medium (M). An extraction unit (112) extracts a spatial frequency component unique to key information from the recorded composite image information. A reconstructing unit (113) reconstructs the security information from extracted spatial frequency component.
   USE - For processing facial images for personal authentication on personal authentication media such as identification (ID) cards.
   ADVANTAGE - Improves the restoration characteristics and visibility of embedded security information without increasing the risk of disclosing the security information.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) image processing apparatus; and
   (2) image processing method.
   DESCRIPTION OF DRAWING(S) - The figure shows the flow diagram of the image processing system.
   recording unit (106)
   frequency component extraction unit (112)
   reconstructing unit (113)
PD EP1471722-A2   27 Oct 2004   H04N-001/32   200476   Pages: 36   English
   US2004215965-A1   28 Oct 2004   G06K-005/00   200476      English
   CA2465088-A1   25 Oct 2004   G06T-001/00   200477      English
   JP2004343712-A   02 Dec 2004   H04N-001/387   200479   Pages: 31   Japanese
   KR2004092456-A   03 Nov 2004   H04N-001/387   200517      
   US6883982-B2   26 Apr 2005   B41J-011/44   200528      English
   TW200426606-A   01 Dec 2004   G06F-017/00   200612      Chinese
   KR605432-B1   31 Jul 2006   H04N-001/387   200728      
   TW280489-B1   01 May 2007   G06F-017/00   200829      Chinese
   JP4227048-B2   18 Feb 2009   H04N-001/387   200914   Pages: 24   Japanese
   JP2009050023-A   05 Mar 2009   H04N-001/387   200918   Pages: 29   Japanese
   EP1471722-B1   08 Apr 2009   H04N-001/32   200925      English
   DE602004020416-E   20 May 2009   H04N-001/32   200934      German
   JP4746663-B2   10 Aug 2011   H04N-001/387   201152   Pages: 29   Japanese
   EP1471722-A3   06 Jul 2005   H04N-001/32   201731      English
UT DIIDW:2004768261
ER

PT P
PN US2004120548-A1; US7194110-B2
TI Feature tracking method for generating facial animation, involves generating candidates for feature point on image frame from information about candidates for feature point on other image frame.
AB    NOVELTY - The method involves initializing feature points on an image frame and generating candidates for each of the feature points on another image frame. Candidates are generated for a feature point on a third image frame from information about candidates for another feature point on the latter image frame. The candidates are generated for the third frame points by selecting a candidate in a neighborhood of a fixed distance.
   USE - Used for performing feature tracking in a video sequence to generate facial animation and to compress image data for video conferencing.
   ADVANTAGE - The method effectively and efficiently performs the tracking of the feature points in the video sequence.
   DETAILED DESCRIPTION - The fixed distance is projected between the feature points from a position of a candidate selected for latter feature points on the latter image frame. INDEPENDENT CLAIMS are also included for the following:
   (a) a machine readable medium having stored sequences of instructions that causes processor to perform feature tracking on image frame
   (b) a feature tracking unit.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an image processing unit.
   Image processing unit (200)
   Feature initialization unit (210)
   Tracking unit (220)
   Initial candidate pool selection unit (221)
   Feature point matching unit (222)
PD US2004120548-A1   24 Jun 2004   G06K-009/00   200449   Pages: 16   English
   US7194110-B2   20 Mar 2007   G06K-009/00   200723      English
UT DIIDW:2004516485
ER

PT P
PN JP2004127285-A; US2004109587-A1; US7340079-B2; US2008085037-A1; US7623688-B2; JP4427714-B2
TI Image recognition apparatus for face recognition of person in security area, has object specification unit which specifies recognition identification corresponding to object image photographed by image pickup unit.
AB    NOVELTY - An object image photographed by an image pickup unit is categorized into attributes corresponding to various photographed situations. A selection unit selects an image database classified according to attribute corresponding to present date counted by sun dial number unit. An object specification unit specifies the recognition identification (ID) corresponding to object image photographed by the image pickup unit.
   USE - For face recognition of person in security area, human interface of pet robot e.g. dog robot, also for photographing logo mark put up on person, shop.
   ADVANTAGE - Image database of optimal attribute is selected automatically and reference image is narrowed down by additional information of a situation, even if user does not perform selection operation intentionally. Thus improvement in recognition rate is achieved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) image recognition processing method; and
   (2) image recognition program.
   DESCRIPTION OF DRAWING(S) - The figure shows a conceptual diagram of the concept of the intimate degree database.(Drawing includes non-English language text).
PD JP2004127285-A   22 Apr 2004   G06T-007/00   200436   Pages: 22   Japanese
   US2004109587-A1   10 Jun 2004   G06K-009/00   200438      English
   US7340079-B2   04 Mar 2008   G06K-009/00   200819      English
   US2008085037-A1   10 Apr 2008   G06K-009/00   200827      English
   US7623688-B2   24 Nov 2009   G06K-009/00   200977      English
   JP4427714-B2   10 Mar 2010   G06T-007/00   201018   Pages: 19   Japanese
UT DIIDW:2004381937
ER

PT P
PN CN1428718-A; CN1428718-B
TI Airport outgoing passenger intelligent identity identification method and system.
AB    NOVELTY - At security-check exit a face image identification technique is used to collect face data, and an identity card identification apparatus is used to automatically identify identity card and card number. Human face image automatic collection, compression and identification technique is adopted to implement real time alarm for indicating a suspect. It adopts large-scale database technique for storing various data materials for a long period.
   USE - The present invention relates to an intelligent identity identification method for outgoing passenger in airport and its system.
PD CN1428718-A   09 Jul 2003   G06F-017/00   200370      Chinese
   CN1428718-B   20 Oct 2010   G06F-017/00   201102      Chinese
UT DIIDW:2003732368
ER

PT P
PN US2003063780-A1; WO2003030085-A1; EP1433118-A1; KR2004037180-A; JP2005505062-W; CN1559051-A; EP1433118-B1; DE60208223-E; DE60208223-T2; CN1276380-C; US7308133-B2
TI Facial image data classification method for neural network, involves comparing proportions of unknown facial image with that of learned model proportions for each known image class.
AB    NOVELTY - A vector with data representing unknown facial image portion to be recognized, is input to a radial basis function (RBF) classifier. The different proportions of the unknown facial image, are classified iteratively by comparing proportions against corresponding proportions of stored learned model image for each known image class, so as to obtain corresponding known image class.
   USE - For recognizing unknown facial image data in neural networks e.g. radial basis function (RBF) network.
   ADVANTAGE - A single classifier is used to recognize multiple profiles of an individual's face, therefore time for classification of the image, and space for storing learned model files are reduced.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) apparatus for classifying facial image data; and
   (2) machine readable storage device storing program for classifying unknown facial image data.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic diagram of the architecture of RBF network.
PD US2003063780-A1   03 Apr 2003   G06K-009/00   200345   Pages: 7   English
   WO2003030085-A1   10 Apr 2003   G06K-009/00   200345      English
   EP1433118-A1   30 Jun 2004   G06K-009/00   200443      English
   KR2004037180-A   04 May 2004   G06K-009/00   200457      
   JP2005505062-W   17 Feb 2005   G06T-007/00   200513   Pages: 31   Japanese
   CN1559051-A   29 Dec 2004   G06K-009/00   200523      Chinese
   EP1433118-B1   21 Dec 2005   G06K-009/00   200604      English
   DE60208223-E   26 Jan 2006      200615      German
   CN1276380-C   20 Sep 2006   G06K-009/00   200711      Chinese
   US7308133-B2   11 Dec 2007   G06K-009/62   200800      English
UT DIIDW:2003480386
ER

PT P
PN JP2001101429-A
TI Facial image observation method involves detecting circular arc shaped border line of predetermined size from obtained images to estimate border line for iris of eye.
AB    NOVELTY - A border line expressing characteristic of a face is obtained from images acquired with image pick-up units. After extracting border lines, each point of border lines obtained from the images is matched to perform a three dimensional measurement process. A circular arc shaped border line with a preset size is detected in the generated three dimensional space to obtain a border line for the iris of an eye.
   USE - In facial image observation apparatus.
   ADVANTAGE - Enables to detect the direction of face or eyes in an image easily.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) Facial image observation apparatus;
   (b) Recording apparatus medium stored with facial image observation program
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of face recognition processing apparatus.
PD JP2001101429-A   13 Apr 2001   G06T-009/20   200205   Pages: 12   Japanese
UT DIIDW:2002036503
ER

PT P
PN JP2000331167-A
TI Facial image comparison for individual authentication in security system, involves performing low-gradation process for dropping gradation of each face image subjected to normalization, masking, monochrome processes.
AB    NOVELTY - The position of size of face that has appeared in the compared two face images, are made in accord by normalization. The face area is extracted by masking preset area of two face image based on preset mask data. The low-gradation process is performed for dropping gradation of each face image which is subjected to monochrome process that converts each face image into monochrome image based on necessity.
   USE - For person authentication in security system for criminal investigation.
   ADVANTAGE - Since only essential information is extracted from image data and compared with other image data, the reliability in authentication and identification of the person is enhanced.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for facial image comparison apparatus.
PD JP2000331167-A   30 Nov 2000   G06T-007/00   200111   Pages: 7   Japanese
UT DIIDW:2001096627
ER

PT P
PN JP11242745-A
TI Facial image reconstruction through three-dimensional image-data acquisition - provides depth-information by triangulation range finding using three image pickup cameras at specific positions while furnishing principal common-zone and differential image-detail.
AB       NOVELTY - The three image pickup cameras (1-3) with specific positional relationship obtains subject (5) image that are digitized for subsequent analysis. These images contain common-zone detail as well as camera-specific differential image- data. Additionally, depth information regard to the facial structure is also acquired through a triangulation range finding procedure.
   USE -   It finds application in image modeling, identification exercises involved in security-system.
   ADVANTAGE -   It employs ordinary illumination and uses nominal amounts of data-processing as well as data to derive precise three-dimensional image configurations. DESCRIPTION OF DRAWING(S) - The figure illustrates the data- processing layout modules involved in the image reconstruction analysis. (1-3) Image pickup cameras; (5) Subject.
PD JP11242745-A   07 Sep 1999   G06T-007/00   199947   Pages: 11   Japanese
UT DIIDW:1999556572
ER

PT P
PN EP474304-A; DE4028191-A; JP4273388-A; EP474304-A3; US5570434-A; EP474304-B1; DE59108393-G; JP3242678-B2
TI Identification circuit for human face - using block analysis of difference images obtained by subtracting moving images.
AB       The identification circuit uses a sequence of video images each of which is structured in blocks, each two images subtracted from one another to obtain a set of difference images. Each block of each difference image is assigned a binary value, to provide a status image which is transformed into a one-dimensional function, with evaluation of the latter to obtain the identification.
   A further image point analysis is pref. also used.
   USE -   For facial recognition system.
PD EP474304-A   11 Mar 1992      199211   Pages: 15   English
   DE4028191-A   12 Mar 1992      199212   Pages: 18   German
   JP4273388-A   29 Sep 1992      199245   Pages: 13   Japanese
   EP474304-A3   16 Dec 1992   H04N-007/137   199344      English
   US5570434-A   29 Oct 1996   G06K-009/64   199649   Pages: 11   English
   EP474304-B1   04 Dec 1996   H04N-007/32   199702   Pages: 17   German
   DE59108393-G   16 Jan 1997   H04N-007/32   199708      German
   JP3242678-B2   25 Dec 2001   G06T-007/20   200203   Pages: 13   Japanese
UT DIIDW:1992081847
ER

PT P
PN CN109716352-A; CN209625231-U; WO2020124306-A1
TI LCD fingerprint module for under display fingerprint identification system, has inverse prism structure that exists between light guide plate and photosensitive surface of fingerprint sensor.
AB    NOVELTY - The module has a liquid crystal panel (12) that is located above a backlight module (13). A fingerprint sensor (3) is located below the backlight module. The backlight module includes a prism structure (6), a light guide plate (131) and a backlight (132). The side end of the light guide plate is provided with the backlight. The prism structure is located at a side of the light guide plate facing the liquid crystal panel. The inverse prism structure exists between the light guide plate and a photosensitive surface of the fingerprint sensor. The prism structure is opposite to the orientation of the prism angle of the inverse prism structure. The light is emitted by the fingerprint light source is reflected and sequentially incident on the photosensitive surface of the fingerprint sensor through the prism structure, the light guide plate and the reverse prism structure.
   USE - LCD fingerprint module for under display fingerprint identification system, for electronic device (all claimed) such as smart phone, notebook computer, wearable device and home appliance.
   ADVANTAGE - The prism light of the prism structure after the prism structure is combined by the reverse prism structure ensures the effective field of view of the fingerprint sensor, and also avoids the fission of the fingerprint image, thereby ensuring the accuracy of fingerprint recognition of the LCD.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for an under display fingerprint identification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a liquid crystal display fingerprint module.
   Fingerprint sensor (3)
   Prism structure (6)
   Liquid crystal panel (12)
   Backlight module (13)
   Light guide plate (131)
   Backlight (132)
PD CN109716352-A   03 May 2019   G06K-009/00   201953   Pages: 21   Chinese
   CN209625231-U   12 Nov 2019   G06K-009/00   201989      Chinese
   WO2020124306-A1   25 Jun 2020   G06K-009/00   202052      Chinese
UT DIIDW:201943461Y
ER

PT P
PN US10204264-B1
TI Method for dynamically scoring implicit interactions with client device, involves calculating total interaction score by interaction analysis server based on client device interaction score that contributes to total interaction score.
AB    NOVELTY - The method (400) involves retrieving client device information associated with a user identifier and corresponding to a period of time by a client device information retriever. A client device interaction score is calculated by a client device interaction score calculator based on correspondences between the client device information and content information. An image interaction score is calculated based on a gaze analysis mechanism by an image interaction analyzer. A total interaction score is calculated by the interaction analysis server based on the image interaction score and the client device interaction score, where the client device interaction score positively contributes to the total interaction score.
   USE - Method for dynamically scoring implicit interactions with a client device.
   ADVANTAGE - The method enables achieving better measurement of person's attention level, thus reducing or avoiding false positives and negatives and potentially saving content delivery bandwidth, eliminating erroneously cached data and increasing efficiency of a scoring system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a server for dynamically scoring implicit interactions with a client device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for dynamically scoring implicit interactions with a client device.
   Method for dynamically scoring implicit interactions with client device (400)
   Step for detecting face in image (402)
   Step for determining that detected face is present in images (404)
   Step for setting index to one (406)
   Step for implementing image interaction analysis (420)
PD US10204264-B1   12 Feb 2019   G06K-009/00   201913   Pages: 17   English
UT DIIDW:201914053G
ER

PT P
PN CN108415188-A; CN108415188-B
TI LCD panel of display device e.g. mobile phone, has semiconductor photodetector that is located in light-shielding region which surrounds open area of white sub-pixel, and arranged on one side of black matrix away from array substrate.
AB    NOVELTY - The LCD panel has a color film substrate (10) and an array substrate (20) which are arranged opposite to each other. The color film substrate is comprised of a base substrate (11) and a black matrix (12). The black matrix is located on one side of the base substrate facing the array substrate, and is positioned in a light shielding area. A semiconductor photodetector (30) is located in the light-shielding region which surrounds the open area of a white sub-pixel, and is arranged on one side of the black matrix away from the array substrate. A distance between the semiconductor photodetector and the white sub-pixel opening region is maintained at less than 5 mu m.
   USE - LCD panel of display device (claimed) such as mobile phone, tablet computer and intelligent wearable device.
   ADVANTAGE - The intensity of the fingerprint signal light is enhanced, and the accuracy of fingerprint recognition is improved.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) display device; and
   (2) fingerprint unlocking method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a sectional view of the LCD panel.
   Color film substrate (10)
   Base substrate (11)
   Black matrix (12)
   Array substrate (20)
   Semiconductor photodetector (30)
PD CN108415188-A   17 Aug 2018   G02F-001/1333   201863   Pages: 27   Chinese
   CN108415188-B   16 Nov 2021   G02F-001/1333   202196      Chinese
UT DIIDW:2018670415
ER

PT P
PN CN108388876-A; WO2019174439-A1; US2020302180-A1; EP3767522-A1; EP3767522-A4
TI Method for identifying image by using terminal device, involves extracting structure characteristic information of target video frame images, and identifying target objects in target video corresponding to attribute type.
AB    NOVELTY - The method involves obtaining a target object of a target video. Target video frame images are extracted in the target video according to object key point information. Multiple key point video frames are generated in the target video. Multiple key point video frames are combined into a key point video frame sequence. Dynamic timing characteristic information of the key point video frame sequence is extracted. Static structure characteristic information of the target video frame image is extracted. The target objects in the target video is identified corresponding to attribute type.
   USE - Method for identifying an image by using a terminal device (claimed).
   ADVANTAGE - The method enables improving facial expression identification accuracy.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for identifying an image by using a terminal device
   (2) a computer storage medium for storing set of instructions for identifying an image by using a terminal device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for identifying an image by using a terminal device. '(Drawing includes non-English language text)'
PD CN108388876-A   10 Aug 2018   G06K-009/00   201857   Pages: 37   Chinese
   WO2019174439-A1   19 Sep 2019   G06K-009/00   201972      Chinese
   US2020302180-A1   24 Sep 2020   G06K-009/00   202079      English
   EP3767522-A1   20 Jan 2021   G06K-009/00   202108      English
   EP3767522-A4   08 Dec 2021   G06K-009/00   202101      English
UT DIIDW:201864149S
ER

PT P
PN US2018144746-A1; US10204625-B2
TI Computer-based method for audio analysis, involves learning audio classifier on third computing device based on analyzing of face within video data and analyzing audio data using audio classifier.
AB    NOVELTY - The method involves obtaining (120) audio data on a second computing device corresponding to the video data. A face is identified (130) within the video data. A first voice from the audio data is associated (140) with the face within the video data. The face is analyzed (150) within the video data for cognitive content. An audio classifier is learned (160) on a third computing device based on the analyzing of the face within the video data. The audio data is analyzed (170) using the audio classifier.
   USE - Computer-based method for audio analysis.
   ADVANTAGE - By detecting a yawn in the audio data and synchronizing the audio data with the video data, learning of the video classifier for audio detection can be improved. The multiple programs or threads are processed simultaneously to enhance utilization of the processor and to facilitate simultaneous functions.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a computer program product for audio analysis; and
   (2) a computer system for audio analysis.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram for audio analysis in the context of video.
   Step for obtaining audio data on second computing device (120)
   Step for identifying face within video data (130)
   Step for associating first voice from audio data (140)
   Step for analyzing face within video data for cognitive content (150)
   Step for learning audio classifier on third computing device (160)
   Step for analyzing audio data using audio classifier (170)
PD US2018144746-A1   24 May 2018   G10L-015/22   201837   Pages: 39   English
   US10204625-B2   12 Feb 2019   G06K-009/00   201912      English
UT DIIDW:201840442V
ER

PT P
PN CN107464136-A
TI Merchandise display method, involves obtaining shelf position information according to recommendation information, transmitting shelf position information to display device associated with image recognition device that arranged in area.
AB    NOVELTY - The method involves receiving a face image by an image identification device. Identity mark corresponding to the face image is determined. Commodity recommendation information corresponding to the identity mark is obtained according to the face image. A shelf identifier is queried according to the commodity recommendation information. Shelf position information is obtained according to the commodity recommendation information. The shelf position information is transmitted to a display device associated with the image recognition device that arranged in a shelf area.
   USE - Merchandise display method.
   ADVANTAGE - The method enables reducing labor cost.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a merchandise display system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a merchandise display method. '(Drawing includes non-English language text)'
PD CN107464136-A   12 Dec 2017   G06Q-030/02   201802   Pages: 21   Chinese
UT DIIDW:201786626P
ER

PT P
PN CN107239766-A
TI Network based three-dimensional human face placing method, involves reconstructing first classification module, and generating second classification module and identifying module, and utilizing generator for receiving input of face image.
AB    NOVELTY - The method involves reconstructing a first classification module. A second classification module and an identifying module are generated, where the reconstructed first classification module comprises two parts of a preset structure and a reconfigurable structure. A generator is utilized for receiving an input of a face image. The face image is converted into a front face. A classifier is utilized for classifying the front face. A face recognition engine is utilized for performing a regularization process of the face image to retain particular characteristic of the face image.
   USE - Network based three-dimensional human face placing method.
   ADVANTAGE - The method enables processing the face image, establishing shape model based on network, and improving face recognition and straightening effect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a network based three-dimensional human face placing method. '(Drawing includes non-English language text)'
PD CN107239766-A   10 Oct 2017   G06K-009/00   201776   Pages: 11   Chinese
UT DIIDW:201770554S
ER

PT P
PN CN107016370-A; CN107016370-B
TI Data enhancement part based shielding face identifying method, involves performing key point detection process and face alignment process, obtaining face identification matching result by matching with original face image.
AB    NOVELTY - The method involves obtaining an obstruction of face image covered with a sunglasses or a mask. A shielding face picture is identified for pre-processing. A pre-treated shielding face picture is obtained. Shielding face image barrier information is obtained after a pre-treatment process. Key information of matching sun glasses is obtained. A key point detection process and a face alignment process are performed according to standard face. Original shielding face is matched with obtained face image after performing the shielding process to obtain a face identification matching result.
   USE - Data enhancement part based shielding face identifying method.
   ADVANTAGE - The method enables avoiding characteristic information loss due to shielding effect, reducing face recognition difficulty, increasing accurate identification rate, improving shielding face recognition accuracy and efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a data enhancement part based shielding face identifying method. '(Drawing includes non-English language text)'
PD CN107016370-A   04 Aug 2017   G06K-009/00   201765   Pages: 15   Chinese
   CN107016370-B   11 Oct 2019   G06K-009/00   201981      Chinese
UT DIIDW:201754475F
ER

PT P
PN CN106909905-A; CN106909905-B
TI Deep learning based multi-mode human face identifying method, involves constructing similarity matrix of image registration set and query set, and obtaining final fusion similarity to perform human face recognition and verification process.
AB    NOVELTY - The method involves extracting an image registration set and a query set. Mode range in a training set is determined. Multi-modality of a certain image is expressed. Human face detecting characteristic on a model is extracted. Similarity across modalities is calculated. A registration set image is obtained. A query set image is represented as a registration collection image. Similarity matrix of the image registration set and the query set is constructed to obtain weighted summation fusion. Final fusion similarity is obtained to perform human face recognition and verification process.
   USE - Deep learning based multi-mode human face identifying method.
   ADVANTAGE - The method enables utilizing set of modalities information by fusion strategy so as to avoid inherent weakness of single mode system. The method enables utilizing various modal information so as to effectively improve human face identifying performance, and thus realizing human face identifying in a rapid and accurate manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning based multi-mode human face identifying method. '(Drawing includes non-English language text)'
PD CN106909905-A   30 Jun 2017   G06K-009/00   201758   Pages: 10   Chinese
   CN106909905-B   14 Feb 2020   G06K-009/00   202017      Chinese
UT DIIDW:2017481439
ER

PT P
PN CN106874898-A; CN106874898-B
TI Depth convolutional neural network model based large-scale human face recognition method, involves performing feature vector extracting process by using eigenvector to obtain identification result of human face.
AB    NOVELTY - The method involves establishing a deep convolutional neural network model based on depth of residual learning. A pre-processing image is obtained according to the deep convolutional neural network model. Pre-processing picture detecting process of human face is performed according to key points of eyes. A reverse propagation loss error updating model parameter is determined according to the deep convolutional neural network model. Feature vector extracting process is performed by using eigenvector to obtain an identification result of the human face.
   USE - Depth convolutional neural network model based large-scale human face recognition method.
   ADVANTAGE - The method enables improving large-scale human face recognition accuracy rate reaches 25%.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a depth convolutional neural network model based large-scale human face recognition method. '(Drawing includes non-English language text)'
PD CN106874898-A   20 Jun 2017   G06K-009/00   201757   Pages: 11   Chinese
   CN106874898-B   30 Mar 2021   G06K-009/00   202131      Chinese
UT DIIDW:201744499N
ER

PT P
PN CN106778657-A
TI Classifying neonatal pain expression useful for evaluating neonatal pain degree by classifying neonatal pain expression images, establishing expression image library, constructing e.g. data layer, and identifying and classifying.
AB    NOVELTY - Method for classifying neonatal pain expression involves collecting and classifying neonatal pain expression images, establishing neonatal pain expression image library, constructing e.g. data layer, and convolution layers, dividing the image of neonatal pain expression into training sample and testing sample, inputting test samples, identifying and classifying the test samples using trained convolution neural network and outputting the test sample belonging to each probability value, and classifying the categories corresponding to the maximum probability values.
   USE - The method is useful for classifying neonatal pain expression (claimed) which is used for evaluating neonatal pain degree.
   DETAILED DESCRIPTION - Method for classifying neonatal pain expression involves (a) collecting neonatal pain expression images, classifying the images based on the different states of the facial expressions of the newborns, and establishing of neonatal pain expression image library, (b) constructing a data layer, 3 convolution layers, 2 completely connected layers, and 1 classification layer convolution neural network, (c) dividing the image of each expression in the neonatal pain expression image database into training sample and testing sample, using training sample data as data input of the convolution neural network, and back propagation algorithm to iterate the convolution neural network, and training and optimizing global parameters softmax convolutional neural network output and decreased loss function value converges to obtain trained convolution neural network, and (d) inputting the test samples, identifying and classifying the test samples using trained convolution neural network and outputting the test sample belonging to each probability value, and classifying the categories corresponding to the maximum probability values as test sample.
TF TECHNOLOGY FOCUS - BIOTECHNOLOGY - Preferred Method: The different neonatal facial expressions are chosen from calmness, crying, mild pain, and severe pain. The step (b) involves constructing (i) data layer by using sample image in the neonatal pain expression library and its corresponding category label as data layer of the convolution neural network, (ii) convolution layer 1, where the first layer is neural network convolution layer using m1-k1x k1 convolution kernel, interval is 1l pixel, sample image is convoluted, the output is sampled and normalized as the next input, convolution layer 2 which is the second layer of the convolutional neural network convolution layer using m2-k2x k2 convolution kernel, and convolution layer 3 using m3-k3x k3 convolution kernel with l3 pixels, and k1, k2, and k3 are gradually decreased, (iii) 2 completely connected layers, where the first layer of the convolution neural network is connected to input of the convolution layer, and (iv) classification layer, where the layer is convolutional neural network classification layer, connecting n output nodes of all-connected layer, and output is the probability value Pj belonging to the jth class, where j is 1, and k is the total number of classes. The softmax loss function is calculated using specific equation involving a predetermined parameters such as F theta , m, k, 1(y(i)=j), x, theta 1- theta k, and T, where F theta is loss function, m is the total number of training samples, k is the total number of classes, 1(y(i)=j) is an instruction function, x is vector formed by the full connection layer output node, theta 1- theta k is model parameter, and T is transpose. The probability values are calculated using specific equation involving predetermined parameters such as identity (y) which is classification result of test sample y.
PD CN106778657-A   31 May 2017   G06K-009/00   201749   Pages: 9   Chinese
UT DIIDW:201738609E
ER

PT P
PN CN106599797-A; CN106599797-B
TI Local parallel neural network based infrared human face identifying method, involves updating connection weight and bias of neuron in network by reverse propagation and gradient descent for realizing infrared face identification.
AB    NOVELTY - The method involves extracting initial convolution characteristics and human face characteristics. Characteristic spectrum is generated. Linear activation is corrected for realizing maximum cell down-sampling operation. Parallel multi-scale convolution characteristics are determined. Different scale information is obtained to obtain multi-scale characteristic spectrum. Classification characteristic vector is generated. Connection weight and bias of a neuron in a network is continuously updated by reverse propagation and gradient descent for realizing infrared face identification.
   USE - Local parallel neural network based infrared human face identifying method.
   ADVANTAGE - The method enables realizing infrared human face identifying operation with identity identification applications in an effective manner.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a local parallel neural network based infrared human face identifying unit. '(Drawing includes non-English language text)'
PD CN106599797-A   26 Apr 2017   G06K-009/00   201733   Pages: 9   Chinese
   CN106599797-B   07 Jun 2019   G06K-009/00   201946      Chinese
UT DIIDW:201729487Y
ER

PT P
PN CN106599883-A; CN106599883-B
TI Convolutional neural network based multi-layer image semantics human face recognition method, involves determining LBP characteristic pattern size, and determining euclidean distance of characteristic semblance groups.
AB    NOVELTY - The method involves carrying out input image de-noising processing operation based on a self-adapting median filtering algorithm. Human face region image normalization processing operation is carried out based on a bilinear interpolation algorithm. Histogram equalization operation is carried out, where the human face image is divided into multi-areas. Local binary pattern (LBP) characteristic of the human face region image is determined. LBP characteristic pattern size is determined. Euclidean distance of characteristic semblance groups is determined.
   USE - Convolutional neural network based multi-layer image semantics human face recognition method.
   ADVANTAGE - The method enables realizing a human face comparison algorithm so as to increase accuracy rate and calculation efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a convolutional neural network based multi-layer image semantics human face. '(Drawing includes non-English language text)'
PD CN106599883-A   26 Apr 2017   G06K-009/00   201732   Pages: 12   Chinese
   CN106599883-B   17 Mar 2020   G06K-009/00   202025      Chinese
UT DIIDW:2017281097
ER

PT P
PN CN106484351-A
TI Method for controlling human face displaying operation by mobile terminal, involves controlling human face displaying operation of sub-screen when human face is not identified by camera to maintain first sub-screen in non-working state.
AB    NOVELTY - The method involves a mobile terminal connected with a foldable flexible screen. The foldable flexible screen is provided with a first sub-screen and a second sub-screen. The first screen is provided with a camera. Judgment is made to check whether the foldable flexible screen is in a folded state by the camera. Human face recognition process is performed by the camera. Human face displaying operation of the second sub-screen is controlled when a human face is not identified by the camera to maintain the first sub-screen in a non-working state.
   USE - Method for controlling human face displaying operation by a mobile terminal (claimed).
   ADVANTAGE - The method enables improving display effect of human face to improve user experience according to result of identifying human face by first sub-screen and the second sub-screen.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a mobile terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for controlling human face displaying operation by a mobile terminal. '(Drawing includes non-English language text)'
PD CN106484351-A   08 Mar 2017   G06F-003/14   201721   Pages: 18   Chinese
UT DIIDW:2017180383
ER

PT P
PN CN106469301-A; CN106469301-B
TI Self-adaptive adjustable human face recognition method, involves collecting human face image, and detecting brightness value of face image, and performing face recognition by using Illumination adjusted face image.
AB    NOVELTY - The method involves collecting a human face image. A brightness value of the face image is detected. Judgment is made to check whether the brightness value is in preset brightness value range. Illumination intensity of fill light is adjusted in the face image if the brightness value is less than preset brightness value range. Face recognition is performed by using the Illumination adjusted face image. A face is detected from the face image and extracted as a face region. A SIFT feature vector is converted into a BOW vector to obtain the brightness value of the face image.
   USE - Self-adaptive adjustable human face recognition method.
   ADVANTAGE - The method enables simply and accurately obtaining a high-quality face image.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a self-adaptive adjustable human face recognition device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a self-adaptive adjustable human face recognition method. '(Drawing includes non-English language text)'
PD CN106469301-A   01 Mar 2017   G06K-009/00   201721   Pages: 12   Chinese
   CN106469301-B   07 May 2019   G06K-009/00   201938      Chinese
UT DIIDW:2017170231
ER

PT P
PN CN106339702-A
TI Multi-feature fusion based human face identification method, involves combining training model with Bayesian matching model for determining highest matching degree of human face image for outputting final identification result.
AB    NOVELTY - The method involves collecting a human face image by using a hardware device. A human face position and a face feature point position are detected based on an Adaboost human face image pre-processing algorithm. Conducting human face illumination normalization process is performed for realizing gray scale image contrast enhancement. Reduction treatment of the human face image is performed to establish a training model. The training model is combined with a Bayesian matching model for determining a highest matching degree of the human face image for outputting a final identification result.
   USE - Multi-feature fusion based human face identification method.
   ADVANTAGE - The method enables collecting sample training extracted characteristics of the human face image so as to effectively improve accuracy of face recognition.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a hardware device. '(Drawing includes non-English language text)'
PD CN106339702-A   18 Jan 2017   G06K-009/00   201715   Pages: 6   Chinese
UT DIIDW:201708480A
ER

PT P
PN CN106295567-A; WO2018028546-A1; CN106295567-B; US2019138791-A1; US10990803-B2
TI Key point locating method, involves judging whether position of human face key point is formed as position of human face key points of target image when another confidence is greater than or equal to preset precision.
AB    NOVELTY - The method involves collecting a target image according to preset configuration. The target image is generated to recognize and locate a target detection area of a face. Another target image is obtained when confidence is greater than or equal to preset threshold value. A judgment is made to check whether a position of a human face key point is formed as a position of human face key points of the target image when another confidence is greater than or equal to a preset precision. A preset model group is provided with a public branch.
   USE - Key point locating method.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a terminal. '(Drawing includes non-English language text)'
PD CN106295567-A   04 Jan 2017   G06K-009/00   201710   Pages: 29   Chinese
   WO2018028546-A1   15 Feb 2018   G06K-009/00   201812      Chinese
   CN106295567-B   12 Apr 2019   G06K-009/00   201928      Chinese
   US2019138791-A1   09 May 2019   G06K-009/00   201934      English
   US10990803-B2   27 Apr 2021   G06K-009/00   202136      English
UT DIIDW:201704374R
ER

PT P
PN US2016379041-A1; JP2017010543-A; KR2017000748-A; CN106295496-A; EP3147827-A1; US10331941-B2; JP6754619-B2; CN106295496-B; CN113743321-A
TI Method for recognizing face appearing in image of camera for use in e.g. security system, involves generating two-dimensional image of user face based on three-dimensional face model, and performing face recognition based on information.
AB    NOVELTY - The method involves generating three-dimensional (3D) shape information based on a personalized 3D face model (230), where the 3D shape information includes a set of 3D surface coordinate values. A normalized two-dimensional (2D) input image (240) of the user face is generated based on the personalized 3D face model. Feature information associated with the user face is generated based on the 3D shape information and pixel color values of the normalized 2D input image. Face recognition is performed based on the feature information.
   USE - Method for recognizing a face appearing in an image of a camera for use in application fields e.g. monitoring/security system and mobile authentication.
   ADVANTAGE - The method enables enhancing performances of neural network models used for the face recognition through the learning procedure, thus increasing accuracy on the face recognition. The method enables utilizing back propagation learning scheme for updating a connection weight of artificial neurons so as to reduce an error. The method enables adjusting pixel values of the depth image, so that the positional error between a feature of the depth image and a corresponding feature of the normalized 2D input image is reduced and/or mitigated.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a non-transitory computer-readable storage medium comprising a set of instructions for recognizing a face appearing in an image
   (2) a face recognition apparatus.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of restoring a 3D shape of a face of a user.
   Landmark (220)
   Personalized 3D face model (230)
   Normalized 2D input image (240)
   Depth image (250)
   Normal image (260)
PD US2016379041-A1   29 Dec 2016   G06K-009/00   201704   Pages: 28   English
   JP2017010543-A   12 Jan 2017   G06T-007/00   201706   Pages: 20   Japanese
   KR2017000748-A   03 Jan 2017   G06K-009/00   201706      
   CN106295496-A   04 Jan 2017   G06K-009/00   201707      Chinese
   EP3147827-A1   29 Mar 2017   G06K-009/62   201723      English
   US10331941-B2   25 Jun 2019   G06K-009/00   201947      English
   JP6754619-B2   16 Sep 2020   G06T-007/00   202076   Pages: 20   Japanese
   CN106295496-B   14 Sep 2021   G06K-009/00   202177      Chinese
   CN113743321-A   03 Dec 2021   G06K-009/00   202201      Chinese
UT DIIDW:201700114U
ER

PT P
PN CN106228137-A
TI Key point location based abnormal human face detecting method for automated teller machine (ATM), involves determining abnormal face based on classification results of key response vector.
AB    NOVELTY - The method involves performing (1) image pre-processing, to obtain a real-time monitoring image by eliminating noise in the image. The video image is enhanced under complex lighting conditions. The human face detection is performed to detect (2) whether the image comprises position and size information of the face. The facial key point position is detected (3). The key point response value is obtained. The key response vector is inputted to a classifier. The abnormal face is determined (4) based on the classification results of the key response vector.
   USE - Key point location based abnormal human face detecting method for automated teller machine (ATM).
   ADVANTAGE - The abnormal human face is detected accurately in real time, to provide alarm in time.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating the abnormal human face detection process. (Drawing includes non-English language text)
   Step for performing image pre-processing (1)
   Step for detecting whether image comprises position and size information of face (2)
   Step for detecting facial key point position (3)
   Step for determining abnormal face (4)
PD CN106228137-A   14 Dec 2016   G06K-009/00   201707   Pages: 11   Chinese
UT DIIDW:201700515U
ER

PT P
PN CN106203333-A
TI Face recognition method, involves storing characteristic information in face feature library to identify face image, and generating reference feature information based on sample face image.
AB    NOVELTY - The method involves inputting an acquired face image to a convolutional neural network to determine probability of the face image. Determination is made that the face image is derived from certain person when the probability is greater than a preset threshold. Characteristic information of the face image input to the convolutional neural network is extracted when the probability is less than the preset threshold value. Characteristic information is stored in a face feature library to identify the face image. Reference feature information is generated based on a sample face image.
   USE - Face recognition method.
   ADVANTAGE - The method enables identifying the face by the convolutional neural network and comparing feature comparison to perform face recognition process so as to ensure effective identification of the human face.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a face recognition method. '(Drawing includes non-English language text)'
PD CN106203333-A   07 Dec 2016   G06K-009/00   201705   Pages: 15   Chinese
UT DIIDW:201678192J
ER

PT P
PN CN106204780-A
TI Deep learning cloud service and human face recognition based student attendance checking system, has memory storage unit connected with face detecting module that is fixed with regression classifier.
AB    NOVELTY - The system has a wireless data transmission module connected with a cloud server through a network. The cloud server is provided with a deep learning network training module that is connected with a regression classifier. A memory storage unit is provided with a personnel information database and an attendance record database. The deep learning network training module obtains training human face video/image. The memory storage unit is connected with a face detecting module that is fixed with a regression classifier.
   USE - Deep learning cloud service and human face recognition based student attendance checking system.
   ADVANTAGE - The system has high storage efficiency, high identification efficiency, high working efficiency and better robustness.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a deep learning cloud service and human face recognition based student attendance checking method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep learning cloud service and human face recognition based student attendance checking method. '(Drawing includes non-English language text)'
PD CN106204780-A   07 Dec 2016   G07C-001/10   201703   Pages: 15   Chinese
UT DIIDW:201678162A
ER

PT P
PN CN106022209-A; CN106022209-B
TI Distance estimation based human face detecting method, involves judging whether camera collected image is same as pre-set image, determining human face size of continuous area, and calculating distance between human face and camera.
AB    NOVELTY - The method involves starting a camera on a mobile terminal. A camera collected image is obtained. A human face training sample is obtained according to a preset human face recognition algorithm. A judgment is made to check whether the camera collected image is same as a pre-set image. Human face size of a continuous area is determined. A distance between the human face and the camera is calculated. A judgment is made to check whether a human face is detected on the camera collected image. Face skin color information is obtained.
   USE - Distance estimation based human face detecting method.
   ADVANTAGE - The method enables realizing distance estimation based human face detecting process in a simple manner and increasing screen impaired vision.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a distance estimation based human face detecting device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a distance estimation based human face detecting method. '(Drawing includes non-English language text)'
PD CN106022209-A   12 Oct 2016   G06K-009/00   201674   Pages: 14   Chinese
   CN106022209-B   17 Sep 2019   G06K-009/00   201973      Chinese
UT DIIDW:201667689R
ER

PT P
PN CN105931303-A
TI Intelligent safety railway line passenger transportation system, has human face image collecting module for collecting passenger face image, where judgment is made to check whether present face image is stored in cloud data center.
AB    NOVELTY - The system has a station identity verification sub-system provided with a first identity information obtaining module and a first human face image collecting module. The first identity information obtaining module determines passenger identity information. A ticket check out station sub-system is provided with a ticket identification module. A second human face image collecting module collects real time collecting passenger face image to obtain latest image information from a cloud data center, where judgment is made to check whether present face image is stored in the cloud data center.
   USE - Intelligent safety railway line passenger transportation system.
   ADVANTAGE - The system provides security aspect of a verification server by a robot, increases service quality and verification efficiency and reduces manual labor intensity.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an intelligent safety railway line passenger transportation system passing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an intelligent safety railway line passenger transportation system. '(Drawing includes non-English language text)'
PD CN105931303-A   07 Sep 2016   G07B-011/00   201669   Pages: 11   Chinese
UT DIIDW:2016602146
ER

PT P
PN US2016251081-A1; US9481459-B2
TI Apparatus for deploying an autonomous air vehicle to reduce risk in high risk interactions with humans, comprises an autonomously controlled air vehicle that is installed with an airborne communications module.
AB    NOVELTY - The apparatus comprises an autonomously controlled air vehicle that has an airborne communications module. An airborne processing unit is connected with a sensor module. A base station (801) is installed with a communications unit. A processing unit is provided with multiple beacons. Multiple object tracking units are installed on a landing pad. A bidirectional communication is provided by the airborne communications module with communications unit. An unit is provided in the processing unit for performing object, facial and voice recognition.
   USE - Apparatus for deploying an autonomous air vehicle to reduce risk in high risk interactions with humans. Uses include but not limited to military force protection, road checks and pulling over vehicles with drivers.
   ADVANTAGE - The apparatus includes a base station that is installed with host external sensors, and thus easy and enhanced mapping and tracking of objects in a three-dimensional space is ensured.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for deploying autonomously controlled air vehicles to reduce risk during suspect investigation that includes issuing instructions by an autonomously controlled air vehicle.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of an apparatus for deploying an autonomous air vehicle to reduce risk in high risk interactions with humans.
   Autonomous assistant vehicle (800)
   Base station (801)
   Mobile unit (802)
   Relay-processing unit (803)
   Suspicious object (805)
PD US2016251081-A1   01 Sep 2016   B64C-039/02   201658   Pages: 12   English
   US9481459-B2   01 Nov 2016   G05D-001/00   201672      English
UT DIIDW:201652519Q
ER

PT P
PN CN105354543-A
TI Video processing method, involves performing human face tracking process to track frame according to human face identification process, and obtaining final identification result of human face image by using target video.
AB    NOVELTY - The method involves obtaining a target video. The target video is provided with a detecting frame and a track frame. Human face detecting process is performed by using the detection frame. Detection data of a human face image is obtained. Human face identification process is performed according to different human face images. Human face tracking process of the track frame is performed according to the human face identification process. The human face image of identification frame is filtered. Final identification result of the human face image is obtained by using the target video.
   USE - Video processing method.
   ADVANTAGE - The method enables increasing human face recognition efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a video processing device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a video processing method. '(Drawing includes non-English language text)'
PD CN105354543-A   24 Feb 2016   G06K-009/00   201620   Pages: 29   English
UT DIIDW:201615706P
ER

PT P
PN CN105227316-A
TI Mobile internet account number and login based human face image identity verification system, has data communication safety module fixed with server terminal, and mobile terminal for obtaining human face detection comparison result.
AB    NOVELTY - The system has a human face identification module for performing video frequency flow collection. A subcutaneous blood circulation detection camera head is equipped with an image stable detection module. A movable sensor is equipped with a mobile terminal detection matching human face identification module. A human face living body detecting module is connected with the subcutaneous blood circulation detection camera head. A data communication safety module is fixed with a server terminal. A mobile terminal obtains a human face detection comparison result.
   USE - Mobile internet account number and login based human face image identity verification system.
   ADVANTAGE - The system avoids mobile internet APP user input complexity and user information and security code leakage problem in an effective manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a mobile internet account number and login based human face image identity verification method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a mobile internet account number and login based human face image identity verification system. '(Drawing includes non-English language text)'
PD CN105227316-A   06 Jan 2016   H04L-009/32   201613   Pages: 9   English
UT DIIDW:201606433A
ER

PT P
PN CN105184239-A; CN105184239-B
TI Patient face identification based sickroom assist care system, has collecting layer formed with identification layer that is matched with application layer, and expression data base constructing module formed with identification layer.
AB    NOVELTY - The system has a collecting layer formed with an identification layer that is matched with an application layer. An expression data base constructing module is formed with the identification layer. The expression data base construction module is connected with an expression image pre-processing module that is connected with an expression characteristic extracting module. A face characteristic extraction module is connected with an expression identification module. A face identification module is connected with a recognition result transmission module.
   USE - Patient face identification based sickroom assist care system.
   ADVANTAGE - The system realizes facial image check and analysis process to identify complete human face through non-contact so as to ensure identification efficiency. The system determines body state to change emotion fluctuation so as to avoid dangerous accident occur.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a patient face identification based sickroom assist care method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a patient face identification based sickroom assist care system. '(Drawing includes non-English language text)'
PD CN105184239-A   23 Dec 2015   G06K-009/00   201606   Pages: 14   English
   CN105184239-B   01 Jun 2018   G06K-009/00   201838      Chinese
UT DIIDW:2016001647
ER

PT P
PN CN105160312-A
TI Human face similarity matching based residual star face decorate recommendation method, involves obtaining key points of human face, and performing decorating process to obtain human face structure layer.
AB    NOVELTY - The method involves obtaining key points of a human face by adopting a sparse cascade returning model. Standardization process is performed to obtain training sample of the human face. Position of the human face is determined by using a rectangular frame. Initial position of the human face position is obtained by using a face map. The human face training sample is tested according to standardization process. Skin detail layer, and color layer of the human face sample are obtained by using image synthesis compound. Decorating process is performed to obtain a human face structure layer.
   USE - Human face similarity matching based residual star face decorate recommendation method.
   ADVANTAGE - The method enables increasing based residual star face decorate recommendation process efficiency.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face similarity matching based residual star face decorate recommendation method. '(Drawing includes non-English language text)'
PD CN105160312-A   16 Dec 2015   G06K-009/00   201602   Pages: 10   English
UT DIIDW:201583018Q
ER

PT P
PN CN104978550-A; CN104978550-B
TI Large scale face data base based face recognition method, involves obtaining image for identification, and detecting characteristic vector group and training sample of human face image for obtaining PCA pattern matching output result.
AB    NOVELTY - The method involves obtaining an image for identification. A high-quality grayscale image is obtained by an image illumination compensation grey, filtering, removing noise and normalization. A human face image is separated using a Haar-Like small wave feature. The human face image is checked in an image sample by utilizing an AdaBoost cascade classifier. Feature extraction and representation step of the human face image is performed by PCA pattern matching process. A PCA pattern matching output result is obtained for detecting characteristic vector group and training sample of the face image.
   USE - Large scale face data base based face recognition method.
   ADVANTAGE - The method enables identifying human face identification with high speed in wide range face data base.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a large scale face data base based face recognition system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a large scale face data base based face recognition method. '(Drawing includes non-English language text)'
PD CN104978550-A   14 Oct 2015   G06K-009/00   201578   Pages: 14   Chinese
   CN104978550-B   18 Sep 2018   G06K-009/00   201865      Chinese
UT DIIDW:2015694383
ER

PT P
PN CN104504856-A
TI Kinect and human face identification based fatigue driving monitoring method, involves detecting neck by Kinect camera, obtaining human face identification result by human face tracking analysis process, and detecting nozzle blowing part.
AB    NOVELTY - The method involves detecting neck and head part by a Kinect camera. Human face identification result is obtained by human face tracking analysis process. 3D mask Kinect identification of a human face is detected according to a characteristic point extracting human face. A human eye image is detected according to a human face characteristic point. A human eye area identifier is included with the human face image and processed with human eye image grey process and binarization unit process. A nozzle blowing part is detected according to person eye blink frequency change rate.
   USE - Kinect and human face identification based fatigue driving monitoring method.
   ADVANTAGE - The method enables promoting significance of research to fatigue driving aspect.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a kinect and human face identification based fatigue driving monitoring method.'(Drawing includes non-English language text)'
PD CN104504856-A   08 Apr 2015   G08B-021/06   201542   Pages: 15   Chinese
UT DIIDW:2015343500
ER

PT P
PN US2015084950-A1; US9317954-B2
TI Computer-implemented method for providing real-time facial performance capture using adaptive model, involves fitting refined linear model onto input data using adaptive principal component analysis shapes.
AB    NOVELTY - The method involves generating a customized digital model that includes a set of blend-shapes using a three-dimensional scan, where the blend-shapes of the set of blend-shapes represent a characteristic of a subject (108). Input data of the subject is received, where the input data includes video data and depth data. Body deformations of the subject are tracked by fitting the input data using the blend-shapes of the set. A refined linear model is fitted onto the input data using adaptive principal component analysis shapes.
   USE - Computer-implemented method for providing real-time facial performance capture using an adaptive model for baby, patient with movement disorder and rapidly changing users in public place.
   ADVANTAGE - The method enables tracking body deformations of the subject by fitting the input data using the blend-shapes of the set, so that the tracking quality and performance capture workflow of a real-time performance capture system are improved, thus achieving accurate and expressive facial retargeting to a target character, and reducing costs associated with an animation process by reducing the complexity of and the turn-around time for generating accurate depictions of expressions.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a system for providing real-time facial performance capture using an adaptive model
   (2) a computer-readable memory comprising a set of instructions to perform a method for providing real-time facial performance capture using an adaptive model.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a computer system for providing real-time facial performance capture using an adaptive model.
   Computer system (100)
   Computer (102)
   Input sensor (104)
   Connector (106)
   Subject (108)
PD US2015084950-A1   26 Mar 2015   G06T-013/40   201523   Pages: 27   English
   US9317954-B2   19 Apr 2016   G06T-015/00   201628      English
UT DIIDW:2015209889
ER

PT P
PN CN104408440-A; CN104408440-B
TI Step reduction and parallel characteristic-based human face expression identification method involves predicting human face expression identification using support vector machine, after pre-processing parallel characteristic.
AB    NOVELTY - The human face expression identification method involves processing original human face expression image, using Gabor wavelet characteristic extraction process. The group characteristic data is carried out in real number domain. The human face expression identification is classified and predicted using support vector machine, after pre-processing parallel characteristic.
   USE - Step reduction and parallel characteristic-based human face expression identification method.
   ADVANTAGE - The basic expression of human face identification is realized effectively in reliable manner. The identification rate of the human face expression is increased precisely. The public safety video monitoring occasion is used widely.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating Step reduction and parallel characteristic-based human face expression identification method. (Drawing includes non-English language text)
PD CN104408440-A   11 Mar 2015   G06K-009/00   201532   Pages: 12   Chinese
   CN104408440-B   17 Oct 2017   G06K-009/00   201770      Chinese
UT DIIDW:201526661N
ER

PT P
PN CN104361326-A
TI Living body human face recognizing method, involves obtaining face picture information by camera, and detecting non-micro expression of living body if characteristic similarity is not in inner shread value range.
AB    NOVELTY - The method involves obtaining face picture information by a camera. Collected original image pair pre-processing operation is performed by adopting grey illumination compensation process to obtain a gray scale image. The grey scale image classify process is performed by utilizing an input AdaBoost classifier. A key point location of the human face is determined. The key point location is included with a nose, a mouth, a chin and eyes locations. Non-micro expression of a living body is detected if characteristic similarity is not in an inner shread value range.
   USE - Living body human face recognizing method.
   ADVANTAGE - The method enables improving living recognition rate of human face identification and human face recognition safety performance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a living body human face recognizing method.'(Drawing includes non-English language text)'
PD CN104361326-A   18 Feb 2015   G06K-009/00   201527   Pages: 6   Chinese
UT DIIDW:201522810J
ER

PT P
PN CN104348778-A
TI Method for authenticating human face preliminary verification of remote identity authentication system in mobile phone terminal, involves obtaining human face image of mobile phone terminal.
AB    NOVELTY - The method involves obtaining human face image of a mobile phone terminal. A login identity authentication platform is provided with a remote background system through the mobile phone terminal. Primary human face verification process is performed on the mobile phone terminal according to precise identification. Video image of the login identity authentication platform is received according to target person purported identity. Human face three-dimensional depth information is received according to frequency spectrum analysis function.
   USE - Method for authenticating human face preliminary verification of a remote identity authentication system in a mobile phone terminal.
   ADVANTAGE - The method enables increasing working efficiency of remote identity authentication, recognition rate and safety performance.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a method for authenticating human face preliminary verification of a remote identity authentication system in a mobile phone terminal.'(Drawing includes non-English language text)'
PD CN104348778-A   11 Feb 2015   H04L-029/06   201524   Pages: 9   Chinese
UT DIIDW:2015212444
ER

PT P
PN US2014376786-A1; US9063956-B2
TI Computer system e.g. client device, for tagging images e.g. photographs, has multiple computer processors configured to eliminate one of multiple possible tags from untagged face based on highest probability factor.
AB    NOVELTY - The system (1000) has multiple computer processors (1002) configured to determine likelihood for one of multiple possible tags to correctly match an untagged face based on logic rules. The computer processors are configured to identify a highest probability factor among probability factors associated with known persons for multiple untagged faces. The computer processors are configured to assign the first untagged face to the first known person based on the highest probability factor and eliminate one of the possible tags from the second untagged face based on the highest probability factor.
   USE - Computer system e.g. server or client device, for tagging images e.g. photographs and videos. Uses include but are not limited to a smartphone, a personal digital assistant (PDA), a tablet computer, a laptop computer and a desktop computer.
   ADVANTAGE - The system utilizes an image server to analyze metadata of contributed images to find related images and perform facial recognition to automatically identify and tag recognized people in images. The system can automatically adjust enhancement settings for a cluster of similar images based on a user adjusting the enhancement settings for just one of the cluster of images and improves accuracy of the face match suggestions presented to the user based on result of the logic engine. The system can present the faces determined to be more important to the user for identification and confirmation, such that the user can identify the people who are important to the user to base other decisions such as sharing and organization of images, on more important people.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a computer system.
   Computer system (1000)
   Computer processors (1002)
   Chipset (1004)
   Memory (1006)
   Keyboard (1010)
PD US2014376786-A1   25 Dec 2014   G06F-017/30   201501   Pages: 21   English
   US9063956-B2   23 Jun 2015   G06K-009/00   201542      English
UT DIIDW:2014W87019
ER

PT P
PN CN103927521-A
TI Human face identification based license confirming system, has driving license identification unit identifying license number and human face image of owner, and center processing unit verifying driving qualification license of owner.
AB    NOVELTY - The system has an inner room provided with a video collect unit, a driving license identification unit and a center processing unit. A management center is fixed to a driver database. The video collect unit collects license number and a human face image of an owner. A human face image identification unit is fixed with the video collect unit. Two data transmission units are fixed with the management center. The driving license identification unit identifies the license number and the human face image of the owner. The center processing unit verifies a driving qualification license of the owner.
   USE - Human face identification based license confirming system.
   ADVANTAGE - The system improves transportation safety performance and reliability.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face identification based license confirming method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a human face identification based license confirming system.'(Drawing includes non-English language text)'
PD CN103927521-A   16 Jul 2014   G06K-009/00   201467   Pages: 13   Chinese
UT DIIDW:2014S23418
ER

PT P
PN CN103886301-A; CN103886301-B
TI Living human face detecting method, involves obtaining registration of human face image, and receiving identification information of human face image, confirming identification information of human face image based on image characteristic.
AB    NOVELTY - The method involves obtaining registration of a human face image (S1). Identification information of the human face image is received (S2). Judgment is made to determine whether the identity information of the human face image is valid by using a human face living detection classifier (S3). The human face living detection classifier is provided with a register in a specific individual process. Characteristic of the human face image is extracted. The identification information of the human face image is confirmed according to the characteristic of the human face image.
   USE - Living human face detecting method.
   ADVANTAGE - The method enables providing a human face sample synthesis algorithm for registration staff synthetic real face samples or dummy face samples, naturally integrating the human face identification operations and reducing operation load.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a living human face detecting method.'(Drawing includes non-English language text)'
   Step for obtaining registration of human face image (S1)
   Step for receiving identification information of human face image (S2)
   Step for determining whether the identity information of the human face image is valid (S3)
PD CN103886301-A   25 Jun 2014   G06K-009/00   201457   Pages: 11   Chinese
   CN103886301-B   18 Jan 2017   G06K-009/00   201710      Chinese
UT DIIDW:2014Q66133
ER

PT P
PN CN103679118-A; CN103679118-B
TI Living human face detection method, involves collecting current frame picture locating face key point/block, and discarding current frame picture when locating face key point/block time is not successful.
AB    NOVELTY - The method involves collecting a current frame picture locating face key point/block (101). The current frame picture is discarded when locating face key point/block time is not successful (102). The current frame picture is stored when the locating face key point/block time is successful (103). An average difference value is calculated according to the locating face key point/block (105). An average difference value is judged when the average difference value is more than a threshold value (106). A support vector machine (SVM) classifier is provided with the face key point/block.
   USE - Living human face detection method.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a living human face detection system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a living human face detection method.'(Drawing includes non-English language text)'
   Step for collecting current frame picture locating face key point/block (101)
   Step for discarding current frame picture when locating face key point/block time is not successful (102)
   Step for storing current frame picture when locating face key point/block time is successful (103)
   Step for calculating average difference value according to the locating face key point/block (105)
   Step for judging average difference value when the average difference value is more than a threshold value (106)
PD CN103679118-A   26 Mar 2014   G06K-009/00   201433   Pages: 17   Chinese
   CN103679118-B   16 Jun 2017   G06K-009/00   201742      Chinese
UT DIIDW:2014J69423
ER

PT P
PN US2013223694-A1; US8913839-B2
TI Method for demographic analysis of facial landmarks in digital photographs used by e.g. private entity for personal purpose, involves classifying particular facial image as male or female genders based on result of weighted comparison.
AB    NOVELTY - The method involves identifying a set of training vectors, where each training vector represents facial landmarks derived from a respective facial image in a digital photograph (200) of face. An input vector of facial landmarks is identified, where the landmarks are derived from a particular facial image. A feature vector containing a subset of the landmarks, is selected from the input vector. A weighted comparison is performed between the feature vector and each training vector. The particular facial image is classified as male or female genders based on a result of the weighted comparison.
   USE - Method for demographic analysis of facial landmarks in digital photographs used by private, public and government entities for personal, commercial and security purposes.
   ADVANTAGE - The method assures accurate demographic analysis for the facial landmarks.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a INDEPENDENT CLAIMS are also included for the following:
   (1) an article of manufacture including a non-transitory computer-readable medium comprising a set of instructions for performing a demographic analysis method
   (2) a computing system comprising a processor.
   DESCRIPTION OF DRAWING(S) - The drawing shows a digital photograph of a face, along with a set of facial landmarks.
   Digital photograph of face (200)
PD US2013223694-A1   29 Aug 2013   G06K-009/00   201359   Pages: 45   English
   US8913839-B2   16 Dec 2014   G06K-009/68   201482      English
UT DIIDW:2013M74480
ER

PT P
PN CN103258191-A
TI Human face identification cell based access control system, has image collecting module obtaining human face image, where image sequence extracts human face characteristic data that is stored with face characteristic database.
AB    NOVELTY - The system has a human face characteristic database storing a permanent zone of human face image and character data. An image collecting module collects human face video information and obtains the human face image in a human face detecting module for pre-processing the human face image. An image sequence of the human face detecting module extracts human face characteristic data. The human face characteristic data is stored in the face characteristic database. A peripheral hardware device is provided with a main machine, a sensor and a warning system.
   USE - Human face identification cell based access control system.
   ADVANTAGE - The system has stable running speed, and avoids illumination changing influences.
   DESCRIPTION OF DRAWING(S) - The drawing shows a functional block diagram of a human face identification cell based access control system. '(Drawing includes non-English language text)'
PD CN103258191-A   21 Aug 2013   G06K-009/00   201375   Pages: 6   Chinese
UT DIIDW:2013U75306
ER

PT P
PN CN102982322-A
TI Principal component analysis and linear discriminant analysis image reconstruction based human face identification method, involves obtaining human face feature vector, and calculating Euclidean distance vector based on minimum distance.
AB    NOVELTY - The method involves calculating optimal projection matrix, and mapping a test image to human face characteristic sub-space. A training image is utilized to extract the test image. Face space of the training image is calculated corresponding to minimum Euclidean distance between the test image and a human face image. The human face image is projected to the characteristic sub-space. Human face feature vector is obtained. Euclidean distance vector is calculated based on the minimum Euclidean distance.
   USE - Method for identification of human face based on principal component analysis and linear discriminant analysis image reconstruction.
   ADVANTAGE - The method enables extracting human face characteristics in a better manner, improving expandability and human face recognition rate in an effective manner and carrying out face training without re-processing human face characteristic sub-space.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart illustrating a principal component analysis and linear discriminant analysis image reconstruction based human face identification method. '(Drawing includes non-English language text)'
PD CN102982322-A   20 Mar 2013   G06K-009/00   201347   Pages: 12   Chinese
UT DIIDW:2013K38307
ER

PT P
PN CN102831392-A; CN102831392-B
TI Far-distance iris tracking and collecting device, has infrared light source fixed with long-focus camera, where long-focus camera is connected with two-degree-of-freedom rotating tripod head.
AB    NOVELTY - The device has a binocular camera (1) connected with a signal output end of a computer. The computer collects human face information of a long-focus camera (2). A two-degree-of-freedom rotating tripod head (3) is connected with a signal input end of the computer. An infrared (IR) light source (4) is fixed with the long-focus camera. The long-focus camera is connected with the two-degree-of-freedom rotating tripod head.
   USE - Far-distance iris tracking and collecting device.
   ADVANTAGE - The device has low target interference recognition algorithm precision range.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a far-distance iris tracking and collecting method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a far-distance iris tracking and collecting device.
   Binocular camera (1)
   Long-focus camera (2)
   Two-degree-of-freedom rotating tripod head (3)
   Light source (4)
PD CN102831392-A   19 Dec 2012   G06K-009/00   201325   Pages: 19   Chinese
   CN102831392-B   24 Jun 2015   G06K-009/00   201557      Chinese
UT DIIDW:2013E68698
ER

PT P
PN CN102332093-A; CN102332093-B
TI Authentication device for identifying palm and human face fusion, has storage module which is connected with human face identifying unit and palm print recognition unit.
AB    NOVELTY - The device has a main processing module which comprises an image obtaining module that is electrically connected with control unit. The control unit of face identifying unit and palm print recognition unit is connected with human face identifying unit. A storage module is connected with human face identifying unit and palm print recognition unit. The image obtaining module is used for shooting biology characteristic image according to instruction of control unit and sending biometric image to control unit.
   USE - Authentication device for identifying palm and human face fusion.
   ADVANTAGE - The safety of the biological feature identification can be improved and the device cost can be reduced.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for the identification method of palm and human face.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of the authentication device for identifying palm and human face fusion. (Drawing includes non-English language text)
PD CN102332093-A   25 Jan 2012   G06K-009/00   201215   Pages: 15   Chinese
   CN102332093-B   15 Jan 2014   G06K-009/00   201420      Chinese
UT DIIDW:2012C09369
ER

PT P
PN CN102332095-A; CN102332095-B
TI Human face motion tracking method involves determining number of human face characteristic point.
AB    NOVELTY - The method involves extracting characteristics point of face video image and human face outline characteristic. The number of human face characteristic point is determined. The human face characteristic points and three-dimensional movement model is used for tracking the human face in the video image if human face characteristic point is greater than the pre-set threshold value. The human face outline characteristic is used to move if the human face feature point is less than or equal to the preset threshold value.
   USE - Human face motion tracking method.
   ADVANTAGE - The process can accurately track head move so as to avoid the loss of the determined human face characteristic point.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) human face motion tracking system; and
   (2) enhanced reality method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the human face motion tracking process. (Drawing includes non-English language text)
PD CN102332095-A   25 Jan 2012   G06K-009/00   201215   Pages: 13   Chinese
   CN102332095-B   08 May 2013   G06K-009/00   201366      Chinese
UT DIIDW:2012C09367
ER

PT P
PN CN102262727-A
TI Real-time human face image quality monitoring method for client collecting terminal, involves collecting face image intelligent prompt and image quality by human face image quality detection system.
AB    NOVELTY - The method involves collecting client terminal face image. Human face detection is performed by a human face detection locating system to obtain human face image. Exact position of divided face is automatically detected. Face quality is evaluated. Face image intelligent prompt and image quality are collected by the human face image quality detection system. A detection is made to determine whether collected object is with presence of glass reflect light and wearing glasses.
   USE - Method for monitoring real-time human face image quality by a client collecting terminal.
   ADVANTAGE - The method allows effectively improving subsequent image comparison accuracy and reducing false alarm rate, so as to ensure reliability of information identification.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a real-time human face image quality monitoring method. '(Drawing includes non-English language text)'
PD CN102262727-A   30 Nov 2011   G06K-009/00   201202   Pages: 7   Chinese
UT DIIDW:2011Q42533
ER

PT P
PN CN201965714-U
TI Intelligent home early-warning security system, has monitor arranged in monitoring room for displaying images of illegal intruders and audible and visual alarm device, and module whose output interface sends filtered image data in recorder.
AB    NOVELTY - The system has Wireless fidelity (WIFI) whose output interface is in wireless communication connection with a wireless micro hard disk and sends an acquired image data in the wireless micro hard disk. An output interface of a General packet radio service (GPRS) module is in wireless communication connection with a wireless hard disk recorder and sends a filtered image data in the wireless hard disk recorder. A monitor arranged in a monitoring room for displaying images of illegal intruders and an audible and visual alarm device.
   USE - Intelligent home early-warning security system.
   ADVANTAGE - The system is based on face recognition technique, can automatically detect the access conduct, determine whether the access conduct is legal, and execute intelligent alarm.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of an intelligent home early-warning security system.'(Drawing includes non-English language text)'
PD CN201965714-U   07 Sep 2011   G08B-013/196   201166   Pages: 8   Chinese
UT DIIDW:2011M45917
ER

PT P
PN US2011091079-A1; TW201115483-A; US8300891-B2; TW405134-B1
TI Facial image recognition system for driver of vehicle, has warning unit coupled to image processing unit, where warning unit emits warning signal when image processing unit determines that identification of driver is changed.
AB    NOVELTY - The system has an image processing unit electrically coupled to an image capturing unit. The image processing unit utilizes a facial frame selecting and position correcting method to frame facial images to correct the facial images according to positional relations among facial features of the facial images when a vehicle is stopped. A warning unit is electrically coupled to the image processing unit, where the warning unit emits a warning signal when the image processing unit determines that the identification of the driver is changed.
   USE - Facial image recognition system for use as anti-theft system for a driver of a vehicle.
   ADVANTAGE - The system can effectively determine whether the driver is changed.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view illustrating that processes involved in an identification comparison algorithm.
PD US2011091079-A1   21 Apr 2011   G06K-009/00   201130   Pages: 11   English
   TW201115483-A   01 May 2011   G06K-009/62   201254      Chinese
   US8300891-B2   30 Oct 2012   G06K-009/00   201272      English
   TW405134-B1   11 Aug 2013   G06K-009/62   201412      Chinese
UT DIIDW:2011E22705
ER

PT P
PN US2010208954-A1; TW201032145-A; US8204284-B2; TW382350-B1
TI Fingerprint identifying system, has set of microstructure layers formed on one of top and bottom faces of light-transmissive finger press plate, and image-capturing unit located below light-transmissive finger press plate.
AB    NOVELTY - The system has a light source (3), and a light-transmissive finger press plate (4) located for receiving light from the light source. The plate has a top face (41) contacting a finger (C), a bottom face (42), and a set of microstructure layers (43) formed on one of the top and bottom faces of the light-transmissive finger press plate for guiding the light received from the light source for scattering uniformly.. An image-capturing unit (5) is located below the light-transmissive finger press plate. A light-diffusing member i.e. planar plate, is located below the finger press plate.
   USE - Fingerprint identifying system.
   ADVANTAGE - The utilization of the microstructure layers can homogeneously or uniformly scatter light to top face of the light-transmissive finger press plate such that fingerprint identification rate can be enhanced. The fingerprint identifying system can be assembled in a simple and convenient manner, and can be produced at a low cost, as configuration and density of the microstructures of the microstructure layers can be specified according to practical requirement.
   DESCRIPTION OF DRAWING(S) - The drawing shows a sectional view of a fingerprint identifying system.
   Finger (C)
   Light source (3)
   Light-transmissive finger press plate (4)
   Image-capturing unit (5)
   light units (31)
   Top face (41)
   Bottom face (42)
   Microstructure layers (43)
   Microstructures (431)
PD US2010208954-A1   19 Aug 2010   G06K-009/00   201058   Pages: 13   English
   TW201032145-A   01 Sep 2010   G06K-009/00   201127      Chinese
   US8204284-B2   19 Jun 2012   G06K-009/00   201240      English
   TW382350-B1   11 Jan 2013   G06K-009/00   201330      Chinese
UT DIIDW:2010K37163
ER

PT P
PN US2010045788-A1; US8229178-B2
TI Method for personal identification of e.g. elderly people, using biometric information, involves extracting sub-images from palm print and palm vein images, and matching extracted features with information in database to authenticate person.
AB    NOVELTY - The method involves switching between visible and near infrared light, and acquiring palm print image and palm vein image from a person under the visible and the near infrared light. Sub-images are extracted from the palm print image and the palm vein image based on a region of interest. Multiple features are extracted from the sub-images, and the extracted multiple features are matched with stored information in a database to authenticate a person. The person is rejected if the palm print image or the palm vein image is determined as non-skin material.
   USE - Method for personal identification of a person e.g. manual laborer and elderly people, using biometric information. Uses include but are not limited to palm print, palm vein, face, iris, voice, fingerprint, hand geometry and signature.
   ADVANTAGE - The method ensures the personal identification of the person in an easy and reliable manner and is effective against spoof attacks on the palm print image based system.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a personal identification system comprising a light controller configured to control a lighting unit.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic front view of an image capture device.
   Image acquisition device (100)
   Light controller (102)
   Lighting unit (103)
   Charge-coupled device camera (104)
   Analog to digital converter (105)
PD US2010045788-A1   25 Feb 2010   H04N-007/18   201030   Pages: 10   English
   US8229178-B2   24 Jul 2012   G06K-009/00   201249      English
UT DIIDW:2010C21992
ER

PT P
PN CN101618280-A; CN101618280-B
TI Human head imitation robot device, has sensor perceptive system provided with two small charge coupled device sensors, and movement control card that outputs movement control command to drive corresponding motors.
AB    NOVELTY - The device has an eyeball movement unit (1) provided with two eyeballs, an eyeball drive mechanism, two eyeball servo motors, two eyelids, an eyelid drive mechanism and a servo motor. A sensor perceptive system is provided with two small charge coupled device (CCD) sensors (3) i.e. component object model (COM)-based streaming media process development package DirectShow (RTM: multimedia framework and application programming interface) sensors. A movement control command is output via a movement control card (5) to drive corresponding motors.
   USE - Human head imitation robot device.
   ADVANTAGE - The device is capable of representing human facial expressions, and has multiple personalized perceptive functions such as sense of smell, touch and sight.
   DETAILED DESCRIPTION - The COM-based streaming media process development package DirectShow (RTM: multimedia framework and application programming interface) sensors are manufactured on a basis of an ActiveMovie and Video for Windows (RTM: operating system with a graphical user interface) by Microsoft (RTM: American multinational computer technology corporation). An image collection card is arranged on a main board by a peripheral component interconnect (PCI).
   DESCRIPTION OF DRAWING(S) - The drawing shows a perspective view of a human head imitation robot device.
   Eyeball movement unit (1)
   Small charge coupled device sensors (3)
   Voice recognition single chip (4)
   Movement control card (5)
   Rear support (6)
PD CN101618280-A   06 Jan 2010   A63H-013/00   201010   Pages: 28   Chinese
   CN101618280-B   23 Mar 2011   A63H-013/00   201143      Chinese
UT DIIDW:2010A73957
ER

PT P
PN EP2133819-A2; JP2009301170-A; KR2009129357-A; US2009309878-A1; CN101604387-A; TW201003570-A; JP4569670-B2; CN101604387-B; EP2133819-A3
TI Image processing apparatus e.g. digital still camera for identifying face of person has identifying unit which identifies face contained in target image by comparing two-dimensional image generated by generating unit against target image.
AB    NOVELTY - The image processing apparatus (100) has projecting unit which projects registered face image containing portion of face onto surface of three-dimensional model. A transforming unit transforms three-dimensional model on basis of orientation of face contained in target image. An identifying unit identifies face contained in target image by comparing two-dimensional image generated by generating unit against target image.
   USE - Image processing apparatus e.g. digital still camera and digital video camera for identifying face of person.
   ADVANTAGE - Provides an apparatus that can accurately identify an individual by using face images containing person's faces by providing an identifying unit that identifies face contained in target image by comparing two-dimensional image generated by generating unit against target image, while reducing load of face identification process.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an image processing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows the block diagram of functional configuration of an example of image processing apparatus.
   Image processing apparatus (100)
   Optical system (111)
   Face detection unit (120)
   Eye detection unit (130)
   Normalizing unit (140)
PD EP2133819-A2   16 Dec 2009   G06K-009/00   201002   Pages: 55   English
   JP2009301170-A   24 Dec 2009   G06T-001/00   201002   Pages: 43   Japanese
   KR2009129357-A   16 Dec 2009   G06T-007/00   201002      
   US2009309878-A1   17 Dec 2009   G06T-015/20   201002      English
   CN101604387-A   16 Dec 2009   G06K-009/00   201003      Chinese
   TW201003570-A   16 Jan 2010   G06T-007/00   201047      Chinese
   JP4569670-B2   27 Oct 2010   G06T-001/00   201071   Pages: 40   Japanese
   CN101604387-B   18 Apr 2012   G06K-009/00   201234      Chinese
   EP2133819-A3   28 Nov 2012   G06K-009/00   201278      English
UT DIIDW:2009S31828
ER

PT P
PN WO2009128784-A1; TW200943197-A; US2011188738-A1; US8374422-B2; TW383325-B1; SG165716-A1; SG165716-B
TI Properties measurement method for identifying face expressions of image objects which is widely applicable to different areas involves identifying face expression of image object based on profile.
AB    NOVELTY - The properties measurement method involves identifying image reference points having three-dimensional coordinates based on feature portions of three-dimensional head object (800). The three-dimensional mesh is manipulated or deformed by compensating mesh reference points accordingly towards image reference points. A difference is identified between the displaced image reference points for obtaining a profile. The difference is obtained by comparing inter-configuration of mesh reference points and image reference points. The face expression of image object is identified based on the profile.
   USE - Properties measurement method for identifying face expressions of image objects which is widely applicable to different areas. Uses include but are not limited to image understanding, psychological studies, facial nerve grading in medicine, face image compression, synthetic face animation, video indexing, robotics and virtual reality.
   ADVANTAGE - Implements the method for providing improved face detection to enable face expressions identification of image objects.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device readable medium.
   DESCRIPTION OF DRAWING(S) - The drawing shows an identification of face expression of the synthesized three-dimensional head object.
   Three-dimensional head object. (800)
PD WO2009128784-A1   22 Oct 2009   G06T-017/00   200971   Pages: 28   English
   TW200943197-A   16 Oct 2009   G06K-009/00   201015      Chinese
   US2011188738-A1   04 Aug 2011   G06K-009/00   201151      English
   US8374422-B2   12 Feb 2013   G06K-009/00   201312      English
   TW383325-B1   21 Jan 2013   C08K-005/00   201336      Chinese
   SG165716-A1   29 Nov 2010      201410      English
   SG165716-B   30 Apr 2013   G06T-017/00   201411      English
UT DIIDW:2009Q27836
ER

PT P
PN CN101464950-A; CN101464950-B
TI Video face identification and search method, involves transmitting density function of identity probability, and obtaining identification score based on maximum a-posteriori probability rule to supply video face identification result.
AB    NOVELTY - The method involves establishing an initialization model of a face identification model, and processing model updating for the initialization face identification model. Identification and search of video face is processed to give a testing sequence and a class model to accumulate a sequence identification information in a video. Density function of identity probability is transmitted according to a time axis information. The identification score based on a maximum a-posteriori probability (MAP) rule is obtained to supply the video face identification result to a user.
   USE - Video face identification and search method.
   ADVANTAGE - The method enables fully establishing a model training frame based on unsupervised study, and enables evolution of the initialization model to be class models with different formats according to the space distribution of training sequence. The method provides distribution of the space data by adjusting the gauss mixed number of the face class model.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a video face identification and search method.'(Drawing includes non-English language text)'
PD CN101464950-A   24 Jun 2009   G06K-009/00   200947   Pages: 17   Chinese
   CN101464950-B   04 May 2011   G06K-009/00   201167      Chinese
UT DIIDW:2009L09232
ER

PT P
PN EP1868158-A2; JP2007334623-A; US2007291998-A1; EP1868158-A8; CN101089875-A; KR2007119463-A; KR831122-B1
TI Face authentication apparatus for use in entrance and exit management apparatus, authenticates person based on recognition of image of face from the face information stored in advance.
AB    NOVELTY - An acquisition unit acquires an image including the face of the moving person. A face detection unit detects the face of the person from the acquired image. A face recognition unit recognizes the image of the detected face from the face information stored in advance. An authentication unit authenticates the person based on the recognition result of face recognition unit.
   USE - For use in entrance and exit management apparatus (claimed).
   ADVANTAGE - The face recognition process is performed by using several face images, so that the face authentication of the walker is attained quickly with high precision.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) face authentication method; and
   (2) entrance and exit management apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the entrance and exit management apparatus.
PD EP1868158-A2   19 Dec 2007   G06T-007/00   200806   Pages: 31   English
   JP2007334623-A   27 Dec 2007   G06T-007/00   200806   Pages: 21   Japanese
   US2007291998-A1   20 Dec 2007   G06K-009/00   200806      English
   EP1868158-A8   02 Apr 2008   G06T-007/00   200825      English
   CN101089875-A   19 Dec 2007   G06K-009/00   200829      Chinese
   KR2007119463-A   20 Dec 2007   G06K-009/52   200841      
   KR831122-B1   20 May 2008   G06K-009/52   200870      
UT DIIDW:2008A86486
ER

PT P
PN US2007217659-A1; JP2007249556-A; CN101038683-A; KR2007093780-A; KR769101-B1
TI Personal identification system for computer, has biometrics data collating section extracting group from set of collation groups based on characteristic data of collation object person.
AB    NOVELTY - The system (10) has a biometrics data inputting section e.g. fingerprint sensor, for inputting collation biometrics data (d1, d2, d3) of a collation object person, and a characteristic data obtaining section for obtaining characteristic data e.g. characteristic points, of the collation object person. A biometrics data collating section extracts a group from the set of collation groups based on the characteristic data of the collation object person, and collates the collation biometrics data with registration biometrics data correlated with the particular group.
   USE - Used with biometrics data e.g. fingerprint, palm print, finger shape, palm shape, voice, retina, ride, face image, dynamic signature, blood vessel pattern and keystroke, that is utilized for a individual person entering and exiting a condominium at an entrance, and during a transaction at an automated teller machine realized by a computer.
   ADVANTAGE - The biometrics data collating section extracts a group from set of collation groups based on the characteristic data of the collation object person, thus enabling the collation biometrics data to be collated with registration biometrics data of a number of registered object persons in a short time without requiring primary verification using identifications.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for personal identification
   (2) a computer readable medium has set of instruction for inputting collation biometrics data.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block representation showing a personal identification system.
   Biometrics data (d1, d2, d3)
   Personal identification system (10)
PD US2007217659-A1   20 Sep 2007   G06K-009/00   200775   Pages: 22   English
   JP2007249556-A   27 Sep 2007   G06T-007/00   200775   Pages: 19   Japanese
   CN101038683-A   19 Sep 2007   G07C-009/00   200810      Chinese
   KR2007093780-A   19 Sep 2007   G06K-009/00   200810      
   KR769101-B1   23 Oct 2007   G06K-009/00   200841      
UT DIIDW:2007803965
ER

PT P
PN WO2007103494-A2; US2007211938-A1; WO2007103494-A3; US7711146-B2
TI Image re-identification method for performing person re-identification based on overall appearance of individual involves comparing images based on correspondence map and region signatures to perform image re-identification.
AB    NOVELTY - The method involves obtaining images; generating a correspondence map between the images; and defining region signatures for one or more regions comprising the images. The images are compared based on the correspondence map and the region signatures to perform image re-identification.
   USE - For performing person re-identification based on the overall appearance of an individual.
   ADVANTAGE - Provides the ability to perform person re-identification based on the overall appearance of an individual as opposed to passive biometrics such as face and gait. Establishes correspondences between image parts and generates signatures that are invariant to variations in illumination and pose and to the dynamic appearance of loose or wrinkled clothing. The invariant signatures are generated by combining normalized color and salient edge histograms. Generates salient edges that are robust to changes in appearance of clothing. Fits an articulated model to each individual to establish correspondences between parts.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an image re-identification system.
   DESCRIPTION OF DRAWING(S) - The figure shows the high-level illustration of a system for re-identifying an image.
   Image re-identification system (10)
   Individual (12)
   Camera (14,16,18)
PD WO2007103494-A2   13 Sep 2007   G06K-009/00   200765   Pages: 26   English
   US2007211938-A1   13 Sep 2007   G06K-009/00   200765      English
   WO2007103494-A3   01 Nov 2007   G06K-009/00   200774      English
   US7711146-B2   04 May 2010   G06K-009/00   201030      English
UT DIIDW:2007701860
ER

PT P
PN EP1830305-A1; JP2007229360-A; US2007206098-A1; CN101030245-A; KR2007090725-A; KR853163-B1; CN100533463-C; EP1830305-B1; DE602006009756-E; US7978259-B2; JP4745084-B2
TI Object`s e.g. palm, image capturing apparatus e.g. camera, for biometric authentication, has circuit board for mounting image sensor and light-emitting devices, and optical unit housed inside ring of ring-shaped light guide.
AB    NOVELTY - The apparatus has a circuit board (20) for mounting an image sensor (30) e.g. complementary metal oxide semiconductor (CMOS) image sensor, for receiving light reflected from an object. Light-emitting devices (LEDs) (22, 24) are mounted on the circuit board in peripheral positions of the image sensor. A ring-shaped light guide (10) guides the light emitted from the LEDs and illuminates an image capturing range. An optical unit (34) housed inside the ring of the ring-shaped light guide guides the reflected light from the illuminated object in the range to the sensor.
   USE - Used for capturing an image of an object e.g. living human body portion such as palm and eye, by illuminating the object and receiving reflected light from the object, for recognizing living body features such as fingerprints of limbs, eye retinas, face, iris, hand skin pattern of palm and finger, vein and blood vessels, for personal identification in biometric authentication.
   ADVANTAGE - The apparatus captures an object by uniformly irradiating a predetermined range of the object with light emitted from the light-emitting devices, thus performing a wide range illumination upon an object, and realizing satisfactory image capturing with reduced cost. The image capturing apparatus is manufactured easily with reduced cost. The image sensor and light emitting devices are mounted on the same circuit board, thus miniaturizing the image capturing apparatus.
   DESCRIPTION OF DRAWING(S) - The drawing shows an exploded structural view of the image capturing apparatus.
   Ring-shaped light guide (10)
   Circuit board (20)
   Light-emitting devices (22, 24)
   Image sensor (30)
   Polarizing plate (32)
   Optical unit (34)
   Aperture (50)
   Control substrate (60)
   External connector (62)
   Holder cover (68)
PD EP1830305-A1   05 Sep 2007   G06K-009/20   200773   Pages: 30   English
   JP2007229360-A   13 Sep 2007   A61B-005/117   200773   Pages: 32   Japanese
   US2007206098-A1   06 Sep 2007   H04N-005/225   200773      English
   CN101030245-A   05 Sep 2007   G06K-009/00   200810      Chinese
   KR2007090725-A   06 Sep 2007   H04N-005/225   200810      
   KR853163-B1   20 Aug 2008   H04N-005/225   200909      
   CN100533463-C   26 Aug 2009   G06K-009/00   200965      Chinese
   EP1830305-B1   14 Oct 2009   G06K-009/20   200967      English
   DE602006009756-E   26 Nov 2009   G06K-009/20   200978      German
   US7978259-B2   12 Jul 2011   H04N-005/222   201145      English
   JP4745084-B2   10 Aug 2011   A61B-005/117   201152   Pages: 30   Japanese
UT DIIDW:2007779951
ER

PT P
PN US2007154096-A1; US7643659-B2
TI Facial feature e.g. eye location, locating method for e.g. mobile device, involves locating eye location by generating intensity response map and applying K-mean clustering to map to determine eye location.
AB    NOVELTY - The method involves locating an eye location by generating an intensity response map, and applying K-mean clustering to the map to determine the eye location, where the map is generated by applying a 3-rectangle filter to pixels in an eye slice from a facial area. An eye corner location is located by preprocessing pixels in an eye patch from the facial area by applying logarithm transform and grayscale stretching to generate a grayscale eye patch. A chin location is located by applying angle constrained gradient analysis to reject locations in a chin search region from the facial area.
   USE - Used for mobile and handheld devices for locating a facial feature e.g. eye location, eye corner location, mouth corner location, chin location, and cheek location, on a facial area (claimed).
   ADVANTAGE - The method enables rapid detection of a face and facial features from the face from an image on mobile and handheld devices.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart of a method for detecting a face and facial features on the face from an image.
PD US2007154096-A1   05 Jul 2007   G06K-009/46   200753   Pages: 37   English
   US7643659-B2   05 Jan 2010   G06K-009/00   201003      English
UT DIIDW:2007543902
ER

PT P
PN US2006204058-A1; KR2006063599-A; US7734067-B2
TI User recognition method of robot system, involves calculating similarity between user feature vector and user feature template which includes cluster having user feature vector as its component.
AB    NOVELTY - A user feature vector is extracted from an input facial image. A similarity is calculated between the extracted feature vector and a user feature template which includes a cluster having a user feature vector as a component of the cluster. An enrolled user of the feature template is recognized as the user of input image when the calculated similarity exceeds a predetermined recognition threshold value.
   USE - For recognizing user from input facial image, used by robot system.
   ADVANTAGE - The user is recognized under varying lighting and user pose conditions with facial appearances, in real time without complication.
   DESCRIPTION OF DRAWING(S) - The figure shows a schematic view of template and cluster of user feature information in user recognition system.
   original facial images (201-205)
   feature vectors (F1-F5)
PD US2006204058-A1   14 Sep 2006   G06K-009/00   200675   Pages: 15   English
   KR2006063599-A   12 Jun 2006   G06K-009/46   200675      
   US7734067-B2   08 Jun 2010   G06K-009/00   201038      English
UT DIIDW:2006724041
ER

PT P
PN WO2006051607-A1; EP1811456-A1; CN101057257-A; JP2006544712-X; US2008130961-A1; CN100565583-C; JP4501937-B2; US7936902-B2; EP1811456-B1; EP1811456-A4
TI Face feature point detector in personal computer, acquires error estimates representing difference between current position of each node arranged in face image and position of corresponding feature point.
AB    NOVELTY - The detector acquires error estimates representing difference between current position of each node arranged in face image and position of corresponding feature point, from information on correlation of difference between node feature values acquired in state that the nodes are arranged in correct and wrong positions. The positions of feature points on image are estimated based on error estimates and current node positions.
   USE - In information processing apparatus e.g. personal computer (PC), digital camera, digital cam corder, mobile telephone, digital video recorder, digital photo printer and mini-laboratory machine.
   ADVANTAGE - The position of feature point is detected from the face image at high speed, without performing shape correction processing or high-density sampling.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) face feature point detection method;
   (2) feature point detector; and
   (3) feature point detection method.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the face feature point detector. (Drawing includes non-English language text).
PD WO2006051607-A1   18 May 2006   G06T-007/20   200637   Pages: 57   Japanese
   EP1811456-A1   25 Jul 2007   G06K-009/62   200750      English
   CN101057257-A   17 Oct 2007   G06T-007/20   200819      Chinese
   JP2006544712-X   29 May 2008   G06T-007/60   200838   Pages: 41   Japanese
   US2008130961-A1   05 Jun 2008   G06K-009/00   200838      English
   CN100565583-C   02 Dec 2009   G06T-007/20   201029      Chinese
   JP4501937-B2   14 Jul 2010   G06T-007/60   201046   Pages: 33   Japanese
   US7936902-B2   03 May 2011   G06K-009/00   201129      English
   EP1811456-B1   28 Sep 2011   G06K-009/62   201163      English
   EP1811456-A4   01 Sep 2010   G06T-007/20   201742      English
UT DIIDW:2006363583
ER

PT P
PN US2005238207-A1; US7356168-B2
TI Biometric verification system for e.g. fingerprint verification, has verification engine including discriminator fusion component that analyzes distortion scores to obtain combined confidence score of biometric sets.
AB    NOVELTY - The system has a verification engine including discriminators that analyzes biometric inputs for a set of biometric sets to generate a set of distortion scores. A discriminator fusion component (122) in the engine analyzes the distortion scores to obtain a combined confidence score of the biometric sets. A training engine trains the discriminators and includes training data, a score analyzer and a subset priority analyzer.
   USE - Biometric verification system for e.g. fingerprint verification, face recognition and voice verification.
   ADVANTAGE - The system effectively determines whether a user is authentic enrolled user or an impostor.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a method for training a biometric verification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a verification engine.
   Verification engine (100)
   Pre-processor component (102)
   Feature vector extraction component (114)
   Discriminator fusion component (122)
   Classifier fusion model component (124)
PD US2005238207-A1   27 Oct 2005   G06K-009/00   200577   Pages: 20   English
   US7356168-B2   08 Apr 2008   G06K-009/00   200826      English
UT DIIDW:2005757011
ER

PT P
PN US2005117783-A1; KR2005053130-A; KR571826-B1; US8150109-B2
TI Face recognition apparatus, has face verification unit comparing feature vectors of input facial image and vectors of images corresponding to each candidate, and recognizing input facial image.
AB    NOVELTY - The apparatus has a face retrieval unit (10) performing a face retrieval of an input facial image and outputting confidence values of stored facial images. A candidate selection unit determines candidates that are selected among the facial images. A face verification unit (13) compares feature vectors of the input facial image and vectors of images corresponding to each candidate, and recognizes the input facial image.
   USE - Used for recognizing a face.
   ADVANTAGE - The apparatus recognizes the face by selecting candidate images by performing the face retrieval with respect to the input facial image and compares the input facial image and the selected candidate images, thus achieving high recognition accuracy and an improved recognition speed.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a computer readable storage medium encoded with processing instructions for causing a processor to execute a face recognition method
   (B) a method of increasing a speed of a facial recognition operation.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a face recognition apparatus.
   Face retrieval unit (10)
   Face database (11)
   Candidate selection unit (12)
   Face verification unit (13)
   Candidate determinator (122)
PD US2005117783-A1   02 Jun 2005   G06K-009/00   200542   Pages: 8   English
   KR2005053130-A   08 Jun 2005   G06K-009/00   200641      
   KR571826-B1   17 Apr 2006   G06K-009/00   200724      
   US8150109-B2   03 Apr 2012   G06K-009/00   201224      English
UT DIIDW:2005416602
ER

PT P
PN JP2005084815-A
TI Human face recognition apparatus for security management, determines whether appropriate lighting environment for face recognition, is obtained and outputs determination result.
AB    NOVELTY - A recognition unit (105) recognizes the candidate by collating the characteristic information extracted by an extraction unit (103) with the dictionary information stored in the storage unit (104). A determination unit (106) determines whether the appropriate lighting environment for face recognition, is obtained. An output unit (107) outputs the determination result.
   USE - For recognizing human face for security management.
   ADVANTAGE - The characteristic information of the person's face is detected correctly for a long period of time.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) human face recognition method; and
   (2) passing control apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows a block diagram of the human face recognition apparatus. (Drawing includes non-English language text).
   extraction unit (103)
   torage unit (104)
   recognition unit (105)
   determination unit (106)
   output unit (107)
PD JP2005084815-A   31 Mar 2005   G06T-007/00   200536   Pages: 26   Japanese
UT DIIDW:2005348426
ER

PT P
PN WO2005024698-A2; US2005084179-A1; EP1671258-A2; JP2007504562-W; WO2005024698-A3; EP1671258-A4
TI Iris recognition method involves detecting image comprising face and eyes of target object using wide-field-of-view camera and detecting sequence of high resolution iris image using narrow-field-of-view camera.
AB    NOVELTY - The image comprising face and eyes of a target object (102), is detected by a wide-field-of-view (WFOV) camera and the sequence of high resolution iris image is detected by a narrow-field-of-view (NFOV) camera. The detected images are processed to perform iris recognition.
   USE - For recognizing iris.
   ADVANTAGE - Since large sensing area is covered on detecting the high resolution iris image by the narrow-field-of-view-camera, iris is captured while the target is moving around.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) iris recognizing apparatus;
   (2) recording medium storing iris recognition program; and
   (3) single image formation method.
   DESCRIPTION OF DRAWING(S) - The figure shows the schematic view of the iris sensing and acquisition system.
   iris sensing and acquisition system (100)
   target subject (102)
PD WO2005024698-A2   17 Mar 2005   G06K-000/00   200530   Pages: 19   English
   US2005084179-A1   21 Apr 2005   G06K-009/32   200530      English
   EP1671258-A2   21 Jun 2006   G06K-009/00   200643      English
   JP2007504562-W   01 Mar 2007   G06T-003/00   200718   Pages: 11   Japanese
   WO2005024698-A3   24 Nov 2005   G06K-009/00   201218      English
   EP1671258-A4   19 Mar 2008   G06K-009/00   201747      English
UT DIIDW:2005295358
ER

PT P
PN US2004258309-A1; US7289662-B2
TI 3-D model generating method, involves searching best matches of persistent feature groupings for subset of uncalibrated image capturing device locations to stitch models to form overall three-dimensional model of scene`s portion.
AB    NOVELTY - The method involves stitching together three-dimensional models viewed from a subset of uncalibrated image capturing device locations. An act of finding spatially local persistent feature groupings is iteratively performed for a subset of the locations. Best matches of the groupings are searched to stitch the models together to form an overall three-dimensional model of a portion of a scene, where the model is output.
   USE - Used for generating three-dimensional model from still imagery or video stream, from uncalibrated view that is obtained using an uncalibrated image capturing device e.g. a still camera, a video camera, a magnetic resonance imaging (MRI) recording mechanism, an ultrasound recording apparatus, an internet web camera, a direct satellite link, a video cassette recorder (VCR), a digital versatile disc (DVD) player, and imaging recording media such as a compact disk (CD), a digital versatile disk/digital video disk (DVD), a floppy disk, a magnetic tape, a removable hard drive, a printed picture, a scanned document, a faxed document, a digital camera, a video cassette, a magnetic resonance imaging (MRI) recording media, an ultrasound recording media, and a solid-state recording media.
   ADVANTAGE - The method generates overall 3-D models from the still imagery or the video streams from the uncalibrated views captured from multiple uncalibrated image capturing device locations. The method automatically aligns the 3D models to be stitched together without the need of manually approximately registering the points in common between the models, thus requiring minimum or little human intervention.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (A) a system for generating three-dimensional models from uncalibrated views
   (B) a computer program product for generating three- dimensional models from uncalibrated views.
   DESCRIPTION OF DRAWING(S) - The drawing shows an image depicting intensity blocks around the features extracted from an uncalibrated frontal camera view of a face.
   Images (500, 502, 504, 506, 508, 510, 512, 514)
PD US2004258309-A1   23 Dec 2004   G06K-009/46   200510   Pages: 42   English
   US7289662-B2   30 Oct 2007   G06K-009/00   200772      English
UT DIIDW:2005089003
ER

PT P
PN GB2395779-A; WO2004051551-A1; EP1565870-A1; JP2006508461-W; US2006104487-A1; CN1717695-A; CN1320490-C; WO2004051551-A8
TI Human face detection apparatus has predictor that predicts face position using face position detected by another detector when detector having high detection threshold fails to detect face position.
AB    NOVELTY - A face position predictor predicts the face position in next image in a test order of video sequence using the face position detected by another face detector in case face detector having high detection threshold fails to detect face within predetermined threshold image distance.
   USE - For detecting human face in video sequence of successive images.
   ADVANTAGE - The human face detection algorithm automatically knows whether face detected by successive frame belongs to the same person or different person. Thus meta data is easily generated from this algorithm.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) video conferencing apparatus;
   (2) surveillance apparatus;
   (3) detected face tracking method;
   (4) face detecting method;
   (5) computer software having program for detecting human face; and
   (6) providing medium for providing program for detecting human face.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the human face detection apparatus.
PD GB2395779-A   02 Jun 2004   G06T-007/20   200445   Pages: 55   English
   WO2004051551-A1   17 Jun 2004   G06K-009/00   200445      English
   EP1565870-A1   24 Aug 2005   G06K-009/00   200556      English
   JP2006508461-W   09 Mar 2006   G06T-007/20   200620   Pages: 50   Japanese
   US2006104487-A1   18 May 2006   G06K-009/00   200634      English
   CN1717695-A   04 Jan 2006   G06K-009/00   200639      Chinese
   CN1320490-C   06 Jun 2007   G06K-009/00   200777      Chinese
   WO2004051551-A8   28 Apr 2005   G06K-009/00   201728      English
UT DIIDW:2004470494
ER

PT P
PN JP2003015816-A; JP4898026-B2
TI Human face and eye recognition apparatus for use in motor vehicle, detects direction of eye from photographed image after detecting opening-closing of eye.
AB    NOVELTY - The detectors detect the direction of face and detect whether the user's eyes are opened in the photographed image. Another detector (43) detects the direction of eyes from the photographed image, after detecting the opening-closing of the eye.
   USE - Human face and eyes recognition apparatus for motor vehicle, cars.
   ADVANTAGE - Highly accurate image processing is obtained, and incorrect tracking of direction of eyes and face is reduced.
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of the face and eyes recognition apparatus. (Drawing includes non-English language text).
   Detector (43)
PD JP2003015816-A   17 Jan 2003   G06F-003/033   200316   Pages: 14   Japanese
   JP4898026-B2   14 Mar 2012   G06F-003/033   201220   Pages: 17   Japanese
UT DIIDW:2003162763
ER

PT P
PN US2002150280-A1; US7155036-B2
TI Face detection method for automatic human face recognition system, involves identifying submatrices matching distance and angle criteria to detect presence of face in image.
AB    NOVELTY - Submatrices matching facial features are determined by comparing the amplitude of the vector computed for the submatrices with a threshold. Distance and angle criteria are established for relationships among the determined submatrices. Submatrices matching the distance and angle criteria are identified to detect the presence of the face in the image.
   USE - Face detection method for automatic human face recognition systems for surveillance systems, photo identification database searching system and automatic photo retouching system.
   ADVANTAGE - Detects human faces rapidly and efficiently under a wide range of luminance conditions, head orientations, scale and rotation and eliminates the need for templates for different angles.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) Electronic storage medium; and
   (2) Face detection apparatus.
   DESCRIPTION OF DRAWING(S) - The figure shows the flowchart explaining the face detection process.
PD US2002150280-A1   17 Oct 2002   G06K-009/00   200319   Pages: 26   English
   US7155036-B2   26 Dec 2006   G06K-009/00   200702      English
UT DIIDW:2003198242
ER

PT P
PN EP1100039-A2; JP2001141411-A; KR2001051651-A; TW538380-A; US7031500-B1; KR758516-B1; EP1100039-A3
TI Semiconductor device for fingerprint recognition, forms insulating film on the chip, covering electrodes formed on chip.
AB    NOVELTY - Pad electrodes (2a) are formed on semiconductor chip (1), and are electrically separated from electrodes (2). Insulating film (3) on chip, covers electrodes (2) when plane of insulating film which is not on a side of chip, is taken as one plane. Distance between chip surface and surface of film (3) facing plane of pad electrode, is larger than that between surfaces of chip and film (3) facing electrode (2).
   USE - For fingerprint recognition in security system at entrance and exit of offices, computer networks and personal identification tools in a portable terminal.
   ADVANTAGE - The structure of the semiconductor chip ensures that when a finger is brought close to the substrate, static electricity is not discharged to the electrode but into static electricity drawing wiring thus preventing damage to the chip due to static electricity.
   DESCRIPTION OF DRAWING(S) - The figure shows the cross-sectional view of the semiconductor chip.
   Semiconductor chip (1)
   Electrodes (2,2a)
   Insulating film (3)
PD EP1100039-A2   16 May 2001   G06K-009/20   200145   Pages: 12   English
   JP2001141411-A   25 May 2001   G01B-007/28   200146   Pages: 9   Japanese
   KR2001051651-A   25 Jun 2001   G06K-009/00   200172      
   TW538380-A   21 Jun 2003   G06K-009/00   200377      Chinese
   US7031500-B1   18 Apr 2006   G06K-009/00   200627      English
   KR758516-B1   14 Sep 2007   G06K-009/00   200839      
   EP1100039-A3   08 May 2002   G06K-009/20   201732      English
UT DIIDW:2001419748
ER

PT P
PN JP2000222576-A; JP3307354-B2
TI Person identification for illumination control robot, by referring linear discrimination dictionary, to determine similarity between information in that dictionary and characteristic information from person image.
AB    NOVELTY - Face feature extraction unit converts front face image extracted by front face alignment unit from partial image of head of persons, into characteristics. Information face identification unit identifies the person by referring linear discrimination dictionary for determining similarity between information registered in linear discrimination dictionary to extracted characteristic information.
   USE - For person identification of illumination control robot for home.
   ADVANTAGE - Identification accuracy is improved even when identification is performed under the environment where fluctuation of illumination condition is intense, as detection is performed using face distance information and front face characteristics.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (a) person identification apparatus;
   (b) recording medium;
   (c) robot
   DESCRIPTION OF DRAWING(S) - The figure shows the block diagram of component of person identification apparatus.
PD JP2000222576-A   11 Aug 2000   G06T-007/00   200053   Pages: 30   Japanese
   JP3307354-B2   24 Jul 2002   G06T-007/00   200255   Pages: 28   Japanese
UT DIIDW:2000568618
ER

PT P
PN EP855666-A2; JP10262951-A; KR98070742-A; US6144756-A; EP855666-A3
TI Finger verification system - recognises particular person related to finger according to impedance distribution pattern group obtained by measurement based on number of electrodes.
AB       The system includes a case with a number of grooves (3,5) disposed on a rear face. A number of electrode faces (7,9) are provided in each grooves and formed with a number of electrodes arranged in the same direction as an extension direction of the grooves.
   A recognition device (23) recognises a particular person related to the finger according to an impedance distribution pattern group obtained by measurement based on the number of electrodes. The case is provided with the number of the grooves on a rear face and contains a rotary body (60) rotatable around a shaft(61) vertical to the rear face.
   ADVANTAGE -   Capable of carrying out personal recognition regardless of operator's gripping style.
PD EP855666-A2   29 Jul 1998   G06K-009/00   199834   Pages: 43   English
   JP10262951-A   06 Oct 1998   A61B-005/107   199850   Pages: 18   Japanese
   KR98070742-A   26 Oct 1998   G08B-006/00   199953      
   US6144756-A   07 Nov 2000   G06K-009/00   200059      English
   EP855666-A3   15 Sep 1999   G06K-009/00   201735      English
UT DIIDW:1998389980
ER

PT P
PN EP524119-A1; FR2677151-A1; CA2110427-A; US5515168-A; EP524119-B1; DE69222099-E; ES2109327-T3; CA2110427-C
TI Identification and classification appts. of human form in clothing industry - uses camera facing screen to take views of body from different angles and neural net processing to classify body.
AB       The classification system takes two-dimensional views of an object and has a module for acquisition, extraction and processing of views. The processing extracts a characteristic dimensional parameter, and the body is defined by these parameters.
   The views are taken by a camera facing a high contrast screen. Both camera and screen can be rotated around the subject. The processing uses a neural net program to classify the subject. It also has a three-dimensional image generator.
   ADVANTAGE -   Rapid classification, with simple hardware, of human forms.
PD EP524119-A1   20 Jan 1993      199303   Pages: 9   French
   FR2677151-A1   04 Dec 1992      199305      French
   CA2110427-A   02 Jun 1995   A61B-005/117   199535      French
   US5515168-A   07 May 1996   G01B-011/02   199624   Pages: 8   English
   EP524119-B1   10 Sep 1997   G06K-009/00   199741   Pages: 12   French
   DE69222099-E   16 Oct 1997   G06K-009/00   199747      German
   ES2109327-T3   16 Jan 1998   G06K-009/00   199810      Spanish
   CA2110427-C   24 May 2005   A61B-005/117   200538      French
UT DIIDW:1993020112
ER

PT P
PN US10322728-B1; WO2019161766-A1; CN111741884-A; EP3755597-A1; EP3755597-A4
TI Method for determining distress of driver of vehicle, involves generating warning by multiple processors based on classification indicating impaired driver behavior.
AB    NOVELTY - The method (200) involves receiving (205) inputs by multiple processors from a multiple of sensors. The received inputs are processed (210) by the multiple processors to obtain a driver heat change estimate. A normal driving model offline is generated using normal driving thermal images of the driver. A probability density function (PDF) is applied to the comparison result to obtain the deviation score for the driver heat change estimate. A machine learning algorithm is executed (225) by the multiple processors to classify driver behavior as normal or impaired based on the deviation scores. A warning is generated (230) by the multiple processors based on the classification indicating impaired driver behavior.
   USE - Method for determining distress of driver of vehicle.
   ADVANTAGE - The method helps the machine learning algorithm to track the changes over time, and predict if the driver present is distress and be prone to road rage. The driver feedback is used with reinforcement learning algorithms by updating the distress or road rage detector models using the buffered streams. By combining all the modalities along side the thermal signature of both the driver's face and hands produces a more generic and more robust model resistant to false positives.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a system for determining distress of a driver of a vehicle; and
   (2) a non-transitory computer-readable medium for determining distress of a driver of a vehicle.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for detection of driver and passenger distress.
   Method for determining distress of driver of vehicle (200)
   Step for receiving inputs by multiple processors from multiple of sensors (205)
   Step for processing the received inputs by the multiple processors to obtain a driver heat change estimate (210)
   Step for executing a machine learning algorithm by the multiple processors (225)
   Step for generating a warning by the multiple processors (230)
PD US10322728-B1   18 Jun 2019   A61B-005/16   201947   Pages: 30   English
   WO2019161766-A1   29 Aug 2019   B60W-050/08   201966      English
   CN111741884-A   02 Oct 2020   B60W-050/08   202084      Chinese
   EP3755597-A1   30 Dec 2020   B60W-050/08   202104      English
   EP3755597-A4   21 Apr 2021   B60W-050/08   202134      English
UT DIIDW:201951246U
ER

PT P
PN CN108446600-A
TI Vehicle driver fatigue monitoring and pre-warning system, has processing and feature extracting unit for obtaining eye characteristic information, and fatigue judging unit for determining whether fatigue degree of driver is detected.
AB    NOVELTY - The system has a face detection and tracking module for obtaining a face image. The face detection and tracking module performs face image analyzing operation. A feature point positioning unit performs key characteristic point positioning process. A normalization processing and feature extracting unit performs face image normalization processing operation. The normalization processing and feature extracting unit obtains eye characteristic information, nozzle characteristic information and head characteristic information. A fatigue judging unit determines whether PERCLOS fatigue degree of a driver is detected.
   USE - Vehicle driver fatigue monitoring and pre-warning system.
   ADVANTAGE - The system has high driver fatigue detecting efficiency.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a vehicle driver fatigue monitoring and pre-warning method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a vehicle driver fatigue monitoring and pre-warning system. '(Drawing includes non-English language text)'
PD CN108446600-A   24 Aug 2018   G06K-009/00   201861   Pages: 16   Chinese
UT DIIDW:2018680624
ER

PT P
PN CN108189804-A; CN108189804-B
TI Human face identification method for vehicle, involves performing identity authentication based on face database and face feature value, and if authentication is successful.
AB    NOVELTY - The method involves receiving (S101) face feature value data of the authorized user at the vehicle terminal and storing the face database local to the vehicle terminal. A face image of a user of a vehicle driving position is obtained (S103). The face feature extraction on the face image is performed (S104) to obtain a face feature value of the user. The identity authentication is performed (S105) based on the face database and the face feature value, and if the authentication is successful. The vehicle is controlled to perform a personalized setting associated with the user.
   USE - Human face identification method for vehicle.
   ADVANTAGE - The waste of hardware resources of the vehicle system and the slow speed of the traditional face recognition is eliminated.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a face recognition system for a vehicle.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a process of human face identification. (Drawing includes non-English language text)
   Step for receiving face feature value data of the authorized user at the vehicle terminal and storing the face database local to the vehicle terminal (S101)
   Step for obtaining face image of a user of a vehicle driving position (S103)
   Step for performing face feature extraction on the face image to obtain a face feature value of the user (S104)
   Step for performing identity authentication is based on the face database and the face feature value, and if the authentication is successful (S105)
   Step for determining whether the authentication is successful (S106)
PD CN108189804-A   22 Jun 2018   B60R-025/25   201845   Pages: 15   Chinese
   CN108189804-B   07 Aug 2020   B60R-025/25   202066      Chinese
UT DIIDW:201850818R
ER

PT P
PN CN108171207-A
TI Method for recognizing human face object based on video sequence by using electronic device, involves obtaining identification information of face object, where pre-set characteristic database is provided with tag identification information.
AB    NOVELTY - The method involves responding to detect an image frame in an acquired video sequence that contains a human face object. The human face object is tracked to extract multiple target image frames from the video sequence. Extracting characteristics of the human face object is performed, where the object is included with multiple target image frames. Identification information of the human face object is obtained based on the extracted characteristic in a pre-set human face characteristic database, where the pre-set human face characteristic database is provided with tag identification information.
   USE - Method for recognizing a human face object based on video sequence by using an electronic device (claimed).
   ADVANTAGE - The method enables improving accuracy of face recognition and determining efficiency of video content attribute.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a video sequence based human face object recognition device
   (2) a computer readable storage medium for storing a set of instructions to recognize human face object.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for recognizing a human face object based on video sequence. '(Drawing includes non-English language text)'
PD CN108171207-A   15 Jun 2018   G06K-009/00   201845   Pages: 18   Chinese
UT DIIDW:201849005B
ER

PT P
PN EP3309728-A1; US2018108120-A1; CA2981552-A1; US10210603-B2; EP3309728-B1
TI Robotic system for product location mapping, has shift distance is applied to location of mobile base along aisle provided by primary navigation component.
AB    NOVELTY - The robotic system has a mobile base (20) that includes an operatively associated primary navigation component is configured to determine a location of the mobile base within an operating environment. A master control unit (24) is associated with the mobile base and several image capture devices. The corrected location of the mobile base is calculated by determining a shift distance associated with aligning the overlapping portions of the sequential overlapping images. The shift distance is applied to the location of the mobile base along the aisle provided by the primary navigation component.
   USE - Robotic system for product location mapping.
   ADVANTAGE - The pre-filtering on the images are performed to remove flat areas that bias the results. The stitch images together are eliminated to conduct planogram-level image analysis for product facing detection and avoids the extra computation time and resources required to stitch images and image quality errors in the stitched images due to robot positional errors. The separate low resolution imaging system eliminates the computation resources and IQ errors with stitching images together to generate a planogram image for analysis and provides the option of running a low-res only mission at a faster speed which aligns with expected faster rate of change of the analysis content.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a continuous motion store profile generation system; and
   (2) a store profile generation system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic elevational view of a store profile generation system.
   Mobile base (20)
   Control unit (24)
   Drive Wheels (34)
   Sensors (36)
   Support frame (46)
PD EP3309728-A1   18 Apr 2018   G06Q-010/08   201828   Pages: 65   English
   US2018108120-A1   19 Apr 2018   G06T-005/00   201829      English
   CA2981552-A1   17 Apr 2018   B25J-009/18   201829      English
   US10210603-B2   19 Feb 2019   G06T-005/00   201914      English
   EP3309728-B1   26 Jan 2022   G06Q-010/08   202208      English
UT DIIDW:2018298048
ER

PT P
PN CN107862299-A; CN107862299-B
TI Live human face detection method based on near-infrared and visible light binocular camera, involves training step of living face classification model, which includes collecting training samples, including several images.
AB    NOVELTY - The method involves training step of living face classification model, which includes collecting training samples, including real face image GNIR photographed under near-infrared camera, near-infrared artificial face image NNIR, and visible light face image VNIR. The live face detection step is performed in which separately capturing image video of object to be detected under near-infrared camera and visible light camera satisfies detection duration, image of near-infrared camera is recorded as first image video, and visible light camera is marked as second image video.
   USE - Live human face detection method based on near-infrared and visible light binocular camera.
   ADVANTAGE - The live human face detection method based on near-infrared and visible light binocular camera greatly improves the accuracy rate and provides guaranteed security. The method doesnBt require users to cooperate with the machine to make corresponding actions or expressions. Thus the user's experience is improved.
PD CN107862299-A   30 Mar 2018   G06K-009/00   201828   Pages: 8   Chinese
   CN107862299-B   06 Aug 2021   G06K-009/00   202168      Chinese
UT DIIDW:201826570T
ER

PT P
PN CN107832677-A
TI Human face recognizing method, involves obtaining two-dimensional image and depth image, and carrying out human face detection and recognition process, and performing skin detection and/or three-dimensional detection operation.
AB    NOVELTY - The method involves obtaining two-dimensional image and depth image. A human face detection and recognition process is carried out based on two-dimensional image and depth image. A skin detection and/or three-dimensional detection operation is performed by using the depth image, where the two-dimensional image includes color image or structure light image. The skin detection process is performed by using the color image and reflection characteristic of structured light image. Environmental light intensity is determined when ambient light intensity is higher than threshold value.
   USE - Human face recognizing method.
   ADVANTAGE - The method enables realizing living body detection by using two- dimensional image for skin detection and effectively differentiating original face, removing photos, videos, model or mask and impersonation attack and improving security level of human face recognition.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face recognizing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face recognizing method. '(Drawing includes non-English language text)'
PD CN107832677-A   23 Mar 2018   G06K-009/00   201825   Pages: 10   Chinese
UT DIIDW:201824230H
ER

PT P
PN CN107609517-A; CN107609517-B
TI Computer vision based classroom behavior detecting system, has behavior judging sub-module for obtaining action behavior of students in classroom according to student head state and action and storing action behavior in data storage module.
AB    NOVELTY - The system has a data pre-processing sub-module (111) for converting classroom video information into an image sequence. A human pose estimation module (112) performs two-dimensional (2D) human pose estimation for a panorama image to identify position information of a human body key part. A behavior judging sub-module (118) obtains action behavior of students in the classroom according to a student head state obtained by a head pose estimation module (117) and action detected by an action detection sub-module (115) and stores the action behavior of the students in a data storage module (12).
   USE - Computer vision based classroom behavior detecting system.
   ADVANTAGE - The system detect normal 2D video information without using hardware requirement, and automatically identify student identity by a face recognition algorithm, student seats and dynamic change of a scene, and performs body position and seat matching process to obtain a student position.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a computer vision based classroom behavior detecting system. '(Drawing includes non-English language text)'
   Data storage module (12)
   Data pre-processing sub-module (111)
   Human pose estimation module (112)
   Action detection sub-module (115)
   Head pose estimation module (117)
   Behavior judging sub-module (118)
PD CN107609517-A   19 Jan 2018   G06K-009/00   201811   Pages: 9   Chinese
   CN107609517-B   30 Oct 2020   G06K-009/00   202092      Chinese
UT DIIDW:201808048L
ER

PT P
PN CN107590482-A
TI Method for generating information of server, involves inputting captured image to convolutional neural network to obtain feature information, analyzing image feature information, and determining key point position of each face in image.
AB    NOVELTY - The method involves obtaining a to-be-detected image. Face detection is performed on the to-be-detected image to obtain a human face region. The human face region in the to-be-detected image is indicated by a human face area information. The human face region indicated by the human face region information is enlarged. A captured image is input to a convolutional neural network to obtain image feature information. Image feature information is analyzed. A key point position of each human face in the image is determined.
   USE - Method for generating information of a server (claimed).
   ADVANTAGE - The method enables improving generated information in a server.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for generating information of a server
   (2) a computer readable storage medium comprising a set of instructions for generating information of a server.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for generating information of a server. '(Drawing includes non-English language text)'
PD CN107590482-A   16 Jan 2018   G06K-009/00   201810   Pages: 18   Chinese
UT DIIDW:201807068Y
ER

PT P
PN CN107578007-A
TI Multi-feature fusion based deep learning face recognizing method, involves forming restricted Boltzmann machine with first and second hidden layers, and calculating accuracy rate of unknown sample through fine-tuning process.
AB    NOVELTY - The method involves extracting a histogram of oriented gradient (HOG) characteristic of test samples to describe multiple attributes of a testing sample. A normalization process is performed after reducing dimension characteristic. Two feature vectors are merged to obtain a characteristic vector. Deep belief network is constructed with first and second hidden layers by adopting a deep belief network (DBN) algorithm. A restricted Boltzmann machine is formed with the first and second hidden layers. An accuracy rate of an unknown sample is predicted and calculated through a fine-tuning process.
   USE - Multi-feature fusion based deep learning face recognizing method.
   ADVANTAGE - The method enables combining multi-characteristics of the test samples, and improving accuracy, stability and adaptability of complex scene algorithm.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating a multi-feature fusion based deep learning face recognizing method. '(Drawing includes non-English language text)'
PD CN107578007-A   12 Jan 2018   G06K-009/00   201809   Pages: 16   Chinese
UT DIIDW:201806034Q
ER

PT P
PN CN107220621-A
TI Method for recognizing human face by terminal, involves determining to perform face identification, obtaining face image by lifting face recognition, and performing face identification operation of acquisition according to human face image.
AB    NOVELTY - The method involves determining to perform face identification. An environmental light brightness is detected. Judgment is made to check whether the environment light brightness is lower than a brightness threshold value. A face image is obtained by utilizing lifting face recognition. A face identification operation of an acquisition is performed according to a human face image. A shooting parameter of a camera device is adjusted. An infrared identifying device mode is started. A lighting lamp is controlled to provide an auxiliary light of a brightness level.
   USE - Method for recognizing human face by terminal.
   ADVANTAGE - The method enables realizing high ambient brightness and high accuracy of face recognition.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a device for recognizing human face by terminal
   (2) a non-temporary computer storage medium comprises set of instructions for recognizing human face by terminal.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a method for recognizing human face by terminal. '(Drawing includes non-English language text)'
PD CN107220621-A   29 Sep 2017   G06K-009/00   201774   Pages: 19   Chinese
UT DIIDW:201769723F
ER

PT P
PN CN107085721-A
TI Face recognition based intelligent tour class managing system, has audio and video module for performing audio and video signal frequency division multiplexing process for feedback information and sending digital signal to cloud server.
AB    NOVELTY - The system has a tour class management cloud server for storing real-time recorded teaching video, and analyzing and evaluating teaching quality according to the recorded teaching video. A teacher terminal starts course recording process and opens an evaluation port. An audio and video module performs audio and video signal frequency division multiplexing process for feedback information received from an audio and video acquisition device to obtain a modulated digital signal, and sends the modulated digital signal to the tour class management cloud server.
   USE - Face recognition based intelligent tour class managing system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic view of a face recognition based intelligent tour class managing system.
PD CN107085721-A   22 Aug 2017   G06K-009/00   201772   Pages: 9   Chinese
UT DIIDW:201760119A
ER

PT P
PN CN107045618-A; CN107045618-B
TI Human facial expression identification method, involves obtaining neural network by deep learning model according to training images of facial expressions, and determining facial expression of detection image based on classification result.
AB    NOVELTY - The method involves extracting local features and integral feature of a detection face image. The local feature, the integral feature and the detection image are inputted to a deep learning model. A classification result of the detection image is obtained for different facial expressions. A deep neural network is obtained by the deep learning model according to training images of the different facial expressions, the local feature and the integral feature of each training images. The facial expression of the detection image is determined based on the classification result.
   USE - Human facial expression identification method.
   ADVANTAGE - The method enables improving accuracy of facial expression identification.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human facial expression identification device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human facial expression identification method. '(Drawing includes non-English language text)'
PD CN107045618-A   15 Aug 2017   G06K-009/00   201767   Pages: 20   Chinese
   CN107045618-B   03 Jul 2020   G06K-009/00   202057      Chinese
UT DIIDW:201756912Y
ER

PT P
PN CN107038429-A
TI Multi-task based deep learning cascade face alignment method, involves determining attribute label of key point, predicting face attribute of key point and human face, and outputting face attribute of key point and human face in real-time.
AB    NOVELTY - The method involves determining attribute label of a key point and a mark face. A human face image is obtained by using grey scaling and normalization process. A face alignment model is trained using a convolutional neural network. The convolution layer is arranged on a normalization layer, an active layer and a cell layer. The normalization layer is arranged on a connection layer after predicting a branch network by the connection layer. Face attribute of the key point and the human face are predicted. The face attribute of the key point and the human face are outputted in real-time.
   USE - Multi-task based deep learning cascade face alignment method.
   ADVANTAGE - The method enables realizing multi-task learning process of a dual-layer network so as to realize face key point location detection and improve face alignment of facial expression, posture, gender for shielding robustness.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a multi-task based deep learning cascade face alignment method. '(Drawing includes non-English language text)'
PD CN107038429-A   11 Aug 2017   G06K-009/00   201766   Pages: 9   Chinese
UT DIIDW:2017557227
ER

PT P
PN CN106778684-A
TI Deep neural network model establishing method, involves obtaining human face information and classification information, collecting human face information, and establishing independent depth neural network models according to loss function.
AB    NOVELTY - The method involves obtaining human face information and classification information. Human face information is classified according to the classification information. The human face information is collected by normalization process. Information in a deep neural network model is obtained. The deep neural network model is established according to loss function and training parameter to obtain an optimized depth neural network model. The depth neural network model is independent depth neural network models. Each independent depth neural network models are established according to loss function.
   USE - Deep neural network model establishing method.
   ADVANTAGE - The method enables improving face posture identifying clarity to improve identification success rate.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a multiple-depth fusion neural network based human face recognizing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a deep neural network model establishing method. '(Drawing includes non-English language text)'
PD CN106778684-A   31 May 2017   G06K-009/00   201749   Pages: 16   Chinese
UT DIIDW:201738608L
ER

PT P
PN CN106778585-A; WO2018103525-A1; CN106778585-B; US2019205623-A1; US10817708-B2
TI Human face key point tracking method for identifying authentication, involves performing multi face recognition of current frame, and capture of image needed in video stream until image in video stream is identified.
AB    NOVELTY - The method involves obtaining (101) current image in video stream, and current frame. The face key point coordinates of current frame, and confidence of face key point coordinates of previous frame image are obtained (102). The coordinates of face key points of current frame are calculated (103) when confidence level is higher than default threshold. The multi face recognition of current frame is performed (104), confidence of face key points coordinates of current frame is calculated, and capture of image needed in video stream is performed until image in video stream is identified.
   USE - Human face key point tracking method for identifying authentication.
   ADVANTAGE - The tracking method greatly reduces the detection time, improves the processing efficiency thus saving consumption of resources, and performing real-time calculation at the mobile terminal. The multi-face face key tracking effect is achieved. The accuracy of recognition ius improved.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for human face key point tracking device.
   DESCRIPTION OF DRAWING(S) - The drawing shows the flow chart illustrating the human face key point tracking method. (Drawing includes non-English language text)
   Step for obtaining current image in video stream (101)
   Step for obtaining face key point coordinates of current frame, and confidence of face key point coordinates of previous frame image (102)
   Step for calculating coordinates of face key points of current frame (103)
   Step for performing multi face recognition of current frame, calculating confidence of face key points coordinates of current frame, and performing capture of image needed in video stream until image in video stream is identified (104)
PD CN106778585-A   31 May 2017   G06K-009/00   201747   Pages: 22   Chinese
   WO2018103525-A1   14 Jun 2018   G06K-009/00   201840      Chinese
   CN106778585-B   16 Apr 2019   G06T-007/246   201933      Chinese
   US2019205623-A1   04 Jul 2019   G06K-009/00   201950      English
   US10817708-B2   27 Oct 2020   G06K-009/00   202088      English
UT DIIDW:2017386112
ER

PT P
PN CN106778669-A
TI Method for identifying target object based on unmanned aerial vehicle, involves switching to next target object to be identified, if first face basic identification information does not match face standard identification information.
AB    NOVELTY - The method involves acquiring (100) first face basic identification information of a target object. The first face basic identification information is compared (200) with face standard identification information in a previously established standard database. A lock instruction is sent (300a) to lock the target object, if the first face basic identification information matches the face standard identification information. The next target object to be identified is switched (300b), if the first face basic identification information does not match the face standard identification information.
   USE - Method for identifying target object based on unmanned aerial vehicle.
   ADVANTAGE - The information collected by the unmanned aerial vehicle to realize the target object identification is realized.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a device for identifying target object based on unmanned aerial vehicle.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow chart of a method for identifying target object based on unmanned aerial vehicle. (Drawing includes non-English language text)
   Step for acquiring first face basic identification information of a target object (100)
   Step for comparing first face basic identification information with face standard identification information (200)
   Step for sending a lock instruction to lock the target object (300a)
   Step for switching to the next target object to be identified (300b)
   Step for restoring the first face basic identification information to the second face basic identification information (400)
PD CN106778669-A   31 May 2017   G06K-009/00   201747   Pages: 18   Chinese
UT DIIDW:2017386092
ER

PT P
PN WO2017015390-A1; US2018211099-A1; US10860837-B2
TI Apparatus for deep multi-task learning framework for face detection, landmark localization, pose estimation and gender recognition, has first module configured to generate class independent region proposals to provide region.
AB    NOVELTY - The apparatus has a first module of three modules. The first module (210) is configured to generate class independent region proposals to provide a region. A second module (220) of the at least three modules is configured to classify the region as face or non-face using multi-task analysis. A third module (230) of at least three modules is configured to perform post-processing on classified region. The second module comprises five convolutional layers with three fully connected layers. The third module comprises one of iterative region proposal or landmark-based non-maximum suppression.
   USE - Apparatus for deep multi-task learning framework for face detection, landmark localization, pose estimation and gender recognition.
   ADVANTAGE - The post processing can boost the face detection score and improve the performance of individual tasks. The learning multiple correlated tasks simultaneously can build a synergy and improves the performance of individual tasks.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is included for a method for deep multi-task learning framework for face detection, landmark localization, pose estimation and gender recognition.
   DESCRIPTION OF DRAWING(S) - The drawing shows the schematic diagram of a modular system.
   First module (210)
   Second module (220)
   Third module (230)
PD WO2017015390-A1   26 Jan 2017   G06K-009/00   201711   Pages: 27   English
   US2018211099-A1   26 Jul 2018   G06K-009/00   201850      English
   US10860837-B2   08 Dec 2020   G06K-009/00   202000      English
UT DIIDW:201708025L
ER

PT P
PN CN106250822-A
TI Face identification based student attention level monitoring system, has attention level monitoring module for judging identity information is matched with face image, where attention level is zero when information is matched with image.
AB    NOVELTY - The system has a system body provided with a student face database. A student face collection module collects student identity information. A face matching module is connected with the student face database and the student face collection module. An attention level monitoring module is connected with the student face collection module and the face matching module. The attention level monitoring module judges the collected identity information is matched with the face image, where student attention level is zero when the collected identity information is matched with the student face image.
   USE - Face identification based student attention level monitoring system.
   ADVANTAGE - The system can effectively monitor and analyze student attention level and improves teaching quality.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a face identification based student attention level monitoring method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a block diagram of a face identification based student attention level monitoring system. '(Drawing includes non-English language text)'
PD CN106250822-A   21 Dec 2016   G06K-009/00   201707   Pages: 10   Chinese
UT DIIDW:201702446W
ER

PT P
PN CN106203376-A; CN106203376-B
TI Human face key point location method, involves inputting face image input to convolutional neural network, and comparing output of face image before frame with human face image by recurrent neural network to predict human face image.
AB    NOVELTY - The method involves inputting a face image input to a convolutional neural network. Key point characteristic of the face image is determined by the convolutional neural network. Extracted feature is inputted to a recurrent neural network. Output of face image before frame is compared with a human face image by a recurrent neural network to predict the human face image. Multiple subsets are selected and inputted to multiple neural networks to determine final human face key point position, where each subset of human face key point are adjacent in space.
   USE - Human face key point location method.
   ADVANTAGE - The method enables predicting human face in an accurate manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face key point locating device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face key point location method. '(Drawing includes non-English language text)'
PD CN106203376-A   07 Dec 2016   G06K-009/00   201703   Pages: 23   Chinese
   CN106203376-B   10 Apr 2020   G06K-009/00   202043      Chinese
UT DIIDW:201681318X
ER

PT P
PN CN106096557-A; CN106096557-B
TI Semi-supervised learning based fuzzy training sample face expression identifying method, involves obtaining calculated distance as classification result, and selecting maximum distance corresponding to classification result.
AB    NOVELTY - The method involves identifying an unknown face image. An expression image of a human face is obtained by a camera. Human face expression image pre-processing operation is performed. Local binary pattern (LBP) feature of the extracting face is extracted to generate dimensions of characteristic vectors. Unknown face image LBP features updating process is performed. Distance between two-classified STSVM models on a plane of an unknown face sample is calculated. The calculated distance is obtained as a classification result. Maximum distance is selected corresponding to a classification result.
   USE - Semi-supervised learning based fuzzy training sample face expression identifying method.
   ADVANTAGE - The method enables improving expression identification rate.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a semi-supervised learning based fuzzy training sample face expression identifying method. '(Drawing includes non-English language text)'
PD CN106096557-A   09 Nov 2016   G06K-009/00   201678   Pages: 11   Chinese
   CN106096557-B   18 Jan 2019   G06K-009/00   201909      Chinese
UT DIIDW:201671631B
ER

PT P
PN CN105518709-A; WO2016149944-A1; US2016371537-A1; US10262190-B2; CN105518709-B
TI Human face recognition method involves predicting characteristic point based on first description and second description operator, detecting first characteristic point and matching identification image with first preset threshold value.
AB    NOVELTY - The method involves obtaining the human face to be the identification image. The first characteristic point representing collective description of each description operator of the first characteristic information is extracted (S130). The second characteristic point description of the second collective description operator is obtained (S140). The characteristic point is predicted (S150) based on the first description and the second description operator. The first characteristic point is detected (S160). The identification image is matched (S170) with the first preset threshold value.
   USE - Human face recognition method.
   ADVANTAGE - The human face recognition is performed effectively.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are included for the following:
   (1) a human face recognition system; and
   (2) a computer program product for human face recognition.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating the human face recognition method. (Drawing includes non-English language text)
   Step for obtaining first characteristic point representing collective description of each description operator (S130)
   Step for obtaining second characteristic point description of second collective description operator (S140)
   Step for predicting characteristic point (S150)
   Step for predicting first characteristic point (S160)
   Step for matching identification image with first preset threshold value (S170)
PD CN105518709-A   20 Apr 2016   G06K-009/00   201640   Pages: 25   English
   WO2016149944-A1   29 Sep 2016   G06K-009/00   201665      Chinese
   US2016371537-A1   22 Dec 2016   G06K-009/00   201701      English
   US10262190-B2   16 Apr 2019   G06K-009/00   201928      English
   CN105518709-B   09 Aug 2019   G06K-009/00   201963      Chinese
UT DIIDW:201633274G
ER

PT P
PN CN105512624-A; CN105512624-B
TI Human smiling face image identifying method, involves performing human smiling face image detection process, performing gray level image processing operation, and realizing human smiling face identification operation.
AB    NOVELTY - The method involves performing human smiling face image detection process. Zooming size of the human smiling face image is extracted. The human smiling face image is converted into a gray level image. A convolution nerve network is established. Different class expression mark information of the human smiling face image is obtained. Gray level image processing operation is performed. An expression characteristic value of the human face smiling image is extracted by the convolution nerve network. Human smiling face identification operation is realized.
   USE - Human smiling face image identifying method.
   ADVANTAGE - The method enables carrying out human smiling face image identification operation with high quality, improving human smiling face image identification judging accuracy, satisfying human smiling face image identification requirements and increasing user experience.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human smiling face image identifying device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human smiling face image identifying method. '(Drawing includes non-English language text)'
PD CN105512624-A   20 Apr 2016   G06K-009/00   201631   Pages: 16   English
   CN105512624-B   21 Jun 2019   G06K-009/00   201948      Chinese
UT DIIDW:201625891C
ER

PT P
PN CN105488685-A
TI Intelligent building sales reception system, has human face recognition hardware module connected with background managing module and APP front end module, where background management module receiving human face information.
AB    NOVELTY - The system has a human face recognition hardware module connected with a background managing module and an APP front end module. The background management module receives human face information. The APP front end module is connected with a user room for checking logging-information. A video monitoring camera is connected with the human face identification hardware module. A PC terminal receives submitted information. A manager terminal displays a map.
   USE - Intelligent building sales reception system.
   ADVANTAGE - The system has high operational reliability and safety.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an intelligent building sales reception system implementing method.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an intelligent building sales reception system. '(Drawing includes non-English language text)'
PD CN105488685-A   13 Apr 2016   G06Q-030/00   201631   Pages: 11   English
UT DIIDW:201626169D
ER

PT P
PN CN105426870-A; CN105426870-B
TI Human face key point locating method, involves detecting initial key points in human face image based on prediction model, adjusting original key point position of face image, and performing local image characteristic classifying process.
AB    NOVELTY - The method involves detecting initial key points in a human face image based on a display shaped prediction model. Original key point position of the human face image is adjusted to determine key point position of the human face image. Judgment is made to check whether the key point position of the human face image is identified by a verification classifier to obtain a judging result. An average human face image sample key point position vector value is calculated. Local image characteristic classifying process is performed.
   USE - Human face key point locating method.
   ADVANTAGE - The method enables increasing human face key point locating speed and robustness of human face key point location in a better manner.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a human face key point locating device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a human face key point locating method. '(Drawing includes non-English language text)'
PD CN105426870-A   23 Mar 2016   G06K-009/00   201624      Chinese
   CN105426870-B   24 Sep 2019   G06K-009/00   201975      Chinese
UT DIIDW:201618923X
ER

PT P
PN WO2015176657-A1; CN105093492-A; CN105095844-A; CN105095845-A; CN105100633-A; IN201617043819-A; US2017124392-A1; CN106716451-A; CN105093492-B; US10169651-B2; US2019147242-A1; US10534959-B2; CN105095844-B; US2020167562-A1; CN106716451-B; US10929660-B2; US2021209360-A1; US2021240981-A1; US11295129-B2
TI Method for compensating iris identification application for identifying e.g. pupil of user eye, involves collecting iris characteristics of user by iris camera module, and providing supplementary light source for iris camera module.
AB    NOVELTY - The method involves collecting iris characteristics of a user by an iris camera module (10) of an iris recognition device. A supplementary light source for the camera module is provided, where the supplementary light source provided by a fill light component (20) reduces reflective spots on iris or makes reflective spots in areas other than the iris when the iris recognition device collects the iris characteristics of the user, an iris region forms uniform light and a light sending element includes a horizontal viewing angle greater than vertical video site angle of the camera module.
   USE - Method for compensating an iris identification application for identifying iris e.g. sclera and pupil, of a user eye.
   ADVANTAGE - The method enables providing the supplementary light source by the fill light component to reduce reflective spots on iris or to make reflective spots in areas other than the iris when the iris recognition device collects the iris characteristics of the user, thus improving precision of the collected iris characteristics of the user.
   DETAILED DESCRIPTION - INDEPENDENT CLAIMS are also included for the following:
   (1) a method for manufacturing an iris recognition device
   (2) an iris recognition device
   (3) an iris and human face recognition system
   (4) a face characteristic construction method
   (5) an online pay method
   (6) a lens
   (7) a method for manufacturing an iris image data collecting module.
   DESCRIPTION OF DRAWING(S) - The drawing shows a schematic block diagram of a double-eye iris image data acquisition module. '(Drawing includes non-English language text)'
   Iris camera module (10)
   Fill light component (20)
   Image sensor chip (111)
   Lens component (112)
   Print circuit board component (113)
TF TECHNOLOGY FOCUS - INDUSTRIAL STANDARDS - The method enables connecting an iris identification device and a background processing module through Wi-Fi, Li-Fi, internet, communication network and bluetooth.
PD WO2015176657-A1   26 Nov 2015   G06K-009/62   201580   Pages: 139   Chinese
   CN105093492-A   25 Nov 2015   G02B-013/18   201601      Chinese
   CN105095844-A   25 Nov 2015   G06K-009/00   201601      English
   CN105095845-A   25 Nov 2015   G06K-009/00   201601      English
   CN105100633-A   25 Nov 2015   H04N-005/235   201601      English
   IN201617043819-A   07 Apr 2017   G06K-009/62   201727      English
   US2017124392-A1   04 May 2017   G06K-009/00   201731      English
   CN106716451-A   24 May 2017   G06K-009/62   201737      Chinese
   CN105093492-B   26 Jun 2018   G02B-013/18   201844      Chinese
   US10169651-B2   01 Jan 2019   H04N-005/33   201902      English
   US2019147242-A1   16 May 2019   G06K-009/00   201936      English
   US10534959-B2   14 Jan 2020   H04N-005/225   202007      English
   CN105095844-B   24 Apr 2020   G06K-009/00   202038      Chinese
   US2020167562-A1   28 May 2020   G06K-009/00   202044      English
   CN106716451-B   24 Jul 2020   G06K-009/62   202062      Chinese
   US10929660-B2   23 Feb 2021   H04N-005/33   202118      English
   US2021209360-A1   08 Jul 2021   G06K-009/00   202157      English
   US2021240981-A1   05 Aug 2021   G06K-009/00   202165      English
   US11295129-B2   05 Apr 2022   H04N-005/33   202229      English
UT DIIDW:2015740151
ER

PT P
PN CN105005779-A
TI Interactive action and anti-fake recognition based human face verification method, involves registering state information of face image by initial logging user, and stopping verification test when verification failure exceeds preset time.
AB    NOVELTY - The method involves registering static state information of human face image by an initial logging user. Static state characteristics of human face image are extracted for comparing the registered characteristics. Face actions of the user are analyzed corresponding to static state matched face image information and the registrant face image information. A user image verification test is processed according to characteristic information comparison and face action analysis. The verification test is stopped when verification failure exceeds preset time.
   USE - Interactive action and anti-fake recognition based human face verification method.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for an interactive action and anti-fake recognition based human face verification system.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flowchart illustrating an interactive action and anti-fake recognition based human face verification method. '(Drawing includes non-English language text)'
PD CN105005779-A   28 Oct 2015   G06K-009/00   201578   Pages: 11   Chinese
UT DIIDW:2015721777
ER

PT P
PN CN104978548-A; CN104978548-B
TI 3D ASM based line sight estimating method, involves determining active shape model location, determining third-dimensional coordinate value according to two-dimensional key point, and establishing frontage viewpoint estimation model.
AB    NOVELTY - The method involves obtaining a human face image by using a camera. Frontage user training data is determined according to the human face image. Camera shooting process is performed by using two map sheets. A two-dimensional (2D) key point location is determined according to the training data. An active shape model (ASM) location is determined according to the 2D key point location. A third-dimensional (3D) coordinate value is determined according to 2D key point. A frontage viewpoint estimation model is established by using human eye line sight estimation sample.
   USE - 3D ASM based line sight estimating method.
   ADVANTAGE - The method enables increasing human face head posture establishing efficiency and head movement adaptability.
   DETAILED DESCRIPTION - An INDEPENDENT CLAIM is also included for a 3D ASM based line sight estimating device.
   DESCRIPTION OF DRAWING(S) - The drawing shows a flow diagram illustrating a 3D ASM based line sight estimating method. '(Drawing includes non-English language text)'
PD CN104978548-A   14 Oct 2015   G06K-009/00   201576   Pages: 17   Chinese
   CN104978548-B   25 Sep 2018   G06K-009/00   201866      Chinese
UT DIIDW:2015694384
ER

EF